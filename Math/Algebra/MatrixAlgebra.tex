\input{../../style.tex}

\title{Matrix Algebra}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents

\chapter{Basic Definitions}

\pagenumbering{arabic}

\chapter{Linear Equations}

The fundamental problem in linear algebra is to solve a system of $m$ equations in $n$ variables. One might write this system as
%
\[ \sum a_{1j} x_j = b_1 \quad \dots \quad \sum a_{mj} x_j = b_m. \]
%
The quantities $\{ a_{ij} \}$ are known, the \emph{coefficients} of the equation, as are the quantities $\{ b_j \}$, and the goal is to figure out the value of the unknown values $\{ b_j \}$. The tricks we describe to solve this equation were known in China, and later Japan, from the years 200BC. But they were formalized in Europe by Carl Gauss in the 19th century, and so the technique is called \emph{Gaussian elimination}. To make the calculus of Gaussian elimination more easy to see, we will write these equations in matrix form. If $A$ is the $m \times n$ matrix with coefficients $\{ a_{ij} \}$, $\mathbf{b}$ is the column vector with entries $\{ b_j \}$, and $\mathbf{x}$ is the column vector of unknowns $\{ x_i \}$, then we can write the equation as $A\mathbf{x} = \mathbf{b}$.

The trick to Gaussian elimination is to successively write these equations in \emph{equivalent ways} that are successively more simple than one another. There are three \emph{elementary operations} one can perform, that can easily be seen to lead to an equivalent system of equations:
%
\begin{itemize}
    \item Swapping one equation with another equation.
    \item Multiplying both sides of an equation by a non-zero number.
    \item Adding one equation to another equation.
\end{itemize}
%
We claim that these three rules can be used to put these equations in a simplified form in which it is easy to determine solutions of the equation.

At each step of the algorithm, we identify a \emph{pivot column}, and identify an equation

\end{document}