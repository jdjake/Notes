\input{../../style.tex}

\title{Differential Geometry}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents
\pagenumbering{arabic}

%\chapter{Curves in the Plane}

%The main ideas of differential geometry emerged from the beautiful analysis of curves in the plane, which illustrated the careful balance between the study of local and global properties of mathematical shapes. We will use the study of curves to illustrate the main strategies for the understanding of the `differential shapes' we will eventually come to call manifolds. Starting with the basic definition of a curve in the plane, we proceed to define the basic concepts of curvature and its consequences.

%\section{Parameterizations of Curves}

%Curves have been familiar to us since we got our hands on crayons in preschool. But the intuitive idea of curves contains so many subtle considerations that it is difficult to define mathematically what is curve is. In many common situations, we can define a curve as a subset of the plane \smallskipatisfying a certain algebraic equation. For instance, the circle can be thought of the set, or locus of points $(x,y) \in \mathbf{R}^2$ satisfying the equation $x^2 + y^2 = 1$; the spiral of archimedes can be seen as the set of points $(r,\theta)$, expressible in polar coordinates, for which $r = \theta$. However, intuitive notions on curves, such as direction, do not appear easy to express in terms of the set of points which lie on the curve. Depending on the techniques one wishes to use to analyze the theory of curves, it is useful to use one of many different mathematical definitions. In differential geometry, we wish to use the `smoothness' of some curves to define their mathematical properties, and the nicest way we can introduce smooth curves is by the idea of parameterization.

%To order for a curve to include all the information we need in our analysis, we require the set of points to be endowed with more structure. One way to get a direction on a curve is to think of the curve as a point evolving over time, tracing out the shape which we think of as the curve. In other words, we sometimes like to think of curves in terms of {\bf parameterizations}, a continuous map $c: I \to \mathbf{R}^2$ from an interval $I$ into the plane. We can think of $t \in I$ as a particular time, so that a point lies at $c(t)$ at time $t$, and the point evolves continuously over time. A parameterization $c$ not only gives us a set $c(I)$ of points lying on a curve, known as the {\bf trace} of the parameterization, but also a direction induced from the fact that an interval goes from left to right, and more dynamical structure resulting from the continuity of the map. We think of a parameterization as {\it defining} a particular curve in the plane.

%\begin{example}
%    The map $c(t) = (t^3 - 4t, t^2 - 4)$ is a parameterized differentiable curve. Note that $\alpha(2) = (0,0) = \alpha(-2)$, so parameterizations allow the easy description of curves which cross over a point multiple times.
%\end{example}

%In some sense, a parameterization gives too much information than is needed about a curve. This is because curves don't lie in a space naturally equipped with time, so a parameterization introduces more structure than a curve naturally has. For instance, we might think of the curves $c_1(t) = (\cos t, \sin t)$ and $c_2(t) = (\cos t + a, \sin t + a)$ as defining the same curve, except they start `at different time points'. To obtain the right balance of information, we do what is now a standard trick, working backwards by defining a curve to be the set of all parameterizations which `define the same curve'. In the case above, we have $c_1(t) = c_2(t + a)$, so we can obtain one parameterization from the other by `changing time slightly'. More generally, we say two parameterizations $c_1: I \to \mathbf{R}^2$ and $c_2: J \to \mathbf{R}^2$ are {\bf reparameterizations} of one another if there is a homeomorphism $g: I \to J$ with $c_1(t) = c_2(g(t))$, and we define a {\bf topological curve} as an equivalence class of paramterizations which are identified by reparameterization.

%\begin{example}
%    The unit circle, viewed as a continuous curve, is just the equivalence class of parameterizations containing the one parameterization $c_1: [0,2\pi] \to \mathbf{R}$, where $c_1(t) = (\cos t, \sin t)$. The curve defined by $c_2: [0,2\pi] \to \mathbf{R}$ defined by $c_2(t) = (\cos k t, \sin k t)$ is a different curve than the one defined by $c_1$, even though it has the same trace, and can be viewed as $k$ `connected' copies of the unit circle, or a three dimensional spiral squished onto a page.
%\end{example}

%Since $g(t) = -t$ is a reparameterization of $\mathbf{R}$, a topological curve has no sense of direction. One way we can introduce direction is by explicitly including it in our definition. We say a reparameterization $g: I \to J$ is {\bf orientation preserving} if $g$ is an increasing function. Then we can define an {\bf oriented topological curve} as an equivalence class of parameterizations under oriented reparameterizations. Because every reparameterization is either increasing or decreasing, this splits every topological curve into two curves, one going in the `increasing' direction relative to one parameterization, and the other going in the `decreasing' direction.

%\section{Smooth Curves}

%In our case, we wish to specialize the study of topological curves to curves with a well defined tangent line. In this case, curves like $c(t) = |t|^{1/2}$ will not have the properties we wish to study, since the curve has no tangent line at zero. We define a differentiable curve of order $C^k$  in terms of parameterizations $c: I \to \mathbf{R}$ which are $C^k$, in the sense that the first $k$ derivatives of $c^1$ and $c^2$ are continuous. Of course, we identify these parameterizations if they have a reparameterization which is also $C^k$, and this gives us the general definition we desire. The term {\bf smooth} is often reserved for the differentiable curves of order $C^\infty$.

%\begin{example}
%    The {\bf Spiral of Archimedes} can be thought of as the smooth curve defined in polar coordinates by the equation $r = \theta$, which also has the parameterization $c: (0,\infty) \to \mathbf{R}^2$ defined by $c(t) = (t \cos t, t \sin t)$. One can synthetically find the tangent line at a point $P$ on the spiral of archimedes by considering a point $Q$ on $OP$ a unit length from $OQ$, rotating $P$ by a right angle anticlockwise to form the point $R$, and then considering the line through $P$ parallel to $QR$.
%\end{example}

\part{Foundations}

\chapter{Topological Considerations}

In some form of mathematical heaven, all objects would exist in the linear realm, where problems are easy to solve. Unfortunately, we live in the real world.  When a physicist describes the motion of a robot's arm, rigidity forces the joints to move along curves bound to a sphere, forced never to move in a linear fashion. When an algebraic geometer studies the curve described by the equation $X^2 + Y^3 = 5$, he must analyze a shape which bends and curves, never straight. Differential geometry gives the mathematician tools to cheat; in many cases, the shapes we study may not be non-linear, but are at least {\it locally linear}, so around each point we can recover the methods used in the linear case. The challenge is to use these local methods to obtain global results, revealing the geometry of the shape we study.

\section{Manifolds and Atlases}

Topology attempts to describe the properties of space invariant under continuous stretching and squashing. Differential geometry extends this description to spatial properties constant when space is stretched and squashed, but not `bent'. Four centuries of calculus have established differentiability in the Euclidean spaces $\RR^n$. A basic environment to extend the notions of differentiability to topological spaces are those which are locally similar to $\RR^n$. A \emph{topological manifold} is a Hausdorff topological space $M$ such that for each point $p \in M$, there exists an open neighbourhood $U \subset M$ containing $p$, and a non-negative integer $n \geq 0$, such that $U$ is homeomorphic to an open subset of $\RR^n$. If the same $n$ works for all points $p$, we say $M$ is \emph{$n$ dimensional}, and write $\dim M = n$.

\begin{remark}
    It is convenient in the theory to view $\RR^0 = \{ 0 \}$ as a `zero dimensional' space. This makes sense, because most techniques involve the use of linear algebra, and $\RR^0$ is the canonical example of a zero dimensional vector space. We assume our manifolds are Hausdorff because non-Hausdorff manifolds are far and few between in applications of manifold theory to other areas of mathematics, and makes the theory less managable.
\end{remark}

\begin{example}
    Any open subset of $\RR^n$ is a manifold, since an open ball in $\RR^n$ is homeomorphic to $\RR^n$. More generally, the same kind of argument show that any open subset of a manifold is a manifold. These manifolds will be called \emph{open submanifolds} of $M$.
\end{example}

\begin{example}
    If $f: \RR^n \to \RR^m$ is continuous, then we can consider it's graph
    %
    \[ G(f) = \{ (x, f(x)) : x \in \RR^n \}. \]
    %
    The map $\pi: G(f) \to \RR^n$ defined by setting $\pi(x,y) = x$ is a homeomorphism, with inverse $\pi^{-1}(x) = (x,f(x))$, so $G(f)$ is a manifold. Later on, we will see that locally, all \emph{submanifolds} look like graphs.
\end{example}

%\begin{figure}
%\begin{center}
%\begin{tikzpicture}
    % x axis
%    \draw[->] (0,0) -- (6,0);

    % y axis
%    \draw[->] (0,0) -- (0,5);

    % function curve, mark points for projection
%    \draw (0,2) .. controls (3,0) and (4,6) .. (6,3)
%        [postaction={decorate, decoration={markings,
%            mark = between positions 0 and 1 step 0.01 with {\draw (0,0) -- (0,0.1);}
%        }}]
%        [postaction={decorate, decoration={markings,
%            mark = between positions 0 and 1 step 0.1 with {\draw (0,0) -- (0,0.2);}
%        }}];
%\end{tikzpicture}
%\caption{Coordinate lines on $\Gamma(f)$}
%\end{center}
%\end{figure}

\begin{remark}
    The above examples show that any topological space homeomorphic to a topological manifold is also a topological manifold. This is a bad omen, because we want to discuss properties of space which remain invariant under differentiable maps, and differentiability should certainly be a stronger concept than continuity. This indicates we must add additional structure to a manifold in order to distinguish smooth maps from continuous maps. We introduce this structure in the next chapter.
\end{remark}

Geometrically a homeomorphism $x: U \to \mathbf{R}^n$ from an open set $U$ on a manifold can be seen as a way of assigning coordinates to points on the manifold, because we associate with each geometric point $p \in U$ a sequence of numbers $x^1(p), \dots, x^n(p) \in \RR^n$. One way to see the study of manifolds is as an extension of analytic geometry to non-planar topological systems. Indeed, the technologies developed have immediate applications to projective, hyperbolic, and elliptic geometries, and the language of manifolds has become the common language of most modern day geometers.

\begin{example}
    Consider the circle
    %
    \[ S^1 = \{ x \in \mathbf{R}^2 : |x| = 1 \}. \]
    %
    For any proper open subset $I \subset S^1$, we define an \emph{angle function} to be a continuous map $\theta: I \to \RR$ such that $e^{i\theta(x)} = x$ for all $x \in I$. Then $\theta$ is a topological embedding of $U$ in $\RR$, with continuous inverse $\theta^{-1}(t) = e^{it}$. Angle functions exist on any proper open subset of $S^1$, and therefore cover $S^1$. Thus $S^1$ is a 1 dimensional manifold.
\end{example}

%\begin{figure}
%\begin{center}
%\begin{tikzpicture}
%    \draw[postaction={decorate}] (0,0) circle [radius=2];
    %\draw (0,0) -- (0:1.5);
    %\draw (0,0) -- (120:1.5);

%    \foreach \i in {0,30,...,360} {
%        \draw (\i:2) -- (\i:2.2);
%    }
%    \foreach \i in {0,2,...,360} {
%        \draw (\i:2) -- (\i:2.1);
%    }
%\end{tikzpicture}
%\caption{Coordinates on $S^1$ obtained from angle coordinates}
%\end{center}
%\end{figure}

As a manifold, the circle is distinct from an open subset of $\RR^n$ because we cannot put coordinates over the whole space at once; instead, we must analyze the circle piece by piece to determine the structure on the whole space. This is the main trick to manifold theory -- a manifold might be a big nasty object globally, but locally, the space is flat.

\begin{example}
    %
%    \begin{figure}
%    \begin{center}
%    \begin{tikzpicture}[scale=0.8]
%        \draw (0,0) circle [radius=1];
%        \draw (-8,-1) -- (8,-1);

%        \foreach \i [evaluate=\i as \x using 2*sin(\i)/(1-cos(\i))] in {30,45,...,330} {
%            \draw[dashed] (0,1) -- (\x,-1);
%        }
%    \end{tikzpicture}
%    \caption{Projecting $S^1$ onto $\mathbf{R} \times \{ -1 \}$}
%    \end{center}
%    \end{figure}
    %
    The method of stereographic projection gives another system of coordinates on the circle, which generalizes to higher dimension shapes $S^n$. We will project the open subset $S^1 - \{ (1,0) \}$ to the line $\{ -1 \} \times \mathbf{R}$, by taking the intersection of the line between $p$ and $(1,0)$ and the line $\{ -1 \} \times \mathbf{R}$. A formula for the projection from $S^1 - \{ 1, 0 \}$ to the line $\{ -1 \} \times \mathbf{R}$ is given by the continuous function
    %
    \[ f(x,y) = \frac{2y}{1-x}. \]
    %
    %    note that the set of points on the line generated by $p = (x,y)$ and $(1,0)$ can be described as the points of the form $\lambda p + (1 - \lambda)(1,0) = (\lambda x + 1 - \lambda, \lambda y)$. The intersection of this line with $\{ -1 \} \times \mathbf{R}$ is obtained by setting $\lambda x + 1 - \lambda = -1$, which has a unique solution $\lambda = 2/(1-x)$.
    %
    Another calculation shows the inverse is given by
    %
%    is obtained by taking a point $p$ on the line $\{ -1 \} \times \mathbf{R}$, considering the line generated by $p$ and $(1,0)$, and finding the unique point on $S^1 - \{ (1,0) \}$ which lies on this line. If $p = (-1,y)$, we therefore try to find values $\lambda$ such that $(1 - 2\lambda, \lambda y)$ lies on $S^1$, which means $(1 - 2\lambda)^2 + (\lambda y)^2 = 1$, which occurs when $\lambda^2 (4 + y^2) - 4\lambda = 1$, so either $\lambda = 0$ (which means $p = (1,0)$, which obviously lies on $S^1$), or $\lambda = 4/4+y^2$, so we find
    %
    \[ f^{-1}(y) = \left(1 - \frac{8}{4 + y^2} , \frac{4y}{4 + y^2} \right). \]
    %
%    \begin{figure}
%    \begin{center}
%    \begin{tikzpicture}
%        \draw (0,0) circle [radius=2];
%
%        \foreach \i [evaluate=\i as \x using 1 - (8/(4 + \i*\i)),
%                     evaluate=\i as \y using 4*\i/(4 + \i*\i),] in {-120,-118,...,120} {
%            \draw (2*\y,2*\x) -- (2.2*\y, 2.2*\x);
%        }
%
%        \foreach \i [evaluate=\i as \x using 1 - (8/(4 + \i*\i)),
%                     evaluate=\i as \y using 4*\i/(4 + \i*\i),] in {-120,-119.8,...,120} {
%            \draw (2*\y,2*\x) -- (2.1*\y, 2.1*\x);
%        }
%
        % Fill in dark spot
%        \coordinate (a) at (0:0);
%\coordinate (b) at (95:4);
%\coordinate (c) at (85:4);
%
%\draw pic[draw=none,fill=black,angle radius=2.2cm] {angle=c--a--b};
%\draw pic[draw=none,fill=white,angle radius=2cm] {angle=c--a--b};
%
%    \end{tikzpicture}
%    \end{center}
%    \caption{Coordinate Lines on $S^1$ induced by stereoscopic projection}
%    \end{figure}
    %
    Similar calculations show that the on the $n$-dimensional spheres $S^n$, we have a projection map from $S^n - \{ (1,0,\dots,0) \}$ onto $\{ -1 \} \times \mathbf{R}^n$ by the same process, and one can calculate the formula for this projection is given by
%    We leave it to the reader to show that on the $n$-dimensional sphere $S^n = \{ x \in \mathbf{R}^{n+1} : \| x \| = 1 \}$, we have a projection map of $S^n - \{ (1,0,\dots,0) \}$ onto $\{ -1 \} \times \mathbf{R}^n$ by the same process, and the formula is calculated to be
    %
    \[ f(x_1, \dots, x_n) = \frac{2}{1 - x_1}(x_2, \dots, x_n) \]
    %
%    and the inverse takes the form
    %
    \begin{align*}
        f^{-1}(y_2, \dots, y_n) &= \left(1 - \frac{8}{4 + \| y \|^2}, \frac{4y_2}{4 + \| y \|^2}, \dots, \frac{4y_n}{4 + \| y \|^2} \right)\\
        &= \frac{1}{4 + \| y \|^2} \left( \| y \|^2 - 4, 4y_2, \dots, 4y_n \right)
    \end{align*}
    %
    If we project from the point $(-1,0,\dots,0)$ to $\{ 1 \} \times \mathbf{R}^{n-1}$ instead of $(1,0,\dots,0)$, then the homeomorphism defined on $S^1 - \{ (-1,0,\dots,0) \}$ is calculated to be
    %
    \[ g(x_1, \dots, x_n) = \frac{1}{1 + x_1}(x_2, \dots, x_n) \]
    %
    \[ g^{-1}(y_1, \dots, y_{n-1}) = \frac{1}{4 + \| y \|^2} \left( 4 - \| y \|^2, 4y_2, \dots, 4y_n \right) \]
    %
    Since any element of $S^n$ is contained in the domain of one of these projections, we conclude $S^n$ is a manifold.
\end{example}

When we analyze manifolds, it is convenient to consider not only homeomorphisms onto $\RR^n$, but also maps onto open subsets of $\mathbf{R}^n$. For a manifold $M$, we call a pair $(x,U)$ a \emph{coordinate chart} if $U$ is an open subset of $M$, and $x$ is a homeomorphism from $U$ to an open subset of $\RR^n$. A \emph{coordinate ball} is a chart $(x,U)$, where $x$ is a homeomorphism of $U$ and the open unit ball in $\RR^n$. Similarily, we can define coordinate cubes, or other shapes. Letters like $x,y$ and $z$ are often used for charts, to make it easy to confuse coordinates $x = (x^1,x^2, \dots, x^n) \in \RR^n$ for a point in $\RR^n$ with coordinates $x(p) = (x^1(p), x^2(p), \dots, x^n(p))$ for $p \in U$. It is often very useful to trick your brain into the coordinate way of thinking, and the only disadvantage is that things can be slightly confusing when working in Euclidean space itself.

\begin{example}
    Consider the set $M(n) = M(n;\RR)$ of $n \times n$ matrices with entries in the real numbers. We can identify $M(n)$ with $\mathbf{R}^{n \times n}$ by considering only the coefficients of the matrix, and this tells us $M(n)$ is a topological manifold of dimension $n^2$. More generally, the set $M(n,m)$ of $n \times m$ real matrices has dimension $nm$. The determinant map $\det: M(n) \to \mathbf{R}$ acts as a polynomial in the entries of the matrix, so the function is continuous, and so the general linear group $GL(n) = \{ M \in M(n) : \det(M) \neq 0 \}$ is an open submanifold of $M(n)$. The set $M(n;\CC)$ of $n \times n$ complex matrices is also a $2n$ dimensional manifold, as is the set $GL(n;\CC)$ of invertible complex-coefficient matrices.
\end{example}

\begin{example}
    Let $M(n,m;k) \subset M(n,m)$ be the set of $n \times m$ matrices of rank $k$. For any matrix $M \in M(n,m;k)$, there are permutation matrices $P$ and $Q$ such that
    %
    \[ PMQ = \begin{pmatrix} A & B \\ C & D \end{pmatrix} \]
    %
    where $A \in GL(k)$. We can then define $L(N) \in M(n,m)$ by setting
    %
    \[ L(N) = PNQ = \begin{pmatrix} A(N) & B(N) \\ C(N) & D(N) \end{pmatrix}. \]
    %
    The map $L$ is rank preserving and continuous. The continuity implies that there exists $U \subset M(n,m)$ such that $A(N)$ is invertible for all $N \in U$. The matrix
    %
    \[ \begin{pmatrix} I_k & 0 \\ -C(N)A(N)^{-1} & I_{n-k} \end{pmatrix} \]
    %
    is invertible, and so $L(N)$ has the same rank as
    %
    \begin{align*}
        \begin{pmatrix} I_k & 0 \\ -C(N)A^{-1}(N) & I_{n-k} \end{pmatrix} &\begin{pmatrix} A(N) & B(N) \\ C(N) & D(N) \end{pmatrix}\\
        &= \begin{pmatrix} A(N) & B(N) \\ 0 & D(N) - C(N)A^{-1}(N)B(N) \end{pmatrix}.
    \end{align*}
    %
    Thus each $N \in U$ has rank $k$ if and only if $D(N) = C(N)A^{-1}(N)B(N)$, i.e.
    %
    \[ U \cap M(n,m;k) = \left\{ N \in U : D(N) = C(N) A^{-1}(N) B(N) \right\}. \]
    %
    In particular, we may specify an element of $M(n,m;k)$ by an element $A$ of $GL(k)$, an element $B$ of $M(k,n-k)$, and an element $C \in M(k,m-k)$. Thus $M(n,m;k)$ is a manifold with dimension
    %
    \[ k^2 + k(n-k) + k(m-k) = k(n+m-k). \]
\end{example}

The manifold $M(n,m;k)$ is interesting, because its canonical embedding in Euclidean space is essentially just expressed along canonical coordinates obtained from the coefficients of the matrix entries by forgetting a few coefficients, yet the space cannot be globally linear because of the rank $k$ requirement. The best way to see this is to consider $M(2,1;1)$, which consists of vectors in $\mathbf{R}^2$ of the form $(a,0)$ and $(0,b)$, with $a,b \neq 0$. This is the union of the $x$ and $y$ axis, with the origin removed, and is a $1$ manifold. At any point you can travel left and right, or up and down, just not too far!

%\begin{figure}
%\begin{center}
%\begin{tikzpicture}
%    \draw[thick] (3,0) -- (-3,0);
%    \draw[thick] (0,3) -- (0,-3);
%    \draw[fill=white, draw=none] (0,0) circle [radius = 0.1];
%\end{tikzpicture}
%\end{center}
%\caption{The space $M(2,1;1)$.}
%\end{figure}

\section{Constructing Abstract Manifolds}

Sometimes, you may wish to place a topological structure on a set, with the purpose of making it into a manifold, without having any topological structure to begin with. In most cases, the set is contained in a bigger space with some topological structure, and you can just surplant the relative topology on the subspace. But in certain cases, there is no canonical topological structure to start working with. The next theorem provides a useful strategy for placing a topology on a space and simultaneously showing it is a manifold.

\begin{theorem}
    Let $M$ be a set together with a cover $\{ U_\alpha \}$ and a family of bijections $\{ x_\alpha \}$, where $x_\alpha$ maps $U_\alpha$ onto an open subset of Euclidean space. Suppose also that
    %
    \begin{itemize}
        \item For any $\alpha, \beta$, $x_\alpha(U_\alpha \cap U_\beta)$ is open, and $x_\alpha \circ x_\beta^{-1}$ is continuous.
        \item If $a, b \in M$, then either there is a set $U_\alpha$ with $a,b \in U_\alpha$, or there are disjoint sets $U_\alpha, U_\beta$ such that $a \in U_\alpha$, $b \in U_\beta$.
    \end{itemize}
    %
    Then there is a unique topological structure on $M$ making it into a Hausdorff manifold, such that each $(x_\alpha, U_\alpha)$ is a chart.
\end{theorem}
\begin{proof}
    Consider the topological structure on $M$ generated by all sets of the form $x_\alpha^{-1}(V)$, where $V$ is an open subset of $x_\alpha(U_\alpha)$. The first condition of this theorem implies that these sets form a base for a topology; if $W_\alpha$ is an open set of $x_\alpha(U_\alpha)$, and $W_\beta$ is an open subset of $x_\beta(U_\beta)$, then set $V_\alpha = x_\alpha^{-1}(W_\alpha)$ and $V_\beta = x_\beta^{-1}(W_\beta)$. Then
    %
    \[ x_\alpha(V_\alpha \cap V_\beta) = W_\alpha \cap (x_\alpha \circ x_\beta^{-1})(x_\beta(U_\alpha) \cap W_\beta) \]
    %
    is an intersection of open sets, and is therefore open, and so $W_\alpha \cap W_\beta = x_\alpha^{-1}(V_\alpha \cap V_\beta)$ is an element of the basis. Note this calculation shows that the maps $x_\alpha$ are open in the topology induced by this basis, hence the maps $x_\alpha$ are homeomorphisms. The space $M$ is also Hausdorff by the second property, since two points $p,q$ in a common set $U_\alpha$ can be separated as in Euclidean space, or separated by two disjoint charts $U_\alpha$ and $U_\beta$. Thus $M$ is a manifold.
\end{proof}

\begin{remark}
    If the family $\{ U_\alpha \}$ has a countable subcover, then the manifold $M$ is also second countable.
\end{remark}

\begin{remark}
    There is also a variant of the theorem where the charts $\{ x_\alpha \}$ can be bijections into open submanifolds of a family of manifolds rather than open subsets of Euclidean space. The proof of this extension is elementary.
\end{remark}

\begin{remark}
	In the next chapter, we will define the notion of a smooth structure on a manifold. If $x_\alpha \circ x_\beta^{-1}$ is a smooth map from $x_\beta(U_\alpha \cap U_\beta)$ to $x_\alpha(U_\alpha \cap U_\beta)$, then it is easy to see that $M$ also has a unique smooth structure such that each $(x,U)$ is a smooth coordinate chart.
\end{remark}

\begin{example}
    Let $V$ be an $n$ dimensional vector space. If $E = (e_1, \dots, e_n)$ is an ordered basis of $V$, then we can associate a bijective map $x_E: V \to \RR^n$ such that
    %
    \[ x_E^{-1}(a_1, \dots, a_n) = a_1e_1 + \dots + a_ne_n. \]
    %
    If $F = (f_1, \dots, f_n)$ is another ordered basis, then $x_F \circ x_E^{-1}$ is an invertible linear map, and therefore a homeomorphism. Thus there is a unique topology and smooth structure on $V$ such that each map $x_E$ is a diffeomorphism.
\end{example}

The last example shows that if $V$ and $W$ are finite dimensional vector spaces, then the family $L(V,W)$ of linear maps has a natural topology and smooth structure making it into a manifold homeomorphic to $\RR^n$. We will use this example to construct another manifold below which places a topological structure on the set of vector spaces of a given dimension.

\begin{example}[Grassmannian]
    Let
    %
    \[ G(k,n) = \{ V \subset \RR^n : \dim(V) = k \} \]
    %
    be the family of all $k$ dimensional subspaces of $\RR^n$. For each $W \in G(n-k,n)$, let $U_W = \{ V \in G(k,n): V \cap W = (0) \}$. We will show $\{ U_W : W \in G(n-k,n) \}$ can be made into a natural compatible covering of $G(k,n)$, which makes the space into a $k(n-k)$ dimensional manifold. To construct bijections, we will repeatedly rely on the fact that for any $V \in G(k,n)$, there exists $W \in G(n-k,n)$ such that $V \cap W = (0)$. We call $W$ a \emph{complemented} subspace of $V$. In particular, we note this implies that the family $\{ U_W \}$ covers $G(n,k)$. Moreover, this family has a subcover of finitely many charts. For instance, one can take the family of all $n-k$ dimensional spaces $W$ spanned by rational coefficient vectors. This implies that $G(k,n)$ is second countable.

    Now fix $W$, and pick $V_0$ such that $\RR^n = V_0 \oplus W$. We can then find projections $\pi: \RR^n \to V_0$ and $\psi: \RR^n \to W$ such that $x = \pi(x) + \psi(x)$ for all $x \in \RR^n$. For each $W$, we will establish a bijection between $U_W$ and $L(V_0,W)$, which is cannonically isomorphic to $\RR^{k(n-k)}$. Given a linear map $T: V_0 \to W$, set $V_T = \{ v_0 + Tv_0 : v_0 \in V_0 \}$. Given $V \in U_W$, the map $\pi: V \to V_0$ is a bijection. Thus we can define a unique linear transformation $T \in L(V_0,W)$ such that $T(\pi(v)) = \psi(v)$ for each $v \in V$. It is clear that $V_T = V$. Conversely, if $V_T = V_S$, then for each $v_0 \in V_0$, there is $v_0' \in V_0$ such that $v_0 + T(v_0) = v_0' + S(v_0')$. But this means that $v_0 - v_0' = S(v_0') - T(v_0) \in V_0 \cap W$, so $v_0 = v_0'$ and $T(v_0) = S(v_0')$, implying $S = T$. Thus the correspondence between $U_W$ and $L(V_0,W)$ is a bijection. Since $L(V_0,W)$ is naturally homeomorphic to $k(n-k)$ space, after we prove the regularity of this choice we will have shown $G(n,k)$ is a $k(n-k)$ manifold.

    To show we get a Hausdorff topological structure, let $V_0,V_1 \in G(n,k)$, and suppose $V_0 \cap V_1$ has dimension $m$. Then $V_0 + V_1$ is a space of dimension $2k - m$, and so we can choose $W \in G(n,(n - k) - (k - m))$ complementing $V_0 + V_1$, and $W_0, W_1$ complementing $V_0$ and $V_1$. Since $W$ intersects $V_0$ and $V_1$ trivially, it now suffices to prove that $V_0 + V_1$ has a subspace of dimension $k - m$ that intersects $V_0$ and $V_1$ trivially. Note that both
    %
    \[ X = W_0 \cap (V_0 + V_1)\ \ \ \ \ Y = W_1 \cap (V_0 + V_1) \]
    %
    have dimension $k - m$. If we consider any isomorphism $T: X \to Y$, then the space $V_T$ is $k-m$ dimensional and intersects both $V_0$ and $V_1$ trivially. Thus $V_T + W$ is the required subspace.  

    A more difficult task is to show that the charts are compatible with one another. First, we show that the set $U_{W_0} \cap U_{W_1}$ corresponds to an open subset of linear transformations. Let $V_0$ and $V_1$ complement $W_0$ and $W_1$, and let $\pi, \psi, \nu$, and $\eta$ denote projections onto $V_0, W_0, V_1$, and $W_1$ respectively. The image of $U_{W_0} \cap U_{W_1}$ in $L(V_0,W_0)$ consists of maps $T: V_0 \to W_0$ such that $\nu \circ (1 + T)$ is an isomorphism. The association $T \mapsto \nu \circ (1 + T)$ is continuous, which shows that the set of all such isomorphisms is open.

    Now how does $U_{W_0} \cap U_{W_1}$ transform between $L(V_0,W_0)$ and $L(V_1,W_1)$? Consider a subspace $V \in U_{W_0} \cap U_{W_1}$ mapped to a transformation $T: V_0 \to W_0$, and a transformation $S: V_1 \to W_1$. Since $V$ intersects $W_0$ and $W_1$ trivially, the projection maps $\pi|_V$ and $\nu|_V$ are isomorphisms, and we have
    %
    \[ T = \psi \circ (\pi|_V)^{-1} \quad\text{and}\quad S = \eta \circ (\nu|_V)^{-1}. \]
    %
    Note that the map $(1 + T): V_0 \to V$ is an isomorphism, and so
    %
    \[ S = \eta \circ (1 + T) \circ (1 + T)^{-1} \circ (\nu|_V)^{-1} = [\eta \circ (1 + T)] \circ [\nu \circ (1 + T)]^{-1} \]
    %
    a formula now expressed independently of $V$, which holds over all choices of $T$, which shows the map $T \mapsto S$ is continuous. This concludes the construction of a manifold structure for $G(k,n)$.
\end{example}

\begin{remark}
    If $V$ is a finite dimensional vector space, then the construction above, which is essentially basis invariant, can be transferred over to show the space $G_k(V)$ of $k$ dimensional subspaces of $V$ is also a manifold. A special case is the space $G_1(V)$ of lines through the origin in $V$, known as the \emph{projectivization} of $V$ and denoted by $\mathbf{P}(V)$.
\end{remark}

\section{Basic Properties of Manifolds}

Many proofs about manifolds use a reliable trick. First, we conjure forth local homeomorphisms to $\mathbf{R}^n$. Then we transport nice properties of $\mathbf{R}^n$ across the homeomorphism, thereby inducing the properties on the manifold. In fact, the general philosophy of manifold theory is that most properties of $\mathbf{R}^n$ will carry across to arbitrary spaces that look locally like $\mathbf{R}^n$. Thus we can perform linear algebra on spaces that are not really linear!

\begin{theorem}
    Every manifold is locally compact.
\end{theorem}
\begin{proof}
    Let $M$ be a manifold, let $p \in M$ be a point, and let $U$ be a neighbourhood of $p$. Then there is a coordinate chart $(x,V)$, where $V \subset U$ and $p \in V$. If $B$ is a closed ball around $x(p)$ in $x(U)$, then $B$ is a compact neighbourhood of $x(p)$, hence $x^{-1}(B)$ is a compact neighbourhood of $p$, contained in $U$. Thus $M$ has a basis of pre-compact coordinate balls, and is therefore locally compact.
\end{proof}

The same method shows that every manifold is locally path-connected, and thus locally connected. The next problem requires more foresight on the reader, though the basic technique used is exactly the same.

\begin{theorem}
    A connected manifold is path-connected.
\end{theorem}
\begin{proof}
    Let $M$ be a connected manifold, and fix $p \in M$. Let
    %
    \[ U = \{ q \in M: p\ \text{and $q$ are path connected} \}. \]
    %
    Since $M$ is locally path connected, $U$ is open. Now suppose $q$ is a limit point of $U$. Take some coordinate ball $(x,V)$ around $q$. Then $V$ contains some $r \in U$, which is path connected to $p$, and we can then patch this path with a path from $r$ to $q$ to obtain a path from $p$ to $q$. Thus $U$ is open, closed, and non-empty, so $U = M$.
\end{proof}

Since every manifold is locally connected, any manifold can be split up into the disjoint sum of its connected components. It therefore often suffices to study connected manifolds, since any manifold is the disjoint union of connected manifolds.

\begin{example}
    $GL(n)$ is not connected, since $\det(GL(n))$ is disconnected. We now show $GL(n)$ consists of two path connected components, those matrices with positive determinant, and those matrices with negative determinant. To do this, we shall construct paths in $GL(n)$ reducing invertible matrices to certain canonical forms. In our proof, we shall use the fact that $GL_n(\mathbf{R})$ can be described as tuples of $n$ linearly independent vectors in $\mathbf{R}^n$, which simplifies notation. If $v_1, \dots, v_n$ are linearly independent, consider adding one vector to another.
    %
    \[ (v_1, \dots, v_p, \dots, v_q, \dots, v_n) \mapsto (v_1, \dots, v_p + v_q, \dots, v_q, \dots, v_n) \]
    %
    These vectors are path connected in $GL_n(\mathbf{R})$ by the path
    %
    \[ t \mapsto (v_1, \dots, v_p + t v_q, \dots, v_q, \dots, v_n) \]
    %
    Similarily, we may subtract rows from one another. Next, consider multiplying a row by a scalar $\gamma > 0$,
    %
    \[ (v_1, \dots, v_p, \dots, v_n) \mapsto (v_1, \dots, \gamma v_p, \dots, v_n) \]
    %
    These matrices are path connected by
    %
    \[ t \mapsto \begin{pmatrix} v_1 & \dots & [(1-t) + t \gamma]v_p & \dots & v_n \end{pmatrix}^t \]
    %
    We cannot perform this technique if $\gamma < 0$, because then $(1-t) + t \gamma = 0$ for some choice of $t$, and the resulting vectors become linearly dependent. We should not expect to find a path when $\gamma < 0$, since multiplying by a negative number reverses the sign of the determinant, and we know from the continuity of the determinant that the sign of the determinant determines at least two connected components. The same reasoning shows we can't necessarily swap two rows. Fortunately, we don't need these operations -- we may use the path-connected elementary matrices to reduce any matrix to a canonical form. A modification of the Gauss Jordan elimination algorithm (left to the reader as a simple exercise) shows all matrices can be path-reduced to a matrix of the form
    %
    \[ \begin{pmatrix} 1 & 0 & \dots & 0 & 0 \\ 0 & 1 & & 0 & 0 \\ 0 & 0 & \ddots & 0 & 0 \\ 0 & 0 &  & 1 & 0 \\ 0 & 0 & \dots & 0 & \pm 1 \end{pmatrix} \]
    %
    One matrix has determinant greater than zero, the other has determinant less than zero. Thus $GL(n)$ consists of two homeomorphic path-connected components: the matrices with determinant greater than zero, and the component with determinant less than zero, reflecting the linear transformations which preserve orientation, and the ones which reverse orientation. This is quite a different situation from the space $GL(n,\mathbf{C})$ of invertible matrices over the complex numbers, which is connected for any $n$.
\end{example}

Another useful fact of a manifold, if the dimenison of the manifold is the same, is that all the local parts of space look identical to one another. We say a topological space $X$ is \emph{homogenous} if, for every two points $p,q \in X$, there is a homeomorphism $f: X \to X$ with $f(p) = q$.

\begin{theorem}
    Every connected manifold is homogenous.
\end{theorem}
\begin{proof}
    Begin by considering a closed unit disk $\mathbf{D}^n \subset \RR^n$. Let $p,q$ lie in the interionr of the disk. Then there is a homeomorphism $f: \RR^d \to \RR^d$ which fixes all points outside of the interior of $\mathbf{D}^n$. Let $\psi$ be a smooth, non-negative function supported on a closed set contained in the interior of the unit disk, with $\psi(x) = 1$ for all points $x$ on the line segment between $p$ and $q$. Consider the vector field $v(x) = \psi(x) (q - p)$. Then there is a one parameter family of diffeomorphisms $\{ \varphi_t \}$ such that $\varphi_0$ is the identity map, and for each $x \in \RR^n$ and $t \in \RR$,
    %
    \[ \frac{d\varphi_t(x)}{dt} = \psi(x) (q - p). \]
    %
    In particular, for each $x$ on the line segment between $p$ and $q$,
    %
    \[ \frac{d\varphi_t(x)}{dt} = q - p, \]
    %
    so $\varphi_1(p) = q$. Thus we can set $f = \varphi_1$.

    Now let $M$ be a connected manifold, fix $p \in M$, and consider
    %
    \[ U = \{ q \in M: \text{there is a homeomorphism $f: M \to M$ such that $f(p) = q$.} \} \]
    %
    If $q \in U$, let $(x,U)$ be a coordinate chart centered at $q$ and with $x(U) = \RR^n$. If $B$ is the open unit disk in $\RR^n$, then for any $r \in B$, there is a homeomorphism $\tilde{f}: \RR^d \to \RR^d$ with $\tilde{f}(0) = r$. We can then define a homeomorphism $f: M \to M$, by setting $f(x) = x$ for all $x \in M - x^{-1}(\mathbf{D}^n)$, and setting $f = x^{-1} \circ f \circ x$ for all $x \in U$, then $f(q) = r$, and we can compose this homeomorphism with a homeomorphism $g: M \to M$ with $g(p) = q$, which gives $r \in U$. Thus $U$ is open. A similar argument shows also that $U$ is closed, thus $U = M$.
\end{proof}

\begin{remark}
    The same argument shows for any $p,q \in M$, there is a diffeomorphism $f: M \to M$ such that $f(p) = q$.
\end{remark}

\section{Construction in the Category of Manifolds}

The category {\sf Man} of topological manifolds has useful constructions which are good to know as tools for constructing more manifolds from simpler ones. For instance, if $M$ and $N$ are manifolds, then the disjoint union $M \cup N$ is a manifold. Here are some more constructions.

\begin{theorem}
    If $M$ and $N$ are manifolds, then $M \times N$ is a manifold.
\end{theorem}
\begin{proof}
    Products of homeomorphisms onto open subsets of $\mathbf{R}^n$ suffice.
\end{proof}

\begin{example}
    Since $S^1$ is a one dimensional manifold, the manifold $\mathbf{T}^2 = S^1 \times S^1$ is a two dimensional manifold, known as the torus, because we have an embedding $f: \mathbf{T}^2 \to \RR^3$ given in coordinates by
    %
    \[ f(\theta_1,\theta_2) = (\cos(\theta_1) [2 + \cos(\theta_2)],\sin(\theta_1) [2 + \cos(\theta_2)], \sin(\theta_2)), \]
    %
    whose image is a donut in three dimensional space. More generally, the $n$ torus $\mathbf{T}^n = S^1 \times \dots \times S^1$ is an $n$ manifold.
\end{example}

\begin{example}[Surfaces of Revolution]
    If $M$ is a 1-manifold in the subplane of $\mathbf{R}^3$ defined by
    %
    \[ X = \{ (x,y,0): x,y > 0 \} \]
    %
    Then the space obtained by rotating $M$ around the $y$ axis is a 2-manifold, known as a surface of revolution, and homeomorphic to $M \times S^1$.
\end{example}

Under some restrictions, we can consider quotient spaces of manifolds which are manifolds. In the most simplest case, if $f: M \to N$ is a locally injective open surjective map, and $M$ is a manifold, then $N$ is a manifold, because sufficiently small charts on $M$ can be pushed forward giving charts on $N$. Later, we will see more interesting examples of quotient manifolds, generated by the actions of Lie groups.

\begin{example}[The M\"{o}bius Strip]
    Consider the group action of $\ZZ$ on $(-\infty,\infty) \times \RR$ by letting $n \cdot (x,y) = (x + n, (-1)^n y)$. The quotient space is the M\"{o}bius strip $\mathbf{M}$. The projection map $\pi: (-\infty,\infty) \times \RR \to \mathbf{M}$ is open and locally injective, so $\mathbf{M}$ is a manifold. By throwing away points, we find $M$ can also be obtained as a quotient of $[-1,1] \times (-1,1)$ by identifying $(-1,x)$ with $(1,-x)$, for each $x \in (0,1)$.
\end{example}

\begin{example}[Projective Space]
    Consider the group action of $\{ -1, 1 \}$ on $S^n$ given by reflection. We can then consider a quotient space obtained by identifying opposite sides of the sphere. The projection is open and locally injective, so the resulting quotient space, denoted $\RR \mathbf{P}^n$, known as real projective space. Another way to describe the space is as $\mathbf{R}^{n+1} - \{ 0 \}$, where $x$ and $\lambda x$ are identified, for $\lambda \neq 0$. To obtain explicit charts on $\mathbf{R} \mathbf{P}^n$, define $x: S^n \to \mathbf{R}^n$ by
    %
    \[ x(a_1, \dots, a_{n+1}) = \frac{1}{a_i} (a_1, \dots, \widehat{a_i}, \dots, a_{n+1}) \]
    %
    This map is continuous whenever $a_i \neq 0$. For all points $p$, $x(p) = x(-p)$, so the chart descends to a map on $\mathbf{R} \mathbf{P}^n$ instead. It even has a continuous inverse
    %
    \[ x^{-1}(b_1, \dots, b_n) = \left[ b_1, \dots, 1, \dots, b_n \right] \]
    %
    Since our maps cover the space, $\mathbf{R} \mathbf{P}^n$ is an $n$ dimensional manifold.
\end{example}

\begin{example}
    A similar space can be formed in the case of complex scalars, by viewing $\mathbf{R}^{2n}$ as $\mathbf{C}^n$. We then identify $z$ with $\lambda z$, for $\lambda \in \mathbf{C} - \{ 0 \}$ to form the quotient space $\mathbf{CP}^n$ from $\mathbf{C}^{n+1} - \{ 0 \}$. For $n = 1$, $\mathbf{P} \mathbf{C}^1$ is just the Riemann sphere, a 2 manifold. In general, $\mathbf{CP}^n$ is a 2n dimensional real manifold, or $n$ `complex' dimensions. We may consider the chart
    %
    \[ x(z_1, \dots, z_{n+1}) = \frac{1}{z_i} (z_1, \dots, \widehat{z_i}, \dots, z_{n+1}) \]
    %
    with inverse
    %
    \[ x^{-1}(w_1, \dots, w_n) = [w_1, \dots, 1, \dots, w_n] \]
    %
    where we consider complex multiplication instead of real multiplication. The space formed is known as complex projective space.
\end{example}

Even though $\mathbf{RP}^2$ is locally trivial, the space is very strange globally, and cannot be embedded in $\mathbf{R}^3$. Nonetheless, the geometry our eyes percieve is modelled very accurately by the spherical construction of projective space. We don't see the really weird part of $\mathbf{R} \mathbf{P}^2$, since our eye cannot see the full circumpherence of vision, but these topological problems occur in the algorithms involved in patching portions of vision across the entire circumpherence.

\begin{example}[Gluing Surfaces]
    Let $M$ and $N$ be connected $n$-manifolds. We shall define the connected sum $M \# N$ of the two manifolds. There are two sets $B_1$ and $B_2$ in $M$ and $N$ respectively, both homeomorphic to the closed unit ball in $\mathbf{R}^n$. Then there is a homeomorphism $h:\partial B_1 \to \partial B_2$, and we may define the connected sum as
    %
    \[ M \# N = (M - B_1^\circ) \cup_h (N - B_2^\circ) \]
    %
    The topological structure formed is unique up to homeomorphism, but this requires some tough topology to show for general manifolds. An example application is the construction of the $n$-holed torus $T \# T \# \dots \# T \# T$, which is a surface embeddable in $\mathbf{R}^3$.
\end{example}

\section{Euclidean Neighbourhoods are Open}

In these notes, we consider a neighbourhood as in the French school, as any subset containing an open set, regardless of whether it is open or not. Nonetheless, let $M$ be a manifold, and take a point $p$ with neighbourhood $U$ homeomorphic to $\mathbf{R}^n$, lets say, by some continuous function $f: U \to \mathbf{R}^n$. Then $U$ contains an open set $V$, and $f(V)$ is open in $\mathbf{R}^n$, so that $f(V)$ contains an open ball $W$ around $f(x)$. But then $W$ is homeomorphic to $\mathbf{R}^n$, and $f^{-1}(W)$ is a neighbourhood of $x$ open in $V$ (and therefore open in $M$) homeomorphic to $\mathbf{R}^n$. This complicated discussion stipulates that we may always choose open neighbourhoods in the definition in a manifold. Remarkably, it turns out that all neighbourhoods homeomorphic to $\mathbf{R}^n$ {\it must} be open; to prove this, we require an advanced theorem of algebraic topology.

% Draw construction above

\begin{theorem}[Invariance of Domain]
    If $f:U \to \mathbf{R}^n$ is a continuous, injective function, where $U$ is an open subset of $\mathbf{R}^n$, then $f(U)$ is open, and $f$ is an embedding of $U$ in $\RR^n$.
\end{theorem}

Classically, analysts called a connected open set a domain, and so this theorem shows that the property of being a domain is invariant under continuous injective maps from $\RR^n$ to itself. In multivariate calculus, the inverse function theorem shows this for differentiable mappings with non-trivial Jacobian matrices across its domain; invariance of domain stipulates that the theorem in fact holds for any such continuous map $f$ on an open domain. The theorem can be proven in an excursion in some basic algebraic topology. In an appendix to this chapter, we shall prove the theorem based on the weaker assumption of the Jordan curve theorem, which is slightly more intuitive than invariance of domain, but still requires strong techniques in algebraic topology to prove.

\begin{lemma}
    If $U \subset \mathbf{R}^n$ and $V \subset \mathbf{R}^m$ are open, then $U \cong V$ implies $n = m$.
\end{lemma}
\begin{proof}
    If $n < m$, consider the projection $\pi: \mathbf{R}^n \to \mathbf{R}^m$
    %
    \[ \pi(x_1, \dots, x_n) = (x_1, \dots, x_n, 0, \dots, 0) \]
    %
    Clearly no subset of $\pi(\mathbf{R}^n)$ is open. But if $f: V \to U$ is a homeomorphism, then $\pi \circ f: V \to \mathbf{R}^m$ is continuous and injective, so $\pi(V) \subset \pi(\mathbf{R}^n)$ is open by invariance of domain.
\end{proof}

The \emph{dimension} of a point on a manifold is the dimension of the euclidean space which is locally homeomorphic to a neighbourhood of the point. When a manifold is connected, one can show simply that the dimension across the entire space is invariant, and we may call this the \emph{dimension of the manifold}. As shorthand, we let $M^n$ denote a manifold $M$ which is $n$ dimensional.

\begin{corollary}
    The dimension of a point on a manifold is unique.
\end{corollary}
\begin{proof}
    Let $U$ and $V$ be two non-disjoint neighbourhoods of a point homeomorphic to $\mathbf{R}^n$ and $\mathbf{R}^m$ by $f:U \to \mathbf{R}^n$ and $g:V \to \mathbf{R}^m$. Then $U \cap V$ is also open, and homeomorphic to open sets of $\mathbf{R}^n$ and $\mathbf{R}^m$. We conclude $n = m$.
\end{proof}

\begin{theorem}
    Any subset of a manifold locally homeomorphic to Euclidean space is open in the original topology.
\end{theorem}
\begin{proof}
    Let $M$ be a manifold, and $U \subset M$ homeomorphic to $\mathbf{R}^n$ by a function $f$. Let $x \in U$ be arbitrary. There is an open neighbourhood $V$ of $x$ that is homeomorphic into $\mathbf{R}^n$ by a function $g$. Since $V$ is open in $M$, $U \cap V$ is open in $U$, so $f(U \cap V)$ is open in $\mathbf{R}^n$. We obtain a one-to-one continuous function from $f(U \cap V)$ to $g(U \cap V)$ by the function $g \circ f^{-1}$. It follows by invariance of domain that $g(U \cap V)$ is open in $\mathbf{R}^n$, so $U \cap V$ is open in $V$, and, because $V$ is open in $M$, $U \cap V$ is open in $M$. In a complicated manner, we have shown that around every point in $U$ there is an open neighbourhood contained in $U$, so $U$ itself must be open.
\end{proof}

Really, this theorem is just a generalized invariance of domain for arbitrary manifolds -- since the concept of a manifold is so intertwined with Euclidean space, it is no surprise we need the theorem for $\mathbf{R}^n$ before we can prove the theorem here.

\section{Equivalence of Regularity Properties}

Many important results in differentiable geometry require spaces with more stringent properties than those that are merely Hausdorff. At times, we will want to restrict ourselves to topological manifolds with these properties. Fortunately, most of these properties are equivalent.

\begin{theorem}
    For any manifold, the following properties are equivalent:
    %
    \begin{enumerate}
        \item[(1)] Every component of the manifold is $\sigma$-compact.
        \item[(2)] Every component of the manifold is second countable.
        \item[(3)] The manifold is metrizable.
        \item[(4)] The manifold is paracompact (so every compact manifold is metrizable).
    \end{enumerate}
\end{theorem}

\begin{lemma}[$1) \to (2$]
    Every $\sigma$-compact, locally second countable space is globally second countable.
\end{lemma}
\begin{proof}
    Let $X$ be a locally second countable space, equal to the union of compact sets $\bigcup_{i = 1}^\infty A_i$. For each $x$, there is an open neighbourhood $U_x$ with a countable base $\mathcal{C}_x$. If, for some $A_i$, we consider the set of $U_x$ for $x \in A_i$, we obtain a cover, which therefore must have a finite subcover $U_{x_1}, U_{x_2}, \dots, U_{x_n}$. Taking $\bigcup_{i = 1}^n \mathcal{C}_{x_i}$, we obtain a countable base $\mathcal{C}_i$ for all points in a neighbourhood of $A_i$. Then, taking the union $\bigcup_{i = 1}^\infty \mathcal{C}_i$, we obtain a countable base for $X$.
\end{proof}

\begin{lemma}[$2) \to (3$]
    If a manifold is second countable, then it is metrizable.
\end{lemma}
\begin{proof}
    This is a disguised form the Urysohn metrization theorem, proved in a standard course in general topology. If you do not have the background, you will have to have faith that this lemma holds. All we need show here is that a second countable manifold is regular, and this follows because every locally compact Hausdorff space is Tychonoff.
\end{proof}

\begin{lemma}[$3) \to (1$]
    Every connected, locally compact metrizable space is $\sigma$-compact.
\end{lemma}
\begin{proof}
    Consider any connected, locally compact metric space $(X,d)$. For each $x$ in $X$, let
    %
    \[ r(x) = \frac{\sup \{ r \in \mathbf{R} : \overline{B}_r(x)\ \text{is compact} \}}{2} \]
    %
    Since $X$ is locally compact, this function is well defined and positive for all $x$. If $r(x) = \infty$ for any $x$, then $\{ \overline{B}_n(x) : n \in \mathbf{Z} \}$ is a countable cover of the space by compact sets. Otherwise, $r(x)$ is finite for every $x$. Suppose that
    %
    \[ d(x,y) + r' < 2r(x) \]
    %
    By the triangle inequality, this tells us that $\overline{B}_{r'}(y)$ is a closed subset of $\overline{B}_{r(x)}(x)$, which is hence compact. This shows that, when $d(x,y) < r(x)$,
    %
    \[ r(y) \geq r(x) - \frac{d(x,y)}{2} \]
    %
    Put more succinctly, this equation tells us that the function $r:X \to \mathbf{R}$ is continuous:
    %
    \[ |r(x) - r(y)| < \frac{d(x,y)}{2} \]
    %
    This has an important corollary. Consider a compact set $A$, and let
    %
    \[ A' = \bigcup_{x \in A} \overline{B}_{r(x)}(x) \]
    %
    We claim that $A'$ is also compact. Consider some sequence $\{ x_i \}$ in $A'$, and let $\{ a_i \}$ be elements of $A$ for which $x_i \in \overline{B}_{r(a_i)}(a_i)$. Since $A$ is compact, we may assume $\{ a_i \}$ converges to some $a$. When $d(a_i, a) < r(a)/2$,
    %
    \[ r(a_i) < r(a) + r(a)/4 \]
    %
    and so
    %
    \[ d(a,x_i) \leq d(a,a_i) + d(a_i,x_i) < r(a)/2 + [r(a) + r(a)/4] = 7r(a)/4 \]
    %
    Since we chose $r(a)$ to be half the supremum of compact sets, the sequence $x_k$ will eventually end up in the compact ball $B_{3r(a)/4}(a)$, and hence will converge.

    If $A$ is a compact set, we will let $A'$ be the compact set constructed above. Let $A_0$ consist of an arbitrary point $x_0$ is $X$, and inductively, define $A_{k+1} = A_k'$, and $A = \bigcup_{i = 0}^\infty A_k$. Then $A$ is the union of countably many compact sets. $A$ is obviously open. If $x$ is a limit point of $A$, then there is some sequence $\{ x_i \}$ in $A$ which converges to $x$, so $r(x_i) \to r(x)$. If $|r(x_i) - r(x)| < \varepsilon$, and also $d(x_i,x) < r(x) - \varepsilon$, then $x$ is contained in $B_{r(x_i)}(x_i)$, and hence if $x_i$ is in $A_k$, then $x$ is in $A_{k+1}$. Thus $A$ is non-empty and clopen, so $X = A = \bigcup A_k$ is $\sigma$-compact.
\end{proof}

\begin{lemma}[$4) \to (1$]
    A connected, locally compact, paracompact space is $\sigma$ compact.
\end{lemma}
\begin{proof}
    Consider a locally-finite cover $\mathcal{C}$ of precompact neighbourhoods in a space $X$. Fix $x \in X$. Then $x$ intersects finitely many elements of $\mathcal{C}$, which we may label $U_{1,1}, U_{1,2}, \dots, U_{1,n_1}$. Then
    %
    \[ U_1 = \overline{U_{1,1}} \cup \overline{U_{1,2}} \cup \dots \cup \overline{U_{{1,n_1}}} \]
    %
    intersects only finitely more elements of $\mathcal{C}$, since the set is compact, and we need only add finitely more open sets $U_{2,1}, \dots, U_{2,n_2}$, obtaining
    %
    \[ U_2 = \overline{U_{2,1}} \cup \dots \cup \overline{U_{2,n_2}} \]
    %
    Continuing inductively, we find an increasing sequence of compact neighbourhoods. Then $U = \bigcup U_i$ is open because a neighbourhood of $y \in U_k$ is contained in $U_{k+1}$. If $y$ is a limit point of $U$, take a neighbourhood $V \in \mathcal{C}$, which must intersect some $U_k$. Then $y \in U_{k+1}$, so $U$ is closed. We conclude $X = U$ is $\sigma$ compact.
\end{proof}

\begin{lemma}[$1) \to (4$]
    A $\sigma$ compact, locally compact Hausdorff space is paracompact.
\end{lemma}
\begin{proof}
    Let $X = \bigcup C_i$ be a locally compact, $\sigma$-compact space. Since $C_1$ is compact, it is contained in an open precompact neighbourhood $U_1$. Similarily, $C_2 \cup \overline{U_1}$ is contained in a precompact neighbourhood $U_2$ with compact closure. We find $U_1 \subset U_2 \subset \dots$, each with compact closure, and which cover the entire space. Now let $\mathcal{U}$ be an arbitrary open cover of $X$. Each $V_k = U_{k} - \overline{U_{k-2}}$ (letting $U_{-2} = U_{-1} = U_0 = \emptyset$) is open, and its closure $\overline{V_k}$ is a closed subset of compact space, hence compact. Since $\mathcal{U}$ covers $\overline{V_k}$, it has a finite subcover $U_1, \dots, U_n$, and we let
    %
    \[ \mathcal{V}_1 = (U_1 \cap V_1), (U_2 \cap V_1), \dots, (U_n \cap V_1) \]
    %
    be a collection of refined open sets which cover $V_1$. Do the same for each $V_k$, obtaining $\mathcal{V}_2, \mathcal{V}_3, \dots$, and consider $\mathcal{V} = \bigcup \mathcal{V}_i$. Surely this is a cover of $X$, and each point is contained only in some $\mathcal{V}_k$ and $\mathcal{V}_{k+1}$, so this refined cover is locally finite.
\end{proof}

\section{Boundaries}

There is an addition family of `manifolds with sides' which often occurs in the theory of differential geometry. A \emph{manifold with boundary} is a topological space $M$ such that for each $x \in M$, there exists a neighbourhood $U$ of $x$ and a homeomorphism $f:U \to \RR^n$, \emph{or} a homeomorphism $f: U \to \mathbf{H}^n$, where
%
\[ \mathbf{H}^n = \{ x \in \RR^n: x_1 \geq 0 \}. \]
%
Invariance of domain can be used to show that only one of the two homeomorphisms can be constructed around a point. Given a manifold with boundary $M$, we let $M^\circ$ denote the \emph{interior of the manifold}, the family of all points which have neighbourhoods homeomorphic to $\RR^n$, and $\partial M$ denote the \emph{boundary of the manifold}, the family of all points which have neighbourhoods homeomorphic to $\mathbf{H}^n$. Of course, we can consider \emph{coordinate half balls} $(x,U)$, where $x$ is a homeomorphism between $U$ and the $B \cap \mathbf{H}^n$, where $B$ is the open unit ball centered at the origin.

\begin{theorem}
    If $M^n$ is a manifold with boundary, then $\partial M$ is a manifold of dimension $n-1$.
\end{theorem}
\begin{proof}
    Let $x$ be a point in $\partial M$, and let $U$ be a neighbourhood homeomorphic to $\mathbf{H}^n$ by a map $f:U \to \mathbf{H}^n$. Consider the points in $U$ that map to the boundary plane under $f$,
    %
    \[ V = \{ y \in U : f(y) = (0,x_2, \dots, x_n) \} \]
    %
    We contend that $V = U \cap \partial M$, so that $V$ is a neighbourhood of $x$ in the relative topology, and since $V$ is homeomorphic to $\mathbf{R}^{n-1}$, this will show that $\partial M$ is an $n-1$ dimensional manifold. It is easy to see that $V \subset U \cap \partial M$. Conversely, the other points in $U - V$ have a neighbourhood homeomorphic to $\mathbf{R}^n$, so that they do not lie in $\partial M$. Thus $U - V \subset M^\circ \cap U$, and this completes the proof.
\end{proof}

\begin{example}
    $\mathbf{H}^n$ is the easiest example of a manifold with boundary. It's boundary consists of $\{ 0 \} \times \mathbf{R}^{n-1}$, which is an $n - 1$ manifold. Another manifold with boundary is the unit disc $\mathbf{D}^n = \{ x \in \mathbf{R}^n : \|x\| \leq 1 \}$. We have already shown that the discs boundary, $\partial \mathbf{D}^n = S^{n-1}$, is an $n - 1$ manifold.
\end{example}

\section{Optional: A Proof of Invariance of Domain}

For this section, we will prove invariance of domain, relying on two unproved (but `obviously true') theorems.

\begin{theorem}[The Generalized Jordan Curve Theorem]
    Every subspace $X$ of $\mathbf{R}^n$ homeomorphic to $S^{n-1}$ splits $\mathbf{R}^n - X$ into two components, and $X$ is the boundary of each.
\end{theorem}

\begin{theorem}
    If a subspace $Y$ of $\mathbf{R}^n$ is homeomorphic to the unit disc $\mathbf{D}^n$, then $\mathbf{R}^n - Y$ is connected.
\end{theorem}

We'll put on the finishing touches to Invariance of Domain now. Hopefully this will give you intuition to why the theorem is true.

\begin{lemma}
    One of the components of $\mathbf{R}^n - X$ is bounded, and the other is unbounded. We call the bounded component the \emph{inside} of $X$, and the unbounded component the \emph{outside}.
\end{lemma}
\begin{proof}
    Since $X$ is homeomorphic to $S^n$, it is a compact set, and therefore contained in some ball $B$. $\mathbf{R}^n - B$ is connected, so therefore one component of $\mathbf{R}^n - X$ is contained within $B$. Since $B$ is bounded, this component is bounded. If both components are bounded, we conclude that the union of the two components plus $X$ is bounded, a contradiction. Therefore the other component is unbounded.
\end{proof}

\begin{lemma}
    If $U \subset \mathbf{R}^n$ is open, $A \subset U$ is homeomorphic to $S^n$, $f:U \to \mathbf{R}^n$ is one-to-one and continuous, and $A \cup (\text{inside of}\ A)$ is homeomorphic to $\mathbf{D}^n$, then $f(\text{inside of}\ A) = \text{inside of}\ f(A)$.
\end{lemma}
\begin{proof}
    Since $f$ is continuous, $f(\text{inside of}\ A)$ is connected, and is therefore contained either entirely within the outside of $f(A)$ or the inside of $f(A)$. The same is true of $f(\text{outside of}\ A)$. The difference is that, due to compactness, $f(A \cup (\text{inside of}\ A))$ is homeomorphic to $A \cup (\text{inside of}\ A)$, and in connection, homeomorphic to $\mathbf{D}^n$. Therefore $\mathbf{R}^n - f(A \cup \text{inside of}\ A)$ is connected. It follows that $f(\text{inside of}\ A)$ is a component of $\mathbf{R}^n - f(A)$, so it is equal to either the inside of outside of space. Since $f(\text{inside of}\ A)$ is contained within a bounded ball, we conclude that it is equal to the inside.
\end{proof}

\begin{theorem}[Invariance of Domain]
    If $f:U \to \mathbf{R}^n$ is an injective continuous function, where $U$ is an open subset of $\mathbf{R}^n$, then $f(U)$ is open, and therefore $f$ is homeomorphic onto its image.
\end{theorem}
\begin{proof}
    Let $V$ be an arbitrary open subset of $U$. We must show $f(V)$ is also open. Let $x \in V$ be arbitrary, and consider a closed ball $\overline{B}$ containing $x$, and contained in $V$. The boundary of $\overline{B}$ is homeomorphic to $S^{n-1}$, and the interior $B$ is equal to the inside of $\overline{B}$. By the lemma (2.4) above, we conclude that
    %
    \[ f(B) = \text{inside of}\ f(\partial B) \]
    %
    Since $\partial B$ is closed in $\mathbf{R}^n$, the inside is open, hence $f(B)$ is open. By an extension of this argument, we have shown the the image of any open set is open, so invariance of domain is proved.
\end{proof}

The unproved theorems we rely on here require quite advanced techniques in algebraic topology, which we will not discuss in these notes for quite some time.








\chapter{Differentiable Structures}

As a topological space, we know when a map between manifolds is continuous, but when is a map differentiable? What we seek is a definition abstract enough to work on any manifold, yet possessing the same properties of differentiable functions on $\mathbf{R}^n$.

\section{Defining Differentiability}

Let us be given a map $f:M \to N$ between manifolds. Given a correspondence $b = f(a)$, a reasonable inquiry would be to consider two charts $(x,U)$ and $(y,V)$, where $U$ is a neighbourhood of $a$ and $V$ is a neighbourhood of $b$. We obtain a map $y \circ f \circ x^{-1}$, defined between open subsets of Euclidean space. We have thus `expressed $f$ in coordinates', and we can consider $f$ differentiable at $p \in M$ if $y \circ f \circ x^{-1}$ is differentiable at $x(a)$. Unfortunately, without additional structure this idea is doomed to fail, since charts on a topological manifold are only defined up to a homeomorphism.

\begin{example}
    Consider the chart $y: \mathbf{R} \to \mathbf{R}$, $y(t) = t^3$, and let $x$ be the identity chart. If $f(x) = \sin(x)$, then $x \circ f \circ x^{-1} = f$ is differentiable, yet
    %
    \[ (x \circ f \circ y^{-1})(t) = \sin( t^{1/3} ) \]
    %
    is not differentiable at the origin.
\end{example}

If we are to stick with this definition, we must identify additional structure on topolical manifolds. Our method will be to identify charts which are `correct', and ignore `non-differentiable' charts. Two charts $(x,U)$ and $(y,V)$ are \emph{$C^\infty$ related}, if either $U \cap V = \emptyset$, or $U \cap V$ is nonempty, and the two functions
%
\[ y \circ x^{-1} : x(U \cap V) \to y(U \cap V) \]
%
\[ x \circ y^{-1} : y(U \cap V) \to x(U \cap V) \]
%
are $C^\infty$ functions. One can see a chart as laying a blanket down onto a manifold. Two charts are $C^\infty$ related if, when we lay them down over each other, they contain no creases! The fact that manifolds do not have a particular preference for coordinates is both a help and a hindrance. On one side, it forces us to come up with elegant, coordinate free approaches to geometry. On the other end, these coordinate free approaches can become quite abstract!

A \emph{smooth} or \emph{$C^\infty$ atlas} for a manifold $M$ is a family of $C^\infty$ related charts $\{ (x_\alpha, U_\alpha) \}$, such that $\{ U_\alpha \}$ is a cover of $M$. A maximal atlas is called a \emph{smooth structure} on a manifold, and a manifold together with a smooth structure is called a \emph{smooth} or \emph{differentiable manifold}. The coordinate charts in the atlas of a smooth manifold are referred to as smooth, or curvilinear. In the literature, each map $y \circ x^{-1}$ is known as a \emph{transition map}, so that we say an atlas for a manifold has $C^\infty$ transition maps. From now on, when we mention a chart on a differentiable manifold, we implicitly assume the chart is the member of the smooth structure of the manifold.

Let $f:M \to N$ be a map between two smooth manifolds. Then $f$ is \emph{differentiable} at $p \in M$ if it is continuous at $p$, and if for some chart $(x,U)$ on $M$ and $(y,V)$ on $N$ with $p \in U$ and $f(p) \in V$, the map
%
\[ y \circ f \circ x^{-1}: x(f^{-1}(V) \cap U) \to \RR^m \]
%
is differentiable at $x(p)$. The function $f$ itself is \emph{differentiable} if it is differentiable at every point on its domain, or correspondingly, if $y \circ f \circ x^{-1}$ is differentiable for any two charts $x$ and $y$. We say the function is $C^k$, if all partial derivatives of $y \circ f \circ x^{-1}$ of order $\leq k$ are continuous, and $C^\infty$, or \emph{smooth} if partial derivatives of all orders are continuous. If $f: M \to N$ is bijective, and both $f$ and $f^{-1}$ are $C^k$ differentiable maps, then we say $f$ is a $C^k$ \emph{diffeomorphism}. In these notes, a diffeomorphism will always refer to a smooth diffeomorphism, i.e. a $C^\infty$ diffeomorphism, unless otherwise specified. Since differentiability is a {\it local} condition on Euclidean spaces, and manifolds are locally Euclidean spaces, the smooth structure, which preserves the differentiability across all local charts, allows us to define differentiability on arbitrary manifolds.

\begin{lemma}
    Every atlas extends to a unique smooth structure.
\end{lemma}
\begin{proof}
Let $\mathcal{A}$ be an atlas for a manifold $M$, and consider the set $\mathcal{A}^*$, which is the union of all atlases containing $\mathcal{A}$. We shall show that $\mathcal{A}^*$ is also an atlas, and therefore necessarily the unique maximal one. Let $(x,U)$ and $(y,V)$ be two charts in $\mathcal{A}^*$, with $U \cap V \neq \emptyset$. For each $p \in U \cap V$, let $(z,W)$ be a chart in $\mathcal{A}$ with $p \in W$. Then for $a \in y(U \cap V \cap W)$, we have
%
\[ (x \circ y^{-1})(a) = [(x \circ z^{-1}) \circ (z \circ y^{-1})](a). \]
%
By assumption, $x \circ z^{-1}$ and $z \circ y^{-1}$ are $C^\infty$ maps on $U \cap V \cap W$, since $x$ and $y$ are in some atlas containing $z$. Thus $x \circ y^{-1}$ is $C^\infty$ on $U \cap V \cap W$. Since $p$ was arbitrary, we can take unions over all neighbourhoods $W$ of points to conclude that $x \circ y^{-1}$ is smooth on $U \cap V$. Of course, this argument also implies $y \circ x^{-1}$ is smooth, so $x$ and $y$ are $C^\infty$ related. Thus $\mathcal{A}^*$ is also an atlas.
\end{proof}

This implies, in particular, that we may specify a smooth structure by just giving a family of $C^\infty$ related transition maps covering a manifold, which is almost always easier to specify than giving a complete maximal atlas.

\begin{corollary}
    If $x$ is a chart defined on a differentiable manifold $M$, and is $C^\infty$ related to each map in a generating atlas $\mathcal{A}$, then $x$ is in the smooth structure generated by $\mathcal{A}$.
\end{corollary}

\begin{lemma}
    Let $f: M \to N$ be a map, and suppose there are smooth charts $(x_1,U_1)$ and $(y_1,V_1)$ on $M$ and $N$, with $p \in U_1 \cap f^{-1}(V_1)$, such that $y_1 \circ f \circ x_1^{-1}$ is differentiable at $x_1(p)$. Then if $(x_2,U_2)$ and $(y_2,V_2)$ are smooth charts on $M$ and $N$, with $p \in U_2 \cap f^{-1}(V_2)$, then $y_2 \circ f \circ x_2^{-1}$ is differentiable at $x_2(p)$.
\end{lemma}
\begin{proof}
    We find that for $a \in x_2(U_1 \cap U_2 \cap f^{-1}(V_1) \cap f^{-1}(V_2))$,
    %
    \[ (y_2 \circ f \circ x_2^{-1})(a) = [(y_2 \circ y_1^{-1}) \circ (y_1 \circ f \circ x_1^{-1}) \circ (x_1 \circ x_2^{-1})](a). \]
    %
    Clearly $x_1 \circ x_2^{-1}$ is differentiable at $x_2(p)$, $y_1 \circ f \circ x_1^{-1}$ is differentiable at $x_1(p)$, and $y_2 \circ y_1^{-1}$ is differentiable at $y_1(f(p))$, so the chain rule implies $y_2 \circ f \circ x_2^{-1}$ is differentiable at $x_2(p)$.
\end{proof}

For simplicity, in these notes we assume almost all the objects we consider are smooth, i.e. $C^\infty$. This is mostly a convenience, since it means we can freely differentiate as many times as we desire, and do not have to constantly list the number of times an object must be differentiated before we can apply a certain technique. Transferring these results to less smooth objects is just a matter of checking how many times an object is differentiable.

However, sometimes it is useful to only ascribe a `$C^k$ structure' to a manifold, for $k \geq 1$, rather than a `$C^\infty$ structure'. Two charts $(x,U)$ and $(y,V)$ are \emph{$C^k$ related} if $U \cap V = \emptyset$, or $U \cap V \neq \emptyset$ and $y \circ x^{-1}$ is a $C^k$ map on Euclidean space. Thus a $C^k$ manifold is a manifold equipped with a $C^k$ atlas, and on a $C^k$ manifold we can consider differentiable maps, and moreover, $C^i$ maps for all $i \leq k$. On the other hand, it does not make sense to talk about $C^i$ maps for $i > k$, or $C^\infty$ maps. Topologically speaking, the family of $C^k$ manifolds is no more general than a $C^\infty$ manifold, since a deep result of differential topology shows any $C^k$ manifold can be given a $C^\infty$ structure. But sometimes we can equip a manifold with a $C^k$ atlas naturally, but it is not natural to equip it with a smooth structure. Our first example of this occurs later on in this chapter during our discussion of submanifolds.

We can also consider an even more rigid structure than a $C^\infty$ atlas. If we require transition maps between charts to be (real) analytic, and we build up an atlas of charts compatible with respect to this requirement, known as a \emph{$C^\omega$ atlas}, we obtain a \emph{real-analytic manifold}. If the dimension of the manifold is equal to $2n$, and we identify $\RR^{2n}$ with $\CC^n$, then we can require our transition maps to be holomorphic, we obtain the family of complex manifolds.

\begin{example}
    Let $M$ be a manifold, and $U$ an open submanifold. Then we can define an atlas on $U$, by considering all smooth charts $(x,V)$ on $M$, with $V \subset U$. This is a maximal atlas, and is the unique smooth structure such that the following properties hold:
    %
    \begin{enumerate}
        \item If $f: M \to N$ is differentiable, then $f|_U: U \to M$ is differentiable.
        \item The inclusion map $i:U \to M$ is differentiable.
        \item If $f: N \to M$ is differentiable, and $f(N) \subset U$, then $f: N \to U$ is differentiable.
    \end{enumerate}
    %
    Later on, we will see these results can also be extended to lower dimensional `submanifolds' of $M$.
\end{example}

\begin{example}
    Consider the manifold $\mathbf{R}^n$, and define a smooth structure by considering the generating atlas containing only the identity map. This defines a smooth structure on $\mathbf{R}^n$, such that
    %
    \begin{enumerate}
        \item $x$ is a chart on $\mathbf{R}^n$ if and only if $x$ and $x^{-1}$ are $C^\infty$ maps.
        \item A map $f:\mathbf{R}^n \to \mathbf{R}^m$ is differentiable in the sense of a manifold if and only if it is differentiable in the classical sense.
        \item A map $f:M \to \mathbf{R}^n$ is differentiable if and only if each coordinate map $f^i:M \to \mathbf{R}$ is differentiable.
        \item A chart $x:U \to \mathbf{R}^n$ is a diffeomorphism from $U$ to $x(U)$.
    \end{enumerate}
    %
    Our definition thus contains the classical calculus as a special case.
\end{example}

We note that the transition maps given to define all the topological manifolds in the previous chapter are all $C^\infty$ related to one another. It is left as an exercise to check this is true. Thus all the topological manifolds in the previous chapter are also differentiable manifolds. There are some manifolds which do not have a continuous structure -- the creases in the manifold cannot be evened out. But these occur in certain pathological situations that we won't touch upon in these introductory notes.

\begin{example}
    On $\RR^2 - \{ 0 \}$, we can consider the polar coordinate system. A \emph{polar coordinate} system $((\theta, r),U)$ is a chart such that $r(z) = |z|$ for all $z \in U$, and on $U$,
    %
    \[ x = r \cos(\theta) \quad\text{and}\quad y = r \sin(\theta). \]
    %
    Assuming knowledge of differential forms we will prove later, the two equations above imply that on $U$,
    %
    \[ dr = \frac{xdy + ydx}{(x^2 + y^2)^{1/2}}, \]
    %
    and
    %
    \[ d\theta = \frac{x dy - y dx}{x^2 + y^2}. \]
    %
    The form on the right is well defined for all points in $\RR^2 - \{ 0 \}$, and one can easily verify it is closed. By Poincare's lemma, if $U$ is simply connected, this implies there exists a smooth function $\theta: U \to \RR$ which agrees with the form on the right. Another purely algebraic argument shows that $dx = d(r \cos(\theta))$, and thus there exists a constant $A$ such that $x = r \cos(\theta) + A$. As $z \to 0$, $r(z) \to 0$, and so
    %
    \[ 0 = \lim_{z \to 0} x = \lim_{z \to 0} r \cos(\theta) + A = A, \]
    %
    so $A = 0$. A similar argument shows that $y = r \sin(\theta)$. Thus we know polar coordinate exist on any simply connected subset of $\RR^2 - \{ 0 \}$.
\end{example}

\begin{example}
    On $\mathbf{R}^3 - \RR e_3$, we can consider \emph{spherical coordinate systems}, i.e. coordinate charts $((r,\phi,\theta), U)$ such that for each $(x,y,z) \in U$,
    %
    \begin{align*}
        x &= r \cos(\theta) \cos(\phi)\\
        y &= r \cos(\theta) \sin(\phi)\\
        z &= r \sin(\theta).
    \end{align*}
    %
    One can verify that the map $(a,b,c) \mapsto (a \cos(b) \cos(c), a \cos(b) \sin(c), a \sin(b))$ has derivative
    %
    \[ \begin{pmatrix} \cos(b) \cos(c) & -a \sin(b) \cos(c) & - a \cos(b) \sin(c) \\ \cos(b) \sin(c) & - a \sin(b) \sin(c) & a \cos(b) \cos(c) \\ \sin(b) & a \cos(b) & 0 \end{pmatrix}. \]
    %
    The determinant of this, after simplification, is
    %
    \[ -a^2 \cos(b) (\cos^2(b) \cos^2(c) + \sin^2(b) \cos^2(c) + \sin^2(c)). \]
    %
    Thus the map has full rank precisely when $b \not \in \pi/2 + n \ZZ$, and thus the inverse function theorem guarantees the existence of a spherical coordinate system locally about any point in $\RR^3 - \{ 0 \}$ except those points of the form $(0,0,t)$. As in the last example, the Poincare lemma will guarantee the existence of a spherical coordinate system on any simply connected subset of $\RR^3 - \RR e_3$.
\end{example}

\begin{example}
    On $\RR^3 - \RR e_3$, we can also consider cylindrical coordinate systems, i.e. triples $(r,\theta,z)$ such that
    %
    \[ x = r \cos \theta \]
    %
    \[ y = r \sin \theta \]
    %
    \[ z = z \]
    %
    One verifies by the inverse function theorem that cylindrical coordinates exist on any simply connected subset of $\RR^3 - \RR e_3$.
\end{example}

\begin{example}
    More generally, on the set $\RR^n - \text{span}(e_3, \dots, e_d)$, we can consider generalized spherical coordinate systems, given by maps $(r,\theta_1, \dots, \theta_{d-1})$, such that
    %
    \begin{align*}
        x_1 &= r \cos(\theta_1) \dots \cos(\theta_n)\\
        x_2 &= r \cos(\theta_1) \dots \cos(\theta_{n-1}) \sin(\theta_n)\\
        x_3 &= r \cos(\theta_1) \dots \cos(\theta_{n-2}) \sin(\theta_{n-1})\\
        \vdots\\
        x_d &= r \sin(\theta_1).
    \end{align*}
    %
    Generalized spherical coordinate systems exist on any simply connected subset of $\RR^n - \text{span}(e_3, \dots, e_d)$. Of course, such systems are exponentially more complicated to use.
\end{example} 

\begin{example}
    The differentiable structure on $S^n$ is defined by the stereographic projection maps. Equivalently, we may also define this structure on $S^1$ by the angle functions. The generalized spherical coordinates also specify this structure, but require a rotation in higher dimensions due to singularities when $n \geq 2$, so results get exponentially more complicated. To verify that the angle functions are differentiable, we note that if $(\theta,U)$ and $(\psi,V)$ are angle functions, then $\theta \circ \psi^{-1}$ is just a translation by a multiple of $2\pi$ on each connected component of $\psi(U \cap V)$, hence $C^\infty$. Indeed, if
    %
    \[ \psi^{-1}(t) = e^{it} = \theta^{-1}(t') \]
    %
    then $t = t' + 2 \pi n$ for some unique integer $n$. Define $f(t) = n$, giving us a map $f: U \cap V \to \mathbf{Z}$. This map is continuous because if $t_i \to t$, then
    %
    \[ (\theta \circ \psi^{-1})(t_i) \to (\theta \circ \psi^{-1})(t) \]
    %
    so $t_i + 2 \pi f(t_i) \to t + 2 \pi f(t)$, hence $f(t_i) \to f(t)$. The continuity of $f$ implies that $f$ is constant on every connected component of $U \cap V$.
\end{example}

\begin{example}[Differentiable Product]
    If $M$ and $N$ are differentiable manifolds, we may consider an atlas on $M \times N$ with the differentiable structure generated by all maps $x \times y$, where $x$ is a chart on $M$ and $y$ is a chart on $N$. From this definition, we find $f \times g: X \to M \times N$ is differentiable if and only if $f: X \to M$ and $g: X \to N$ are differentiable. This is the unique differentiable structure on $M \times N$ which has this property, since the property determines the charts on the manifold.
\end{example}

\begin{example}[Differentiable Quotients and $\mathbf{P}^n$]
    If $N$ is a quotient space of a differentiable manifold $M$ whose projection $\pi:M \to N$ is locally injective, then we may ascribe a differentiable structure to it. We take all smooth charts $x:U \to \mathbf{R}^n$ on $M$ such that $U$ is homeomorphic to $\pi(U)$ by $\pi$. We may then push the chart onto $N$, and all the charts placed down on $N$ will be $C^\infty$ related. As a covering, this can be extended to a maximal atlas. In fact, this is the unique structure on $N$ which causes $f: N \to X$ to be differentiable if and only if $f \circ \pi: M \to X$ is differentiable. For instance, this gives the projective plane $\mathbf{P}^n$ and the M\"{o}bius strip $\mathbf{M}$ a canonical smooth structure.
\end{example}

\begin{example}
    Smooth structures on manifolds are {\it not} unique. Let $\mathbf{R}_1$ be the canonical smooth manifold on $\mathbf{R}$. Let $\mathbf{R}_2$ be the smooth structure on $\mathbf{R}$ generated by the map $x$, such that $x(t) = t^3$. Then $\mathbf{R}_1$ and $\mathbf{R}_2$ are diffeomorphic. Let $x:\mathbf{R}_2 \to \mathbf{R}_1$ be our diffeomorphism. It is surely bijective. Let $y$ be a chart on $\mathbf{R}_2$. We must verify that $y = z \circ x$, where $z$ is a chart on $\mathbf{R}_1$. We may show this by verifying that $y \circ x^{-1} = z$, and $x \circ y^{-1} = z^{-1}$ is $C^\infty$ on $\mathbf{R}_1$. But this was exactly why $y$ was a chart on $\mathbf{R}_2$ in the first place, hence the map is a diffeomorphism. Later, we will show that most manifolds which lies in Euclidean space has a natural smooth structure, however, and this will be taken as the canonical smooth structure in any of these cases.
\end{example}

\section{Smooth Functions on a Manifold}

The set of all scalar-valued differentiable maps defined on a manifold $M$ form an algebra $C^\infty(M)$. Note that a continuous map $f: M \to N$ induces an algebra homomorphism $f^\#: C(N) \to C(M)$ defined by $f^\#(g) = g \circ f$. If $f$ and $g$ are $C^\infty$, then $g \circ f$ is $C^\infty$, and so we may restrict $f^\#$ to a map from $C^\infty(N)$ to $C^\infty(M)$. We see therefore that the `map' $C^\infty$ defines a contravariant functor from the category of differential manifolds to the category of algebras over the real numbers.

\begin{lemma}
    A continuous map $f:M \to N$ between manifolds is smooth if and only if $f^\#(C^\infty(N)) \subset C^\infty(M)$.
\end{lemma}
\begin{proof}
    Suppose $f^\#(C^\infty(N)) \subset C^\infty(M)$. Let $(y,V)$ be a chart on $N$ at a point $q$, and let $(x,U)$ be a chart on $M$ at $p \in f^{-1}(p)$. By assumption, each $y^i \circ f = f^\#(y^i)$ is differentiable, so that $y^i \circ f \circ x^{-1}$ is differentiable. But this implies $y \circ f \circ x^{-1}$ is differentiable, so $f$ is differentiable.
\end{proof}

\begin{theorem}
    A homeomorphism $f:M \to N$ is a diffeomorphism if and only if $f^\#$ is an isomorphism between $C^\infty(N)$ and $C^\infty(M)$.
\end{theorem}
\begin{proof}
    Given $g = f^{-1}$, we note that $(g \circ f)^\# = f^\# \circ g^\#$ is just the identity map. Thus if $f$ is a diffeomorphism, Then $f^\# \circ g^\#$ is the identity, so that $f^\#$ is invertible, and hence an isomorphism. Conversely, if $f^\#$ is a bijection between $C^\infty(N)$ and $C^\infty(M)$, then $f^\#(C^\infty(N)) = C^\infty(M) \subset f^\#(C^\infty(M))$, so $f$ is differentiable, and $g^\#(C^\infty(M)) = C^\infty(N) \subset C^\infty(N)$, so $g$ is also differentiable, hence $f$ is a diffeomorphism.
\end{proof}

\begin{remark}
    More generally, $f$ is $C^k$ if and only if $f^\#(C^k(N)) \subset C^k(M)$, and a $C^k$ diffeomorphism if and only if $f$ maps $C^k(N)$ isomorphically to $C^k(M)$.
\end{remark}

Suppose we know $C^\infty(M)$ for all manifolds $M$. Then we may recover the smooth structure on $M$, which is the set of diffeomorphisms from open subsets of $M$ to open subsets of euclidean space. We can actually define a $C^\infty$ manifold in a completely algebraic way. Given some topological space $X$, suppose that from the sheaf of continuous functions on $X$, we consider a subsheaf $C^\infty(U)$, such that every point $p \in M$ has a neighbourhood $U$ and $x^1, \dots, x^n \in C^\infty(U)$ such that $x = (x^1, \dots, x^n)$ is a homeomorphism of $U$ with an open subset of $\mathbf{R}^n$, and $f \in C^\infty(U)$ if and only if $f \circ x^{-1}$ is a $C^\infty$ function. Then there is a unique $C^\infty$ structure on $X$ such that the $C^\infty$ real-valued maps on any open set $U$ are precisely the elements of $C^\infty(U)$. These facts constitute the foundation of the algebraic viewpoint of function theory, which attempts to uncover the nature of manifolds solely by determing how geometric influences the algebraic properties of the commutative algebra $C^\infty(M)$. You can actually get pretty far with this approach: Nestruev's book ``Smooth Manifolds and Observables'' attempts to introduce differential geometry solely in this manner, but we prefer to introduce the geometric and algebraic approach simultaneously for maximal insight.

\section{Partial Derivatives}

In calculus, when a function is differentiable, we obtained a derivative, a measure of a function's local change. On manifolds, determining an analogous object is difficult due to the lack of a specified coordinate system. For now, we shall stick to structures corresponding to some particular set of coordinates. Consider a differentiable map $f \in C^1(M)$. We have no conventional coordinates to consider partial derivatives on, but if we fix some chart $x:U \to \mathbf{R}^n$ on $M$, we obtain a differentiable map $f \circ x^{-1}$, and we define, for a point $p \in U$,
%
\[ \left. \frac{\partial f}{\partial x^k} \right|_p = D_k(f \circ x^{-1})(x(p)) \]
%
Geometrically, this is the change in the function $f$ when we trace the function along the coordinate lines from the map $x$; literally, if we define a curve $c(t) = (f \circ x^{-1})(x(p) + te_k)$, then
%
\[ c'(0) = \left.\frac{\partial f}{\partial x^k}\right|_p \]
%
Sometimes we use the notation $\partial_{x^k} f$ for the partial derivative in the $k$'th direction, which simplifies the notation in heavy calculations.

\begin{theorem}
    If $(x,U)$ and $(y,V)$ are charts at a point $p$, and $f: M \to \RR$ is differentiable at $p$, then
    %
    \[ \left. \frac{\partial f}{\partial x^i} \right|_p = \sum_j \left. \frac{\partial y^j}{\partial x^i} \right|_p \left. \frac{\partial f}{\partial y^j} \right|_p \]
\end{theorem}
\begin{proof}
    We just apply the chain rule in Euclidean space.
    %
    \begin{align*}
        \left.\frac{\partial f}{\partial x_i}\right|_p &= D_i(f \circ x^{-1})(x(p)) = D_i((f \circ y^{-1}) \circ (y \circ x^{-1}))(x(p))\\
        &= \sum D_j(f \circ y^{-1})(y(p)) D_i(y_j \circ x^{-1})(x(p)) = \sum \left.\frac{\partial f}{\partial y_j}\right|_p \left.\frac{\partial y_j}{\partial x_i}\right|_p. \qedhere
    \end{align*}
\end{proof}

\begin{example}
    Let us compute the laplacian on $\mathbf{R}^2$ in polar coordinates.
    %
    \[ \bigtriangleup f = \frac{\partial f^2}{\partial x^2} + \frac{\partial f^2}{\partial y^2} \]
    %
    To do this, we need to relate partial differentives by the chain rule. If $(r,\theta)$ is the polar coordinate chart, and $(x,y)$ the standard chart on $\mathbf{R}^2$, then
    %
    \[ x = r \cos(\theta)\ \ \ \ \ y = r \sin(\theta) \]
    %
    (note that this is a relation between functions, and can be applied pointwise at any point on the charts). Thus the matrix of partial derivatives is
    %
    \[ \begin{pmatrix} \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} \end{pmatrix} = \begin{pmatrix} \cos \theta & -r \sin \theta \\ \sin \theta & r \cos \theta \end{pmatrix} \]
    %
    We can invert this matrix to obtain the partial derivatives with respect to $x$ and $y$. We have
    %
    \[ \begin{pmatrix} \frac{\partial r}{\partial x} & \frac{\partial r}{\partial y} \\ \frac{\partial \theta}{\partial x} & \frac{\partial \theta}{\partial y} \end{pmatrix} = \frac{1}{r} \begin{pmatrix} r \cos(\theta) & r \sin(\theta) \\ -\sin(\theta) & \cos(\theta) \end{pmatrix} = \begin{pmatrix} \cos(\theta) & \sin(\theta) \\ -\frac{\sin(\theta)}{r} & \frac{\cos(\theta)}{r} \end{pmatrix} \]
    %
    Now we apply the chain rule. We have
    %
    \[ \frac{\partial}{\partial x} = \cos(\theta) \frac{\partial}{\partial r} - \frac{\sin(\theta)}{r} \frac{\partial}{\partial \theta}\ \ \ \ \ \ \ \ \ \ \frac{\partial}{\partial y} = \sin(\theta) \frac{\partial}{\partial r} + \frac{\cos(\theta)}{r} \frac{\partial}{\partial \theta} \]
    %
    So
    %
    \begin{align*}
        \frac{\partial^2 f}{\partial x^2} &= \left( \cos(\theta) \frac{\partial}{\partial r} - \frac{\sin(\theta)}{r} \frac{\partial}{\partial \theta} \right) \left( \cos(\theta) \frac{\partial f}{\partial r} - \frac{\sin(\theta)}{r} \frac{\partial f}{\partial \theta} \right)\\
        &= \cos^2(\theta) \frac{\partial^2 f}{\partial r^2} + \frac{\cos(\theta) \sin(\theta)}{r^2} \frac{\partial f}{\partial \theta} - \frac{\cos(\theta) \sin(\theta)}{r} \frac{\partial^2 f}{\partial r \partial \theta}\\
        &+ \frac{\sin^2(\theta)}{r} \frac{\partial f}{\partial r} - \frac{\sin(\theta) \cos(\theta)}{r} \frac{\partial^2 f}{\partial \theta \partial r} + \frac{\sin(\theta) \cos(\theta)}{r^2} \frac{\partial f}{\partial \theta} + \frac{\sin^2(\theta)}{r^2} \frac{\partial^2 f}{\partial \theta^2}
    \end{align*}
    %
    \begin{align*}
        \frac{\partial^2 f}{\partial y^2} &= \left( \sin(\theta) \frac{\partial}{\partial r} + \frac{\cos(\theta)}{r} \frac{\partial}{\partial \theta} \right) \left( \sin(\theta) \frac{\partial f}{\partial r} + \frac{\cos(\theta)}{r} \frac{\partial f}{\partial \theta} \right)\\
        &= \sin^2(\theta) \frac{\partial^2 f}{\partial r^2} - \frac{\cos(\theta) \sin(\theta)}{r^2} \frac{\partial f}{\partial \theta} + \frac{\cos(\theta) \sin(\theta)}{r} \frac{\partial^2 f}{\partial r \partial \theta}\\
        &+ \frac{\cos^2(\theta)}{r} \frac{\partial f}{\partial r} + \frac{\sin(\theta) \cos(\theta)}{r} \frac{\partial^2 f}{\partial \theta \partial r} - \frac{\sin(\theta) \cos(\theta)}{r^2} \frac{\partial f}{\partial \theta} + \frac{\cos^2(\theta)}{r^2} \frac{\partial^2 f}{\partial \theta^2}
    \end{align*}
    %
    All the nastiness cancels out by use of the trigonometric identities, and we find
    %
    \[ \bigtriangleup f = \frac{\partial f}{\partial r^2} + \frac{1}{r} \frac{\partial f}{\partial r} + \frac{1}{r^2} \frac{\partial^2 f}{\partial \theta^2} \]
    %
    Which gives a simple radial description of the Laplacian. It makes sense that such a radial expansion exists, because the Laplacian describes the second averages of $f$ on balls around a point.
\end{example}

Partial derivatives also satisfy a nice `derivation' property, which we leave to the reader to calculate. It is essentially the product rule for partial derivatives. We shall see later that this property characterizes the partial derivative maps in the space of linear maps on $C^\infty(M)$, which relates to the `intrinsic' tangent space of a manifold.

\begin{lemma}
    If $f$ and $g$ are differentiable maps from a manifold $M$ to $\mathbf{R}$, then
    %
    \[ \left.\frac{\partial fg}{\partial x^i}\right|_p = f(p)\left.\frac{\partial g}{\partial x^i}\right|_p + g(p)\left.\frac{\partial f}{\partial x^i}\right|_p  \]
\end{lemma}

The partial derivatives provide immediate quantities to measure the rate of change of a real-valued differentiable function on a manifold. Next, we will consider a `coordinate-independent' way to measure this rate of change for any two functions, by the introduction of `infinitisimals' on manifolds.

\section{The Differential Map}

The coordinate operators allow us to extend differentiation to functions in $C^\infty(M)$, for any manifold $M$. However, defining the derivative of a differentiable map $f: M \to N$ between arbitrary manifolds is more tricky. We could define the derivative coordinatewise at a point $p$ by considering the derivative $D(y \circ f \circ x^{-1})(x(p))$, for some coordinate systems $x$ and $y$. The trouble is that in this way we can only talk of properties of the derivative that are invariant under the coordinate systems chosen. The trouble here is that there is no `definite' space the operator is defined over -- as we change the coordinate systems, the space changes. To obtain a `universal' coordinate independent map, we need to form a `universal' space which represents all coordinates at the same time, upon which the derivative operators are invariant. This space is known as the tangent bundle. There are deep and elegant constructions for this bundle, but for now, we only require the absolute basics.

Vectors $v$ in $\mathbf{R}^n$ are often pictured as starting at the origin, and ending at the point with the same coordinates as $v$. But it is often convenient to picture these vectors as starting at a different beginning point than the origin. We introduce the notation $v_p$, where $p$ is a point in Euclidean space, and $v$ is a vector, to be the vector beginning at $p$ and ending at $p + v$. One way to think of these values are as points $p$ in $\mathbf{R}^n$ with an added `first order' infinitisimal shift in the direction $v$. If $U$ is an open subset of $\mathbf{R}^n$, we can define the \emph{tangent bundle} of $U$ to be the space $TU = \{ v_p : p \in U, v \in \mathbf{R}^n \}$. At each point $p \in U$, the \emph{fibre} at $p$, denoted $T_pU$, consists of all tangent vectors beginning at $p$, which can be made into a vector space by defining $v_p + w_p = (v + w)_p$ and $\lambda v_p = (\lambda v)_p$. We now generalize this process to arbitrary manifolds not necessarily lying in Euclidean space.

There are many complex and elegant ways to form the tangent bundle on a differentiable manifold. We will eventually discuss them in time, but we now construct the space in the most quick and dirty way. On a differentiable manifold $M$, we locally specify the tangent bundle in charts, and then patch them together into a reasonable structure on the entire manifold. For each point $p$, we let the fibre $T_pM$ consist of equivalence classes of tuples of the form $(x,v)_p$, where $v \in \mathbf{R}^n$, and $(x,U)$ is a chart with $p \in U$, where we identify $(x,v)_p$ and $(y,w)_p$ if $w = D(y \circ x^{-1})(p)(v)$. In other words, $w$ is identified by $v$ if it is obtained from $v$ by a change in coordinates. Putting this altogether gives the required tangent vectors $[x,v]_p$. The fibres $T_pM$ are still vector spaces, obtained by defining $[x,v]_p + [x,w]_p = [x,v+w]_p$, and $\lambda [x,v]_p = [x,\lambda v]_p$. The set $TM$ is then constructed as the union of the vector spaces $TM_p$, and is the natural generalization of the tangent bundle to an open subset of Euclidean space.

If $f: U \to V$ maps an open subset $U \subset \mathbf{R}^n$ to $V \subset \mathbf{R}^m$, and is differentiable, then we can consider the derivatives $Df(p)$ at $p \in U$, which describe how the function acts locally around $p$. Since a differentation indicates that the space is `locally linear', then the derivative should act on tangent vectors by the actual linear map. In other words, we can put all the derivatives together to define the \emph{covariant derivative} of $f$, denoted $f_*$, by $f_*(v_p) = (Df(p)(v))_{f(p)}$. The linear map obtained by restriction of $f_*$ to the domain $T_p U$ and codomain $T_{f(p)} V$ is denoted $f_*|_p$.

For a differentiable map $f:M \to N$, we can define $f_*: TM \to TN$ by mapping $[x,v]_p$ to $[y,w]_{f(p)}$, where $w = D(y \circ f \circ x^{-1})(p)(v)$. This is easily shown to be independent of the coordinates chosen. Thus we obtain linear maps $f_*|_p$ from $TM_p$ to $TN_{f(p)}$. The chain rule for derivative can be succinctly represented by the fact that if $f: M_1 \to M_2$ and $g: M_2 \to M_3$ are differentiable maps, then $(g \circ f)_* = g_* \circ f_*$. We say that $f$ has \emph{rank $k$} at a point $p$ if $f_*|_p$ is a rank $k$ linear map. The next section will show the rank gives essentially all the local information about a differentiable map if we are free to choose arbitrary smooth coordinate systems.


\section{The Rank Theorems}

The rank theorems provide the existence of coordinates on a manifold which simplify how the maps operate immensely. They are essentially an extension of the Euclidean inverse and implicit function theorems to general smooth functions on manifolds.

\begin{theorem}
    If $f: M \to N$ is a smooth map with rank $k$ at a point $p$, there is a coordinate chart $(x,U)$ at $p$ and a coordinate chart $(y,V)$ at $f(p)$ such that for each $a \in x(U \cap f^{-1}(V))$,
    %
    \[ (y \circ f \circ x^{-1})(a_1,\dots,a_n) = (a_1,\dots,a_k,*,\dots,*), \]
    %
    where the asterixes denote arbitrary smooth functions of $a$.
\end{theorem}
\begin{proof}
    Let $(x,U)$ and $(y,V)$ be arbitrary coordinate systems around $p$ and $f(p)$. By a permutation the coordinates of $x$ and $y$, we may guarantee that the matrix
    %
    \[ \left( \left.\frac{\partial y^i \circ f}{\partial x_j}\right|_p \right)_{i,j = 1}^k \]
    %
    is invertible. Define a map $z:U \cap f^{-1}(V) \to \mathbf{R}^n$ around $p$ by setting $z^i = y^i \circ f$, for $1 \leq i \leq k$, and $z^i = x^i$ otherwise. The matrix
    %
    \[ D(z \circ x^{-1})(p) = \begin{pmatrix} \left( \left.\frac{\partial y^i \circ f}{\partial x^j}\right|_p \right)_{i,j = 1}^k & X \\ 0 & I \end{pmatrix} \]
    %
    is invertible, hence, by the inverse function theorem, $z \circ x^{-1}$ is a diffeomorphism in a neighbourhood of $x(p)$. It follows that for some neighbourhood $W$ of $p$, $(z,W)$ is a smooth coordinate system at $p$. For $1 \leq i \leq k$, $(y_i \circ f \circ z^{-1})(a_1,\dots, a_n) = a_i$, which completes the proof.
\end{proof}

\begin{corollary}
    If $f: M \to N$ is smooth, and has rank $k$ in a neighbourhood of a point $p$, then we may choose coordinate systems $(x,U)$ and $(y,V)$ around $p$ and $f(p)$ such that if $a \in x(U \cap f^{-1}(V))$,
    %
    \[ (y \circ f \circ x^{-1})(a_1, \dots, a_n) = (a_1, \dots, a_k, 0, \dots, 0). \]
\end{corollary}
\begin{proof}
    Choose $x$ and $y$ as in the theorem above. Then
    %
    \[ D(y \circ f \circ x^{-1})(p) = \begin{pmatrix} I & 0 \\ * & \left.\left( \frac{\partial y_i \circ f}{\partial x_j} \right)\right|_p \end{pmatrix} \]
    %
    Since $f$ is rank $k$, the matrix in the bottom right corner must vanish in a neighbourhood of $p$. Therefore, for $a$ in a small neighbourhood of $x(p)$, the value $(y \circ f \circ x^{-1})(a)$ depends only on $a_1, \dots, a_k$. Write $(y \circ f \circ x^{-1})(a) = (a_1,\dots,a_k,\psi_{k+1}(a_1,\dots,a_k), \dots, \psi_n(a_1,\dots,a_k))$. Then we can define a chart $(z,V)$ around $f(p)$ by setting $z^i = y^i$, for $i \leq k$, and for $i > k$,
    %
    \[ z^i(q) = y^i(q) - \psi_{k+1}(y^1(q), \dots, y^k(q)). \]
    %
    Then for $q$ in this neighbourhood, we find
    %
    \[ D(z \circ y^{-1})(q) = \begin{pmatrix} I & 0 \\ * & I \end{pmatrix}, \]
    %
    So $z$ is a coordinate system, and
    %
    \[ z \circ f \circ x^{-1}(a_1, \dots, a_n) = (a_1, \dots, a_k, 0, \dots, 0). \qedhere \]
\end{proof}

In particular, this theorem enables us to understand full rank maps.

\begin{corollary}
    If $f: M^n \to N^m$ is rank $m$ at $p$, then for any coordinate system $y$ around $f(p)$, there exists a coordinate system $x$ at $p$ such that
    %
    \[ y \circ f \circ x^{-1} (a_1, \dots, a_n) = (a_1, \dots, a_m) \]
\end{corollary}
\begin{proof}
    Applying the last theorem, we note in the beginning of the proof of the theorem that we need not rearrange the coordinates of $y$ in the case that the matrix is rank $m$, just the coordinates of $x$.
\end{proof}

\begin{corollary}
    If $f: M^n \to N^m$ is rank $n$ at $p$, then for any coordinate system $x$ at $p$, there exists a coordinate system $y$ at $f(p)$ such that
    %
    \[ y \circ f \circ x^{-1} (a_1, \dots, a_n) = (a_1, \dots, a_n, 0, \dots, 0) \]
\end{corollary}
\begin{proof}
    If $f$ is rank $n$ at a point, then it is rank $n$ on a neighbourhood, since the set of full rank matrices is open. Choose coordinate systems $u$ and $v$ such that $u \circ f \circ v^{-1}(a^1, \dots, a^n) = (a^1, \dots, a^n, 0, \dots, 0)$. Define a map $\lambda$ on $\mathbf{R}^m$ by $\lambda(a^1, \dots, a^m) = (x \circ v^{-1}(a^1, \dots, a^n), a^{n+1}, \dots, a^m)$. Then $\lambda$ is a diffeomorphism, hence $\lambda \circ y$ is a coordinate system, and
    %
    \begin{align*}
        (\lambda \circ y) \circ f \circ x^{-1} (a^1, \dots, a^n) &= \lambda \circ (y \circ f \circ v^{-1}) \circ (v \circ x^{-1}) (a^1, \dots, a^n)\\
        &= \lambda (v \circ x^{-1} (a^1 \dots a^n), 0 \dots 0)\\
        &= (a^1 \dots a^n, 0 \dots 0)
    \end{align*}
    %
    and we have found the chart required.
\end{proof}

\section{Immersions, Submersions, and Covers}

The charts above are all locally constructed, but we can use global topological properties to infer properties of an entire map from the rank of it's covariant derivative at points. Call a smooth map $f:M \to N$ an \emph{immersion} if $f_*|_p$ is injective for all points $p$, and a \emph{submersion} if $f_*|_p$ is always surjective. In terms of rank, $f$ is an immersion if the rank of $f$ at $p \in M$ is the dimension of $T_pM$, and a submersion if the rank of $f$ is the dimension of $T_{f(p)}N$.

\begin{theorem}
    Let $f: M \to N$ be a map between smooth manifolds. Then
    %
    \begin{enumerate}
        \item[(a)] If $f$ is injective and has locally constant rank, then $f$ is an immersion.
        \item[(b)] If $f$ is surjective, has constant rank, $M$ is second-countable, and $N$ has the same dimension throughout, then $f$ is a submersion.
    \end{enumerate}
\end{theorem}
\begin{proof}
    If $p$ is a point, if the dimension of $N$ is $m$ at $f(p)$, and if $f$ has rank $k < m$ in a neighbourhood of $p$, then there are coordinate systems $x$ and $y$ such that
    %
    \[ (y \circ f \circ x^{-1})(a^1, \dots, a^n) = (a^1, \dots, a^k, 0, \dots, 0) \]
    %
    which clearly isn't injective if $k < n$. This proves (a). To prove (b), consider a cover of $f$ by charts $(x_\alpha, U_\alpha)$, where $f(U_\alpha) \subset V_\alpha$ for some chart $(y,V_\alpha)$ on $N$. Since $M$ is second-countable, it is also Lindel\"{o}f, and we may therefore assume the cover is countable. Let $N$ have dimension $m$, and let $f$ have rank $k < n$. The coordinate systems can be chosen so that
    %
    \[ (y \circ f \circ x^{-1})(a^1, \dots, a^n) = (a^1, \dots, a^k, 0, \dots, 0) \]
    %
    This shows us that $(y \circ f \circ x_\alpha^{-1})(x(U_\alpha))$ is nowhere dense in $y(V_\alpha)$, and thus, $f(U_\alpha)$ is nowhere dense in $N$. It then follows by the Baire category theorem for locally compact topological spaces that $\bigcup f(U_\alpha) = f(M)$ is nowhere dense on $N$, which implies $f$ is not surjective, a contradiction.
\end{proof}

Variants of this proof can be adapted to various other cases, where $f$ may not have constant rank, and $N$ has non-constant dimension. We avoid discussing them in detail, because they are complicated to state, but obvious to adapt to any particular situation by working out the details there.

\begin{theorem}
    If $f:M^n \to N^m$ is a submersion, then $f$ is an open map.
\end{theorem}
\begin{proof}
    If $p \in M$, pick a neighbourhood $x$ around $p$ and $y$ around $f(p)$ such that
    %
    \[ x \circ f \circ y^{-1}(a^1, \dots, a^n) = (a^1, \dots, a^m) \]
    %
    This map is open, showing $f$ is locally open, and thus open on its entire domain, since openness is a local property.
\end{proof}

\begin{corollary}
    A submersion from a compact manifold to a connected manifold with the same dimension is surjective.
\end{corollary}
\begin{proof}
    If $M$ is compact, and $f: M \to N$ an immersion, then $f(M)$ is compact, hence closed, and since $f$ is open, $f(M)$ is also an open subset of $N$. But then $f(M) = N$, since it is an open, closed, non-empty set.
\end{proof}

The idea of a submersive map $f: M \to N$ is very closely related to its family of smooth sections $s: N \to M$, maps such that $f \circ s = \text{id}_N$. More precisely, a submersive map is closely related to its local smooth sections over neighbourhoods of $N$.

\begin{theorem}
    A smooth map $f: M \to N$ is a submersion if and only if every $p \in M$ is in the image of some local section.
\end{theorem}
\begin{proof}
    If $f$ is a submersion, then there are local coordinates $x$ and $y$ around every point where $(y \circ f \circ x^{-1})(a^1, \dots, a^n) = (a^1, \dots, a^m)$. Given any point $p_0 \in M$, consider a point $q_0 \in N$ with $y^1(q_0) = x^1(p_0), \dots, y^m(q_0) = x^m(p_0)$, and define the map $s(q) = x^{-1}(y(q), x^{m+1}(p), \dots, x^n(p))$, then $s$ is the required section. Conversely, if $s: U \to M$ is a local section, then $\text{id} = f_* \circ s_*$, and therefore the rank of $f_*$ at $p$ must be at least the rank of $\text{id}$, so it has full rank.
\end{proof}

\section{Submanifolds}

All this discussion of ranks and immersions is most useful to studying submanifolds. consider two smooth manifolds $M$ and $N$, and suppose $i: N \to M$ is an injective immersion. With this structure, we say $N$ is an \emph{immersed submanifold} of $M$. If $i$ is also a \emph{topological embedding}, then we say $N$ is a \emph{submanifold} of $M$. For a smooth submanifold, the smooth structure on $N$ making $i$ an immersion is then unique. This is because if $(x,U)$ is any chart on $N$, then there is a chart $(y,V)$ on $M$ such that $(y \circ i \circ x^{-1})(a) = (a,0)$. Conversely, if $(y,U)$ is a chart on $N$ such that $U \cap i(N) = \{ p \in U : y^{k+1}(p) = \dots = y^n(p) = 0 \}$, and we set $y = (\tilde{y},0)$, then $\tilde{y} \circ i$ is a diffeomorphism, and therefore a chart on $N$, which uniquely specifies the atlas.

\begin{remark}
    If $N \subset M$ is a $k$ dimensional manifold under the relative topology, then $N$ has a smooth structure making the inclusion map $i: N \to M$ an immersion if and only if for each $p \in N$, there exists a neighbourhood $U$ of $p$ in $M$, and a chart $(x,U)$ on $M$, such that
    %
    \[ U \cap N = \{ p \in U : x^{k+1}(p) = \dots = x^n(p) = 0 \}. \]
    %
    The first $k$ coordinates can then be taken as a chart as in the last paragraph. Thus $N$ has a natural $C^\infty$ structure. If we can only choose a $C^k$ chart $(x,U)$ above instead of a $C^\infty$ chart, then $N$ only has a natural $C^k$ structure. Thus, for instance, it wouldn't be natural to consider
    %
    \[ M = \{ (x,y) \in \RR^2: y = x^2 \sin(1/x) \} \]
    %
    as a $C^2$ manifold, but only a $C^1$ manifold, even though one can extend the natural $C^1$ structure on $M$ to a $C^\infty$ structure. Most of the manifolds we have specified are $C^\infty$ submanifolds of $\RR^n$ for some $n$, and one can verify that all the differentiable structure we've given to these manifolds is the only canonically induced by the submanifold structure.
\end{remark}

\begin{example}
    The biggest utility is when these manifolds lie in Euclidean space. Classically, a $k$-dimensional $C^m$ manifold was a subset $M$ of some Euclidean space $\mathbf{R}^n$, such that at ever $x \in M$, there is an open subset $U$ of {\it Euclidean space} containing $x$, an open subset $V$ of $\mathbf{R}^n$, and a $C^m$ diffeomorphism $f: U \to V$ such that $f(U \cap M) = V \cap \mathbf{R}^k \times \{ 0 \}$. All these diffeomorphisms, restricted to $M$, are $C^m$ related to one another, because they are the restricted of diffeomorphisms on the whole of $\mathbf{R}^n$, and give a $C^m$ differentiable structure onto $M$. Conversely, any submanifold of Euclidean space has this property because of the rank theorems, because if $i: M \to \mathbf{R}^n$ is an immersion, then we can extend any coordinate system on $M$ to a coordinate system on an open subset of $\mathbf{R}^n$, such that $M$ is flat in this coordinate system. The natural choice of a differentiable structure on any manifold lying in Euclidean space is this differentiable structure, and, as you can check, this is the differentiable structure we have chosen for all the manifolds we have used in past examples, which naturally can be embedded in Euclidean space.
\end{example}

\begin{example}
    Our knowledge of immersion can now be used to show every submanifold of $\RR^N$ can be identified locally as a `graph' of a function. Let $M$ be a $k$ dimensional submanifold of $\RR^N$, and fix $p \in M$. Then $T_p M$ can be identified with a $k$-dimensional subspace of $T_p \RR^N$. If $T_p M$ is spanned by $(x_1)_p, \dots, (x_k)_p$ for $x_1, \dots, x_k \in \RR^N$, then we can find $k$ distinct indices $i_1, \dots, i_k$ such that for each $j \in \{ 1, \dots, k \}$, $(x_j)_{i_j} \neq 0$. We consider the linear map $y: \RR^N \to \RR^k$ given by $y(x) = (x_{i_1}, \dots, x_{i_k})$. Since $y$ is linear, $y_*(x_p) = y(x)_{y(p)}$, and our choice of basis implies that $y_*|_p$ is injective when restriction to $T_p M$. Thus $y|_M$ is an immersion in a neighbourhood $U$ of $p$ in $M$. If $U$ is further restricted, the rank theorem also implies that $y$ restricts to a coordinate chart on $U$. If we let $V = y(U)$. Then there exists a function $g: V \to \RR^N$ such that for each $p \in U$, $g(y(p)) = p$. For simplicity, if we permute coordinates such that $i_1 = 1, \dots, i_k = k$, then this means that
    %
    \[ U = \{ (y^1, \dots, y^k, g^{k+1}(y), \dots, g^N(y): y \in V) \}. \]
    %
    Thus $M$ is locally the graph of the function $g$.
\end{example}

An important method of finding submanifolds of $\mathbf{R}^n$ is by specifying the submanifolds as a level set of Euclidean space. For instance, the sphere $S^n$ can be defined as the level set $f^{-1}(1)$ of the map $f(x) = \| x \|$. Provided that $\nabla f$ does not vanish on the domain level set, the level set is always a manifold, and this fact can be generalized to maps between arbitrary manifolds.

\begin{theorem}
    If $f: M^n \to N^m$ has constant rank $k$ in a neighbourhood of the points mapping to $q \in N$, then $f^{-1}(q)$ is a closed $n - k$ submanifold of $M$.
\end{theorem}
\begin{proof}
    If $f(p) = q$, and $f$ has rank $k$ at $p$, then we may write
    %
    \[ y \circ f \circ x^{-1}(a_1, \dots, a_n) = (a_1, \dots, a_k, 0 ,\dots, 0) \]
    %
    for some charts $(x,U)$ and $(y,V)$, with $y(q) = 0$. This means that
    %
    \[ U \cap f^{-1}(q) = \{ r \in U : x^1(r) = \dots = x^k(r) = 0 \}, \]
    %
    so $f^{-1}(q)$ is specified by $n-k$ dimensional slices in $M$, and can therefore be given a $n-k$ submanifold structure.
\end{proof}

\begin{example}
    If $M$ is a 1-submanifold of $\mathbf{R}^2$, specified as the level set of a function $f: \mathbf{R}^2 \to \mathbf{R}$, $M = \{ (x,y) \in \mathbf{R}^2: f(x,y) = 0 \}$, then the surface of revolution about the $z$-axis related to $M$ can be identified as the space
    %
    \[ N = \left\{ (x,y,z) \in \mathbf{R}^3: f \left( \sqrt{x^2 + y^2}, z \right) = 0 \right\} \]
    %
    It is a surface, because the differential of the defining level set mapping has rank 1 at any point in a neighbourhood of $N$ (because we can use the chain rule, and the fact that the map $(x,y,z) \mapsto (\sqrt{x^2 + y^2}, z)$ has full rank at every point).
\end{example}

\begin{example}
    The special linear group $SL(n) = SL_n(\mathbf{R})$ is the set of invertible matrices with determinant one. Since the determinant is a multilinear function, we can find the determinant via the formula
    %
    \[ D(\det)(v_1, \dots, v_n)(w_1, \dots, w_n) = \sum_{k = 1}^n \det(v_1, \dots, w_k, \dots, v_n) \]
    %
    where $M(n,n)$ is identified with $(\mathbf{R}^n)^n$. Then for any $(v_1, \dots, v_n) \in GL_n(\mathbf{R})$,
    %
    \[ D(\det)(v_1, \dots, v_n)(v_1, \dots, v_n) = n \det(v_1, \dots, v_n) \neq 0. \]
    %
    Thus $\det$ has rank 1 at every point (full rank), and $SL(n)$ is a (closed) submanifold of $GL(n)$ dimension $n^2 - 1$.
\end{example}

\begin{example}
    The orthogonal group $O(n)$ is the set of matrices $M$ such that $MM^t = I$. Then $O(n)$ is closed, for the map $\psi: M \mapsto MM^t$ is continuous, and $O(n) = \psi^{-1}(I)$. $\psi$ maps $M(n,n)$ into the set of symmetric matrices, which is a subspace of $M(n,n)$ of dimension $n(n+1)/2$. If we take the $i$'th diagonal entry of $MM^t = I$, we obtain that
    %
    \[ v_{i1}^2 + v_{i2}^2 + \dots + v_{in}^2 = 1 \]
    %
    This implies that $M$ lies on $(S^{n-1})^n \subset \mathbf{R}^{n^2}$, so $O(n)$ is closed and bounded, and therefore compact. Consider the diffeomorphism $R_A: B \mapsto BA$ of $GL(n)$, for a fixed $A \in GL(n)$. We also have $\psi \circ R_A = \psi$, for $A \in O(n)$. We conclude that
    %
    \[ D(\psi)(A) \circ D(R_A)(I) = D(\psi \circ R_A)(I) = D(\psi)(I) \]
    %
    Since $R_A$ is a diffeomorphism, $D(\psi)(A)$ has the same rank as $D(\psi)(I)$. Let us find this rank. Explicitly, we may write the projections of $\psi$ as
    %
    \[ \psi^{ij}(M) = \sum_{k = 1}^n M_{ik}M_{jk} \]
    %
    Then
    %
    \[ \left.\frac{\partial \psi^{ij}}{\partial M^{lk}}\right|_M = \begin{cases} 2 M_{ik} & i = j = l \\ M_{ik} & j = l \\ M_{jk} & i = l \\ 0 & \text{elsewise} \end{cases} \]
    %
    In particular, at the identity,
    %
    \[ \left.\frac{\partial \psi^{ij}}{\partial M^{lk}}\right|_I = \begin{cases} 2 & i = j = k = l \\ 1 & j = k = l \\ 1 & i = k = l \\ 0 & \text{elsewise} \end{cases} \]
    %
    It follows that $\psi_*(TGL(n)_I)$ is the space of all symmetric matrices in $TGL(n)_I$, which has dimension $n(n+1)/2$. Thus $\psi$ has constant rank $n(n+1)/2$, and we find the space of orthogonal matrices has dimension
    %
    \[ n^2 - n(n+1)/2 = n(n-1)/2 \]
    %
    as a differentiable manifold.
\end{example}

\begin{example}
    Every orthogonal matrix has determinant $\pm 1$. The special orthogonal group $SO(n)$ is the set of orthogonal matrices of determinant one, and is an open submanifold of $O(n)$. It's actually a connected component. To see this, consider the obvious (Lie-group) action of $SO(n)$ on $S^{n-1}$. For $n \geq 2$, it is easy to see that the action is transitive: if $v_1 \in S^{n-1}$ is given, we can extend $v_1$ to an orthonormal basis $(v_1, \dots, v_n)$; possibly permuting this basis, the basis can be assumed oriented, and then the orthogonal matrix mapping $e_i$ to $v_i$ is in $SO(n)$.
    %
    %In fact, for $n = 2$, $SO_2$ is diffeomorphic to the torus $S^1 = \mathbf{T}$, because the action of an orthogonal $2 \times 2$ matrix can be determined by its action at a single point on $\mathbf{T}$, in particular, where it is moved. The particular map is given in angle coordinates by
    %
    %\[ \theta \mapsto \begin{pmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{pmatrix} \]
    %
    %which is easily seen to be differentiable, because if $M \mapsto \left( \begin{smallmatrix} a & b \\ c & d \end{smallmatrix} \right)$ is orthogonal, then the relations $ac = -bd$ and $ad = bc$, imply that $a = d$ and $b = -c$, and $a^2 + c^2 = 1$ The map $M \mapsto a + ic$ is then the inverse map into $\mathbf{T}$.
    %
    For a fixed $p \in S^{n-1}$, the action $f(M) = Mp$ is surjective. If $N$ is fixed, $g(M) = NM$, and $h(p) = Np$, then we have a commutative diagram
    %
%    \begin{center}
%    \begin{tikzcd}
%        SO(n) \arrow[r, "f"] \arrow[d, "g"] & S^{n-1} \arrow[d, "h"]\\
%        SO(n) \arrow[r, "f"] & S^{n-1}
%    \end{tikzcd}
%    \end{center}
    %
    The vertical maps are diffeomorphisms, hence, taking differentials of all maps in the diagram, we conclude that $h_* \circ f_* = f_* \circ g_*$, and since $h_*$ and $g_*$ are rank preserving, we conclude that the rank of $f_*$ at any $M$ is the same as it's rank at $NM$ (this argument works for any `differentiable group', and any `differentiable action', a notion we will define precisely later when we introduce Lie groups). Because $SO(n)$ is second countable, we conclude that $f$ is a submersion, hence it is open. In the one dimensional case, the map isn't surjective, but in this case $SO(1)$ and $S^0$ are discrete spaces, so the map $M \mapsto Mp$ is trivially open. In any dimension $\geq 2$, the stabilizer of the unit vector $e_n$ is the subgroup of matrices of the form
    %
    \[ \begin{pmatrix} X & 0 \\ 0 & 1 \end{pmatrix} \]
    %
    where $X \in SO(n-1)$, so the stabilizer is actually diffeomorphic to $SO(n-1)$. Using the orbit stabilizer theorem, we find that for $n \geq 2$, $S^{n-1}$ is homeomorphic to $SO(n)/SO(n-1)$. Using induction, we may assume that $SO(n-1)$ is connected. But for $n \geq 2$, $S^{n-1}$ is connected. The theorem then follows from the general fact that if $H$ is a closed subgroup of $G$, and $H$ and $G/H$ are connected, then $G$ is connected.
\end{example}

\begin{remark}
    In the lower dimensional matrix groups there are standard coordinate systems which are often employed. On $SO(2) = \mathbf{T}$, the standard coordinate system is obtained by taking angles. In $SO(3)$ it is easy to prove using the spectral theorem that any rotation $R$ is given by an oriented rotation about the plane perpendicular to a vector $x$ with $Rx = x$. Thus a rotation is given by picking a line $l \in \mathbf{RP}^2$ and an angle $\theta \in [0,2\pi]$. More concretely, we can use Euler angles. It turns out that in three dimensions, every rotation can be given first by a rotation about the $z$ axis, known as {\it yaw}, {\it pitch}, which is a further rotation about the $y$ axis, and {\it roll}, which is rotation about the $x$ axis. That is, we can describe almost every rotation by three angles $\phi$, $\theta$, and $\psi$, which describes the rotation
    %
    \[ \begin{pmatrix} \cos \psi & \sin \psi & 0 \\ - \sin \psi & \cos \psi & 0 \\ 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 1 & 0 & 0 \\ 0 & \cos \theta & \sin \theta \\ 0 & - \sin \theta & \cos \theta \end{pmatrix} \begin{pmatrix} \cos \phi & \sin \phi & 0 \\ -\sin \phi & \cos \phi & 0 \\ 0 & 0 & 1 \end{pmatrix} \]
    %
    The singularities of this coordinate system result in computational glitches known as gimbal locks when implemented to represent rotations in coordinate systems.
\end{remark}

Similar results about level sets can be obtained for manifolds with boundary.

\begin{theorem}
    If $f \in C^\infty(M)$, where $M$ is a manifold with boundary, and $f_*|_p \neq 0$ for any $p$ with $f(p) = a$ or $f(p) = b$, where $a,b \in \mathbf{R}$, then $f^{-1}([a,b])$ is a submanifold of $M$ with boundary with the same dimension as the original space.
\end{theorem}
\begin{proof}
    Clearly $f^{-1}((a,b))$ is a open subset of $M$, so it is a smooth manifold with boundary at any point $p$ with $f(p) \in (a,b)$. The boundary of this set is contained within $f^{-1}[a,b]$, and it actually equal to this, since $f_*|_p \neq 0$ for any $p$ with $f(p) = a$ or $f(p) = b$. At $p$ with $f(a) = p$, we can find a chart $(x,U)$ in $M$ with $p \in U$ such that $f(p) = x^1(p)$ for $a \in U$. Restricting $x$ to $V = \{ q \in U : x^1(p) \in [a,a+\varepsilon) \}$ for some $\varepsilon > 0$ then gives a boundary chart at $p$.
\end{proof}

Immersed submanifolds behave almost as well as submanifolds, apart from the odd inconsistency of dropping differentiable maps to subdomains. Let $i: N_0 \to N$ be an injective immersion between two manifolds, and let $g: M \to N$ be a differentiable function with $g(M) \subset N_0$. Then we can consider the induced function $\tilde{g}: M \to N_0$ such that $g = i \circ \tilde{g}$. It is natural to ask whether the map $g$ is differentiable.

\begin{example}
    Immerse $(-\pi, \pi)$ in $\mathbf{R}^2$ via the lemniscate map $f(t) = (\sin 2t, \sin t)$. The map $g:(-\pi, \pi) \to (-\pi, \pi)$ defined by
    %
    \[ g(x) = \begin{cases} \pi + x &: x < 0 \\ 0 &: x = 0 \\ x - \pi &: x > 0 \end{cases} \]
    %
    is not even continuous, yet $f \circ g$ is differentiable.
\end{example}

Continuity is all that can go wrong in this situation.

\begin{theorem}
    Let $i: N_0 \to N$ be an injective immersion of $M$ in $N$, and suppose $f: M \to N$ is differentiable, with $f(M) \subset N_0$. If the induced map $\tilde{f}: M \to N_0$ is continuous, then $\tilde{f}$ is differentiable.
\end{theorem}
\begin{proof}
    Fix $p \in M$, and let $f(p) = q$. Choose a coordinate chart $(\tilde{y},U_0)$ on $N_0$ with $q \in U_0$, and then find a coordinate chart $(y,U)$ on $N$, such that $U_0 = U \cap i(N_0)$, and $(y \circ i \circ \tilde{y}^{-1})(a) = (a,0)$, so $\pi \circ y \circ i = \tilde{y}$, where $\pi$ is projection onto the first set of coordinates. Since $\tilde{f}$ is continuous, $\tilde{f}^{-1}(U_0)$ is an open neighbourhood of $p$, and so we can choose a coordinate system $(x,V)$ around $p$, with $V \subset \tilde{f}^{-1}(U_0)$. We know $y \circ f \circ x^{-1}$ is smooth, but this means that
    %
    \begin{align*}
        \tilde{y} \circ \tilde{f} \circ x^{-1} &= ( \pi \circ y \circ i) \circ \tilde{f} \circ x^{-1}\\
        &= \pi \circ y \circ f \circ x^{-1},
    \end{align*}
    %
    which is smooth, so $\tilde{f}$ is smooth.
\end{proof}

In particular, this is never a problem for submanifolds.

\begin{theorem}
    The following are sufficient to conclude that an injective immersion $f: M \to N$ is an embedding.
    %
    \begin{enumerate}
        \item[(a)] $f$ is open or closed.
        \item[(b)] $f$ is proper (the inverse image of compact sets are compact).
        \item[(c)] $M$ is compact.
        \item[(d)] The dimension of $M$ is equal to the dimension of $N$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    An injective open or closed continuous map is a topological embedding, so (a) is fairly trivial. If $M$ is compact, then $f$ is closed, and if the dimension of $M$ is equal to the dimension of $N$, $f$ is open (by looking at the coordinate charts), so (c) and (d) follow from (a). Similarily, if $f$ is proper and it's codomain is locally compact, so $f$ is closed, and hence an embedding. To see why, we fix a closed set $C \subset M$. Suppose $y \not \in f(C)$. Pick a precompact neighbourhood $V$ of $y$. Then $C \cap f^{-1}(\overline{V})$ is compact, so $f(C) \cap \overline{V}$ is compact. In particular, since $y \not \in f(C) \cap \overline{V}$, we can find an open neighbourhood $W$ of $y$ not containing any points in $f(C) \cap V$. But this means $V \cap W$ is an open neighbourhood of $y$ not containing any points in $f(C)$, so $f(C)$ is closed.
\end{proof}

One can think of a proper map $f: M \to N$ as being one which maps `points near infinity' on $M$ to `points near infinity' on $N$, this being certainly true for the model case of proper maps $f: \RR \to \RR$.

\section{Manifolds and Measure}

The Lebesgue measure on $\mathbf{R}^n$ gives us a nice way to calculate the volume of various sorts of objects. A subset $E$ has measure zero if, for any $\varepsilon$, there is a cover of $E$ by open sets $U_1, U_2, \dots$ with $\sum |U_k| < \varepsilon$. Suppose that $\mu$ is a measure on a differentiable manifold $M$ behaving locally like the Lebesgue measure, in the sense that for any chart $(x,U)$, the pushforward measure $x_*(\mu)$ is absolutely continuous with respect to the Lebesgue measure. If $\mu(E) = 0$, then $x_*(\mu)(x(E)) = 0$ for every chart $x$, and conversely, assuming $M$ is second countable, if this is true for every chart, then $\mu(E) = 0$. Thus it makes sense, without any measure on the manifold, to say a set $E$ has \emph{measure zero} if it can be covered by countably many charts $(x_i,U_i)$, where $x_i(E \cap U_i)$ has measure zero for each $i$.

\begin{lemma}
    If $f \in C^1(M,N)$, where $M$ and $N$ are smooth manifolds with the same dimension, and if $E \subset M$ has measure zero, then $f(E)$ has measure zero.
\end{lemma}
\begin{proof}
    Begin with the case $M = N = \mathbf{R}^n$. Let $E$ be a compact rectangle in $\mathbf{R}^n$, and suppose that $\| \partial_i f^j \|_{L^\infty(E)} \leq K$ for all $i$ and $j$. Then for $x,y \in E$, by tracing the path from $x$ to $y$ along the coordinate lines,
    %
    \[ |f(x) - f(y)| \leq \sum_{i = 1}^n |f^i(x) - f^i(y)| \leq K \sum_{i = 1}^n \sum_{j = i}^n |x^j - y^j| \leq Kn^2 |x-y| \]
    %
    If $E$ has measure zero, then, by the $\sigma$ compactness of $\mathbf{R}^n$, we may assume $E$ is compact, so that $E$ is contained in some rectangle, and suppose on this rectangle that the partial derivative bound holds. If $E$ is covered by rectangles $U_1, U_2 \dots$ with $\sum |U_i| \leq \varepsilon$, then each $U_i$ is mapped into a ball of radius $Kn^2 \text{diam}(U_i)$, whose volume agrees with $|U_i|$ up to a constant $C$ depending on $K$ and $n$, and so $f(E)$ is covered by balls with total measure $\leq C\varepsilon$, and we may let $\varepsilon \to 0$. Now in general on any manifold, let $E$ be covered by charts $(x_1,U_1), (x_2,U_2), \dots$ with $x_k(E \cap U_k)$ a set of measure zero, in such a way that there are charts $(y_1,V_1), (y_2,V_2), \dots$ with $U_k \subset f^{-1}(V_k)$ (we may take an initial covering of $E$ by charts, and then duplicate the charts, suitably small to be contained in the $V_k$). Then each $(y_k \circ f \circ x^{-1})(x(E \cap U_k)) = y_k(f(E \cap U_k))$ has measure zero, and so $f(E) = \bigcup f(E \cap U_k)$ has measure zero.
\end{proof}

If the rank of a map $f: \mathbf{R}^n \to \mathbf{R}^m$ has rank $< m$, then intuitively, it maps small neighbourhoods of points to sets close to hyperplanes, which have measure zero. We will show this idea shows the image of the set of low rank points always has measure zero on a manifold. Call a point $p$ on $M$ \emph{criticial} for a map $f:M \to N$ if $f$ is not a submersion at $p$, and \emph{regular} if the map is a submersion at $p$. A point $q \in N$ is then called a \emph{regular value} if all points in $f^{-1}(q)$ are regular. Otherwise, $q$ is known as a \emph{critical value}.

\begin{theorem}[Sard]
    If $f: M^n \to N^n$ is $C^1$, and $M$ is second countable, then the set of critical values is a set of measure zero in $N$.
\end{theorem}
\begin{proof}
    We begin with the case $N = \mathbf{R}^n$, and $M$ an open subset of $\mathbf{R}^n$. Let $C$ be a closed cube in $M$ with side lengths $l$, and let $B$ denote the critical values in $C$. Then $f$ is uniformly continuously differentiable on $C$, and so for any $\varepsilon$, for large enough $N$, we can divide $C$ into $N^n$ cubes $S_k$ of side length $l/n$, such that for $x,y \in S_k$,
    %
    \[ |f(y) - f(x) - Df(x)(y - x)| < \varepsilon |y - x| \leq \varepsilon \frac{l \sqrt{n}}{N} \]
    %
    and we also know that $f$ is Lipschitz on $C$, so there is $M$ such that
    %
    \[ |f(y) - f(x)| < M|y - x| < \frac{lM \sqrt{n}}{N} \]
    %
    If $B$ intersects $S_k$, then there exists $x \in S_k$ such that $Df(x)(y-x)$ lies in an $n-1$ dimensional subspace of $\mathbf{R}^n$, and so the two inequalities about imply that $f$ maps $S_k$ into a `$n$ dimensional cylinder', formed by taking a ball of radius $lM \sqrt{n}/N$ in $n-1$ dimensional space as the base of the cylinder, and a height of length $2 \varepsilon l\sqrt{n}/N$. This shape has volume equal, up to a constant depending on $n$, by
    %
    \[ \left( \frac{2 \varepsilon l \sqrt{n}}{N} \right) \left( \frac{lM \sqrt{n}}{N} \right)^{n-1} = 2 \varepsilon \frac{l^n n^{n/2} M^{n-1}}{N^n} \]
    %
    But we have $N^n$ cubes, which implies the set $f(B \cap C)$ can have volume at most
    %
    \[ 2 \varepsilon l^n n^{n/2} M^{n-1} \]
    %
    and we may then let $\varepsilon \to 0$ to conclude $f(B \cap C)$ is a set of measure zero. But since $\mathbf{R}^n$ is the union of countably many cubes $C$, we conclude that $B$ itself has measure zero. The general case for second countable manifolds follows from taking a countable set of coordinate charts covering $M$, and then applying the Euclidean Sard's theorem.
\end{proof}

Sard's theorem has a more general form which is very useful in differential topology.

\begin{theorem}[Sard]
    If $f: M^n \to N^m$ is $C^k$, and $k \geq \max(1,n-m+1)$, then the set of critical values is a set of measure zero in $N$.
\end{theorem}

Our case occurs when $n = m$. It is easy to prove the case where $m > n$. The tricky case is where $m < n$.

\begin{theorem}
    If $f:M^n \to N^m$ is $C^1$, $M^n$ is connected, and $n < m$, then $f(M)$ has measure zero in $N^m$.
\end{theorem}
\begin{proof}
    Consider the map $g: M \times \mathbf{R}^{m-n} \to N$, defined by $g(p,x) = f(p)$. Then all values of $g$ are critical, and so $g(M \times \mathbf{R}^{m-n}) = f(M)$ has measure zero by the previous case of Sard's theorem we proved.
\end{proof}

Let us now prove the difficult case where $m < n$. We shall only prove the theorem for smooth maps. The proof for $C^k$ maps with $k < \infty$ is \emph{essentially} the same proof, but involving some more advanced technology, i.e. the Whitney extension theorem, which enables us to boost the smoothness of functions in certains ituations.

\begin{theorem}
    If $n > m$, and $f: M^n \to N^m$ is a $C^{n-m+1}$ map, then the set of critical values has measure zero in $N$.
\end{theorem}

In this situation, we cannot simply rely on the fact that neighbourhoods of critical points are mapped to neighbourhoods of lower dimensional sets. Thus we must rely on the higher order smoothness in some way. It clearly suffices to prove the theorem when $M = U$ is an open subset of $\RR^n$, and $N = \RR^m$. We let $C \subset U$ be the set of all critical points. We must show $f(C)$ has measure zero. For each $s \in \{ 1, \dots, n \}$, we let $C_s$ be the set of all critical points $x$ such that $f_\alpha(x) = 0$ for all $|\alpha| \leq s$.

\begin{lemma}
    If $s > n/m - 1$, then $f(C_s)$ has measure zero.
\end{lemma}
\begin{proof}
    Let $I \subset U$ be a closed cube with sides parallel to the axis. It suffices to show $f(C_s \cap I)$ has measure zero. By Taylor's theorem and compactness, we can write $f(y) = f(x) + R(x,y)$, where for $x \in B_s \cap I$ and $y \in I$, $|R(x,y)| \lesssim |x - y|^{s+1}$. Choose a large integer $N$, and divide $I$ into $N^n$ cubes with sidelength $\sim 1/N$. Then for any such subcube $J$ that intersects $C_s$, $f(J)$ is contained in a ball of radius $O(1/N^{s+1})$. Thus $f(J)$ has measure at most $O(1/N^{m(s+1)})$. But this means that $f(B_s)$ has measure at most $O(N^{n-m(s+1)})$. If $s > n/m-1$, this is $o(1)$ as $N \to \infty$.
\end{proof}

The inequality $n/m - 1 < n - m + 1$ is certainly true for $m = 1$ and $m = 2$. For $m \geq 3$, the inequality is equivalent to the inequality
%
\[ n < m \left( \frac{m - 2}{m - 1} \right) = m \left( 1 - \frac{1}{m - 1} \right) \]
%
But this clearly implies $n < m$. Thus $f(C_{n-m+1})$ has measure zero.

We now prove Sard's theorem by induction on $n$, the case $n = 1$ holding by application of previous cases. We write
%
\[ C = (C - C_1) \cup (C_1 - C_2) \cup \dots \cup (C_{n-m} - C_{n-m+1}) \cup C_{n-m+1}. \]
%
We know $f(C_{n-m+1})$ has measure zero, so it suffices to prove $f(C_s - C_{s+1})$ has measure zero for $s < r$, and that $f(C - C_1)$ has measure zero.

We can write $C - C_1 = \bigcup_{i,j} A_{ij}$, where $A_{ij}$ is the set of critical points $x$ with $D_i f_j(x) \neq 0$. Let us consider the case $A_{11}$, for simplicity in notation. Then for each $x_0 \in A_{11}$, the implicit function theorem guarantees that there exists a neighbourhood $V$ of $x_0$, a neighbourhood $W$ of $\RR^n$, and a diffeomorphism $u: W \to V$ such that for $(t,y) \in U$, $f_1(u(t,y)) = f_1(x_0) + t$. For each $t$, we let $W_t = \{ y \in \RR^{n-1}: (t,y) \in W \}$ and define $h_t: W_t \to V$ by setting
%
\[ h_t(y) = (f_2(u(t,y)), \dots, f_n(u(t,y))). \]
%
Then $y \in W_t$ is a critical point for $f \circ h_t$ if and only if $(t,y)$ is a critical point for $f$. Applying Sard's theorem in the case $n-1$, we conclude that $f(A_{11}) \cap \{ t \} \times \RR^{m-1} \cap V)$ has measure zero. But then by Fubini's theorem, we conclude that $f(A_{11} \cap V)$ has measure zero. But since $A_{11}$ can be covered by countably many such neighbourhoods $V$, this means that $f(A_{11})$ has measure zero. The same technique shows $f(A_{ij} \cap)$ has measure zero for all indices $i,j$, the proof only requiring a permutation of coordinates.

Now we address the analysis of $C_s - C_{s+1}$, with $s < r$. We write $C_s - C_{s+1} \subset \bigcup_{\alpha,i} C(\alpha,i)$, where $\alpha$ is a multiindex with $|\alpha| = s$, $i,j \in \{ 1, \dots, n \}$, and $C(\alpha,i,j)$ is the set of points $x$ with $D_\alpha f(x) = 0$, but $D_i D_\alpha f_j(x) \neq 0$. For simplicity, we shall show $f(C(\alpha,1,1))$ has measure zero, the general case obtained by permuting coordinates. By the implicit function theorem, for each $x_0 \in C(\alpha,i)$, there exists a neighbourhood $V$ of $x_0$, an open set $W \subset \RR^n$, and a diffeomorphism $g: W \to V$ such that for each $(t,y) \in W$,
%
\[ f_\alpha(g(t,y),y) = (t, z_2(t,y), \dots, z_m(t,y)). \]
%
Define $W_0 = \{ y : (0,y) \in W \}$. Clearly $C(\alpha,1,1) \cap V$ is contained in the set of points $(g(0,y),y)$, with $y \in W_0$, such that $y$ is critical for the map $h_0 : W_0 \to \RR^m$ given by setting $h_0(y) = f(g(0,y),y)$. But by induction we know the image of the critical points for $h_0$ form a set of measure zero, from which we obtain that $f(C(\alpha,1,1) \cap V)$ has measure zero. Since $C(\alpha,1,1)$ can be covered by countably many neighbourhoods $V$, we conclude $f(C(\alpha,1,1))$ has measure zero. The general case for $C(\alpha,i,j)$ is treated by very similar methods. This completes our proof of Sard's theorem.

\begin{remark}
    The only change that results in the tight statement of Sard's theorem is replacing the application of the implicit function theorem with something which `boosts smoothness' in the case where $f$ is $C^k$, with $k < \infty$, since repeated applications of the implicit function theorem will reduce smoothness in this case.
\end{remark}

\section{Partitions of Unity}

The use of $C^\infty$ functions relies on the fact that manifolds possess them in plenty. The following theorem gives us our first plethora. First, we detail some $C^\infty$ functions on $\mathbf{R}^n$.

\begin{enumerate}
    \item The map $f:\mathbf{R} \to \mathbf{R}$, defined by
    %
    \[
    g(t) =
    \begin{cases}
        e^{-t} & : t > 0\\
        0 & : \text{elsewhere}
    \end{cases}
    \]
    %
    is $C^\infty$. We have $0 < f(t) < 1$ on $(0,\infty)$, and $f^{(n)}(0) = 0$ for all $n$.
    \item The $C^\infty$ map $g(t) = f(t-1)f(t+1)$ is positive on $(-1,1)$, and zero everywhere else. Similarily, for any $\varepsilon$, there is a map $g_\varepsilon$ which is positive on $(-\varepsilon, \varepsilon)$ and zero elsewhere.
    \item The map 
    %
    \[ l(t) = \begin{cases}
        \left(\int_{-\varepsilon}^t g_\varepsilon \right)/\left(\int_{-\varepsilon}^\varepsilon g_\varepsilon \right) & : t \in (0, \infty) \\
        0 & : \text{elsewise}
    \end{cases} \]
    %
    is $C^\infty$, is zero for negative $t$, increasing on $(0, \varepsilon)$, and one on $[\varepsilon, \infty)$.
    \item There is a differentiable map $h:\mathbf{R}^n \to \mathbf{R}$ defined by $h(x_1, \dots, x_n) = g(x_1) g(x_2) \dots g(x_n)$ which is positive on $(-1, 1)^n$, and zero elsewhere.
\end{enumerate}

With these nice functions in hand, we may form them on arbitrary manifolds.

\begin{theorem}
    If $M$ is a differentiable manifold, and $C$ is a compact set contained in an open set $U$, then there is a differentiable function $f:M \to \mathbf{R}$ such that $f(x) = 1$ for $x$ in $C$, and whose support $\overline{\{ x \in M : f(x) \neq 0 \}}$ is contained entirely within $U$.
\end{theorem}
\begin{proof}
    For each point $p$ in $C$, consider a chart $(x,V)$ around $p$, with $\overline{V} \subset U$, and $x(V)$ containing the open unit square $(-1,1)^n$ in $\mathbf{R}^n$. We may clearly select a finite subset of these charts $(x_k,V_k)$ such that the $x_k^{-1}((-1,1)^n)$ cover $C$. We may define a map $f_k:V_k \to \mathbf{R}$ equal to $h \circ x_k$, where $h$ is defined above. It clearly remains $C^\infty$ if we extend it to be zero outside of $V_k$. Then $\sum f_k$ is positive on $C$, and whose closure is contained within $\bigcup \overline{V_k} \subset U$. Since $C$ is compact, and the function is continuous, $\sum f_k$ is bounded below by $\varepsilon$ on $C$. Taking $f = l \circ (\sum f_k)$, where $l$ is defined above, we obtain the map needed.
\end{proof}

To enable us to define $C^\infty$ functions whose support lie beyond this range, we need to consider a technique to extend $C^\infty$ functions defined locally to manifolds across the entire domain. A \emph{partition of unity} on a manifold $M$ is a family of $C^\infty$ functions $\{ \phi_i : i \in I \}$, and such that the following two properties hold:
%
\begin{enumerate}
    \item The supports of the functions forms a locally finite set.
    \item For each point $p \in M$, the finite sum $\sum_{i \in I} \phi_i(p)$ is equal to 1.
\end{enumerate}
%
If $\{ U_i \}$ is an open cover of $M$, then a partition of unity is subordinate to this cover if it also satisfies (3):
%
\begin{enumerate}
    \item[3.] The closure of each function is contained in some element of the cover.
\end{enumerate}
%
It is finally our chance to use the topological `niceness' established in the previous chapter.

\begin{lemma}[The Shrinking Lemma]
    If $M$ is a paracompact manifold, and $\{ U_i \}$ is an open cover, then there exists a refined cover $\{ V_i \}$ such that for each $i \in I$ there exists $i'$ such that $\overline{V_i} \subset U_{i'}$.
\end{lemma}
\begin{proof}
    Without loss of generality, we may assume $\{ U_i \}$ is locally finite, and $M$ is connected. Then $M$ is also $\sigma$-compact, $M = \bigcup C_i$. Since $C_i$ is compact, and each $p \in C_i$ locally intersects only finitely many $U_i$, then $C_i$ intersects only finitely many $U_i$. Therefore $\bigcup C_i$ intersects only countably many $U_i$, and thus our locally finite cover is countable. Consider an ordering $\{ U_1, U_2, \dots \}$ of $\{ U_i \}$. Let $C_1$ be the closed set $U_1 - (U_2 \cup U_3 \cup \dots)$. Let $V_1$ be an open set with $C_1 \subset V_1 \subset \overline{V_1} \subset U_1$. Inductively, let $C_k$ be the closed set $U_k - (V_1 \cup \dots \cup V_{k-1} \cup U_{k+1} \cup \dots)$, and define $V_k$ to be an open set with $C_k \subset V_k \subset \overline{V_k} \subset U_k$. Then $\{ V_1, V_2 \dots \}$ is the desired refinement.
\end{proof}

\begin{theorem}
    Any cover on a paracompact manifold induces a subordinate partition of unity.
\end{theorem}
\begin{proof}
    Let $\{ U_i \}$ be an open cover of a paracompact manifold $M$. Without loss of generality, we may consider $\{ U_i \}$ locally finite. Suppose that each $U_i$ has compact closure. Choose $\{ V_i \}$ satisfying the shrinking lemma. Apply Theorem (2.13) to $\overline{V_i}$ to obtain functions $\psi_i$ that are 1 on $\overline{V_i}$ and zero outside of $U_i$. These functions are locally finite, and thus we may define $\phi_i$ by
    %
    \[ \phi_i(p) = \frac{\psi_i(p)}{\sum_j \psi_j(p)} \]
    %
    Then $\phi_i$ is the partition of unity we desire.

    This theorem holds for any $\{ U_i \}$ provided Theorem (2.13) holds on any closed set, rather than just a compact one. Let $C$ be a closed subset of a manifold, contained in an open subset $U$, and for each $p \in C$, choose an open set $U_p$ with compact closure contained in $U$. For each $p \in C^c$, choose an open subset $V_p$ contained in $C^c$ with compact closure. Then our previous compact case applies to this cover, and we obtain a subordinate partition of unity $\{ \zeta_i \}$. Define $f$ on $M$ by
    %
    \[ f(p) = \sum_{\overline{\zeta_i} \subset U_p} \zeta_i(p) \]
    %
    Then the support of $f$ is contained within $U$, and $f = 1$ on $C$.
\end{proof}

Partitions of unity allow us to extend local results on a manifold to global results. The utility of these partitions is half the reason that some mathematicians mandate that manifolds must be paracompact -- otherwise many nice results are lost.

\begin{theorem}
    In a $\sigma$-compact manifold $M$, there exists a smooth, real-valued function $f$ such that $f^{-1}((-\infty, t])$ is compact for each $t$.
\end{theorem}
\begin{proof}
    Let $M$ be a $\sigma$-compact manifold with $M = \bigcup B_i$, Where $\overline{B_i}$ is compact, $B_i$ is diffeomorphic to a ball, and the $B_i$ are a locally finite cover. Consider a partition of unity $\{\psi_i\}$ subordinate to $\{B_i\}$, and take the sum
    %
    \[ f(x) = \sum k \psi_k(x) \]
    %
    Then $f$ is smooth, since locally it is the finite sum of smooth functions. If $x \not \in B_1, \dots, B_n$, then
    %
    \[ f(x) = \sum_{k = 1}^\infty k \psi_k(x) = \sum_{k = n+1}^\infty k \psi_k(x) \geq (n+1) \sum_{k = n+1}^\infty \psi_k(x) = (n+1) \]
    %
    Therefore if $f(x) < n$, $x$ is in some $B_i$. Thus $f^{-1}((-\infty, n])$ is a closed subset of $\overline{B_1} \cup \dots \cup \overline{B_n}$, and is therefore compact.
\end{proof}

Other existence proofs also follow naturally.

\begin{lemma}
    If $A$ is a closed subset of a paracompact manifold $M$, there is a differentiable function $f: M \to [0,1]$ with $f^{-1}(0) = A$.
\end{lemma}
\begin{proof}
    Let us begin proving this for $M = \mathbf{R}^n$. Let $\{ U_i \}$ be a countable cover of $\mathbf{R}^n - A$ by open unit balls. For each $U_i$, pick a smooth $f_i : \mathbf{R}^n \to [0,1]$ positive on $U_i$, and equal to zero on $\mathbf{R}^n - U_i$. Define
    %
    \[ \alpha_j = \min \left\{ \left\| \frac{\partial^n f_i}{\partial x_{i_1} \dots \partial x_{i_n}} \right\|_\infty : i \leq j, n \leq j \right\} \]
    %
    Each $\alpha_j$ is well defined, because $f_i$ is $C^\infty$ and tends to zero as we leave $U_i$. Define
    %
    \[ f = \sum_{k = 1}^\infty \frac{f_k}{\alpha_k 2^k} \]
    %
    Then $f$ is differentiable, since if $k \geq n$,
    %
    \[ \frac{1}{\alpha_k 2^k} \frac{\partial^n f_k}{\partial x_{i_1} \dots \partial x_{i_n}} \]
    %
    so the sums of all partial derivatives converge uniformly, and $f^{-1}(0) = C$ because the function is positive for some coefficient everywhere else.

    To address the general case, let $M$ be paracompact, and find a cover of coordinate balls $\{ B_\alpha \}$ for $M - A$. Then we may find a partition of unity $\{ \psi_\alpha \}$ for this family, and find smooth $f_\alpha: M \to \mathbf{R}$ with $f_\alpha^{-1}(0) = A \cap B_\alpha$. But then $f = \sum \psi_\alpha f_\alpha$ is smooth (by local finiteness), and satisfies $f^{-1}(0) = A$.
\end{proof}

\begin{corollary}
    If $A$ and $B$ are disjoint closed subsets of a paracompact manifold $M$, then there is a function $h: M \to [0,1]$ with $h^{-1}(0) = A$, $h^{-1}(1) = B$.
\end{corollary}
\begin{proof}
    Modifying the function obtained in the last theorem, we can find $f: M \to [0,1]$ with $f^{-1}(1) = B$. Denote $f^{-1}(0)$ by $C$. If $g: M \to [0,1/2]$ satisfies $g^{-1}(0) = A$, and if $\psi: M \to [0,1]$ is a bump function on $B$, vanishing on $A \cup C$, then
    %
    \[ h = \psi f + (1 - \psi) g \]
    %
    satisfies $h^{-1}(1) = B$, because for $p \not \in B$, $f(p), g(p) < 1$. Now certainly $h(p) = 0$ for $p \in C$. If $p \in A - C$, $\psi(p) = 0$, so $h(p) = g(p) > 0$, and if $p \not \in A \cup C$, $f(p), g(p) > 0$, so $h(p) > 0$. Thus $h^{-1}(0) = A$, and we have shown $h$ is the needed function in the theorem.
\end{proof}

\section{Differentiable Manifolds with Boundary}

Recall that we may extend differentiability on subsets of Euclidean space in the following way; a map $f: A \to B$ between arbitrary subsets of Euclidean space is differentiable if $f$ can be extended to a differentiable map on an open subset of Euclidean space containing $A$.

\begin{theorem}
    If $f:\mathbf{H}^n \to \mathbf{R}$ is locally differentiable (every point has a neighbourhood on which $f$ can be locally extended to be differentiable), then $f$ is differentiable in the sense defined above.
\end{theorem}
\begin{proof}
    Let $\{ U_\alpha \}$ be a open cover of $\mathbf{H}^n$ in $\mathbf{R}^n$ with smooth functions $g_\alpha:U \to \mathbf{R}$ agreeing with $f$ where the two are jointly defined. Consider a partition of unity $\{ \Phi_\alpha \}$ subordinate to $\{ U_\alpha \}$. Define $g = \sum g_k \Phi_k$, defined on $\bigcup U_\alpha$, a open extension of $\mathbf{H}^n$. Each pair $g_k$ and $\Phi_k$ are differentiable, so the multiplication of the two is differentiable. Since these functions are locally finite, we also have $g$ differentiable across its domain. If $p \in \mathbf{H}^n$, then $g_k(p) = f(p)$. Thus
    %
    \[ g(p) = \sum g_k(p) \Phi_k(p) = \sum f(p) \Phi_k(p) = f(p) \]
    %
    since the $\Phi_k$ sum up to one. Thus $g|_{\mathbf{H}^n} = f$, and we have extended $f$ to be differentiable.
\end{proof}

This allows us to define a notion of differentiable structure for a manifold with boundary. We can extend the notion of two charts being $C^\infty$ consistent, because we have extended differentiability on non open subsets of $\mathbf{H}^n$ to a non-local criterion. Similarily, a map $f: M \to N$ can also be considered differentiable if $y \circ f \circ x^{-1}:x(U) \to y(V)$ can be extended to be a differentiable function on an open subset of Euclidean space. Thus manifolds with boundary have effectively the same theory as manifolds without boundary.

\begin{example}[Differentiable structures on the boundary of a manifold]
    Given a differentiable manifold with boundary $M$, we can assign a unique differentiable structure to $\partial M$ such that the inclusion map is an imbedding. We can generate it from the atlas consisting of the restriction of charts on $M$ to the boundary, projected into an $(n-1)$ dimensional subspace of $\mathbf{R}^n$. The transition maps are easily verified to be $C^\infty$.
\end{example}

One issue with manifolds with boundary is that the rank theorem does not hold. The problem is that, at the boundary, we are restricted in how we reapply coordinates -- we can only consider smooth automorphisms of $\mathbf{H}^n$ rather than $\mathbf{R}^n$. However, for immersions $f$ of manifolds with boundary in manifolds without, we do have coordinate charts for which
%
\[ (y \circ f \circ x^{-1})(a_1, \dots, a_n) = (a_1, \dots, a_n, 0, \dots, 0) \]
%
because in the theorem we currently have, we need not change the coordinates on the manifold with boundary, only on the manifold without. Thus a submanifold with boundary $N$ can be identified in manifolds without boundary $M$ as those subsets which have `half slices', i.e. for each $p \in N$, there is a neighbourhood $U$ of $p$ in $M$ and a coordinate chart $(x,U)$ centred at $p$ such that either
%
\[ U \cap N = \{ q \in U : x^{k+1}(q), \dots, x^n(q) = 0, x^k(q) \geq 0 \}, \]
%
in which case $p \in \partial N$, or
%
\[ U \cap N = \{ q \in U : x^{k+1}(q), \dots, x^n(q) = 0 \}, \]
%
in which case $p \in N^\circ$.




\chapter{The Tangent Bundle}

Historically, calculus was the subject of infinitisimals, differentiable functions which are `infinitisimally linear'. It took over 200 years to make precise the analytical notions defining the field; in the process, infinitisimals vanished from sight, replaced by linear approximations, epsilons and deltas. On manifolds, we cannot discuss global linear approximations, since the space is not globally linear. Thus we must reconcile the use of infinitisimals, since a manifold is `infinitisimally linear'. The construction of the tangent bundle of infinitisimals is therefore integral to the development of modern geometry. One can describe modern differential geometry as studying manifolds with additional structure ascribed to the tangent bundle, and so a further analysis of the tangent bundle we constructed in the last chapter is essential.

\section{A Smooth Structure on the Tangent Bundle}

Recall that if $M$ is a manifold, then the tangent bundle $TM$ is the union of the tangent spaces $T_p M$, for all $p \in M$, where $T_p M$ is the equivalence class of vectors in various charts on the manifold, where $[x,v]_p$ is identified with $[y,w]_p$ if
%
\[ w = D(y \circ x^{-1})(x(p))(v) \]
%
We view elements of $TM$ as vectors whose base points emerge from the manifold at every point. Thus there is a projection map $\pi: TM \to M$, which projects a vector to it's basepoint, i.e. we set $\pi [x,v]_p = p$. On each of the `tangent spaces' $\pi^{-1}(p)$, often denoted $T_pM$ or $M_p$, we have a vector space structure defined by setting
%
\[ \lambda [x,v]_p + \gamma [x,w]_p = [x,\lambda v + \gamma w]_p \]
%
The linearity of derivatives makes this construction well defined. One of the nicest parts about the tangent bundle is that we can express the collection of all derivatives of a differentiable map $f: M \to N$ in all coordinate systems as a single mathematical object, i.e. via the map $f_*: TM \to TN$, defined by setting
%
\[ f_*[x,v]_p = \left[ y, D(y \circ f \circ x^{-1})(x(p))(v) \right]_{f(p)}. \]
%
For a fixed $p$,  we let $f_*|_p$ denote the map from $T_p M$ to $T_{f(p)} N$. This map is linear, since it is really just the linear transformation given by the derivative, but expressed in a coordinate independent fashion. Our goal is to give $TM$ a smooth structure, such that for each smooth function $f$, the function $f_*$ is smooth.

As an initial, simple example, we consider $T\RR^n$. Since the space here can be covered by a single coordinate system, we can identify $T\RR^n$ with $\RR^n \times \RR^n$, where we identify $(p,v)$ with $[\text{id}, v]_p$. Often the vector $(p,v)$ is denoted $v_p$, so we remember which part of the product is the `vector', and which part is the base point. Under the identification of $T\RR^n$ with $\RR^n \times \RR^n$, the tangent bundle on $\RR^n$ possesses both topological and smooth structure, which agrees with our intuitions about how tangent vectors change over time. If $f: \mathbf{R}^n \to \mathbf{R}^m$ is smooth, then $f_*: T\mathbf{R}^n \to T\mathbf{R}^m$ is also a smooth map between the tangent spaces, now viewed as smooth manifolds, since the entries of the derivative matrix change smoothly as we vary the base point.

The most geometric way to obtain a smooth manifold structure for an arbitrary manifold $M$ is to assume $M$ occurs as a smooth submanifold of $\RR^n$, for some $n$. Then the inclusion map $i: M \to \RR^n$ is an imbedding, and therefore induces an injective map $i_*: TM \to T\mathbf{R}^n$. Thus we can identify the vectors in $TM$ with the subset $i_*(TM)$ of $\RR^n \times \RR^n$. Now $i_*(TM)$ is a smooth submanifold of $T\RR^n$. To see this, we let $(x,U)$ be a chart on $\RR^n$, such that $U \cap i_*(M) = \{ p \in U: x^{k+1}(p) = \dots = x^n(p) = 0 \}$. Then it is easy to see that
%
\[ (U \times \RR^n) \cap i_*(TM) = \{ v_p \in U: x^{k+1}(p) = dx^i(v_p) = 0\ \text{for all}\ i \in \{ k+1, \dots, n \} \}. \]
%
so $x_*$ give a `slice chart' for $i_*(TM)$. It is only slightly technical to show this smooth structure does not depend on the particular embedding of $M$ in Euclidean space, so, assuming our manifold already occurs in Euclidean space, this solves our problem of giving the bundle a smooth structure. More generally, this argument shows that if $N$ is a submanifold of $M$, then $TN$ is naturally identified as a subset of $TM$.

\begin{example}
    Consider the smooth manifold $S^1 \subset \RR^2$. Then $TS^1$ can be identified as a subset of $\{ v_x: x \in S^1, v \in \RR^2 \}$. At each point $x \in S^1$, $T_xS^1$ is one dimensional, and is thus spanned by a single nonzero vector $v_x \in T_x(\RR^2)$. In fact, we claim that if $x = (x_1,x_2)$, then $T_xS^1$ is spanned by $(-x_2,x_1)_x$. Indeed, if $\theta: U \to I$ is an angle chart on $S^1$, where $U \subset S^1$ is open and $I$ is an open interval in $\RR$, then $\theta^{-1}(t) = (\cos(t), \sin(t))$. Thus for each $s \in \RR$,
    %
    \[ \theta^{-1}_*(s_t) = s \cdot (-\sin(t), \cos(t)). \]
    %
    But this implies $T_x S^{-1}$ is spanned by $(-\sin(\theta(x)), \cos(\theta(x)) = (-x_2,x_1)$.
\end{example}

\begin{example}
    More generally, we claim that for each $x \in S^n$, $TS^n$ can be identified with the set of all $v_x \in T\RR^{n+1}$ such that $x \in S^n$ and $v \cdot x = 0$. To see this claim, we fix $x \in S^n$. Then we may choose $x_2, \dots, x_{n+1} \in \RR^{n+1}$ such that if we set $x_1 = x$, then $\{ x_1, \dots, x_n \}$ is an orthogonal basis. Then
    %
    \[ S^n = \{ t_1x_1 + \dots + t_{n+1}x_{n+1} : t_1^2 + \dots + t_{n+1}^2 = 1 \}. \]
    %
    We consider the open set $U \subset S^n$ defined by
    %
    \[ U = \{ t_1x_1 + \dots + t_{n+1}x_{n+1} : t_1 > 0, t_1^2 + \dots + t_n^2 = 1 \}. \]
    %
    Then there is a smooth chart $y: U \to B$, where $B$ is the open unit ball in $\RR^n$, given by setting $y(t_1x_1 + \dots + t_{n+1}x_{n+1}) = (t_2, \dots, t_{n+1})$. The map is certainly smooth, satisfies $y(x) = 0$, and the inverse is given by
    %
    \[ y^{-1}(t) = (1 - |t|^2)^{1/2} x_1 + t_2x_2 + \dots + t_{n+1}x_{n+1}. \]
    %
    But computing the derivative map at $t = 0$ gives
    %
    \[ D(y^{-1})(0)(v_2, \dots, v_{n+1}) = v_2x_2 + \dots + v_{n+1} x_{n+1}. \]
    %
    In particular, this means that $T_xS^n$ is spanned by $(x_2)_x, \dots, (x_{n+1})_x$, which was what we needed to show.
\end{example}

This construction is, in low dimensions, the geometrically natural way to view the smooth structure of the tangent space. Since all manifolds can be imbedded in some dimension of Euclidean space, this is certainly a sufficient method to define the tangent space on all manifolds. Nonetheless, it is not entirely elegant because the imbedding is not unique, so there are many different candidates for `the' tangent bundle (though all the candidates will essentially be equivalent), and it is not clear which imbedding will make the tangent bundle easiest to work with.

A similar, but slightly more abstract way to obtain a canonical smooth structure is to use the coordinate charts on a manifold $M$. Given any chart $(x,U)$ on the manifold, the map $x: U \to x(U)$ is a smooth map, and as such we can consider $x_*: TU \to Tx(U)$. We claim that the $x_*$ can be chosen as a consistent family of charts on $TM$. Recall the way we construct abstract smooth manifolds from chapter 2. If $(y,V)$ is another coordinate chart on the manifold, then $x_*(TV \cap TU) = x(U \cap V) \times \mathbf{R}^n$ is an open subset of Euclidean space, and by definition,
%
\[ (y \circ x^{-1})_*(v_p) = (D(y \circ x^{-1})(p)(v))_{(y \circ x^{-1})(p)} \]
%
Since $y \circ x^{-1}$ is $C^\infty$, not only does the map change over points smoothly, but the choice of $D(y \circ x^{-1})$ changes smoothly as $p$ ranges, which means the entire map, viewed as a map between open subsets of $\mathbf{R}^n \times \mathbf{R}^n$, to itself, is smooth. Thus these map are $C^\infty$ related, and combine to give a smooth structure on $TM$, and we have abstractly defined a smooth structure on $TM$. The coordinates $x_*$ into $\mathbf{R}^{2n}$ on $TM$ are often denoted by $(x,dx)$, so $dx^i[x,v]_p = v^i$.

\section{Vector Bundles}

We're not finished with our tangent bundle discussion yet. There are many different perspectives with which we can view the tangent bundle of a manifold, and all are useful at one point or another. The unity in description is best achieved with the concept of a vector bundle. The abstract definition of a vector space eminating from points on a topological space comes from the theory of vector bundles. A \emph{vector bundle} $\xi$ over a topological space $X$ is a topological space $E_\xi$ together with a continuous, surjective projection $\pi_\xi: E \to X$, such that for each $x \in X$, the set $E_x = \pi^{-1}(x)$ has the structure of a finite dimensional vector space, and such that we have local triviality; for each point $x \in X$, there is a neighbourhood $U$ and a homeomorphism $\phi: \pi^{-1}(U) \to U \times \mathbf{R}^n$ which is a linear isomorphism on each fibre for $x \in U$.

The function $\dim(E_x)$ is a locally constant function of $x$, so if $X$ is connected, $\dim(E_x)$ is a constant function of $x$, equal to some integer $n$, and we refer to $\xi$ as a \emph{$n$ dimensional vector bundle}. If $X$ and $E$ are both differentiable manifolds, and the projection maps $\pi$ and triviality maps $\phi$ are differentiable, then we shall call $\xi$ a \emph{smooth vector bundle}.

\begin{example}
    For any topological space $X$, we have trivial $n$ dimensional vector bundles $\varepsilon^n(X)$, given by the space $X \times \RR^n$ and the projection map $\pi(v_x) = x$. In some sense, every vector bundle on $X$ is obtained from `twisting' this trivial bundle globally, while keeping the trivial structure locally.
\end{example}

\begin{example}
    The M\"{o}bius strip $\mathbf{M}$ can be seen as a vector bundle over the circle $S^1$. Indeed, $S^1$ can be identified with the orbit space of $\RR$ under the group action $n \cdot x = (x + n)$, and $\mathbf{M}$ as $\RR^2$ under the group action $n \cdot (x,y) = (x + n, (-1)^n y)$. Thus the projection map $\pi(x,y) = x$ filters through the group action and induces a vector bundle structure.
\end{example}

\begin{example}
    If $\xi$ and $\eta$ are bundles on two bundles respectively onto two spaces $X$ and $Y$, we can define a bundle $\xi \times \eta$ on $X \times Y$ by taking the product of the bundle spaces.
\end{example}

If $X$ and $Y$ are spaces with bundles $(\xi,E)$ and $(\eta,Y)$ on two spaces $X$ and $Y$, then a \emph{bundle map} is a pair of continuous maps $f: X \to Y$ and $f_\sharp: E \to F$, where each $f^\sharp_p = (f_\sharp)|_{X_p}$ is linear from $X_p$ to $Y_{f(p)}$. An isomorphism in the category of bundle maps is known as an \emph{equivalence} of bundles. If $X$ and $Y$ are the same space, it is often assumed that $f$ {\it must} be the identity map, so $f_\sharp$ maps $\pi^{-1}(p)$ to $\nu^{-1}(p)$ linearly for each point $p$ in the base space. Since $f$ can be easily obtained from $f_\sharp$, we can describe a bundle map solely by the map $f_\sharp$ between bundles, or more particularly by the linear maps $(f_\sharp)_p: X_p \to Y_p$, which can be viewed as a continuous parameterization of linear maps over $p$. Note that now we have defined an equivalence of bundles, we can work backwards and {\it define} the local triviality conditions of bundles by saying that every vector bundle is locally equivalent to the trivial bundle over suitably small neighbourhoods.

\begin{theorem}
    Let $\xi$ and $\eta$ be bundles over the same base space $X$, then a bundle map $f: \xi \to \eta$ which is an isomorphism on each fibre is a bundle equivalence.
\end{theorem}
\begin{proof}
    Let $(\xi,E)$ and $(\eta,F)$ be bundles. The map $f: E \to F$ is clearly injective, so all that remains is to check the inverse is continuous. But locally at any point $p \in X$, we can consider a neighbourhood $U$ for which there are trivializations $\phi: \pi_\xi^{-1}(U) \to U \times \mathbf{R}^n$ and $\eta: \pi_\eta^{-1}(U) \to U \times \mathbf{R}^n$, and then $g = \eta \circ f \circ \phi^{-1}$ is a bundle map from $U \times \mathbf{R}^n$ to itself which is an isomorphism on each fibre. Since being a homeomorphism is a local condition, it now suffices to prove that a bundle map from a trivial bundle to itself which is an isomorphism on each fibre is an equivalence.

    Note that the map $g$ induces a continuous choice of linear isomorphisms $M: U \to GL(n)$ such that $g(p,v) = (p,M(p)v)$. Clearly such $M(p)$ must exist, and be unique, and then the inverse of $g$ is given by $g(p,v) = (p,M(p)^{-1} v)$. Since the choice of isomorphism $M^{-1}: U \to GL(n)$ is continuous, we conclude that $g$ is an equivalence. Of course, since inversion of matrices is also smooth, it follows that a smooth bundle map which is an isomorphism on each fibre is a smooth bundle equivalence.
\end{proof}

%The tangent bundle of a smooth manifold is a smooth manifold in its own right, since the charts $x_*$ in an atlas are $C^\infty$ related to one another. Introducing notation, we write
%
%\[ x_*(v) = (\dot{x}^1(v), \dots, \dot{x}^n(v))_{(x \circ \pi)(v)} \]
%
%so that the `coordinates' related to $x_*$ are $(x^1 \circ \pi, \dots, x^n \circ \pi, \dot{x^1}, \dots, \dot{x^n})$, though often we abuse notation and just write $x^k \circ \pi$ as $x^k$.

\section{The Space of Derivations}

The algebraists found another characterization of the tangent bundle, which is elegant, but much more abstract then the definition by coordinates. Note that on $\mathbf{R}^n$, the space of vectors $v \in T\mathbf{R}^n_p$ can be identified with the space of directional derivatives, functionals $D_v$ on $C^\infty(M)$ defined by
%
\[ D_v(f)(p) = \lim_{h \to 0} \frac{f(p + hv) - f(p)}{h}. \]
%
This map satisfies the product rule
%
\[ D_v(fg)(p) = f(p) D_v(g)(p) + g(p) D_v(f)(p). \]
%
The idea of the algebraic tangent bundle is to identify tangent vectors with certain functionals on $C^\infty(M)$. This is compatible with the interpretation of tangent vectors as velocities over the manifold. If we are moving across a surface, then the velocity we are travelling at determines how the surface below us changes, and the measurement of this change can be identified in the velocity we are travelling at.

Since $C^\infty(M)$ is a vector space, we may consider the dual space $C^\infty(M)^*$, which is a monstrous vector space consisting of all linear functionals from $C^\infty(M)$ to $\mathbf{R}$. A derivation at a point $p \in M$ is a linear functional $\lambda \in C^\infty(M)^*$ satisfying $\lambda(fg) = f(p) \lambda(g) + g(p) \lambda(f)$. A $C^\infty$ map $f: M \to N$ induces a linear map $f^*: C^\infty(N) \to C^\infty(M)$, defined by $f^*(g) = g \circ f$, which further induces a map $f_*: C^\infty(M)^* \to C^\infty(N)^*$, defined by $[f_*\lambda](g) = \lambda(f^*(g))$. If $\lambda$ is a derivation at $p$, then
%
\begin{align*}
    f_*(gh) = \lambda(f^*(gh)) &= \lambda((gh) \circ f) = \lambda((g \circ f)(h \circ f))\\
    &= g(f(p)) \lambda(h \circ f) + h(f(p)) \lambda(g \circ f)\\
    &= g(f(p)) f_*(h) + h(f(p)) f_*(g)
\end{align*}
%
so $f_*(\lambda)$ is a derivation at $f(p)$. We can directly calculate that $(f_* \circ g_*) = (f \circ g)_*$, so that if $f$ is a diffeomorphism, $f_*$ is an isomorphism. We shall soon find that we can identify the space of derivations at a point $p$ as the space of tangent vectors at $p$. The identification will match the $f_*$ defined here with the $f_*$ defined on the tangent space, providing an elegant algebraic definition of the covariant derivative.

The differential operators
%
\[ \left. \frac{\partial}{\partial x^1} \right|_p, \dots, \left. \frac{\partial}{\partial x^n}\right|_p \]
%
are all derivations. We will show, in fact, that these operators span the space of all derivations, so that the space is $n$ dimensional, and can be identified with the tangent bundle under the map
%
\[ \left. \sum v^i \frac{\partial}{\partial x^i} \right|_p \mapsto [x,v]_p \]
%
This means the set of all derivations {\it is} the tangent bundle, up to a change in notation. In other words, a `tangent' on the manifold can be thought of a way of measuring change, by moving along the manifold at a certain velocity.

\begin{lemma}
    If $f \in C^\infty(M)$ is constant, $\lambda(f) = 0$ for any derivation $\lambda$.
\end{lemma}
\begin{proof}
    By scaling, assume without loss of generality that $f = 1$ for all $p$. Then
    %
    \[ \lambda(f) = \lambda(f^2) = \lambda(f) + \lambda(f) = 2 \lambda(f) \]
    %
    We then just subtract $\lambda(f)$ from both sides of the equation.
\end{proof}

\begin{lemma}
    If $\lambda$ is a derivation, and $f(p) = g(p) = 0$, then $\lambda(fg) = 0$.
\end{lemma}
\begin{proof}
    \[ \lambda(fg) = f(p) \lambda(g) + g(p) \lambda(f) = 0 + 0 = 0 \]
    %
    We verified the proof by direct calculation.
\end{proof}

We will show this space, though lying in a very high dimensional vector space, is actually very low dimensional.

\begin{lemma}
    If $f \in C^\infty(\mathbf{R}^n)$ and $f(0) = 0$, then there exists functions $f_i \in C^\infty(\mathbf{R}^n)$, such that $f(x) = \sum f_i(x) x^i$, and $f_i(0) = \partial_i f(0)$.
\end{lemma}
\begin{proof}
    Define $g(t) = f(tx)$. Then $g'(t) = \sum x^i \partial_i f(tx)$
    %
    \[ f(x) = \int_0^1 g'(t)\ dt = \sum x^i \int_0^1 \partial_i f(tx) = \sum x^i f_i(x) \]
    %
    and each $f_i$ is verified to be $C^\infty$, with $f_i(0) = \partial_i f(0)$.
\end{proof}

\begin{theorem}
    The space of derivations at the origin on $\mathbf{R}^n$ is $n$ dimensional, with basis
    %
    \[ \left.\frac{\partial}{\partial x^1}\right|_0, \dots, \left.\frac{\partial}{\partial x^n}\right|_0 \]
\end{theorem}
\begin{proof}
    Let $f \in C^\infty(\mathbf{R}^n)$. Then $\lambda(f) = \lambda(f - f_0)$, and so we can write $f - f_0 = \sum x^i f_i$, and then
    %
    \[ \lambda(f - f_0) = \sum \lambda(x^i) \frac{\partial f}{\partial x^i}(0) \]
    %
    Thus
    %
    \[ \lambda = \sum \lambda(x^i) \frac{\partial}{\partial x^i} \]
    %
    The independence of each partial derivative derivation is left to the reader.
\end{proof}

We can extend this theorem to arbitrary smooth manifolds.

\begin{lemma}
    If $\lambda$ is a derivation at $p$, and $f$ and $g$ are equal in a neighbourhood of $p$, then $\lambda(f) = \lambda(g)$.
\end{lemma}
\begin{proof}
    We shall prove that if $f = 0$ in a neighbourhood $U$ of $p$, then $\lambda(f) = 0$. Consider a bump function $\psi \in C^\infty(M)$ such that $\psi = 1$ at $p$, and $\psi = 0$ outside of $U$. Then $\psi f = 0$, and
    %
    \[ 0 = \lambda(0) = \lambda(\psi f) = \psi(p) \lambda(f) + f(p) \lambda(\psi) = \lambda(f) \]
    %
    hence $\lambda(f) = 0$.
\end{proof}

If $f$ is only defined in a neighbourhood $U$ of $p$, we may still compute a well-defined value $\lambda(f)$. Consider a function $\psi = 1$ in $V \subset U$, and equal to zero outside of $U$. Then $\psi f \in C^\infty(M)$, and $\lambda(\psi f)$ is invariant of the bump function chosen, by the last lemma. This implies that we can identify the space of derivations at $p \in U$ in $C^\infty(U)$ with $C^\infty(M)$, and we find that derivations really act on the \emph{germ} of functions defined in a neighbourhood of $p$. When we are working with analytic or complex manifolds, we no longer have bump functions to work with, so we must begin by working on the space of derivations defined on germs of analytic or holomorphic functions from the very beginning of the theory.

\begin{theorem}
    The space of derivations at $p$ is $n$-dimensional, and if $(x,U)$ is a chart centered at $p$, then a basis for the space are the partial derivatives
    %
    \[ \left.\frac{\partial}{\partial x^1}\right|_p, \dots, \left.\frac{\partial}{\partial x^n}\right|_p \]
\end{theorem}
\begin{proof}
    By restriction, we can identify derivations at $p$ on $M$ with derivations at $p$ on $U$, which can be identified by diffeomorphism with the derivations at the origin on $x(U) \subset \mathbf{R}^n$, and therefore with the space of derivations at the origin in $\mathbf{R}^n$. This identification maps the partial derivatives at $p$ with respect to the coordinates $x$ onto the partial derivatives at $x(p)$, and this is all that is needed to verify the proof.
\end{proof}

If we collect all derivations at all points on a manifold together, we obtain a structure corresponding exactly to $TM$. The correspondence is
%
\[ [x,v]_p \mapsto \sum_{k = 1}^n v_i \left.\frac{\partial}{\partial x^i}\right|_p \]
%
which induces a topology (and smooth structure) on derivations making the correspondence a homeomorphism (diffeomorphism). We will rarely distinguish between the two sets, and alternate between the two notations depending on which is convenient. For instance, we will often speak of a tangent vector operating on functions, or of a derivation as an element of the tangent space. Ultimately, they are the same mathematical objects, viewed from two different lenses.

\begin{remark}
    We can enlarge the space $C^\infty(M)$ of real-valued smooth functions to the complex vector space of complex-valued smooth functions on $M$. The linear functionals $\lambda$ in the complex dual space $C^\infty(M)^*$ such that $\lambda(f) \in \mathbf{R}$ is $f$ is a real-valued smooth function can then be identified with the original dual space. In the same way, the functionals $\lambda$ such that $\lambda(f)$ is purely imaginary when $f$ is real-valued and smooth can also be identified with the original dual space. If $\lambda$ is any complex linear functional, and we define
    %
    \[ \lambda_1(u + iv) = \text{Re}(\lambda(u)) + i\ \text{Re}(\lambda(v)) \]
    \[ \lambda_2(u + iv) = \text{Im}(\lambda(u)) + i\text{Re}(\lambda(v)) \]
    %
    Then $\lambda_1$ and $\lambda_2$ are complex linear, $\lambda = \lambda_1 + i\lambda_2$, and $\lambda_1$ and $\lambda_2$ are the unique such functions decomposing $\lambda$ such that $\lambda_1(f)$ is real and $\lambda_2(f)$ is purely imaginary when $f$ is real-valued. If $d$ is a derivation on the space of real-valued functions, and if we define $d(u + iv) = du + idv$, then $d$ is also a derivation on complex-valued functions by a trivial calculation. On the other hand, if $d$ is a complex-valued derivation, then
    %
    \[ d_1(uv) = \text{Re}(u(p) dv + v(p) du) = u(p) d_1v + v(p) d_1u \]
    %
    so $d_1$, and, left as an exercise, $d_2$, are easily seen to be a derivations. Thus every complex-valued derivation can be split into two real-valued derivations, and so it follows that every complex-valued derivation at $p$ can be written as
    %
    \[ \sum a_i \left. \frac{\partial}{\partial x_i} \right|_p \]
    %
    where $a_i \in \mathbf{C}$. This is sometimes more natural to work with than the original tangent space over the real numbers, and is a kind of `complexification' of the original tangent space.
\end{remark}

\begin{remark}
If $I$ is the ideal of $C^\infty(M)$ consisting of all functions $f$ with $f(p) = 0$, and $I^2$ is the subspace of $I$ generated by all products of functions in $V$, then the lemma above shows that any derivation on $C^\infty(M)$ vanishes on $I^2$. Conversely, if there is a functional $\lambda: I \to \mathbf{R}$ which vanishes on $I^2$, then it can be extended to a unique derivation on $C^\infty(M)$, by defining, for arbitrary $f \in C^\infty(M)$,
%
\[ \lambda(f) = \lambda(f - f(p)) \]
%
an equation which must hold for any derivation which extends $\lambda$. It then follows that $\lambda$ is a linear operator, and $\lambda$ annihilates constant functions, from which it then follows that
%
\begin{align*}
    \lambda(fg) &= \lambda(fg - f(p)g(p))\\
    &= \lambda([f - f(p)][g - g(p)]) + f(p) \lambda'(g) + g(p) \lambda'(f) - 2 f(p) g(p) \lambda(1)\\
    &= f(p) \lambda'(g) + g(p) \lambda'(f).
\end{align*}
%
Thus derivations on $C^\infty(M)$ can be identified with $(I/I^2)^*$, which we have verified to be finite dimensional, so $V/W$ is finite dimensional. This is often the way to construct the tangent bundle to a more varied class of geometric spaces, in particular, those that occur in algebraic geometry. Unfortunately, it is a little analytically unstable. If $I$ is the ideal of $C^k(\mathbf{R})$ consisting of functions vanishing at the origin, then $I/I^2$
 is infinite dimensional. For a given $f$, we define
 %
 \[ \text{ord}(f) = \sup \left\{ \alpha > 0 : \lim_{x \to 0} f(x) |x|^{-\alpha} = 0 \right\} \]
 %
 By Taylor's theorem, if $f \in C^k(\RR)$, and $f(0) = 0$ we can write
 %
 \[ f(x) = a_1x + \dots + a_{k-1} x^{k-1} + g(x) x^k, \]
 %
 where $g$ is a continuous function with $g(0) = 0$. This means every element $f$ of $I^2$ in $C^1(\RR)$ can be written as $\sum_{i = 2}^k a_i x^k + x^{k+1} g(x)$ for some continuous $g$, and so it follows that $\text{ord}(f) \geq k+1$, if all the $a_i$ vanish, or otherwise, is an integer between $1$ and $k$. But this means that none of the functions $f_\varepsilon(x) = x^{1 + \varepsilon}$, for $\varepsilon \in (0,1)$, lie in $I^2$, yet $f_\varepsilon \in I$. Moreover, $\{ f_\varepsilon \}$ are linearly independant modulo $I^2$, since $\text{ord}(\sum a_i f_{\varepsilon_i}) = k + \min(\varepsilon_i)$, which lies in $(k,k+1)$. This means that the coordinate-based tangent space structure should be used in this setting. Alternatively, it is a difficult, but proven theorem that every $C^k$ manifold has a compatible smooth structure, and we can use this to define the space of derivations on the manifold.
\end{remark}

\section{Curves as Vectors}

There is a third important view of the tangent bundle on a manifold, which is perhaps the most geometrically visual. To construct $M_p$, we consider curves $c: (a,b) \to M$ which pass through $p$ at some time $t$. Given our previous construction of the tangent bundle, we can consider the tangent to the curve $c_*(1_t)$ in $M_p$, which represents the speed of the curve passing through the point; any tangent vector can be put in this form for some curve $c$. Classically, $c_*(1_t)$ is often denoted in Leibnitzian fashion as $dc/dt$, where the additional parameter $t$ is obscured. We could have defined $M_p$ by these curves, provided we identify $c$ and $c'$ if $dc/dt = dc'/dt$. Of course, without the original tangent bundle to work with, stating this isn't so elegant. We have to fix some coordinate system $(x,U)$ at $p$, and identify two curves $c$ and $c'$ with $c(t) = p$, $c(t') = p$ if $(x \circ c)'(t) = (x \circ c')(t')$. Our new tangent bundle is obviously equivalent to our original tangent bundle. This is the closest intrinsic way to visualize the tangent vectors on an manifold. If a manifold does not lie in $\mathbf{R}^n$, it isn't really fair to see tangent vectors as vectors lying `off' the manifold, because the tangent vectors don't really `point' to anything. For instance, in Einsteinian physics, we consider the universe as a 4-dimensional manifold, and seeing the tangent bundle as vectors lying `outside of the universe' seems particularly strange. But the directions we can travel {\it are} visualizable from inside the manifold, and curves describe the way these curves travel.

\section{Sections and Vector Fields}

A \emph{section} on a vector bundle $(\xi,E)$ is a continuous map $f:X \to E$ for which $\pi \circ f$ is the identity map. One can think of a section as a continuous choice of a vector assigned to each point in space. Because of the local triviality property, the space of continuous sections, denoted $\Gamma(E)$ forms a module over the space of real-valued continuous functions on $X$. To begin with, sections provide an interesting way to study the topological behaviour of a vector bundle. A \emph{frame} on an $n$ dimensional vector bundle $\pi: E \to B$ is a family of sections $s_1, \dots, s_n$ such that $\{ s_1(p), \dots, s_n(p) \}$ is a basis at each point.

\begin{theorem}
    There exists a frame $s_1, \dots, s_n: U \to \pi^{-1}(U)$ on a set $U$ if and only if there exists a trivialization $\phi: \pi^{-1}(U) \to U \times \mathbf{R}^n$.
\end{theorem}
\begin{proof}
    If $\phi: \pi^{-1}(U) \to U \times \mathbf{R}^n$ is a trivialization, we can define a {\it local} frame $s_1, \dots, s_n$ on $U$ by setting $s_k(p) = \phi^{-1}(p,e_k)$. On the other hand, let $s_k$ be a local frame defined on a common domain $U$, upon which there is a local trivialization $\psi: \pi^{-1}(V) \to V \times \mathbf{R}^n$ for some $V \subset U$, then the maps $\psi \circ s_k$ can be seen as a series of maps from $V$ to $\mathbf{R}^n$, which can be stacked together, row by row, to form a single map $F: V \to GL(n)$. If we define a map $\phi: \pi^{-1}(U) \to U \times \mathbf{R}^n$ by $\phi(\sum a^i s_i(p)) = (p,a)$, then we find $(\psi \circ \phi^{-1})(p,v) = (p,F(p)(v))$, which is a homeomorphism since it is a bundle equivalence. But by patching together over a covering of $U$ by neighbourhoods $V$, we conclude that $\phi$ is an equivalence everywhere.
\end{proof}

Sections also provide easy ways to form of \emph{subbundles} $F \subset E$ of bundles $\pi: E \to B$, which are subsets which also form vector bundles in the sense that $F$ has the same vector space structure as $E$ ($F_p$ is a subspace of $E_p$ at each point), and under the same topology $F$ has local trivializations.

\begin{theorem}
    A subset $F \subset E$ is an $m$ dimensional subbundle of $E$ if and only if there are $m$ linearly independant sections $s_1, \dots, s_m$ into $F$ in a neighbourhood of each point in the base space $B$.
\end{theorem}
\begin{proof}
    The trivialization gives us the linearly independant sections, and conversely, a modification of our argument as to why frames give local trivializations show $F$ has a local trivialization. Instead of arguments about $GL(n)$, we need to use arguments about constant rank matrices instead, which we leave to the reader to modify from our previous discussion as to why $M(n,m;k)$ is a manifold.
\end{proof}

\begin{example}
    Let $f: \xi \to \eta$ be a bundle map between two vector bundles, such that the map $p \mapsto \text{rank}(f_p)$ is locally constant on the base space of $\xi$, then $\text{Ker}(f)$, the union of all the kernels of $f_p$ on the fibres of $\xi$, forms a subbundle of $\xi$, whose dimension at each point $p$ is the different in dimensions between the dimension of $f$ at $p$ and the rank of $f_p$. By locality, it suffices to prove this for bundles maps $f: \varepsilon^n(X) \to \varepsilon^m(Y)$ on trivial bundles with a constant rank $k$. Notice that in this case $f$ corresponds to a choice of a continuous function from $X$ to $Y$, as well as a continuous choice $M: X \to M(n,m;k)$ such that $f(p,v) = (f(p), M(p)(v))$. Slight modifications to our arguments about $M(n,m;k)$ in the first chapter guarantee that there exists a continuous choice of matrix $N(p) \in GL(n)$ locally around each point $p$ such that
    %
    \[ M(p)N(p) = \begin{pmatrix} A(p) & 0 \\ B(p) & 0 \end{pmatrix} \]
    %
    where $A \in GL(k)$, and $B \in M(n-k,k)$. But then the sections $s_j(p) = (p, N(p)(e_{k+j})$, for $j \leq n-k$, are linearly independent and parameterize the kernel of $f$ locally. It follows from the last theorem that $\text{Ker}(f)$ is a subbundle of $\xi$. Notice that these ideas can also be used to prove that the image of $f$ in $\eta$ is a subbundle, because if we choose a continuous $N(p) \in GL(n)$ such that
    %
    \[ N(p)M(p) = \begin{pmatrix} A(p) & B(p) \\ 0 & 0 \end{pmatrix} \]
    %
    where $A(p) \in GL(k)$ and $B(p) \in M(n,n-k)$, then $s_j(p) = (p, N(p)M(p)(e_j)$ are linearly independent and consistute a frame for the image of $f$.
\end{example}

\begin{example}
    If $f: \xi \to \eta$ is a bundle map injective as a map over base spaces, and the rank of $f$ is locally constant, then we can also make the union $\text{coker}(f)$ of the cokernels of the $f_p$, formed by the union of the cokernels of each $f_q$ (well defined because $f^{-1}(q)$ consists of at most one point) and given the quotient topology, into a vector bundle over the base space of $\eta$. To see this, it suffices to note the image of $f$ is a subbundle of the extension space of $\eta$, so we may choose a frame $s_1, \dots, s_k$ locally mapping into the image. This frame may then be extended locally to a frame on entire fibres of $\eta$, and these added sections, once projected into the quotient space, constitute a trivialization of the cokernel bundle.
\end{example}

\begin{example}
    If $(\xi,E)$ is a bundle on $X$, and $f: Y \to X$ is continuous, we can define an \emph{induced bundle} $f^*(\xi)$ on $X$, by letting the extended space be the fibre product
    %
    \[ Y \times_X E = \{ (y,v) \in Y \times E: f(y) = \pi_\xi(v) \} \]
    %
    If $s_1, \dots, s_n: U \to E$ is a local frame for $\xi$, then the maps $t_k(x) = (x,(s_k \circ f)(x))$ is a local frame on $f^*(\xi)$, showing $f^*(\xi)$ is locally trivial. If $(\eta,F)$ is another bundle on $Y$, with a bundle map $(\phi,\phi_\sharp): \eta \to \xi$, such that $\phi_\sharp = f$, then there is a unique map $\phi \times_X \pi_\eta: F \to Y \times_X E$ with the standard commuting properties, and one can check fairly easily that this is a bundle map, which gives $f^*(\eta)$ a universal property. This universal property can be used fairly simply to check that $g^*(f^*(\pi_\xi))$ is equivalent to $(f \circ g)^*(\pi_\xi)$.
\end{example}

\begin{example}
    If $N$ is a submanifold of another manifold $M$, there are two natural vector bundles on $N$. The first is the tangent bundle $N$. The second is the tangent bundle $TM|_N$. The map $i: N \to M$ induces an injective map $i_*: TN \to TM|_N$ which means we can view $TN$ as a subbundle of $TM|_N$. We call the quotient $TM|_N/TN$, denoted $(TN)^\perp$, the \emph{normal bundle} to $N$ in $M$, since vectors in the quotient can in the case where $M = \RR^n$ (or a Riemannian manifold) be identified with the bundle formed by vectors in $TM$ forming the orthogonal complement to $TN$ at each point.
\end{example}

\begin{example}
    If $\xi$ and $\eta$ are bundles over the same base space $X$, then we can define a new bundle $\xi \oplus \eta$ over $X$ called the \emph{Whitney sum} of the bundles, which using our previous exposition can be most easily described as $\Delta^*(\pi \times \xi)$, where $\Delta: X \to X \times X$ is the diagonal map $\Delta(x) = (x,x)$. Thus it is easy to see it is a vector bundle, and it has the unique property that for any bundle maps $\nu \to \xi$ and $\nu \to \eta$, there is a unique map $\nu \to \xi \oplus \eta$.
\end{example}

\begin{example}
    If $\xi$ is a subbundle of $\eta$, we can form the quotient bundle $\eta/\xi$ by taking the quotient vector space on each fibre. If we take a local frame $s_1, \dots, s_n$ for $\xi$, and extend it to a local frame on $\eta$ by taking additional sections $t_1, \dots, t_n$. Then the projections $\pi \circ t_i$ constitute a local frame for the quotient. The construction of these sections also shows that $\eta$ is isomorphic to the Whitney sum $\eta/\xi \oplus \xi$.
\end{example}

The main sections we see in differential geometry are sections of the tangent bundle on the manifold, and we call these sections \emph{vector fields}. On a smooth vector bundle, we can consider smooth sections, and the smooth vector fields are the vector fields we will really care about. On vector bundles, we denote sections by lower case letters like $s$ or $t$, whereas a vector field is often denoted by capital letters like $X$, $Y$, or $Z$, and the value of a vector field $X$ on a manifold $M$ at a point $p \in M$ is $X_p \in T_p $. Vector fields are so important that we often denote a single vector in the tangent bundle as $X$, as well, which could get confusing if we forget to distinguish the two, but often vector fields act just like vectors in the same was that real valued functions `act' like numbers, so this doesn't cause a problem. Vector fields form a vector space. Locally, around a chart $(x,U)$, we may express the vector field in terms of the basis $\smash{X_p = \sum a^i(p) \partial/\partial x^i (p)}$. This vector field is differentiable or continuous if and only if the functions $a^i$ are smooth or continuous.

The space $\Gamma(TM)$ of all smooth vector fields is itself a vector space, an algebra over $C^\infty(M)$. If $X \in \Gamma(TM)$, and $f \in C^\infty(M)$, then we define a new function $X f \in C^\infty(M)$ by setting $(Xf)(p) = X_p(f)$. What's more, we have the elegant equation $X(fg) = f X(g) + g X(f)$, which expresses the pointwise derivation property globally. In general, a \emph{derivation} is a map $F: A \to A$ on an algebra such that $F(ab) = a F(b) + b F(a)$. If $F: C^\infty(M) \to C^\infty(M)$ is any derivation, then we may then define a vector field $X$ such that $X_p$ is the unique vector satisfying $X_p(f) = F(f)(p)$, and it is easily verified that $X_p \in M_p$, and that $X$ itself is a smooth vector field. This vector field is the unique field which generates a derivation on $C^\infty(M)$, for if locally, $X = \sum a^i \frac{\partial}{\partial x^i}$, then $a^i(p) = X_p(x^i) = F(x^i)(p)$. The derivation corresponding to $X$ is $F$, so $C^\infty$ vector fields and derivations on $C^\infty(M)$ are in one to one correspondence.

\section{Tangents on Manifolds with Boundary}

Given a manifold $M$ with boundary, it is easy to define the tangent bundle on the interior of the manifold, but it is less clear what the fibres of the bundle should look like on the boundary; should they have the same dimension as on the interior, or one dimension less? We shall find it is most convenient, suprisingly, to make the tangent space at the boundary as the dimension of the space on the interior.

The safest option is to look at the space of derivations at points on the boundary of the manifold, which are defined exactly the same as on any manifold without boundary. The locality properties apply, and so it suffices to determine the space $V$ of derivations on $\mathbf{H}^n$. Surely we have the partial derivatives $\partial/\partial x^1, \dots, \partial / \partial x^{n-1}$, so the space is at least $n-1$ dimensional. But perhaps suprisingly, the partial derivative operator $\frac{\partial}{\partial x^n}$ is also still well defined -- to calculate it, we take some $f \in C^\infty(M)$, and extend to some differentiable $\tilde{f}: \mathbf{R}^n \to \mathbf{R}$, and then take partial derivatives. The value we obtain is independent of extension, because we can also calculate the partial derivative as the limit of the quantities $(f \circ x^{-1})(te_n)/t$, as $t \downarrow 0$ from above. Arguing the same way as in our calculation of the space of derivations in $\mathbf{R}^n$, we may take Taylor series to determine that these partial derivatives span the space of all derivations. Thus, viewing $TM$ as the space of derivations over every point, we find that $T_pM$ is $n$ dimensional at the boundary.

An additional feature of the tangent bundle on the boundary of a manifold is we can identify some vectors as `outward' pointing, `inward' pointing, and parallel to the manifold at the boundary. For a chart $(x,U)$ at the boundary, we can write any derivation at $p \in U$ as
%
\[ \sum a_i \frac{\partial}{\partial x^i} \]
%
Recalling that $\mathbf{H}^n = \{ x: x_n \geq 0 \}$, we say the derivation is outward pointing if $a_n < 0$, and inward pointing if $a_n > 0$, and parallel to the manifold if $a_n = 0$. This is well defined even when we vary coordinate systems, because if $y \circ x^{-1}: U \to V$ is a diffeomorphism, where $U,V \subset \mathbf{H}^n$, then by invariance of domain we know that
%
\[ y \circ x^{-1}(t_1, \dots, t_{n-1}, 0) = (f^1(t_1, \dots, t_{n-1}), \dots, f^{n-1}(t_1, \dots, t_{n-1}), 0) \]
%
so that for $p \in \partial M$,
%
\[ \left.\frac{\partial y^n}{\partial x^i}\right|_p = \begin{cases} 0 &: i \neq n \\ \text{positive} &: i = n \end{cases} \]
%
where the fact for $i = n$ follows because $y^n$ is always non-negative, and $(y^n \circ x^{-1})(x(p) + te^n) \to 0$ as $t \downarrow 0$, so that the function $t \mapsto (y^n \circ x^{-1})(p + te_n)$ must be an increasing function in a suitably small half neighbourhood of the origin. Thus we find that if a vector $v$ can be written
%
\[ \sum a_i \frac{\partial}{\partial x^i} = \sum b_j \frac{\partial}{\partial y^j} \]
%
then
%
\[ b_n = \sum a_k \frac{\partial y^n}{\partial x_k} = a_n \frac{\partial y^n}{\partial x^n} \]
%
and we see that the sign of $b_n$ and $a_n$ agree.

\begin{remark}
    In particular, we note that one can use this process to find a globally defined smooth vector field $X$ on $TM|_{\partial M}$ which points outward at each point, since we can form a field locally in each coordinate system and add that up using a partition of unity, noting that outward pointing vectors do not cancel one another out.
\end{remark}

\section{Universality of the Tangent Bundle}

For a map $f:M \to N$, the map $f_*: TM \to TN$ is meant to be a sufficient generalization of the derivative operator on Euclidean space. The fact that this is a `universal' generalization can in fact be proved, once we introduce a categorical viewpoint. Note that the association of $M$ with $TM$ and $f$ with $f_*$ is a {\it functor} from the category of $C^\infty$ manifolds to the category of smooth vector bundles, such that
%
\begin{itemize}
    \item The bundle $T\mathbf{R}^n$ is isomorphic to $\varepsilon^n(\mathbf{R}^n)$ by a trivialization $t_n$ such that for any map $f: \mathbf{R}^n \to \mathbf{R}^m$, the diagram
    %
%    \begin{center}
%    \begin{tikzcd}
%        T\mathbf{R}^n \arrow{d}{f_*} \arrow{r}{t_n} & \varepsilon^n(\mathbf{R}^n) \arrow{d}{\text{old\ $f_*$}}\\
%        T\mathbf{R}^m \arrow{r}{t_m} & \varepsilon^m(\mathbf{R}^m)
%    \end{tikzcd}
%    \end{center}
    %
    commutes, where $\text{old $f_*$}$ is defined by $(p,v) \mapsto (f(p), Df(p)(v))$.
    \item If $U \subset M$ is an open submanifold, there are equivalences
    %
    \[ u_{U,M}: TU \to (TM|_U) \]
    %
    such that if $i: U \to M$ is the inclusion map, then
    %
%    \begin{center}
%    \begin{tikzcd}
%        & (TU|_M) \arrow{rd} &\\
%        TU \arrow{ru}{u_{U,M}} \arrow{rr}{i_*} & & TM
%    \end{tikzcd}
%    \end{center}
    %
    commutes, and for any differentiable $f:M \to N$, we have a diagram
    %
%    \begin{center}
%    \begin{tikzcd}
%        & TU \arrow{ld}{i_*} \arrow{rd}{(f|_U)_*} &\\
%        TM \arrow{rr}{f_*} & & TN
%    \end{tikzcd}
%    \end{center}
\end{itemize}
%
The first bullet says that the functor, restricted to Euclidean spaces, is naturally equivalent to the functor which associates $\mathbf{R}^n$ with $\varepsilon^n(\mathbf{R}^n)$ and $f: \mathbf{R}^n \to \mathbf{R}^m$ with the old definition of the differential. The second says that the family of maps $u_{U,M}$ is a natural transformation between the tangent functor and the restriction of the functor to open submanifolds.

We shall verify that any functor satisfying these properties is unique up to a natural equivalence. We shall first explore the properties of functors satisfying the properties above. So until necessary, we will let $M \to TM$ stand for such a functor. These properties will be obvious for the standard tangent functor, but as long as we don't use any facts about our constructed tangent bundle, the theorem will remain true for any of the other functors.

Given a chart $(x,U)$ on a manifold $M$, we have a chain of isomorphisms
%
\[ (TM)|_U \xleftarrow{u_{U,M}} TU \xrightarrow{x_*} Tx(U) \xrightarrow{u_{x(U), \mathbf{R}^n}} (T\mathbf{R}^n)|_{x(U)} \xrightarrow{t_n|_{x(U)}} \varepsilon^n(x(U)) \]
%
Define $\alpha_x: (TM)|_U \to \varepsilon^n(x(U))$ to be the chain of compositions. We shall try and understand the properties of $\alpha_x$ independent of the properties of our tangent bundle, because if we have some other functor $M \mapsto T'M$, with other equivalences $t_n'$ and $u_{U,M}'$, then we have $\beta_x: (T'M)|_U \to \varepsilon^n(x(U))$ defined exactly as $\alpha_x$ is defined, and we can consider $\beta_x^{-1} \circ \alpha_x: (TM)|_U \to (T'M)|_U$. Provided that this map is independent of $x$, we can put the maps together for all $x$, and obtain an equivalence between $TM$ and $T'M$.

\begin{lemma}
    If $V \subset U$ is open, and $y = x|_V$, then $\alpha_y = (\alpha_x)|_V$.
\end{lemma}
\begin{proof}
    Denote the inclusion maps by $i: U \to M$, $\iota: V \to M$, $j : V \to U$, and $k : y(V) \to x(U)$. Then consider the diagram
    %
%    \begin{center}
%    \begin{tikzcd}
%        (TM)|_U \arrow[bend left=20]{rrrr}{\alpha_x} & TU \arrow{l}[above]{u_{U,M}} \arrow{r}{x_*} & T(x(U)) \arrow{r}{u_{x(U), \mathbf{R}^n}} & (T\mathbf{R}^n)|_{x(U)} \arrow{r}{(t_n)|_{x(U)}} & \varepsilon^n(\mathbf{R}^n)|_{x(U)}\\
%        (TM)|_V \arrow[bend right=20]{rrrr}{\alpha_y} \arrow{u} & TV \arrow{u}{j_*} \arrow{l}{u_{V,M}} \arrow{r}{y_*} & T(y(V)) \arrow{u}{k_*} \arrow{r}{u_{y(V), \mathbf{R}^n}} & (T\mathbf{R}^n)|_{y(V))} \arrow{r}{(t_n)|_{y(V)}} \arrow{u}{} & \varepsilon^n(\mathbf{R}^n)|_{y(V)} \arrow{u}{}\\
%    \end{tikzcd}
%    \end{center}
    %
    We verify commutativity by verifying the commutativity of each square. The first square follows by breaking the diagram into triangles.
    %
%    \begin{center}
%    \begin{tikzcd}
%        TM|_U \arrow{rd} & & TM|_V \arrow{ll} \arrow{ld}\\
%        & TM &\\
%        TU \arrow{ru}{i_*} \arrow{uu}{u_{U,M}} & & TV \arrow{lu}[above]{\iota_*} \arrow{ll}{j_*} \arrow{uu}[right]{u_{V,M}}
%    \end{tikzcd}
%    \end{center}
    %
    These subtriangles commutes by the universal properties of the functor, and these gives us the commutativity of the square. The second square follows by functoriality, because $x \circ j = k \circ y$. The third square commutes for the same reason the first square commutes, and the last square obviously commutes.
\end{proof}

\begin{lemma}
    If $A \subset \mathbf{R}^n$ and $B \subset \mathbf{R}^m$ are open, and $f: A \to B$ is $C^\infty$, then the diagram
    %
%    \begin{center}
%    \begin{tikzcd}
%    TA \arrow{r}{u_{A,\mathbf{R}^n}} \arrow{d}{f_*} & (T\mathbf{R}^n)|_A \arrow{r}{t_n|_A} & \varepsilon^n(\mathbf{R}^n)|_A \arrow{d}{\text{old $f_*$}} \\
%    TB \arrow{r}{u_{B,\mathbf{R}^m}} & (T\mathbf{R}^m)|_B \arrow{r}{t_m|_B} & \varepsilon^m(\mathbf{R}^m)|_B
%    \end{tikzcd}
%    \end{center}
    %
    commutes.
\end{lemma}
\begin{proof}
    First, assume that $f$ can be extended to a map $g: \mathbf{R}^n \to \mathbf{R}^m$. Denoting inclusions by $i: A \to \mathbf{R}^n$ and $j : B \to \mathbf{R}^m$. Then we have a huge diagram, whose subtriangles and subsquares all obviously commute.
    %
%    \begin{center}
%    \begin{tikzcd}
%       & (T\mathbf{R}^n)|_A \arrow{r}{t_n|_A} \arrow{d} & \varepsilon^n(\mathbf{R}^n)|_A \arrow{d} \arrow[bend left=80]{dd}{\text{old}\ f_*}\\
%    TA \arrow{r}[below]{i_*} \arrow{ru}{u_{A,\mathbf{R}^n}} \arrow{d}{f_*} & T\mathbf{R}^n \arrow{d}[left]{g_*} \arrow{r}{t_n} & \varepsilon^n(\mathbf{R}^n) \arrow{d}{\text{old $g_*$}}\\
%    TB \arrow{r}{j_*} \arrow{dr}[below left]{u_{B,\mathbf{R}^m}} & T\mathbf{R}^m \arrow{r}{t_m}     & \varepsilon^m(\mathbf{R}^m)\\
%       & (T\mathbf{R}^m)|_B \arrow{r}{t_m|_B} \arrow{u} & \varepsilon^m(\mathbf{R}^m)|_B \arrow{u}
%    \end{tikzcd}
%    \end{center}
    %
    and this diagram contains the other diagram as a subdiagram, since we know that $\text{old}\ g_*$ and $\text{old}\ f_*$ agree on $A$ (since $A$ is an open subset), so that the theorem is proved in this case.

    In general, we might not be able to extend the entire map $f$ to all of $\mathbf{R}^n$, but we can at least find a function $g$ for each $p \in A$ such that $g$ agrees with $f$ on a neighbourhood $A' \subset A$ of $p$. Then we have a commutative diagram
    %
%    \begin{center}
%    \begin{tikzcd}
%        TA \arrow{rr}{u_{A,\mathbf{R}^n}} \arrow[bend right=50]{ddd}[left]{f_*} & & (T\mathbf{R}^n)|_A \arrow{r}{t_n|_A} & \varepsilon^n(\mathbf{R}^n)|_A \arrow[bend left=50]{ddd}[right]{\text{old $g_*$}}\\
%        & (TA)|_{A'} \arrow{lu}\\
%        TA' \arrow{ru}{u_{A',A}} \arrow{uu}{i_*} \arrow{d}{(f|_{A'})_*} \arrow{rr}{u_{A', \mathbf{R}^n}} & & (T\mathbf{R}^n)|_{A'} \arrow{uu} \arrow{r}{t_n|_{A'}} & \varepsilon^n(\mathbf{R}^n)|_{A'} \arrow{uu} \arrow{d}[left]{\text{old}\ (f|_{A'})_*}\\
%        TB \arrow{rr}{u_{B,\mathbf{R}^m}} & & (T\mathbf{R}^m)|_{B} \arrow{r}{t_m|_B} & \varepsilon^m(\mathbf{R}^m)|_B
%    \end{tikzcd}
%    \end{center}
    %
    where $i: A' \to A$ is the inclusion map. Every subshape but the top left and bottom rectangle obviously commutes. First, note that the bottom rectangle is just an instance of this theorem, but where we can extend our map to all of $\mathbf{R}^n$, so we have already argued it's commutativity. To see that the top left square commutes, we extend it to a larger diagram. Defining $j: A \to \mathbf{R}^n$ to be the inclusion, we have a diagram
    %
%    \begin{center}
%    \begin{tikzcd}
%        & T\mathbf{R}^n\\
%        TA \arrow{ru}{j_*} \arrow{rr}{u_{A,\mathbf{R}^n}} &  & (T\mathbf{R}^n)|_A \arrow{lu} \\
%        TA' \arrow{u}{i_*} \arrow{rr}{u_{A', \mathbf{R}^n}} & & (T\mathbf{R}^n)|_{A'} \arrow{u}
%    \end{tikzcd}
%    \end{center}
    %
    To prove that $u_{A,\mathbf{R}^n} \circ i_* = u_{A',\mathbf{R}^n}$, it suffices to prove that $j_* \circ i_* = u_{A', \mathbf{R}^n}$, because $j_*$ is equal to $u_{A,\mathbf{R}^n}$ when viewed as functions without a specified codomain. But $j \circ i$ is just the inclusion of $A'$ in $\mathbf{R}^n$, so this fact is obvious. We have verified enough commutativity to conclude that the composition
    %
    \[ TA \xrightarrow{f_*} TB \xrightarrow{u_{B,\mathbf{R}^m}} (T\mathbf{R}^m)|_B \xrightarrow{t_m|_B} \varepsilon^m(\mathbf{R}^m)|_B \]
    %
    is equal to the composition
    %
    \[ TA \xrightarrow{u_{A,\mathbf{R}^n}} (T\mathbf{R}^n)|_A \xrightarrow{t_n|_A} \varepsilon^n(\mathbf{R}^n)|_A \xrightarrow{\text{old\ $g_*$}} \varepsilon^m(\mathbf{R}^m)|_B \]
    %
    on $TA|_{A'}$, and since $\text{old}\ g_*$ is equal to $\text{old}\ f_*$ on $(TA)|_{A'}$, we have verified the theorem over $A'$. Since $A'$ was arbitrary, we obtain commutativity over all of $A$.
\end{proof}

\begin{lemma}
    If $(x,U), (y,V)$ are two coordinates charts with $p \in U \cap V$, then $\beta_y^{-1} \circ \alpha_y$ and $\beta_x^{-1} \circ \alpha_x$ agree at $p$.
\end{lemma}
\begin{proof}
    We may assume $U = V$, because our first lemma shows the theorem is true in general otherwise. Assuming this is true, we consider the diagram
    %
%    \begin{center}
%    \begin{tikzcd}
%        & & T(x(U)) \arrow{r}{u_{x(U),\mathbf{R}^n}} \arrow{dd}{(y \circ x^{-1})_*} & (T\mathbf{R}^n)_{x(U)} \arrow{r}{t_n|_{x(U)}} & \varepsilon^n(\mathbf{R}^n)|_{x(U)} \arrow{dd}{\text{old}\ (y \circ x^{-1})_*}\\
%        (TM)|_U & TU \arrow{l}{u_{U,M}} \arrow{ru}{x_*} \arrow{rd}{y_*}\\
%        & & T(y(U)) \arrow{r}{u_{y(U),\mathbf{R}^n}} & (T\mathbf{R}^n)|_{y(U)} \arrow{r}{t_n|_{y(U)}} & \varepsilon^n(\mathbf{R}^n)|_{y(U)}
%    \end{tikzcd}
%    \end{center}
    %
    The triangle obviously commutes, and the rectangle commutes by the second lemma. This implies that $\alpha_y = \text{old}\ (y \circ x^{-1})_* \circ \alpha_x$, and $\beta_y = \text{old}(y \circ x^{-1})_* \circ \beta_x$. The desired result follows immediately.
\end{proof}

Putting together all $\beta_x^{-1} \circ \alpha_x: (TM)|_U \to (T'M)|_U$, we now have a well defined equivalence $e_M$ from $TM$ to $T'M$. Now we need only prove that this is in fact a natural equivalence -- that is, for any $f: M \to N$, $e_N \circ f_* = f_\sharp \circ e_M$. Let's prove this first for $f: \mathbf{R}^n \to \mathbf{R}^m$. First, note that $e_{\mathbf{R}^n} = (t_n')^{-1} \circ t_n$, because most of the maps involved in the construction of the equivalence are trivial. The properties of the tangent map imply that the squares and triangles of the diagram
%
%\begin{center}
%\begin{tikzcd}
%    T\mathbf{R}^n \arrow{r}{t_n} \arrow[bend left=30]{rr}{e_{\mathbf{R}^n}} \arrow{d}{f_*} & \varepsilon^n(\mathbf{R}^n) \arrow{d}{\text{old}\ f_*} & T'\mathbf{R}^n \arrow{l}[above]{t'_n} \arrow{d}{f_\sharp}\\
%    T'\mathbf{R}^m \arrow[bend right=30]{rr}{e_{\mathbf{R}^m}} \arrow{r}{t'_m} & \varepsilon^m(\mathbf{R}^m) & T\mathbf{R}^m \arrow{l}[above]{t'_m}
%\end{tikzcd}
%\end{center}
%
commute, and this shows the naturality equation holds. Second, note that $e_M \circ i_* = i_\sharp \circ e_U$, where $i: U \to M$ is the inclusion of an open submanifold of $M$, where $(x,U)$ is a chart, because $e_M$ is essentially formed by putting together all $i_\sharp \circ e_M \circ i_*^{-1}$. Similarily, we find that if $(x,U)$ is a chart, then $e_{x(U)} \circ x_* = x_\sharp \circ e_U$. Putting all this together, we prove that, given $f:M \to N$, $e_N \circ f_* = f_\sharp \circ e_M$ holds at a neighbourhood of each point. Fixing some point $p$, we choose a chart $(x,U)$ containing the point, and a chart $(y,V)$ containing $f(p)$, such that $f(U) \subset V$. We may assume $x(U) = \mathbf{R}^n$ and $y(V) = \mathbf{R}^m$. If $g = y \circ f \circ x^{-1}$, then
%
%\begin{center}
%\begin{tikzcd}
%    T\mathbf{R}^n \arrow[bend left=20]{rrrrr}{g_*} \arrow{d}{e_{\mathbf{R}^n}} & TU \arrow{l}{x_*} \arrow{r}{i_*} \arrow{d}{e_U} & TM \arrow{d}{e_M} \arrow{r}{f_*} & TN \arrow{d}{e_N} & TV \arrow{d}{e_V} \arrow{l}{j_*} \arrow{r}{y_*} & T\mathbf{R}^m \arrow{d}{e_{\mathbf{R}^m}}\\
%    T'\mathbf{R}^n \arrow[bend right=20]{rrrrr}{g_\sharp}  & T'U \arrow{l}{x_\sharp} \arrow{r}{i_\sharp} & T'M \arrow{r}{f_\sharp} & T'N & T'V \arrow{l}{j_\sharp} \arrow{r}{y_\sharp} & T'\mathbf{R}^m
%\end{tikzcd}
%\end{center}
%
Then we have justified that all the squares in the diagram but the middle one commute, as do the top and bottom triangles, and the rectangle as a whole (in the sense that $e_{\mathbf{R}^m} \circ g_* = g_\sharp \circ e_{\mathbf{R}^n}$). But a final diagram chase justifies that $f_\sharp \circ e_M$ is equal to $e_N \circ f_*$ on $TM|_U$. Since $(x,U)$ was arbitrary, we have shown that the property is a natural equivalence in full. Thus

\begin{theorem}
    There is a unique functor from the category of $C^\infty$ manifolds to the category of vector bundles which satisfies the bullet point properties we denoted at the beginning of the section.
\end{theorem}

The beauty of this categorical proof is that it depends on very little of the structure of $C^\infty$ manifolds. The proof easily extends to $C^k$ manifolds, and even to $C^\omega$ manifolds. It also shows uniqueness of association to $C^\infty$ vector bundles, and effectively settles the question of how well the tangent bundle represents the linear property of differentiable maps on spaces.

\section{Orientation}

The key idea of differential geometry is that classical geometric concepts (which do not `really' hold ground rigorously) can be given a rigorous standing when reinterpreted as some structure on the tangent bundle. These are normally just simple extensions of linear algebraic constructions, applied over each fibre of the tangent bundle. The first, easiest concept to introduce, is orientation. In a {\it real} vector space $V$, each tuple $(v_1, \dots, v_n)$ gives rise to a linear isomorphism $T: \mathbf{R}^n \to V$ such that $T(e_i) = v_i$. Given another basis $(w_1, \dots, w_n)$, we obtain another isomorphism $S: \mathbf{R}^n \to V$ with $S(e_i) = v_i$, and therefore a linear operator $T \circ S^{-1}: \mathbf{R}^n \to \mathbf{R}^n$. The determinant of this operator is non-zero and therefore positive or negative. We say these tuples are {\it equally oriented} if the determinant of this operator is positive, and {\it oppositely oriented} if the determinant is negative. This divides the bases of $V$ into two equivalence classes, and an \emph{orientation} for $V$ is a choice of one of these classes. The equivalence class of some basis $(v_1, \dots, v_n)$ shall be denoted $[v_1, \dots, v_n]$, so that if $V$ is an oriented vector space with fixed orientation $\mu$, then $(v_1, \dots, v_n)$ is oriented if and only if $[v_1, \dots, v_n] = \mu$. Given two vector spaces $V$ and $W$ with a fixed orientation, a linear isomorphism $T: V \to W$ is orientation preserving if $T$ maps oriented bases $(v_1, \dots, v_n)$ in $V$ to an oriented bases $(Tv_1, \dots, Tv_n)$ in $W$. Either an isomorphism is orientation preserving or orientation reversing -- it maps oriented bases $(v_1, \dots, v_n)$ to unoriented bases $(Tv_1, \dots, Tv_n)$.

The key reason that orientation exists is that the determinant of an operator is always positive or always negative. We see the same phenomenon occur when considering equivalences $f: \varepsilon^n(X) \to \varepsilon^n(X)$, which can be written
%
\[ f(p,v) = \left(p, \sum a_{ij}(p) v^i e_j \right) \]
%
where the $a_{ij}: X \to \mathbf{R}$ change continuously, and since the matrix $(a_{ij})$ is always invertible, it's determinant is either always positive or always negative. Thus we may define an orientation to the bundle $\varepsilon^n(X)$ as a choice of orientation on each fibre, such that every equivalence of the form is orientation preserving on all fibres, or orientation reversing. Our reasoning shows that there are only two choices of orientation on $\varepsilon^n(X)$.

Now on an arbitrary vector bundle, defining such a vector bundle may be impossible, because we have to patch local equivalences up across the whole space. An orientation for an arbitrary bundle $\pi: E \to B$ is a choice of orientation $\mu_p$ on each fibre $B_p$, such that any local trivialization $t: \pi^{-1}(U) \to U \times \mathbf{R}^n$ around a connected set $U$ is either orientation preserving on all fibres, or orientation reversing on all fibres. To verify that a particular orientation is `consistant', we need only verify it for a set of connected open sets which cover the bundle, because trivializations are already orientation reversing or preserving when they are locally trivialized, as we just calculated noted. A bundle is called orientable if it has an orientation, and a differentiable manifold is called orientable if its tangent bundle is orientable. An oriented manifold is a manifold with a fixed orientation on its tangent bundle, and if $f: M \to N$ is a local diffeomorphism between oriented manifolds of the same dimension, we say $f$ is orientation preserving if $f_*: TM \to TN$ is orientation preserving on each fibre.

\begin{example}
Since $T\mathbf{R}^n$ is equivalent to $\varepsilon^n(\mathbf{R}^n)$, Euclidean space is an orientable manifold, with a canonical orientation induced by the canonical choice of basis, the unit vectors $\mu_p = [(e_1)_p, \dots, (e_n)_p]$. If $U$ is an open subset of $\mathbf{R}^n$, then $TU$ can be embedded in $T\mathbf{R}^n$, and we can define an orientation $\mu_p$ in the same way. More generally, if $\pi: E \to B$ is an orientable bundle, and if $F \subset E$, then the bundle $\pi|_F$ is also orientable.
\end{example}

\begin{example}
The spheres $S^n$ are all orientable -- to obtain the orientation, we note that, viewing $TS^n$ as a subset of $\varepsilon^{n+1}(\mathbf{R}^{n+1})$, we see that $p_p \not \in TS^n$ for any $p \in S^n$. We may then define an orientation on $S^n$ by letting $[v_1, \dots, v_n]$ be an oriented basis at $p$ if $[p,v_1, \dots, v_n]$ is oriented in $\mathbf{R}^n$. More generally, if $M$ is a manifold with boundary with orientation $\mu$, then $\partial M$ has a natural orientation by saying a basis $(v_1, \dots, v_{n-1})$ of $\partial M_p$ is orientable if $[w, v_1, \dots, v_{n-1}] = \mu$ for any outward pointing vector $w \in M_p$. If we consider the orientation on $\mathbf{R}^{n-1}$ as a subset of $\mathbf{H}^n$, we see that we obtain $(-1)^n$ times the standard orientation. The reason for this choice will become clear when we talk about integration on manifolds, in which orientation plays a key role.
\end{example}

\begin{example}
For any manifold $M$, $TM$ is always orientable as a differentiable manifold. We calculate the transition map between two charts induced by the coordinate maps $(x,U)$ and $(y,V)$ on $M$ to be
%
\begin{align*}
    (y^1, \dots, &y^n, \dot{y}^1, \dots, \dot{y}^n) \circ (x^1, \dots, x^n, \dot{x}^1, \dots, \dot{x}^n)^{-1}[p, v]\\
    &= \left[ (y \circ x^{-1})(p), \sum_{i,j} v^j \left. \frac{\partial y^i}{\partial x^j} \right|_p e_i \right]
\end{align*}
%
and so the matrix of partial derivatives is
%
\[ M = \begin{pmatrix} \left( \frac{\partial y^i}{\partial x^j} \right) & 0 \\ X & \left( \frac{\partial y^i}{\partial x^j} \right) \end{pmatrix} = \begin{pmatrix} N & 0 \\ X & N \end{pmatrix} \]
%
Essentially, we have found that
%
\[ \frac{\partial \dot{y}^i}{\partial \dot{x}^j} = \frac{\partial y^i}{\partial x^j} \]
%
Now
%
\[ \begin{pmatrix} N^{-1} & 0 \\ 0 & I_n \end{pmatrix} \begin{pmatrix} N & 0 \\ X & N \end{pmatrix} \begin{pmatrix} I_n & 0 \\ 0 & N^{-1} \end{pmatrix} = \begin{pmatrix} I_n & 0 \\ X & I_n \end{pmatrix} \]
%
Taking determinants on both sides, we find
%
\[ \frac{\det(M)}{\det(N)^2} = 1 \]
%
and so $\det(M) = \det(N)^2 > 0$. This implies that the family of charts $(x,\dot{x})$ have consistant orientations, and so together they give us an orientation on $T(TM)$. Here we have used the fact that if we have a family of chart $\{ (x_\alpha, U_\alpha) \}$ covering a manifold $M$, and
%
\[ \det \left(\frac{\partial x^i_\beta}{\partial x^j_\alpha}\right) \]
%
is always positive, then the orientations induced from the $(x_\alpha)_*$ combine to give an orientation on $TM$.
\end{example}

We can view orientation, in another language, through the tool of frames, left to the reader to prove. A choice $\mu_p$ of orientation on a bundle $\pi: E \to B$ is consistant if and only if every local frame is either always orientation preserving or orientation reversing on each fibre (we assume the frame has some particular ordering). This is just a restatement of the duality between local frames and trivializations.

\begin{example}
    If $\pi: E \to B$ is a bundle with an orientation $\mu$, and $f: X \to E$, then $f^*(\pi) = \xi: F \to X$ is orientable, which we can obtain by making the bundle map $g: F \to E$ orientation preserving on each fibre. Of course, $f^*(\pi)$ might be orientable even if $\pi$ is not orientable. But, in the special case where we consider $\pi^*(\pi)$, which is a bundle over $E$, then $\pi^*(\pi)$ is orientable if and only if $\pi$ is, because we can view the bundle $\pi$ as the restriction of the bundle $\pi^*(\pi)$ over the set $\{ v \in E: v\ \text{is a zero vector} \}$, which is isomorphic to $B$.
\end{example}

\begin{example}
    If $\pi: E \to B$ and $\xi: F \to B$ are two vector bundles over the same space which are both orientable, then the Whitney sum $\pi \oplus \xi: K \to B$ is also orientable, because the bundle can be covered by local trivializations which are just products of trivializations on each factor, and so we can easily define an orientation here by saying that if $s_k: U \to E$, $t_k: U \to F$ combine to give a local frame of $\pi \oplus \xi$, then this frame is oriented if either both $s_k$ and $t_k$ are oriented in each bundle, or if bundle is not oriented. On the other hand, if $\pi$ is orientable, but $\xi$ is non-orientable, then $\pi \oplus \xi$ is non-orientable. If $s_k: U \to F$ is a frame, then, after perhaps shrinking the neighbourhood, we may find an oriented frame $t_k: U \to E$, and then the frames combine to give a frame of $\pi \oplus \xi$. We could say $s_k$ is oriented if the frame on $\pi \oplus \xi$ is oriented. However, if $\pi$ is a bundle, then $\pi \oplus \pi$ is {\it always} orientable, because for any finite dimensional vector space $V$, $V \oplus V$ has a natural orientation such that, if $(v_1, \dots, v_n)$ is any basis of $V$, then $(v_1,0), \dots, (v_n,0), (0,v_1), \dots, (0,v_n)$ is an oriented basis of $V \oplus V$. This allows us to give a natural orientation to $\pi \oplus \pi$ on each fibre, and it is not difficult to check this orientation is consistant in trivializations.
\end{example}

An assignment of orientation can also been as a `section', not of the tangent bundle, but of an associated `orientation bundle'. Orientations on a vector space do not form a vector space, so this orientation bundle is not a vector bundle, but instead a member of a more general family called `fibre bundles'. Given a topological space $M$, known as a \emph{model space}, and a \emph{base space} $X$, we consider the family of all \emph{$M$ bundles}, which are topological spaces $E$ together with projection maps $\pi: E \to X$ such that each $x \in X$ has a neighbourhood $U$ such that $\pi^{-1}(U)$ is homeomorphic to $M \times U$ which preserves the basepoint in $U$. Of course, we can form the \emph{trivial $M$ bundle} $\varepsilon(X,M) = M \times X$, but there are certainly more interesting bundles.

The orientation bundle associated with any vector bundle $\xi$ over a space $X$ is a fibre bundle with model space $M = \{ -1, 1 \}$. We let $O(\xi)$ have the extension space $X \times \{ -1, 1 \}$. Let the extension space of $\xi$ be denoted $E$, and the projection map $\pi: E \to X$. We let $\pi_O: X \times \{ -1, 1 \}$ be the projection map on $O(\xi)$. For each $p$, we consider an arbitrary linear isomorphism $S_p: E_p \to \RR^n$. The topological structure on $X \times \{ -1, 1 \}$ is then induced such that, if $t: \pi^{-1}(U) \to U \times \RR^n$ is a trivialization of $\xi$, then $(t_*): U \times \{ -1, 1 \}$ is a trivialization of $U \times \{ -1, 1 \}$, given by
%
\[ (t_*)(x_p) = \text{sgn}(\det(t_p \circ S_p^{-1})) \cdot x_p. \]
%
Given another trivialization $s: \pi^{-1}(U) \to U \times \RR^n$, we have
%
\[ ((s_*) \circ (t_*)^{-1})(x_p) = \text{sgn}(\det(s_p \circ t_p^{-1})) \cdot x_p, \]
%
which is continuous because $\text{sgn}(\det(s_p \circ t_p^{-1}))$ is actually a value independant of $p$, since $s_p \circ t_p^{-1}$ is a continuously varying family of elements of $GL(n)$, hence the determinant is either always positive or always negative. Thus we obtain a topological structure on $X \times \{ -1, 1 \}$ which gives it the structure of a fibre bundle. One can view a choice of orientation on $\xi$ as a continuous section of $O(\xi)$, which results in the following theorem.

\begin{theorem}
    If $X$ is locally connected, then $\xi$ is orientable if and only if there exists a global section $s: X \to O(\xi)$, in which case $O(\xi)$ is equivalent to $\varepsilon(X,M)$, corresponding to the two choices of orientation on $\xi$.
\end{theorem}
\begin{proof}
    Suppose $u: X \to O(\xi)$ is a global section of the orientation bundle. Let $p \in X$ be a point, and consider a trivialization $t: \pi^{-1}(U) \to U \times \RR^n$. Then $t_* \circ x: X \to X \times \{ -1, 1 \}$ is continuous, and therefore there exists $a \in \{ -1, 1 \}$ such that $(t_* \circ u)(p) = a_p$ for all $p \in U$. We let $t$ be orientation preserving if $a = 1$, and orientation reversing if $a = -1$. If $t$ is an orientation preserving trivialization on $U$, and $s$ is another orientation preserving trivialization, then $s$ is oriented if and only if $\det(s_p \circ t_p^{-1}) > 0$, because
    %
    \[ (s_* \circ u) = [(s_*) \circ (t_*)^{-1}] \circ [(t_*) \circ u], \]
    %
    and $(s_*) \circ (t_*)^{-1}$ is the map given by multiplication by $\text{sgn}(\det(s_p \circ t_p^{-1}))$. Thus we see that the orientation we have chosen is consistant.

    Conversely, if $\mu$ is an orientation of $\xi$, we can define a section $u: X \to O(\xi)$ such that if $t: \pi^{-1}(U) \to U \times \RR^n$ is an oriented trivialization, then we let $u(x) = (t_*)^{-1}(1_x)$. This is certainly well defined, since if $s$ is any other oriented trivialization, then $s_* \circ t_*^{-1}$ is the identity map, so
    %
    \[ (s_*)^{-1}(1_x) = (s_*)^{-1} \circ (s_* \circ (t_*)^{-1})(1_x) = (t_*)^{-1}(1_x). \]
    %
    Thus we have a global section of $O(\xi)$.
\end{proof}

\begin{example}
    We have already seen that $O(\varepsilon^n(X))$ is homeomorphic to $X \times \{ -1, 1 \}$, so this provides an argument as to why $\varepsilon^n(X)$ is orientable.
\end{example}

\begin{example}
    If $\xi$ is the M\"{o}bius bundle over $S^1$, with the M\"{o}bius strip $\mathbf{M}$ as the extension space, and the projection map $\pi: \mathbf{M} \to S^1$. Then $O(\xi)$ is equivalent to the fibre bundle $\eta$ with extension space $S^1$ and projection map $\pi(t) = 2t$. We essentially argued that $\xi$ was not orientable by tracing this fibre bundle around the manifold.
\end{example}

\section{Whitney's Embedding Theorem}

We now use our results about tangent spaces and differentiability to show that all compact manifolds can be embedded in low dimensional space. This is a easy version of the general Whitney's embedding theorem, which shows this result is true for all manifolds.

\begin{lemma}
    Any compact manifold is imbeddable in some Euclidean space.
\end{lemma}
\begin{proof}
    Let $M$ be a compact manifold. Then there is a finite set of charts $(x_1,U_1), \dots, (x_n,U_n)$ covering $M$. Since the $U_k$ is locally finite, by the shrinking lemma we may choose open sets $V_k \subset U_k$ with $\overline{V_k} \subset U_k$ which cover $M$. Consider a partition of unity $\psi_k$ equal to 1 on $V_k$, and subordinate to $U_k$, and define
    %
    \[ f(p) = (\psi_1(p) x_1(p), \dots, \psi_n(p) x_n(p), \psi_1(p), \dots, \psi_n(p)) = (f_1(p), \dots, f_n(p)) \]
    %
    then $f$ is an immersion of $M$, because if $p \in V_k$, then in a neighbourhood of $p$ we have $f_k(p) = x_k(p)$, so the partial derivative matrix
    %
    \[ \left( \frac{\partial f^i}{\partial x_k^j} \right) \]
    %
    contains the identity as a submatrix, so the map has full rank. The map is also injective, because if $p \in V_m$, and $\psi_k(p) x_k(p) = \psi_k(q) x_k(q)$ and $\psi_k(p) = \psi_k(q)$ for some $q$, then $\psi_m(q) = \psi_m(p) = 1$, so $q \in U_m$, and $x_m(p) = x_m(q)$, so $p = q$. Since $M$ is compact, the immersion is an imbedding, so $M$ is imbeddable in Euclidean space.
\end{proof}

This theorem already has an application, for we can now use it to show all compact manifolds are embeddable in a fairly low dimensional Euclidean space.

\begin{theorem}[Whitney's Embedding Theorem]
    A compact submanifold $M^n$ of $\mathbf{R}^N$ can be embedded in $\mathbf{R}^{2n+1}$.
\end{theorem}
\begin{proof}
    We say a {\it chord} of $M$ is a vector in $\mathbf{R}^N$ of the form $p - q$, for distinct $p,q \in M$. If $U$ is the open subset of $(p,q) \in M \times M$ with $p \neq q$, then the map $f: U \to S^{N-1}$ given by
    %
    \[ f(p,q) = \frac{p-q}{|p-q|} \]
    %
    is $C^\infty$, and since $2n < N-1$, $f(U)$ has measure zero since all values are critical. Similarily, if $V$ is the open set of vectors $v \in TM \subset T\mathbf{R}^N$ with $|v| \neq 0$, then we have a map $g: V \to S^{N-1}$ defined by $g(v_p) = v/|v|$, and again, since $2n < N-1$, $g(V)$ has measure zero. It follows that $f(U) \cup g(V)$ has measure zero, hence $S^{N-1} - f(U) - g(V)$ is non-empty, and so there exists a vector $v$ not parallel to any chord in $\mathbf{R}^N$, and not parallel to any tangent space. It follows that the projection of $M$ onto a hyperplane perpendicular to $v$ is an injective immersion, and since $M$ is compact, it is an imbedding. Continuing this process, we may imbed $M$ into $\mathbf{R}^{2n+1}$.
\end{proof}

\begin{remark}
    If the submanifold is smooth, the embedding is smooth, but more generally, if the submanifold is only $C^k$ for $k \geq 1$, the embedding is still $C^k$.
\end{remark}

We have proven that all compact manifolds can be embedded in some Euclidean space, so that all compact $n$ dimensional manifolds can be embedded in $\mathbf{R}^{2n+1}$. For non-compact manifolds, this argument only shows that these manifolds can be {\it immersed} in $\mathbf{R}^{2n+1}$, not imbedded. We can also easily turn this theorem into an \emph{approximation result}.

\begin{theorem}
    Let $M$ be a compact, $C^k$ manifold. Then for any $C^k$ map $f: M^n \to \RR^m$, with $m \geq 2n + 1$, and for any $\varepsilon > 0$, there is a $C^k$ \emph{embedding} $g: M^n \to \RR^m$ such that $\| f - g \|_{L^\infty(M)} \leq \varepsilon$.
\end{theorem}
\begin{proof}
    Without loss of generality, we may assume $M \subset \RR^N$ for some $N$. Then we can identify $M$ with the graph of $f$, as a subset of $\RR^N \times \RR^m$. Clearly, it suffices to approximate the projection map $\pi: \RR^{N + m} \to \RR^m$ with an embedding. But this is precisely what we have done in the previous theorem.
\end{proof}

With some additional technicalities, the theorem above generalizes to non-compact manifolds.

\begin{lemma}
    For any manifold $M$, there exists a proper map $\rho: M \to [0,\infty)$.
\end{lemma}
\begin{proof}
    Let $\{ U_k \}$ be a cover of precompact sets, and let $\phi_k$ be a partition of unity for this cover. We then define
    %
    \[ \rho = \sum_{k = 1}^\infty k \phi_k. \]
    %
    If $x \in M$ and $\rho(x) \leq N$, then there must exist some $k \leq N$ such that $x \in U_k$. Thus
    %
    \[ \rho^{-1}[0,N] \subset \bigcup_{k = 1}^N U_k, \]
    %
    The set $\bigcup_{k = 1}^N U_k$ has compact closure, and since $\rho^{-1}[0,N]$ is closed in $M$, it is actually compact. If $K \subset \RR$ is any compact set, then $K \cap [0,\infty)$ is compact, hence closed and bounded, and so there is $N$ such that $K \cap [0,\infty) \subset [0,N]$. But then $\rho^{-1}(K) = \rho^{-1}(K \cap [0,\infty))$ is a closed subset of $\rho^{-1}([0,N])$, which is compact, hence $\rho^{-1}(K)$ is compact.
\end{proof}

\begin{theorem}
    Every $n$ dimensional manifold $M$ is embeddable in $\RR^{2n+1}$.
\end{theorem}
\begin{proof}
    The compact argument case certainly gives an injective \emph{immersion} $f_0:M \to \RR^{2n+1}$. Composing this argument with a diffeomorphism of $\RR^{2n+1}$ with its unit ball, we may assume $|f_0(x)| < 1$ for all $x$. Let $\rho: M \to [0,\infty)$ be a proper map, and consider the map $f(x) = f_0(x) + a \rho(x)$ from $M$ to $\RR^{2n+2}$. Now fix a unit vector $a \in \RR^{2n+2}$, and consider the resulting projection $\pi_a \circ f: M \to \RR^{2n+2}$, i.e. the map
    %
    \[ (\pi_a \circ f)(x) = f(x) - (f(x) \cdot a) \cdot a \]
    %
    Suppose $(\pi_a \circ f)_*$ is not injective at $x \in M$, and let $y \in \pi_a \circ f$. We have
    %
    \[ (\pi_a \circ f)_*(v_x) = f_*(v_x) - (f_*(v_x) \cdot a_y) a_y. \]
    %
    Thus if $(\pi_a \circ f)_*(v_x) = 0$, then $f_*(v_x)$ is a scalar multiple of $a_x$. But $TM$ is a manifold with dimension $2n$, hence by Sard's theorem, this is impossible for almost every $a \in S^{2n+1}$. In particular, this means we may choose $a$ such that $\pi_a \circ f$ is an immersion, and moreover, we may assume that $a$ is not a pole of this $S^{2n+1}$, ie. $a \neq (0,\dots,0,\pm 1)$. But then $\pi_a \circ f$ is now actually a \emph{proper map}. This follows because if $a = (a_0,t)$, with $a_0 \in \RR^{2n+1}$ and $t \in \RR$, the last coordinate of $(\pi_a \circ f)(x)$ has magnitude
    %
    \[ |\rho(x) - [(f_0(x) \cdot a_0) + \rho(x) t] t| = |(1 - t^2) \cdot \rho(x) - t \cdot (f_0(x) \cdot a_0)| \geq (1 - t^2) \rho(x) - t |a_0|. \]
    %
    Thus we conclude
    %
    \[ |(\pi_a \circ f)(x)| \geq (1 - t^2) \rho(x) - t |a_0| \geq (1 - t^2) \rho(x) - 1, \]
    %
    and so if $|(\pi_a \circ f)(x)| \leq M$, then
    %
    \[ \rho(x) \leq \frac{M + 1}{1 - t^2}. \]
    %
    In particular, this enables us to conclude that there exists a constant $A$, and for sufficiently large $R$, if $B_R$ is the closed ball of radius $R$ at the origin, then $(\pi_a \circ f)^{-1}(B_R)$ is a closed subset of $\rho^{-1}([0,B_{AM}])$, and in particular, is compact. From this, we can conclude $\pi_a \circ f$ is proper. But then $\pi_a \circ f$ is a proper and injective, and is therefore a homeomorphism onto its image.
\end{proof}

\begin{remark}
    For a given manifold $M^n \subset \RR^N$, viewing $TM$ as a subset of $T\RR^N$, we set
    %
    \[ S(M) = \{ v_x \in TM : |v| = 1 \}. \]
    %
    It is an easy argument that $S(M)$ is a $2n - 1$ dimensional submanifold of $T\RR^N$. The map $f: S(M) \to S^{N-1}$ given by $f(v_x) = v$ is smooth, and therefore by Sard's theorem, forms a set of measure zero if $2n - 1 < N - 1$, i.e. $2n < N$. If $a \in S^{N-1}$ is not in the image of this map, and it then follows that if $\pi_a$ is given by projection onto the hyperplane orthogonal to $a$, then $\pi_a$ is an immersion of $M$ in $\RR^{N-1}$. In particular, since we can embed any $n$ manifold $M$ in $\RR^{2n+1}$, we can also \emph{immerse} $M$ in $\RR^{2n}$. Generalizing the approximation property established from the embedding theorem, we find that if $M$ is compact, and $N \geq 2n$, then for any smooth map $f: M \to \RR^N$ and any $\varepsilon > 0$, there exists a smooth immersion $g: M \to \RR^N$ with $\| f - g \|_{L^\infty(M)} \leq \varepsilon$.
\end{remark}

Whitney's embedding theorem can be improved to show any manifold $M^n$ can be embedded in $\RR^{2n}$, and \emph{immersed} in $\RR^{2n-1}$. However, the approximation theorem above is tight - there are maps $M^n \to \RR^{2n}$ that cannot be approximated by an embedding. Let us indicate some ideas behind this result.

\begin{theorem}
    Let $M^n$ be a compact manifold. Then there exists a map $f: M^n \to \RR^{2n-1}$ which is an immersion except at finitely many points.
\end{theorem}
\begin{proof}
    Let $f: M \to \RR^{2k}$ be an immersion. Then we obtain a map $f_*: TM \to T\RR^{2k}$. In $\pi(v_x) = v$ is the projection map from $T\RR^{2k}$ to $\RR^{2k}$, then we obtain a map $F = \pi \circ f_*$ from $TM$ to $\RR^{2k}$.

    Let $a \in \RR^{2k}$ be a regular value of this map. We claim that there are finitely many vectors in $F^{-1}(a)$. Fix $a \in \RR^{2k}$ and suppose we can find an infinite sequence $v_i$ based at points $p_i \in M$. By compactness, we may assume that the sequence $p_i$ converges to some $p \in M$. But locally, $f$ is an immersion, so we can switch to a coordinate system $(x,U)$ around $p$ such that for sufficiently large $i$,
    %
    \[ v_i = \left. \frac{\partial}{\partial x^1} \right|_{p_i}. \]
    %
    But this means that the sequence $v_i$ converges to the vector $v = \partial_1|_p$, for which $F(v) = a$. Since $F$ is not injective in a neighbourhood of $v$, it cannot be true that $F$ is an immersion at $v$, so $a$ is not a regular value.

    If $a$ is a regular value, and $\pi_a$ is the projection map from $\RR^{2k}$, onto the hyperplane orthogonal to $a$, then $\pi_a \circ f$ is an immersion except at the base-points of $F^{-1}(a)$, completing the proof.
\end{proof}

The exceptional points to the immersion above are known as \emph{cross caps}. Proving further embedding theorems thus requires a knowledge of this caps.

\chapter{The Cotangent Bundle}

We now begin the real task of differentiable geometry, which is to equip the tangent space of a differentiable manifold with enough structure to begin doing some actual geometry. The first idea is that any natural operation on vector spaces can be applied to vector bundles. The essential idea is phrased more precisely using category theory. A covariant endofunctor $F$ on the category of finite dimensional vector spaces associates with any two vector spaces $V$ and $W$ a map from $\text{Hom}(V,W)$ to $\text{Hom}(F(V),F(W))$, and $F$ is called a {\it continuous} functor if the map from $\text{Hom}(V,W)$ to $\text{Hom}(F(V),F(W))$ is continuous for each $V$ and $W$.

\begin{theorem}
    If $F$ is a continuous endofunctor, and $\xi = \pi_0: E \to B$ is any vector bundle, then there is a bundle $F(\xi) = \pi_1: E' \to B$ for which $E'_p = F(E_p)$, and such that every trivialization $\smash{t: \pi^{-1}(U) \to U \times \mathbf{R}^n}$ corresponds to a trivialization $\smash{F(t): \pi_1^{-1}(U) \to U \times F(\mathbf{R}^n)}$.
\end{theorem}
\begin{proof}
    Given $\xi$, define $E' = \bigcup F(E_p)$, with $\pi_1: E' \to B$ projecting $F(E_p)$ onto $p$. A trivialization $t$ corresponds to a set of linear maps $t_p: E_p \to \mathbf{R}^n$, for each $p$ in some open set $U$, and we define $s$ as the map corresponding to the set of maps $F(t_p): F(E_p) \to F(\mathbf{R}^n)$. We declare a topology on $F(\xi)$ by letting the maps $F(t)$ be trivializations. If $t$ and $s$ are two trivializations on a common open set $U$, then $F(t) \circ F(s)^{-1} = F(t \circ s^{-1})$ corresponds to a map from $U \times F(\mathbf{R}^n)$ to itself, which is a homeomorphism if and only if the map $p \mapsto F((t \circ s^{-1})_p)$ from $U$ to $GL(F(\mathbf{R}^n))$ is continuous. But we know that the map $t \mapsto (t \circ s^{-1})_p$ is continuous, and so the continuity of the corresponding map is also continuous because the functor is continuous. By choosing some isomorphism of $F(\mathbf{R}^n)$ with Euclidean space once and for all, we obtain a vector bundle $F(\xi)$. If $f: \xi \to \eta$ is a bundle map, then it is given by maps $f_p: E_p \to F_p$, for each $p$, and we can obtain a map $F(f): F(\xi) \to F(\eta)$ by taking the union of the $F(f_p)$. The continuity of the functor again guarantees that $F(f)$ remains continuous, and is therefore a bundle map.
\end{proof}

Thus a continuous functor on the category of vector spaces extends uniquely to a functor on the category of vector bundles which acts as the original functor on the fibres of bundle maps. \emph{Smooth functors} can be considered similarly, we induces a functor on the category of smooth vector bundles. A similar process can be carried out when $F$ is a functor of multiple variables, or is contravariant, or is only restricted to manifolds of a fixed dimension. We will take these generalizations for granted in the sequel.

\section{The Dual of a Vector Bundle}

The first object we can equip the tangent bundle is with cotangent vectors, which often operate as differentials in certain geometric arguments. Recall that if $V$ is a vector space, then there is a natural vector space $V^*$ associated with it, known as the {\it dual space} of $V$, which is the space of all linear functionals $f: V \to \mathbf{R}$. If $f: V \to W$ is a linear map, we can define another linear map, the dual $f^*: W^* \to V^*$, by setting $f^*(g) = g \circ f$. Thus the dual can be viewed as a {\it contravariant} functor in the category of vector spaces. If $V$ is finite dimensional, and has some basis $(v_1, \dots, v_N)$, then we can construct a {\it dual basis} $(v_1^*, \dots, v_N^*)$ on $V^*$ defined by $v_n^*(v_m) = \delta_{nm}$. This implies the functor $V \mapsto V^*$ is smooth, for if a map $f$ is given in some pair of bases by a matrix $M$, then $f^*$ will be given by the matrix $M^T$ with respect to the dual bases, and the map $M \mapsto M^T$ is smooth. Thus, given any vector bundle $\xi$, we can associate with it the {\it dual bundle} $\xi^*$, whose elements consist of functionals over a particular fibre.

For any finite dimensional vector space $V$, $V^*$ is isomorphic to $V$. But this is an artificial fact emerging from the fact that $V^*$ has the same dimension as $V$, and there is no `canonical' way to identify $V$ with it's dual. This implies that $\xi^*$ is not always equivalent to $\xi$. However, since $V^{**}$ is naturally isomorphic to $V$, we can always identify $\xi^{**}$ with $\xi$. This example can be generalized, and the proof of the equivalence is no more complicated.

\begin{theorem}
    Let $F$ and $G$ be continuous functors on finite dimensional vector spaces naturally isomorphic to one another. Then the extensions of $F$ and $G$ to functors on vector bundles are naturally isomorphic to one another.
\end{theorem}
\begin{proof}
    Let $\eta$ be a natural isomorphism between $F$ and $G$. Given a bundle $(\xi,E)$, we can define a map $\eta_\xi: F(\xi) \to G(\xi)$, which is an isomorphism on each fibre, by putting together the equivalences $\eta_{F(E_p)}: F(E_p) \to G(E_p)$, for each point $p$. The only tricky part is to verify this map is continuous. For any trivialization $t$ of $\xi$ on $U \subset B$, we have a commutative diagram
    %
    \begin{center}
    \begin{tikzcd}
        U \times F(\mathbf{R}^n) \arrow[bend left=20]{rrr}{\eta_{F(\varepsilon^n(U))}} & F(\pi)^{-1}(U) \arrow{l}{F(t)} \arrow{r}{\eta_\xi} & G(\pi)^{-1})(U) \arrow{r}{G(t)} & U \times G(\mathbf{R}^n)
    \end{tikzcd}
    \end{center}
    %
    The diagram above implies that in order to prove $\eta_\xi$ is continuous, it suffices to prove that $\eta_{F(\varepsilon^n(U))}$ is continuous for each set $U$. But this is just $\eta_{F(\varepsilon^n(U))}(v_p) = \eta_{F(\mathbf{R}^n)}(v)_p$, which is obviously continuous, because it is a constant choice of a linear map. Thus $\eta$ truly does extend to a bundle equivalence, and the naturality is clear.
\end{proof}

The most important case of the dual bundle occurs when we study the tangent bundle $TM$, the dual bundle $(TM)^*$ will be known as the \emph{cotangent bundle} of $M$, which we also denote by $T^*M$. It is a smooth vector bundle, so we can consider smooth sections, which we call {\it covector fields}. Unlike elements of $TM$, it is unwise to think of elements of $T^*M$ as vectors pointing in a particular direction. Instead, covectors \emph{act} on vectors; in particular, given a smooth covector field $\omega$, and a vector field $X$, we can define a function $\omega(X)$ by $\omega(X)(p) = \omega_p(X_p)$. If $\omega$ and $X$ are both smooth fields, then $\omega(X)$ will also be smooth.

\begin{example}
    For any smooth real-valued function $f: M \to \mathbf{R}$, we can define a covector field $df: M \to T^*M$, such that $df_p(v) = v(f)$ for each $v \in T_pM$. The covector field $df$ is known as the {\emph differential} of $f$. If $X$ is a vector field, then $df(X) = X(f)$ is a smooth function.
\end{example}

If $(x,U)$ is a coordinate system on $M$, then the $x^i$ are smooth, and so we can consider the local differentials $dx^i$. Now we calculate
%
\[ dx^i_p \left( \left. \frac{\partial}{\partial x^j} \right|_p \right) = \left. \frac{\partial x^i}{\partial x^j} \right|_p = \delta^i_j. \]
%
This calculation implies that the family of vectors $\{ dx^i_p \}$ are precisely the dual basis of the basis $\{ \left. \partial/\partial x^i\right|_p \}$. This means every covector field $\omega$ can be locally written as
%
\[ \omega = \sum \omega \left( \frac{\partial}{\partial x_i} \right) dx^i = \sum \omega_i dx^i. \]
%
The field $\omega$ is continuous/smooth if on this chart if and only if the $\omega_i$ are smooth/continuous. In particular, since $df(\partial_i) = \partial_i(f)$, we obtain the classical formula
%
\[ df = \sum_{i = 1}^n \frac{\partial f}{\partial x^i} dx^i, \]
%
which holds on $U$. If $\sum \omega_i dx^i = \sum \eta_j dy^j$, then
%
\[ \sum \eta_j dy^j = \sum \eta_j \frac{\partial y^j}{\partial x^i} dx^i. \]
%
Thus we find
%
\[ \omega_i = \sum \eta_j \frac{\partial y^j}{\partial x^i}. \]
%
A smooth covector field can be seen as an assignment of $n$ functions to each coordinate system on a manifold, which are related to one another by the equation above, complicated by the resulting topology of the manifold. The advantage of the modern approach using the tangent bundle is that these complications are summarized in the topology of $TM$ rather than in the relations between the various coordinate systems. Just like the fact that a coordinate system $(q,U)$ induces a coordinate system $(q,dq)$ on $TU$, it also induces a coordinate system $(q,p)$ on $T^*U$, where for each $\omega \in T^*U$, $\omega = \sum p_i(\omega) dq^i$.

\begin{example}
    TODO: Move to a later section? Let $M$ be the configuration space of a dynamical system with local generalized coordinates $q_1, \dots, q_n$, and a time independant Lagrangian $L(q,\dot{q})$. Thus we can think of $L$ as a real valued functional $TM$. We now consider the transition of Lagrangian mechanics to Hamiltonian mechanics. The {\it generalized momenta} are defined as
    %
    \[ p_i = \frac{\partial L}{\partial \dot{q}^i} \]
    %
    In classical mechanics, one often assumes that $\det(\partial p_i/\partial \dot{q}^j) \neq 0$, which enables us to use $(q,p)$ as a coordinate system on $TM$. One problem with this is that the coordinates $p$ are no longer geometrically significant. Furthermore, if we change from coordinates $q_0$ to $q_1$, we obtain two different generalized momenta $p^0$ and $p^1$, and we then find that
    %
    \[ p^0_i = \frac{\partial L}{\partial \dot{q}_0^i} = \sum \frac{\partial L}{\partial \dot{q}_1^j} \frac{\partial \dot{q}_1^j}{\partial \dot{q}_0^i} = \sum p^1_j \frac{\partial \dot{q}_1^j}{\partial \dot{q}_0^i} \]
    %
    Thus the momenta behave more like covectors than vectors. In particular, we can obtain a bundle map $p: TM \to T^*M$, by setting $p(q,\dot{q}) = \sum p_i(q,\dot{q}) dq^i$, which is an isomorphism if the determinant criterion is satisfied. The space $T^*M$ is then known as the \emph{phase space} of the dynamical system. The Lagrangian can in most cases in mechanics be expressed as $L(q,\dot{q}) = T(q,\dot{q}) - V(q)$, where $T$ is a kinetic energy, and $V$ a potential energy independent velocity. Often $T$ is a positive definite symmetric quadratic form
    %
    \[ T(q,\dot{q}) = \frac{1}{2} \sum g_{ij}(q) \dot{q}^i \dot{q}^j \]
    %
    where the $g_{ij}$ are known as masses, or at least can be approximated by one. Then
    %
    \[ p_k = \frac{\partial T}{\partial \dot{q}^k} = \sum g_{ik}(q) \dot{q}^i \]
    %
    Now $T$ is a two tensor, which gives $M$ the structure of a Riemannian manifold. And now we see that the momentum is just the covariant version of the velocity vector, under the musical isomorphism. Thus velocity and momentum are duals of one another with respect to the kinetic energy tensor.

%    In terms of our introduced notation, Lagrange's equation takes the form
    %
%    \[ \frac{d}{dt} \frac{\partial L}{\partial \dot{q}^i} = \frac{\partial L}{\partial q^i} \]
    %
%    Write $L(q,\dot{q}) = T(q,\dot{q}) - V(q)$, where $( \partial V/\partial q^i )(0) = 0$, $Q_{ij} = (\partial^2 V/\partial q^i \partial q^j)(0)$ is positive definite, and $2T$ is positive definite. Thus, we can write, to a first approximation,
    %
%    \[ L(q,\dot{q}) \approx \frac{1}{2} \sum g_{ij}(0) \dot{q}^i \dot{q}^j - Q_{ij} q^i q^j \]
    %
%    Under this equation, Lagrange's equation reads
    %
%    \[ \sum g_{ij}(0) \ddot{q}^i = - \sum Q_{ij} q^i \]
    %
%    Suppose that $\sum Q_{ij} v^i = \sum \lambda g_{kj} v^k$, and let $q^i(t) = \sin(\sqrt{\lambda} t) v^i$. Then we find $\ddot{q}^i = - \lambda q^i(t)$, and so
    %
%    \begin{align*}
%    - \sum Q_{ij} q^i(t) &= - \sin(\sqrt{\lambda t}) \sum Q_{ij} v^i = - \lambda \sin(\sqrt{\lambda t}) \sum g_{kj} v^k\\
%    &= \sum g_{kj} \ddot{q}^k(t) = \sum g_{ij} \ddot{q}^i(t)
%    \end{align*}
    %
%    Thus $q$ describes a solution to the equation.

    There is a canonical covariant vector field on $T^* M$ (a section into $T^* T^* M$!) known as the {\it Poincare one form} $\lambda$, given by $\lambda(q,p) = \sum p_i dq^i$. It is not a covariant vector field on $M$ itself, even though it is defined over $dq^i$, and not over $dp_i$, because the coefficients $p_i$ are not functions of $M$. Essentially, we just embed $T^*M$ in $T^*(T^*M)$ in a canonical way. The coordinate independant definition of $\lambda$ is given as follows. Given a one form $\alpha \in T^* M$, since the map $\pi: T^* M \to M$ is differentiable, we can consider $\pi^* \alpha \in T^*(T^* M)$, and this is $\lambda(\alpha)$.

    Let $\phi(x,u)$ be an equation on $\mathbf{R}^n \times \mathbf{R}^m$ homogenous of degree two in the variable $u$, i.e. $\phi(x,\lambda u) = \lambda^2 \phi(x,u)$. This implies
    %
    \begin{align*}
        \langle \nabla_u \phi(x,u), u \rangle = \lim_{\lambda \to 0} \frac{\phi(x,(\lambda + 1)u) - \phi(x,u)}{\lambda} &= \phi(x,u) \lim_{\lambda \to 0} \frac{(\lambda + 1)^2 - 1}{\lambda}\\
        &= 2 \phi(x,u)
    \end{align*}
    %
    If we define $\rho_i = \partial_{u^i}(\phi)$, then $2 \phi = \sum \rho_i u^i$. Thus taking differentials gives $2 d\phi = \sum \rho_i du^i + u^i d\rho_i$. But we also have $d\phi = \sum \rho_i du^i + \partial_{x^i} \phi dx^i$, so subtracting these equations gives $d\phi = u^i d\rho_i - (\partial_{x^i} \phi) dx^i$. But we also have
    %
    \[ d\phi = \frac{\partial \psi}{\partial x^i} dx^i + \frac{\partial \psi}{\partial \rho_i} d\rho_i \]
    %
    so by uniqueness of the expansion we conclude
    %
    \[ u^i = \frac{\partial \psi}{\partial \rho_i}\ \ \ \ \ - \frac{\partial \phi}{\partial x^i} = \frac{\partial \psi}{\partial x^i} \]
    %
    These equations are very useful in mechanics, where $\smash{\phi(x,u) = \sum a_{ij} u^i u^j}$ is a measure of kinetic energy, and $\smash{\rho_i = \sum a_{ij} u^j}$ is some form of generalized momentum.
\end{example}

Classical differential geometers were not afraid to describe $df$ as the `infinitisimal change' with respect to $f$ as the $x_i$ change. Eventually, it was realized that an infinitisimal change can be described as an action on infinitisimal lengths, or tangent vectors. Indeed, we find
%
\[ f(x + y) = f(x) + \sum y^i \left. \frac{\partial f}{\partial x^i} \right|_x + o(|y|^2) = f(x) + df_x(y_x) + o(|y|^2). \]
%
Thus the infinitisimal changes $df$ and $dx^i$ became functions on tangent vectors, which enables the classical notation to be preserved almost unchanged.

If $f: M \to N$ is smooth, then we obtain a differential map $f_*: TM \to TN$. For a fixed $p \in M$, $f_*|_p$ is a linear map from $T_p M$ to $T_{f(p)} N$, and we can thus take adjoints to obtain a dual map $(f_*|_p)^*$ from $T_{f(p)}^*N$ to $T^*_pM$. The fact that $f_*|_p$ is not injective prevents us from putting all these maps together to form a bundle map $f^*: T^* N \to T^* M$ (though if $f$ is an embedding this bundle map is definable). On the other hand, given a covector field $\omega$ on $N$, we can define a \emph{pullback} covector field $f^* \omega$ on $M$ by setting $(f^* \omega)_p = (f^*|_p)^*(\omega_{f(p)})$. Note that for vector fields, there is no way to \emph{pushforward} a vector field $X$ on $M$ to a vector field $f_* X$ on $N$, which makes the analysis of covector fields quite different than that of vector fields.

\section{The Method of Lagrangian Multipliers}

The theory we have developed is enough to obtain an incredibly practical theorem, which has usage almost everywhere in applied mathematics, and has theoretical applications as well. Let $M$ be a differentiable manifold, and let $N$ be a $C^\infty$ submanifold. Given a function $f \in C^\infty(M)$, we may wish to consider maximizing $f$ over $N$. One method of finding the maximum over $f$ is to find a cover of $N$ by coordinate charts $(x_\alpha,U_\alpha)$, and then find points where
%
\[ \frac{\partial f}{\partial x^i_\alpha} = 0 \]
%
for all $i$, which are candidates for extrema. In terms of our notation, if $h = f|_N$, then candidates for extrema are points $p$ for which $dh(p) = 0$. This may prove impractical, especially when the equations for the partial derivatives are cumbersome to solve. The method of Lagrangian multipliers provides an alternate method of proof, applying when $N$ is specified as the level set of some specifying function $g: M \to M'$, where $N = g^{-1}(p)$. We begin by trying to identify $TN$ and $T^*N$ as subbundles of $TM$ and $T^*M$ using the map $g$.

\begin{theorem}
    Let $g: M_1 \to M_2$ be a map such that $g$ has locally constant rank on a neighbourhood of $g^{-1}(p)$. Then the tangent bundle $TN$ can be identified with the subbundle of $TM|_N$ consisting of all vectors $v$ with $g_*(v) = 0$.
\end{theorem}
\begin{proof}
    If $g: M \to M'$ is rank $k$ at $p$, then choose some coordinate system $(x,U)$ on $M$ and $(y,V)$ centred at $f(p)$ such that
    %
    \[ (y \circ g \circ x^{-1})(t^1, \dots, t^n) = (t^1, \dots, t^k, 0, \dots, 0) \]
    %
    Then $(x^{n-k+1}, \dots, x^n)$ is a coordinate system on $N$, and
    %
    \[ g_* \left( \sum a^i \frac{\partial}{\partial x^i} \right) = \sum_{i,j} a^i \frac{\partial y^j \circ g}{\partial x^i} \frac{\partial}{\partial y^j} = \sum_{i = 1}^k a^i \frac{\partial}{\partial y^i} \]
    %
    and so $g_* \left(\sum a^i \frac{\partial}{\partial x^i} \right) = 0$ if and only if $a^i = 0$ for $i = 1, \dots, k$, in which case $\sum a^i \frac{\partial}{\partial x^i}$ is a vector in $TN$.
\end{proof}

\begin{example}
    This is the most natural way to think of tangent vectors on $S^n$, since $S^n$ is $f^{-1}(1)$, where $f(x) = |x|^2$. Since $Df(x)(v) = 2 (x \cdot v)$, $TS^n$ can be identified with the subbundle
    %
    \[ \{ v_p \in T\RR^{n+1}|_{S^n} : p \cdot v = 0  \}. \]
\end{example}

Consider the special case where $g: M^m \to \mathbf{R}^n$ has constant rank $k$ in a neighbourhood of $g^{-1}(x)$. Then we can consider the covector fields $dg^1,\dots,dg^n$. Since
%
\[ g_*(v_p) = (dg^1(v), \dots, dg^n(v))_{g(p)}, \]
%
the space $TN$ can be identified with the subbundle
%
\[ \{ v \in TM|_N : dg^1(v) = \dots = dg^n(v) = 0 \}. \]
%
At each point $p \in N$, $dg^1(p), \dots, dg^n(p)$ span a $k$ dimensional subset of $T^*M$. The adjoint map $\omega_p \mapsto \omega_p|_{T_pN}$ has rank $n-k$, since $N$ is $n-k$ dimensional, which implies that $dg^1(p), \dots, dg^n(p)$ span the kernel of the reduction map.

Now we return our original problem, suppose that $N$ is specified as the level set of a differentiable map $g: M \to \RR^n$, and that $p \in M$ is a critical point for a function $f$ on $N$. This means that $df(p)(v) = 0$ for all vectors $v \in T_pN$. But this means there must exist scalars $\lambda_i$ such that
%
\[ df(p) = \sum \lambda_i dg^i(p). \]
%
Thus we have a coordinate independant way to find a critical points on the set. In the special case where $g: M \to \mathbf{R}$, we need only find $\lambda$ such that $df(p) = \lambda \cdot dg(p)$.

\begin{example}
    Consider finding extrema of the function $f(x,y) = 5x - 3y$, subject to the constraint $x^2 + y^2 = 136$. Then the constraint function is $g(x,y) = x^2 + y^2$, and
    %
    \[ df = 5dx - 3dy\ \ \ \ \ dg = 2x dx + 2y dy \]
    %
    Extrema occur at points $(x,y)$ such that
    %
    \[ 5dx - 3dy = 2 \lambda x dx + 2 \lambda y dy \]
    %
    Which implies that $2 \lambda x = 5$, $2 \lambda y = -3$, and in order for the point $(x,y)$ to occur in the constraint region, we require $25/4 \lambda^2 + 9/4 \lambda^2 = 17/2\lambda^2 = 136$, so $1/16 = \lambda^2$, and so $\lambda = \pm 1/4$. If $\lambda = 1/4$, then we have an extrema $(10, -6)$ and $(-10, 6)$. Since the constraint region is compact, we must have a maxima and minima, and since the function is non-constant, one of these points must be the maixmum, and one the minimum. Since $f(10,-6) = 68$, $f(-10,6) = -68$, $(10,-6)$ is the maxima of $f$, and $(-10,6)$ the minima.
\end{example}

\begin{example}
    Let $T: \mathbf{R}^n \to \mathbf{R}^n$ be a self-adjoint operator, and consider maximizing $\langle Tx, x \rangle$ over $S^{n-1}$. If $f(x) = \langle Tx, x \rangle$, then
    %
    \[ f(x) = \sum_{i,j} T_{ij} x_i x_j \]
    %
    so we may take differentials,
    %
    \[ df(x) = \sum_{i = 1}^n \left( \sum_{j \neq i} (T_{ij} + T_{ji}) x_j + 2 T_{ii} x_i \right) dx^i = 2 \sum_{i,j} T_{ij} x_j dx^i \]
    %
    and the constraint region is defined by the function $g(v) = \sum x_i^2$, so $dg(x) = 2 \sum x_i dx^i$. Since $S^{n-1}$ is compact, extrema exist, and at this extremum point $x$ there is $\lambda$ such that
    %
    \[ \sum_{i,j} T_{ij} x_j dx^i = \lambda \sum x_i dx^i \]
    %
    and so $Tx = \lambda x$, implying that $x$ is an eigenvector of $T$, with eigenvalue $\lambda$. If $V$ is the orthogonal complement of the span of $x$, then $T(V) \subset V$, because if $\langle w, v \rangle = 0$, then $\langle Tw, v \rangle = \langle w, Tv \rangle = \lambda \langle w, v \rangle = 0$, and $T: V \to V$ is still self-adjoint. This implies that if $V$ is non-trivial, we may find another eigenvector in $V$. Continuing this process, we find a sequence of orthogonal eigenvectors $v_1, \dots, v_n$ for $T$, which diagonalizes $T$.
\end{example}

The fact that all orthogonal matrices can be diagonalized gives us a general decomposition results for matrices in $GL(n)$, which is analogous to the fact that a nonzero complex number can be written as $rz$ where $r > 0$ and $|z| = 1$. In general, the result is known as the polar decomposition theorem.

\begin{theorem}
    Any invertible matrix $M$ can be uniquely decomposed as $M_1M_2$, where $M_1$ is orthogonal and $M_2$ is symmetric and positive definite. In fact, $GL(n)$ is diffeomorphic to $O(n) \times \mathbf{R}^{n(n+1)/2}$, where we identify $\mathbf{R}^{n(n+1)/2}$ with the space of symmetric positive definite matrices under the exponential map on the family of symmetric matrices.
\end{theorem}
\begin{proof}
    The uniqueness is easy, because if $M = M_1M_2 = M_1'M_2'$, then
    %
    \[ (M_2')^2 = M_2' (M_1')^{-1} M_1'M_2' = M^tM = M_2M_1^{-1}M_1M_2 = M_2^2 \]
    %
    But since $M_2$ and $M_2'$ are positive definite, this fact implies $M_2 = M_2'$, and therefore that $M_1 = M_1'$. Existence is a bit more tricky. Given $M \in GL(n)$, $MM^t$ is self adjoint and positive definite, so that there is a positive definite matrix $N$ such that $MM^t = N^2$ (If $MM^t$ was diagonal, then $N$ would be easy to find, because we could take square roots along the diagonal, but in general we can just diagonalize $M$ and then take square roots). Then $N^{-1}M$ is orthogonal, because $N^{-1}$ is also positive definite, so
    %
    \[ (N^{-1}M)^t (N^{-1}M) = M^tN^{-2}M = M^t(MM^t)^{-1}M = I \]
    %
    and so $M = N(N^{-1}M)$.

    Now we claim the association $M \mapsto (M_1,M_2)$ is continuous. Since $M_2 = M_1^{-1}M$, it suffices to prove that the map $M \mapsto M_1$ is continuous. Suppose that $M^1, M^2, \dots$ is a sequence in $GL(n)$ converging to $GL(n)$. Then since $O(n)$ is compact, some subsequence $\smash{M^{i_k}_1, M^{i_2}_1, \dots}$ converges to an orthogonal matrix $N$. But now $M^{i_k}_2 = (M^{i_k}_1)^{-1} M^{i_k}$ converges as well, to $N^{-1}M$, and the space of self adjoint matrices is closed so $N^{-1}M$ is self adjoint. Since $M^{i_k} = M^{i_k}_1 M^{i_k}$ converges to $N(N^{-1}M)$, it follows that $N = M_1$ and $N^{-1}M = M_2$, hence $M^{i_k} \to M_1$. But now we conclude that in general $M^k_1 \to M_1$ because this argument can be adapted to show every subsequence contains a further subsequence converging to $M_1$. Since the map $(M_1,M_2) \to M_1M_2$ is continuous, this verifies the decomposition is actually a homeomorphism. In fact, the map $(M_1,M_2) \mapsto M_1 M_2$ is obviously smooth, as is it's inverse, since it is actually analytic in it's matrix entries. So the correspondence is a diffeomorphism.
\end{proof}

\section{Tensors}

We now consider tensors, which are a generalization of covectors as linear functionals to multilinear functionals. If $V$ is a vector space, then we let $T^n(V)$ denote the space of maps $f: V^n \to \mathbf{R}$ which are {\it multilinear}, or linear in each variable. Of course, $T^1(V)$ is just our ordinary dual space $V^*$. If $T$ is an $n$ tensor, and $S$ is an $m$ tensor, then the {\it tensor product} $(T \otimes S)(v,w) = T(v)S(w)$ is an $n + m$ tensor, and the tensor product is an associative, distributive operation which turns the space $\bigoplus_{n = 1}^\infty T^n(V)$ into a graded algebra (by convention, we can let $T^0(V)$ denote real numbers, and define $\lambda \otimes T = \lambda T$, so that $T(V) = \bigoplus_{n = 0}^\infty T^n(V)$ is a graded algebra with identity. In particular, if $f_1, \dots, f_n$ is a basis for $V^*$, then a basis for $T^m(V)$ is given by the elements $f_{i_1} \otimes f_{i_2} \otimes \dots \otimes f_{i_m}$, with $i_1, \dots, i_m \in [n]$, so $T^m(V)$ has dimension $n^m$. Given a linear map $f: V \to W$, we can define a map $f^*: T^n(W) \to T^n(V)$ by setting $f^*(\omega)(v_1, \dots, v_n) = \omega(f(v_1), \dots, f(v_n))$. Thus we obtain a continuous tensor functor, so for each vector bundle $\xi$, we can define the bundle $T^n(\xi)$ of $n$ tensors over $\xi$. In particular, we obtain the bundle $T^n(TM)$ of tensors over the tangent bundle, which we also denote by $T^nM$. Given a coordinate system $x$ around a point $p \in M$, a basis for $T^1_p M$ is given by $dx^1_p, \dots, dx^n_p$ for some coordinate system $x$ around $p$, a basis for $T^m_pM$ is given by $dx^{i_1}_p \otimes \dots \otimes dx^{i_m}_p$. Thus an $m$ tensor field $\omega$ on $M$ can be locally written as
%
\[ \omega = \sum a_{i_1 \dots i_m} dx^{i_1} \otimes \dots \otimes dx^{i_m} \]
%
and $\omega$ is continuous/smooth if and only if the functions $a_{i_1 \dots i_m}$ are. If
%
\[ \sum a_{i_1 \dots i_m} dx^{i_1} \otimes \dots \otimes dx^{i_m} = \sum b_{j_1 \dots j_m} dy^{j_1} \otimes \dots \otimes dy^{j_m}, \]
%
then we have the horrendous conversion formula
%
\[ a_{i_1 \dots i_m} = \sum b_{j_1 \dots j_m} \frac{\partial y^{j_1}}{\partial x^{i_1}} \dots \frac{\partial y^{j_m}}{\partial x^{i_m}}, \]
%
where the products here are just ordinary products of functions.

Though not as often occuring as covariant tensors, contravariant tensor fields are defined similarily, as multilinear functionals operating on $V^*$, and the space of $n$ contravariant tensors is denoted $T_n(V)$. $T_1(V)$ is just $V^{**}$, and is naturally isomorphic to $V$, and we can take tensor products to obtain all elements of $T_n(V)$, so if $V$ has a basis $v_1, \dots, v_n$, then a basis for $T_m(V)$ is given by $v_{i_1} \otimes \dots \otimes v_{i_m}$. Given $f: V \to W$, we obtain $f^*: T_n(V) \to T_n(W)$ in the obvious way, and this functor gives us $T_n(\xi)$ for any vector bundle $\xi$. Putting covariant and contravariant tensors gives us the space $T_n^m(V)$ of mixed tensors, multilinear functions over $m$ copies of $V$ and $n$ copies of $V^*$, and we can therefore form $T^m_n(\xi)$. Over the tangent bundle $TM$, we can form the space $T_n^m(TM)$, also denoted by $T^m_n M$, and vector fields over these sets can be locally written as
%
\[ \sum a_{i_1 \dots i_n}^{j_1 \dots j_m} dx^{i_1} \otimes dx^{i_n} \otimes \frac{\partial}{\partial j_1} \otimes \dots \otimes \frac{\partial}{\partial j_m}, \]
%
where if
%
\begin{align*}
    \sum a_{i_1 \dots i_n}^{j_1 \dots j_m} & dx^{i_1} \otimes dx^{i_n} \otimes \frac{\partial}{\partial x^{j_1}} \otimes \dots \otimes \frac{\partial}{\partial x^{j_m}}\\
    &\ \ \ \ \ = \sum b_{\alpha_1 \dots \alpha_n}^{\beta_1 \dots \beta_m} dy^{\alpha_1} \otimes dy^{\alpha_n} \otimes \frac{\partial}{\partial y^{\beta_1}} \otimes \dots \otimes \frac{\partial}{\partial y^{\beta_m}},
\end{align*}
%
then
%
\[ a_{i_1 \dots i_n}^{j_1 \dots j_m} = \sum_{\alpha_1, \dots, \alpha_n, \beta_1, \dots, \beta_m} b_{\alpha_1 \dots \alpha_n}^{\beta_1 \dots \beta_m} \frac{\partial y^{\alpha_1}}{\partial x^{i_1}} \dots \frac{\partial y^{\alpha_n}}{\partial x^{i_n}} \frac{\partial x^{j_1}}{\partial y^{\beta_1}} \dots \frac{\partial x^{j_m}}{\partial y^{\beta_m}}. \]
%
We call an element of $T_n^m(V)$ an $(m,n)$ tensor, or a tensor with $m$ covariant indices and $n$ contravariant indices.

There are all kinds of algebraic tricks one can do on mixed tensors. Consider the endofunctor which associates a vector space $V$ with the vector space $\text{End}(V)$, and for each isomorphism $f: V \to W$, we consider $f_*: \text{End}(V) \to \text{End}(W)$ by setting $f_*(T) = f \circ T \circ f^{-1}$. This functor is continuous, so we can consider the bundle $\text{End}(\xi)$ for each vector bundle $\xi$. For this space it is only {\it equivalences} $f: \xi \to \phi$ which extend to maps $f_*: \text{End}(\xi) \to \text{End}(\phi)$. Now the mixed tensor space $T^1_1(V)$ is isomorphic to $\text{End}(V)$, where $S: V \to V$ corresponds to the tensor $\omega_S(v,\lambda) = \lambda(S(v))$. If $f: V \to W$ is an isomorphism, and $S: W \to W$ is an endomorphism, then
%
\[ \omega_{f_* S}(v,\lambda) = \lambda((f_* S)(v)) = \lambda((f \circ S \circ f^{-1})(v)) = (f^* \omega_S)(v,\lambda) \]
%
Thus we have found a natural isomorphism between $T^1_1(V)$ and $\text{End}(V)$. More generally, we have a natural isomorphism between $V^* \otimes W$ and $\text{Hom}(V,W)$. Thus $\text{End}(\xi)$ is naturally equivalent to $T_1^1(\xi)$, and so any operation we can perform on $\text{End}(V)$ can be transfered naturally to $T_1^1(V)$. For instance, we can define the {\it trace}, or {\it contraction} of a tensor $\omega \in T_1^1(V)$ to be the trace of the endomorphism $S$ corresponding to $\omega$. If we fix a basis, and write $\smash{\omega = \sum a_i^j e_i^* \otimes e_j}$, then $\smash{\text{trace}(\omega) = \sum a_i^i}$. In particular, a smooth vector field on $T_1^1(TM)$ can be contracted pointwise to produce a smooth function. Similarily, we can always contract a bottom and top index on an $(n,m)$ tensor to form a $(n-1,m-1)$ tensor.

\section{Local Operators and Derivational Viewpoints}

We previously identified the smooth vector fields on a manifold $M$ with the derivations on $C^\infty(M)$. We now identify tensor fields as certain operators on vectors fields. Given an $n$ tensor field $\omega$ and $n$ vector fields $X_1, \dots, X_n$, we can define a smooth function
%
\[ \omega(X_1, \dots, X_n)(p) = \omega(p)(X_1(p), \dots, X_n(p)), \]
%
and $\omega$ operates as a multilinear map on $\Gamma(TM)^n$, where the map is linear not only over real numbers, but also $C^\infty$ functions, since the map is `defined pointwise'. We find that all such multilinear maps are just tensor fields in disguise.

\begin{theorem}
    If $T: \Gamma(TM)^n \to C^\infty(M)$ is a $C^\infty(M)$ multilinear map, there is a unique smooth $n$ tensor field $\omega$ on $M$ such that $T(X_1, \dots, X_n) = \omega(X_1, \dots, X_n)$.
\end{theorem}
\begin{proof}
    We first note that if $X_k = Y_k$ in a neighbourhood of $p$, then
    %
    \[ T(X_1, \dots, X_n)(p) = T(Y_1, \dots, Y_n)(p) \]
    %
    To see this, we find a smooth bump functions $f_1, \dots, f_n$ equal to one in a neighbourhood of $p$ such that $f_kX_k = f_kY_k$, and then
    %
    \begin{align*}
        T(X_1, \dots, X_n)(p) &= f_1(p) \dots f_n(p) T(X_1, \dots, X_n)(p)\\
        &= T(f_1X_1, \dots, f_nX_n)(p)\\
        &= T(f_1Y_1, \dots, f_nY_n)(p)\\
        &= f_1(p) \dots f_n(p) T(Y_1, \dots, Y_n)(p)\\
        &= T(Y_1, \dots, Y_n)(p)
    \end{align*}
    %
    Thus $T$ is locally defined. To show that $T(X_1, \dots, X_n)(p)$ depends only on $X_1(p), \dots, X_n(p)$, it suffices to show that $T(X_1, \dots, X_n)(p) = 0$ if $X_k(p) = 0$, for some $k$. If $(x,U)$ is a coordinate system around $p$, then we can write
    %
    \[ X_n = \sum b_n^i \frac{\partial}{\partial x^i}. \]
    %
    But then
    %
    \[ T(X_1, \dots, X_k, \dots, X_n)(p) = \sum b_1^{i_1}(p) \dots b_n^{i_n}(p) T \left(\partial_{i_1}, \dots, \partial_{i_n} \right)(p) \]
    %
    which shows that $T$ is defined pointwise. We can now set
    %
    \[ \omega(p)(v_1, \dots, v_n) = T(X_1, \dots, X_n)(p) \]
    %
    where $X_k(p) = v_k$, and we have verified this is well defined. $\omega$ is smooth because
    %
    \[ \omega = \sum \omega_{i_1 \dots i_n} dx^{i_1} \otimes \dots \otimes dx^{i_n}. \]
    %
    Thus $\omega_{i_1 \dots i_n} = T(dx^{i_1}, \dots, dx^{i_n})$ is a smooth function.
\end{proof}

Because of this, we do not distinguish tensor fields from their corresponding operators on vector fields. Similar results hold for contravariant tensors, which operate on covector fields.

If $E$ and $F$ are vector bundles on some manifold $M$, we say a map $f: \Gamma(E) \to \Gamma(F)$ is a \emph{local operator} if whenever a section $s \in \Gamma(E)$ vanishes in a neighbourhood of a point $p$, then $f(s)(p) = 0$. We say it is a \emph{pointwise operator} if $f(s)(p) = 0$ whenever $s(p) = 0$.

\begin{example}
    The space $C^\infty(M)$ can be considered as the space of sections over the trivial bundle $\varepsilon^1(M)$. Vector fields $X$, viewed as first order linear operators, operate as linear maps from $C^\infty(M)$ to itself. These operators are local, since if $f$ vanishes around a neighbourhood of a point $p$, then $X(f)(p) = 0$. But they are not point operators since a function can surely vanish at a point, whereas it's derivative might not.
\end{example}

\begin{example}
    We will soon define the exterior derivative operator $d$, which maps alternating tensor fields $\Gamma(\Omega^k(M))$ to $\Omega^{k+1}(M)$. The derivative operator is local, but not pointwise defined.
\end{example}

Because of the existence of bump functions, a local map $f: \Gamma(E) \to \Gamma(F)$ naturally extends to a sheaf morphism from sections of $E$ to sections of $F$. We have essentially verified in the argument above that $C^\infty(M)$ linear maps are pointwise operators. On the other hand,  if $T: \Gamma(E) \to \Gamma(F)$ is a pointwise operator, we can define a bundle map from $E$ to $F$ by letting $T(v_p) = T(s)(p)$, where $s \in \Gamma(E)$ has $s(p) = v_p$. Thus every pointwise operator is $C^\infty(M)$ linear. In general, a $C^\infty(M)$ linear operator can therefore be though as an operator defined pointwise. This is a basic instance of the {\it Serre Swan correspondence}, which says that module structures over $C^\infty(M)$ can be identified with vector bundle structures on $M$.

\section{The Classical Viewpoint}

The classical viewpoint of vectors in the tangent bundle, and tensors in general, is obsessed with the coefficients associated to these objects and the way they transform with respect to coordinate systems. We previously mentioned that for a coordinate chart $(x,U)$, then for any $(n,m)$ tensor field $\omega$, there are functions
%
\[ a_{i_1 \dots i_n}^{j_1 \dots j_m} \in C^\infty(U) \]
%
such that, on $U$,
%
\[ \omega = \sum_{\substack{i_1, \dots, i_n\\j_1, \dots, j_m}} a_{i_1 \dots i_n}^{j_1 \dots j_m} dx^{i_1} \otimes \dots \otimes dx^{i_n} \otimes \frac{\partial}{\partial x^{j_1}} \otimes \dots \otimes \frac{\partial}{\partial x^{j_m}} \]
%
If $(y,V)$ is another coordinate system, then there are functions
%
\[ b_{i_1 \dots i_n}^{j_1 \dots j_m} \in C^\infty(V) \]
%
such that on $V$,
%
\[ \omega = \sum_{\substack{i_1, \dots, i_n\\j_1, \dots, j_m}} b_{i_1, \dots, i_n}^{j_1, \dots, j_m} dy^{i_1} \otimes \dots \otimes dy^{i_n} \otimes \frac{\partial}{\partial y^{j_1}} \otimes \dots \otimes \frac{\partial}{\partial y^{j_m}}. \]
%
Then, restricted to $U \cap V$, we have the monstrous transformation rule
%
\[ a_{i_1 \dots i_n}^{j_1 \dots j_m} = \sum_{\substack{\alpha_1, \dots, \alpha_n\\\beta_1, \dots, \beta_m}} b_{\alpha_1 \dots \alpha_n}^{\beta_1 \dots \beta_m} \frac{\partial y^{\alpha_1}}{\partial x^{i_1}} \dots \frac{\partial y^{\alpha_n}}{\partial x^{i_n}} \frac{\partial x^{j_1}}{\partial y^{\beta_1}} \dots \frac{\partial x^{j_m}}{\partial y^{\beta_m}}. \]
%
Classically, one {\it defines} a $(n,m)$ tensor on a $k$ dimensional manifold as an assignment of $k^{n + m}$ functions to each chart $(x,U)$, defined on $U$, such that the transformation above is satisfied on $U \cap V$, where $(y,V)$ is another coordinate system. An {\it invariant} is a $(0,0)$ tensor, or a smooth function on the entire manifold. Let us consider some examples of this terminology.

\begin{example}
    A classical differential geometer will tells you that the Kronecker delta function $\smash{\delta_i^j}$ is a tensor. What they mean by this is, if we assign the same $n^2$ functions $\smash{\delta_i^j}$ to {\it every} coordinate system, then for any two coordinate systems $x$ and $y$,
    %
    \[ \delta_i^j = \frac{\partial x^j}{\partial x^i} = \sum \frac{\partial x^j}{\partial y^\alpha} \frac{\partial y^\alpha}{\partial x^i} = \sum \delta_\alpha^\beta \frac{\partial x^j}{\partial y^\beta} \frac{\partial y^\alpha}{\partial x^i} \]
    %
    so, mystically, these coordinate systems really do define a smooth tensor field $A$ over $T_1^1(TM)$. To figure out what $A$ {\it really} is, we calculate
    %
    \[ A(X,\omega) = \sum \omega_i X^i = \omega(X) \]
    %
    so $A$ is just the tensor which operates as evaluation of a functional on a vector field. In terms of the identification of $\text{End}(TM)$ with $T_1^1(TM)$, the tensor $A$ corresponds to the identity map.
\end{example}

It is a useful skill to be able to translate the classical language into modern language, and vice versa, and be able to prove things in both settings. Thus in the following, we state theorems in their classical context, prove them by classical means, then reexpress them into modern theorems and prove them in this context.

\begin{theorem}
    If $\sum \nu_i \mu^i$ is an invariant for any covector field $\sum \nu_i dx^i$, then $\sum \mu^i \frac{\partial}{\partial x_i}$ is a well defined vector field.
\end{theorem}
\begin{proof}
    For any covector field $\sum \nu_i dx^i = \sum (\nu')_i dy^i$, we have
    %
    \[ \sum \mu^i \nu_i = \sum \mu'^i \nu'_i \]
    %
    and we have the relation $(\nu')_i = \sum \nu_j \frac{\partial x^j}{\partial y_i}$. If we choose $\nu_\alpha = 1$, and $\nu_i = 0$ otherwise, we find
    %
    \[ \mu^\alpha = \sum \mu^i \nu_i = \sum_i \mu'^i \nu_i' = \sum_{i,j} \mu'^i \frac{\partial x^j}{\partial y_i} \nu_j = \sum_i (\mu')^i \frac{\partial x^\alpha}{\partial y_i} \]
    %
    so
    %
    \[ \sum_i \mu^i \frac{\partial}{\partial x^i} = \sum_{i,j,k} (\mu')^k \frac{\partial x^i}{\partial y_k} \frac{\partial y^j}{\partial x^i} \frac{\partial}{\partial y^j} = \sum_{j,k} (\mu')^k \frac{\partial y^j}{\partial y^k} \frac{\partial}{\partial y^j} = \sum_k (\mu')^k \frac{\partial}{\partial y^k} \]
    %
    is a well defined vector field, and is uniquely determined since we can determine the value of $\mu^i$ at any point $x$ by choosing a smooth vector field $\nu$ such that $\nu_i(x) = 1$, $\nu_j(x) = 0$, and then $\mu^i(x) = \sum \mu^i(x) \nu_i(x)$.
\end{proof}

\begin{remark}
    In modern terms, the functions $\nu_i$ correspond to a $C^\infty(M)$ linear map $T\mu = \sum \mu^i \nu_i$ from $\Gamma(T^*M)$ to $C^\infty(M)$. Such a map is an element of $\Gamma(T^{**}M)$, which is naturally isomorphic to $\Gamma(TM)$, so the linear map corresponds to a unique smooth vector field $\nu$, and the $\nu_i$ are obviously the coordinates of $\nu$.
\end{remark}









\chapter{Lie Derivatives and Differential Equations}

So far, we've managed to construct a manifold, and a coordinate-independant way to measure derivatives on the manifold via the tangent bundle. However, a problem results when we wish to measure the rate of change of a rate of change. But it's difficult to measure the rate of change of vector field $X$, because we cannot compare $X_p$ and $X_q$ for $p \neq q$, as the elements lie in different vector spaces. We can of course switch to a coordinate system for close enough $p$ and $q$, associating with each $X$ a function $F(x) : \mathbf{R}^n \to \mathbf{R}^n$ we can differentiate, but this doesn't really apply to a global perspective in any way, and one cannot naively patch together these maps so that the derivative of a vector field is a vector field. The idea here is to provide a direction in which to take a derivative, which in this case is induced by a vector field. We recall the main existence and uniqueness result from ordinary differential equations.

\begin{theorem}
    If $f: \mathbf{R}^n \to \mathbf{R}^n$ is a locally Lipschitz function, then at any point $p \in \mathbf{R}^n$, there is a suitably small $\varepsilon$ and a unique curve $x: (-\varepsilon, \varepsilon) \to \mathbf{R}^n$ with $x(0) = p$, and $x' = f \circ x$, and if we extend $x$ to have the maximum possible interval domain, then either $x$ is defined everywhere, or $x$ is defined on some interval $(a,b)$, and $\lim_{t \to a} |x(t)| = \lim_{t \to b} |x(t)| = \infty$.
\end{theorem}

The theorem has the following interpretation in the theory of differentiable manifolds. Given any differentiable vector field $X: M \to TM$, and $p \in M$, there is a suitably small $\varepsilon$ and a unique differentiable curve $x: (-\varepsilon,\varepsilon) \to M$ such that $x'(t) = X_{x(t)}$. We shall call a curve satisfying this equation an \emph{integral curve} for $X$. this results obviously follows because we can switch to any particular coordinate system. The unique curves imply that we can define a function on some open subset of $M \times \mathbf{R}$ by $\phi_t(p) = x(t)$, where $x$ is the unique integral curve for $X$ with $x(0) = p$, where we assume such an $x$ exists in the definition of the function. If $\phi_{t+h}(p)$, and $\phi_t(\phi_h(p))$ are all well defined, then $\phi_t(\phi_h(p)) = \phi_{t + h}(p)$ because if $x: (-\varepsilon_0,\varepsilon_0) \to M$ is an integral curve satisfying $x(0) = p$, and $y: (-\varepsilon_1, \varepsilon_1) \to M$ is an integral curve with $y(0) = x(h)$, then the uniqueness theorem tells us that $y(u) = x(h + u)$ where these functions are defined, because $z(u) = x(h + u)$ is also an integral curve with $z(0) = x(h)$. $\phi$ is defined on an open submanifold of $M \times \mathbf{R}$, and it is a very difficult theorem of functional analysis to show that if $X$ is a $C^k$ vector field, then $\phi$ is $C^k$. We will take this for granted in the sequel. Since $\phi_{-t} = \phi_t^{-1}$ on a small enough neighbourhood of every domain, we conclude that each $\phi_t$ is `almost' a diffeomorphism, and is certainly a local diffeomorphism. For compact manifolds, we can obtain a global family of diffeomorphisms.

\begin{theorem}
    If $X$ is compactly supported, then $\phi$ is defined everywhere.
\end{theorem}
\begin{proof}
    If $K$ is the support of $X$, then $\phi_t$ is defined on $K^c$ for all $t$, because if $p \not \in K$, then $x(p) = p$ is an integral curve of $X$. If we cover $K$ by finitely many coordinate charts $(x_1,U_1), \dots, (x_n,U_n)$, such that $\phi_t$ is defined for $|t| < \varepsilon_i$ on $U_i$, then we have define $\phi_t$ on $M$ for all $|t| < \min(\varepsilon_1, \dots, \varepsilon_n)$, and we can then define $\phi$ for arbitrarily large real numbers by considering the composition $\phi^n_t = \phi(nt)$.
\end{proof}

One way to interpret the local uniqueness and existence of vector fields is that all vector fields are the same locally up to a change of coordinates, when the vector field is nonvanishing.

\begin{theorem}
    If $X$ is defined on $M$ with $X_p \neq 0$, then there is a coordinate chart $(x,U)$ around $p$ such that for $q \in U$,
    %
    \[ X_q = \left. \frac{\partial}{\partial x^1} \right|_q. \]
\end{theorem}
\begin{proof}
    It obviously suffices to prove this for $p = 0$, and $M = \mathbf{R}^n$. We may also start with a coordinate chart such that $X_0 = \partial_1$. Consider the induced diffeomorphism $\phi$ from the vector field $X$, and take the map $\alpha: (-\varepsilon, \varepsilon) \times U \to \mathbf{R}^n$, where $U \subset \mathbf{R}^{n-1}$ is a neighbourhood defined by $\alpha(t,x) = \phi_t(0,x)$. Clearly the map is $C^\infty$, and if $f: x(U) \to \mathbf{R}$ is arbitrary, then
    %
    \begin{align*}
        \alpha_* \left( \partial_i(0,a) \right)(f) = \frac{\partial (f \circ \alpha)}{\partial x^i} (0,a) = \frac{\partial f}{\partial x_i}(a)
    \end{align*}
    %
    Thus $\alpha_* \partial_i(0,a) = \partial_i(0,a)$. We also calculate
    %
    \begin{align*}
        \alpha_* \left( \partial_t(0,a) \right) (f) &= \lim_{h \to 0} \frac{f(\phi_{h}(a)) - f(a)}{h} = X_a(f)
    \end{align*}
    %
    so $\alpha_*(\partial_t(0,a)) = X_a$. We conclude that $\alpha_*$ is nonsingular in a neighbourhood of $(0,a)$, and so locally $\alpha$ is invertible, and the inverse, which we call $y$, is a coordinate system. But we know that
    %
    \[ \frac{\partial}{\partial y^1}(a) = \alpha_* \left( \frac{\partial}{\partial t}(a) \right) = X_a, \]
    %
    which completes the proof.
\end{proof}

Note that here we have used the fact that
%
\[ X_p f = \lim_{t \to 0} \frac{f(\phi_t(p)) - f(p)}{t}. \]
%
This limit suggests that the translation maps $\phi_h$ can be used to measure the rate of change along the vector $X_p$. As foreshadowed at the beginning of the chapter, we can use this process to measure the rate of change of many other objects defined on a differentiable manifold.

First, we switch notations, and denote $X(f)$ by $L_X(f)$, and call it the \emph{Lie derivative} of $f$ along $X$. Using this notation, we can also consider the derivatives of covariant vector fields $\omega$, obtaining a new covariant vector field
%
\[ (L_X \omega)_p = \lim_{t \to 0} \frac{(\phi_t^*\omega)_p - \omega_p}{t}. \]
%
%
%If $(x,U)$ around a point $p$, and in this chart, $\omega = \sum a_i dx^i$, then
%
%\begin{align*}
%    (\phi_t^* \omega)_p(\partial_i|_p) &= \omega_{\phi_t(p)} \left( (\phi_t)_*(\partial_i|_p) \right)\\
%    &= \omega_{\phi_t(p)} \left( \sum_j \frac{\partial x^j \circ \phi_t}{\partial x^i} \left. \frac{\partial}{\partial x_j} \right|_{\phi_t(p)} \right)\\
%    &= \sum_j a_j(\phi_t(p)) \frac{\partial x^j \circ \phi_t}{\partial x^i}.
%\end{align*}
%
%Thus
%
%\[ (\phi_t^* \omega)_p = \sum_{i,j} a_j(\phi_t(p)) \frac{\partial x^j \circ \phi_t}{\partial x^i} dx^i(p). \]
%
%In particular, we calculate that
%
%\[ \frac{\partial (\phi_t^* \omega)_p}{\partial t} = \sum_{i,j} \left( X_p(a_j) \frac{\partial x^j \circ \phi_t}{\partial x^i} +  \right) dx^i(p) \]
%
Thus for any vector $v \in M_p$, $\omega((\phi_t)_* v) = \omega(v) + t (L_X \omega)(v) + o(t)$. Similarily, we consider the derivatives of a vector field $Y$ by a vector field $X$, defined by
%
\[ (L_X Y)_p = \lim_{t \to 0} \frac{Y_p - ((\phi_t)_* Y)_p}{t}. \]
%
Here the order of terms is switched because $((\phi_t)_* Y)_p$ is really pushing forward the point on the vector field at $Y_{\phi_{-t}}$, and therefore is looking backwards in time rather than forwards. Thus $((\phi_{-t})_* Y)_p = Y_p + t (L_X Y)_p + o(t)$. Both derivatives exist whenever the fields in question are smooth, because they are just partial derivatives of a suitably defined smooth function on $M \times \mathbf{R}$. Moreover, we will now calculate the derivative in coordinates, which also provides a proof of existence. It will be easiest to compute this once we verify the linearity of the derivative, as well as the product rule.

\begin{theorem}
    If $f \in C^\infty(M)$, $\omega \in \Gamma(T^*M)$, and $X,Y \in \Gamma(TM)$, then
    %
    \begin{itemize}
        \item[(i)] $L_X(f \omega) = (L_X f) \omega + f (L_X \omega)$.
        \item[(ii)] $L_X(f Y) = (L_X f) X + f (L_X Y)$.
        \item[(iii)] $L_X(Y(f)) = (L_X Y)(f) + Y(L_X f)$.
        \item[(iv)] $L_X(\omega(Y)) = \omega(L_X Y) + \omega(L_X Y)$.
    \end{itemize}
\end{theorem}
\begin{proof}
    We write
    %
    \begin{align*}
        L_X(f \omega)(p)(v) &= \lim_{h \to 0} \frac{\phi_h^*(f \omega)(p)(v) - (f \omega)(p)(v)}{h}\\
        &= \lim_{h \to 0} \frac{f(\phi_h(p)) \omega(\phi_h(p))((\phi_h)_*(v)) - f(p) \omega(p)(v)}{h}\\
        &= (FG)'(0)
    \end{align*}
    %
    Where $F(t) = f(\phi_t(p))$, and $G(t) = \omega(\phi_t(p))((\phi_t)_*(v))$. Since $F(0)' = (L_X f)(p)$, and $G(0)' = (L_X \omega)(p)(v)$, we find that
    %
    \[ L_X(f \omega)(p)(v) = f(p) (L_X \omega)(p)(v) + (L_X f)(p) \omega(p)(v) \]
    %
    and this completes the proof of (i). The propositions (ii), (iii), and (iv) are proved in essentially the same way.
\end{proof}

It will be easiest to begin computing the Lie derivative of a covariant vector field. Given $X$, with induced flow $\phi_t$, we calculate that
%
\[ ((\phi_t)^*df)(v_p) = ((\phi_t)_*(v_p))(f) = v_p(f \circ \phi_t) = d(f \circ \phi_t)(v_p) \]
%
Thus using the smoothness of the values involved, we conclude that
%
\[ L_X(df) = \lim_{t \to 0} \frac{d(f \circ \phi_t)(p) - df(p)}{t} = \lim_{t \to 0} d \left( \frac{f \circ \phi_t - f}{t} \right)(p) = d(Xf) \]
%
In particular, this means that if $X = \sum a^i \partial_i$, then
%
\[ L_X dx^i = d(X(x^i)) = d(a^i) = \sum \frac{\partial a^i}{\partial x^j} dx^j. \]
%
Thus if $\omega = \sum b_i dx^i$, then
%
\[ L_X(\omega) = \sum X(b_i) dx^i + b_i L_X(dx^i) = \sum a^j \frac{\partial b_i}{\partial x^j} dx^i + b_i \frac{\partial a^i}{\partial x^j} dx^j \]
%
Thus we see from the coefficients that the Lie derivative involves both the derivatives of the coefficients of $\omega$ and the coefficients of $X$. This makes sense, since $L_X(\omega)$ is neither $C^\infty(M)$ linear in $X$ nor in $\omega$. Now we can use the `product rule' to calculate the Lie derivative of a vector field. $X = \sum a^i \partial_i$ and $Y = \sum b^i \partial_i$, then we know that
%
\begin{align*}
    \sum a^j \frac{\partial b^i}{\partial x^j} &= L_X(b^i) \\
    &= L_X(dx^i(Y))\\
    &= (L_X dx^i)(Y) + dx^i(L_X Y)\\
    &= \sum \frac{\partial a^i}{\partial x^j} b^j + dx^i(L_X Y).
\end{align*}
%
Thus we find
%
\[ L_X Y = \sum dx^i(L_X Y) \partial_i = \sum \left( a^j \frac{\partial b^i}{\partial x^j} - \frac{\partial a^i}{\partial x^j} b^j \right) \frac{\partial}{\partial x^i} \]
%
This very complicated expressions can be represented in a very simple way. It is easy to read off from this formula that for each function $f \in C^\infty(M)$, $(L_X Y)(f) = X(Yf) - Y(Xf)$. We often define the right hand side as the \emph{Lie bracket}, or \emph{commutator} of $X$ and $Y$, denoted $[X,Y]$. We verify quite simply that
%
\begin{align*}
    [X,Y]_p(fg) &= X(Y(fg)) - Y(X(fg))\\
    &= X(gY(f) + fY(g)) - Y(gX(f) + fX(g))\\
    &= X(g)Y(f) + gX(Y(f)) + X(f)Y(g) + fX(Y(g))\\
    &- Y(g)X(f) - gY(X(f)) - Y(f)X(g) - fY(X(g))\\
    &= g [X,Y]_p(f) + f [X,Y]_p(g)
\end{align*}
%
so second order terms cancel out, and so the difference is really a derivation at $p$. There is an easier, coordinate way to do this. The idea is that if $X$, and $f$ are fixed, with flow $\phi_t$, then there is a smooth family of functions $g_t$ such that $f \circ \phi_t = f + t g_t$, and $g_0 = Xf$. We then calculate that
%
\begin{align*}
    ((\phi_t)_* Y)_p(f) &= (\phi_t)_*(Y_{\phi_{-t}(p)})(f) = Y_{\phi_{-t}(p)}(f \circ \phi_t)\\
    &= (Yf)(\phi_{-t}(p)) + t (Yg_t)(\phi_{-t}(p))
\end{align*}
%
and so
%
\[ (L_X Y)(f) = \lim_{t \to 0} \frac{(Yf)(p) - (Yf)(\phi_{-t}(p))}{t} - (Yg_t)(\phi_{-t}(p)) \]
%
The first term is easily as $X_p(Yf)$, and the second is easily seen to converge to $(Yg_0)(p) = Y_p(Xg)$.

The commutator reveals certain properties of the Lie derivative which are not obvious from a first glance. For instance, $[X,Y] = -[Y,X]$, and so as a consequence, $L_X(Y) = -L_Y(X)$, and $L_X(X) = 0$. Secondly, we see that $[aX_1 + bX_2,Y] = a[X_1,Y] + b[X_2,Y]$, so $L_{aX_1 + bX_2} = aL_{X_1} + bL_{X_2}$, giving linearity in the lower argument. Finally, we see the important Jacobi identity
%
\[ [X,[Y,Z]] = [[X,Y],Z] + [Y,[X,Z]], \]
%
which can be seen as a kind of `product rule' for the Lie bracket.

A useful interpretation of the Lie bracket of vector fields is as a measure of `noncommutativity' of two vector fields. We begin by noting that if $X$ is a vector field on $M$ generating a flow $\{ \phi_t \}$, and $\alpha: M \to N$ is a diffeomorphism, then $\alpha_* X$ generates the flow $\{ \alpha \circ \phi_t \circ \alpha^{-1} \}$.

\begin{lemma}
    If $\alpha: M \to M$ is a diffeomorphism, $X \in \Gamma(TM)$ generates a flow $\{ \phi_t \}$, then $\alpha_* X = X$  if and only if $\phi_t \circ \alpha = \alpha \circ \phi_t$ for all $t$.
\end{lemma}
\begin{proof}
    For then $\alpha_* X = X$ generates the flow $\{ \alpha \circ \phi_t \circ \alpha^{-1} \}$, which means that $\alpha \circ \phi_t \circ \alpha^{-1} = \phi_t$. Conversely, if $\phi_t = \alpha \circ \phi_t \circ \alpha^{-1}$, then $\alpha_* X$ generates the flow $\{ \phi_t \}$, so $\alpha_* X = X$.
\end{proof}

\begin{theorem}
    Let $X,Y \in \Gamma(TM)$, generating flows $\{ \phi_t \}$ and $\{ \psi_s \}$. Then $[X,Y] = 0$ if and only if $\phi_t \circ \psi_s = \psi_s \circ \phi_t$ for all $t, s$ for which this equation makes sense.
\end{theorem}
\begin{proof}
    If $\phi_t \circ \psi_s = \psi_s \circ \phi_t$, then the last lemma implies that $(\phi_t)_* Y = Y$ for all $t$. But this means that for all $p \in M$,
    %
    \[ L_X(Y)_p = \lim_{t \to 0} \frac{Y_p - ((\phi_t)_* Y)_p}{t} = 0, \]
    %
    so $[X,Y] = 0$. Conversely, if $[X,Y] = 0$, fix $p \in M$, and consider a curve $c(t) = ((\phi_t)_* Y)_p$ in $M_p$. Then
    %
    \begin{align*}
        c'(t) &= \lim_{s \to 0} \frac{((\phi_{t + s})_* Y)_p - ((\phi_t)_* Y)_p}{s}\\
        &= (\phi_t)_* \left( \lim_{s \to 0} \frac{((\phi_s)_* Y)_{\phi_{-t}(p)} - Y_{\phi_{-t}(p)}}{s} \right)\\
        &= (\phi_t)_* L_X(Y)_{\phi_{-t}(p)} = 0.
    \end{align*}
    %
    Thus $c(t) = c(0) = Y_p$ for all $t$. Since $p$ was arbitrary, this implies $(\phi_t)_* Y = Y$, and we can apply the lemma to conclude $\phi_t \circ \psi_s = \psi_s \circ \phi_t$ for all $s$. Since $t$ was arbitrary, this completes the proof.
\end{proof}

\begin{corollary}
    $X^1, \dots, X^k$ are vector fields on $M$ such that $X^1(p), \dots, X^k(p)$ are linearly independent for each $p \in M$, and if $[X^i,X^j] = 0$ for all $1 \leq i,j \leq k$, then around each point $p \in M$, there exists a coordinate system $(x,U)$ with $p \in U$ and such that
    %
    \[ X^i = \frac{\partial}{\partial x^i}. \]
\end{corollary}
\begin{proof}
    For the flows $\{ \phi^1_t \}, \dots, \{ \phi^n_t \}$, $\phi^i_t \circ \phi^j_s = \phi^j_s \circ \phi^i_t$ for all indices $i,j$ and times $t,s$. Define
    %
    \[ \alpha(t^1, \dots, t^k) = (\phi_{t^1}^1 \circ \dots \circ \phi_{t^k}^k)(p). \]
    %
    Then for each $1 \leq i \leq k$, and for each $f \in C^\infty(M)$,
    %
    \[ \alpha_* \left( \left. \frac{\partial}{\partial t^i} \right|_p \right)(f) = \lim_{s \to 0} \frac{f(\phi_s^i(p)) - f(p)}{s} = X^i_p(f). \]
    %
    Thus
    %
    \[ \alpha_* \left(\left. \partial/\partial t^i \right|_p \right) = X^i_p. \]
    %
    In particular, we conclude $\alpha$ has full rank in a neighbourhood of $0$, and so there exists a coordinate system $(x,U)$ about $p \in M$ such that $(x^i \circ \alpha)(t) = t^i$ for $1 \leq i \leq k$, and $(x^i \circ \alpha)(t) = 0$ for $i > k$. But this means that for $1 \leq i \leq k$,
    %
    \[ X^i_p = \alpha_* \left( \left. \frac{\partial}{\partial t^i} \right|_p \right) = \left. \frac{\partial}{\partial x^i} \right|_p. \]
    %
    Thus we have found the required coordinate system.
\end{proof}

Thus the Lie bracket measures the amount that two vector fields can `fail' to commute. It is much more technical, but we can also obtain a more quantitative version of this statement. Given two vector fields $X$ and $Y$ generating flows $\phi$ and $\psi$ respectively, and given $p \in M$, we can consider the curve
%
\[ c(t) = (\psi_{-t} \circ \phi_{-t} \circ \psi_t \circ \phi_t)(p), \]
%
which travels on the integral curve of $X$ for time $t$, then the integral curve of $Y$ for time $t$, then backwards along the integral curve of $X$ for time $t$, then backwards along the integral curve of $Y$ for time $t$. If $\phi$ and $\psi$ commute, then $c$ is constant. In general, all flows commute `up to first order'.

\begin{lemma}
    $c'(0) = 0$.
\end{lemma}
\begin{proof}
    Let
    %
    \[ \alpha(x,y,z,w) = (\psi_x \circ \phi_y \circ \psi_z \circ \phi_w)(p). \]
    %
    Then the chain rule implies that
    %
    \[ c'(0) = \alpha_* \left( \left. \frac{\partial }{\partial w} \right|_0 + \left. \frac{\partial}{\partial z} \right|_0 - \left. \frac{\partial}{\partial y} \right|_0 - \left. \frac{\partial}{\partial x} \right|_0 \right). \]
    %
    But
    %
    \[ \alpha^* \left( \left. \frac{\partial}{\partial w} \right|_0 \right)(f) = \left. \frac{\partial (f \circ \alpha)}{\partial w} \right|_0 = \lim_{s \to 0} \frac{f(\phi_s(p)) - f(p)}{s} = X_p(f). \]
    %
    \[ \alpha^* \left( \left. \frac{\partial}{\partial z} \right|_0 \right)(f) = \left. \frac{\partial (f \circ \alpha)}{\partial z} \right|_0 = \lim_{s \to 0} \frac{f(\psi_s(p)) - f(p)}{s} = Y_p(f). \]
    %
    \[ \alpha^* \left( \left. \frac{\partial}{\partial y} \right|_0 \right)(f) = \left. \frac{\partial (f \circ \alpha)}{\partial y} \right|_0 = \lim_{s \to 0} \frac{f(\psi_s(p)) - f(p)}{s} = Y_p(f). \]
    %
    \[ \alpha^* \left( \left. \frac{\partial}{\partial x} \right|_0 \right)(f) = \left. \frac{\partial (f \circ \alpha)}{\partial x} \right|_0 = \lim_{s \to 0} \frac{f(\phi_s(p)) - f(p)}{s} = X_p(f). \]
    %
    Thus
    %
    \[ c'(0) = X_p + Y_p - X_p - Y_p = 0. \qedhere \]
\end{proof}

Thus the non-commutativity of $c$ must be encoded in the higher derivatives. \emph{Normally}, for a curve $c$ on a manifold, the second derivative $c''(0)$ \emph{cannot} be defined as a tangent vector. For instance, if $c(t) = (\cos(t), \sin(t))$ is a curve on $S^1$, then $c''(t) = -c(t)$ is not a tangent vector (it actually lies perpendicular to any tangent vector to $S^1$). Thus defining second derivatives intrinsically on a manifold is quite a technical endeavor. However, in our case, the situation is more simple, for if $c$ is a curve on a manifold in $\RR^n$, and $c'(0) = \dots = c^{(k-1)}(0) = 0$, then the vector $c^{(k)}(0)$ \emph{is} always tangent to the curve $c$, because if we define a curve $c_1(t) = c(t^{1/k})$, then Taylor's theorem implies (since $c'(0) = \dots = c^{(k-1)}(0) = 0$) that $c_1$ is differentiable with $c_1'(0) = c^{(k)}(0)$. This suggests that if $c$ is a curve on a manifold $M$ with $c(0) = p$ and $c'(0) = 0$, then we should be able to define $c''(0)$ as an element of $T_pM$. Viewing tangent vectors as differentiable operators, this suggest we define
%
\[ c''(0)(f) = (f \circ c)''(0). \]
%
One calculates that this gives a derivation at $p$, so this actually does actually define $c''(0)$ as an element of $T_p M$.

\begin{theorem}
    $c''(0) = 2 [X,Y]_p$.
\end{theorem}
\begin{proof}
    This is a crazy, but simple calculation using the chain rule. To prevent things from getting excessively crazy, we introduce some clever notation. Define
    %
    \[ \alpha_1(t,h) = \psi_t(\phi_h(p)), \]
    \[ \alpha_2(t,h) = \phi_{-t}(\psi_h(\phi_h(p))), \]
    %
    and
    %
    \[ \alpha_3(t,h) = \psi_{-t}(\phi_{-h}(\psi_h(\phi_h(p)))). \]
    %
    Then for any $f \in C^\infty(M)$,
    %
    \[ \frac{\partial (f \circ \alpha_1)}{\partial t} = Yf \circ \alpha_1, \]
    \[ \frac{\partial (f \circ \alpha_2)}{\partial t} = -Xf \circ \alpha_2, \]
    %
    and
    %
    \[ \frac{\partial (f \circ \alpha_3)}{\partial t} = -Yf \circ \alpha_3. \]
    %
    By the chain rule, we find
    %
    \[ c''(0) = \left. \frac{\partial^2 (f \circ \alpha_3)}{\partial t^2} \right|_0 + 2 \left. \frac{\partial^2 (f \circ \alpha_3)}{\partial t \partial h} \right|_0 + \left. \frac{\partial^2 (f \circ \alpha_3)}{\partial h^2} \right|_0. \]
    %
    We calculate easily that
    %
    \[ \left. \frac{\partial^2 (f \circ \alpha_3)}{\partial t^2} \right|_0 = - \left. \frac{\partial (Yf \circ \alpha_3)}{\partial t} \right|_0 = (Y^2f \circ \alpha_3)(0,0) = (Y^2f)(p). \]
    %
    Next, we calculate by repeated applications of the chain rule that
    %
    \begin{align*}
        \left. \frac{\partial^2 (f \circ \alpha_3)}{\partial t \partial h} \right|_0 &= \left. \frac{\partial (-Yf \circ \alpha_3)}{\partial h} \right|_0\\
        &= \left. \frac{\partial (-Yf \circ \alpha_2)}{\partial t} \right|_0 + \left. \frac{\partial (-Yf \circ \alpha_2)}{\partial h} \right|_0\\
        &= (XYf \circ \alpha_2)(0,0) + \left. \frac{\partial (-Yf \circ \alpha_1)}{\partial t} \right|_0 + \left. \frac{\partial (-Yf \circ \alpha_1)}{\partial h} \right|_0\\
        &= (XYf)(p) - (Y^2f \circ \alpha_1)(0,0) - (XYf)(p)\\
        &= (XY - Y^2 - XY)(f)(p).
    \end{align*}
    %
    Some more liberal applications of the chain rule show
    %
    \begin{align*}
        \left. \frac{\partial^2 (f \circ \alpha_3)}{\partial h^2} \right|_0 &= \left. \frac{\partial^2 (f \circ \alpha_2)}{\partial t^2} \right|_0 + 2 \left. \frac{\partial^2 (f \circ \alpha_2)}{\partial t \partial h} \right|_0 + \left. \frac{\partial^2 (f \circ \alpha_2)}{\partial h^2} \right|_0.
    \end{align*}
    %
    We then calculate
    %
    \[ \left. \frac{\partial^2 (f \circ \alpha_2)}{\partial t^2} \right|_0 = (X^2 f \circ \alpha_2)(0,0) = (X^2 f)(p) \]
    %
    \begin{align*}
        \left. \frac{\partial^2 (f \circ \alpha_2)}{\partial t \partial h} \right|_0 &= \left. \frac{\partial (-Xf \circ \alpha_2)}{\partial h} \right|_0\\
        &= \left. \frac{\partial (-Xf \circ \alpha_1)}{\partial t} \right|_0 + \left. \frac{\partial (-Xf \circ \alpha_1)}{\partial h} \right|_0\\
        &= (-YXf \circ \alpha_1)(0,0) - (X^2 f)(p) = (-YX - X^2)(f)(p),
    \end{align*}
    %
    and
    %
    \begin{align*}
        \left. \frac{\partial^2 (f \circ \alpha_2)}{\partial h^2} \right|_0 &= \left. \frac{\partial^2 (f \circ \alpha_1)}{\partial t^2} \right|_0 + 2 \left. \frac{\partial^2 (f \circ \alpha_1)}{\partial t \partial h} \right|_0 + \left. \frac{\partial^2 (f \circ \alpha_1)}{\partial h^2} \right|_0\\
        &= (Y^2 f)(p) + 2 \left. \frac{\partial (Yf \circ \alpha_1)}{\partial h} \right|_0 + (X^2 f)(p)\\
        &= (Y^2 + 2XY + X^2)(f)(p).
    \end{align*}
    %
    Putting these together, we conclude
    %
    \begin{align*}
        \left. \frac{\partial^2 (f \circ \alpha_3)}{\partial h^2} \right|_0 &= (X^2 f)(p) - 2(YX + X^2)(f)(p) + (Y^2 + 2XY + X^2)(f)(p)\\
        &= (Y^2 + 2[X,Y])(f)(p).
    \end{align*}
    %
    In conclusion, this means that
    %
    \begin{align*}
        c''(0) &= (Y^2f)(p) + 2(XY - Y^2 - XY)(f)(p) + (Y^2 + 2[X,Y])(f)(p)\\
        &= 2[X,Y](f)(p) = 2[X,Y]_p(f).
    \end{align*}
    %
    This completes the calculation.
\end{proof}

\section{Parameter Curves}

If $X_1, \dots, X_k$ are vectors fields with $[X_i,X_j]_p \neq 0$ for some $i,j$, it is not possible to find a coordinate system $(x,U)$ with $p \in U$ such that on $U$,
%
\[ X_i = \frac{\partial}{\partial x^i}. \]
%
On the other hand, it may be possible to find functions $a_1, \dots, a_k$ such that $[X_i/a_i, X_j/a_j] = 0$, in which case we can find a coordinate system $(x,U)$ such that for each $i$, on $U$ we have
%
\[ X_i = a_i \frac{\partial}{\partial x_i}. \]
%
The integral curves of $X_i$ thus lie along the coordinate lines of the system $(x,U)$. In two dimensions, this is always possible.

\begin{theorem}
    If $X,Y$ are linearly independant vector fields on a 2-manifold, then around any point $p$ we can find coordinates $(x,U)$ such that $X = a \partial_{x^1}$ and $Y = b \partial_{x^2}$ for $a,b \in C^\infty(U)$.
\end{theorem}
\begin{proof}
    Without loss of generality, we may assume we are working on an open subset $U$ of $\RR^2$, that $p = 0$, that $X = \partial/\partial x^1$, and that $Y_0 = \partial / \partial x^2$. If $\phi$ is the flow generated by $Y$, then there is a smaller open neighbourhood $V \subset U$ such that for each $x \in U$, there is a unique $a(x)$ and $t(x)$ such that $\phi_{t(x)}(a(x),0) = x$. The map $x \mapsto (a(x),x^2) = (y^1,y^2)$ is $C^\infty$, and we verify that it's Jacobian at $x = 0$ is the identity, so the map gives a coordinate system $y$ in a neighbourhood of the identity. If we fix $y^2$, and vary $y^1$, we travel parallel to the $x$ axis, hence along the integral curves of $X$. Conversely, if we fix $y^1$, and vary $y^2$, then we travel along the integral curves of $Y$.
\end{proof}

\begin{remark}
    If $(x,U)$ gives parameter curves for two vector fields $X$ and $Y$, then so too does $(y,U)$, where $y = (\alpha,\beta) \circ x$ for any diffeomorphisms $\alpha$ and $\beta$ of $\RR$. In particular, for any smooth curve in $M$ with $c(0) = p$ and $c'(t)$ never a multiple of $X$ or $Y$, we can find a coordinate system $(x,U)$ giving parameters curves for $X$ and $Y$ with $x(c(t)) = (t,t)$.
\end{remark}

On the other hand, there exists vector fields $X$, $Y$, and $Z$ in $\RR^3$ such that no coordinate system can locally parameterize integral curves of $X$, $Y$, and $Z$. So two dimensions is the only case where we are guaranteed a successful theory.

\begin{example}
    Let $Y(x,y,z) = (0,1,0)$, $Z(x,y,z) = (0,0,1)$, and let $X(x,y,z) = (1,0,yx)$. Then the integral curves of $Y$ are lines parallel to the $Y$ axis, the integral curves of $Z$ are lines parallel to the $Z$ axis, and the integral curves of $X$ are the $x$ axis, and parabolas in planes parallel to the $xz$ plane, of varying eccentricity in $y$. If $(x,U)$ is a coordinate chart around the origin, such that as $x^1$ varies we travel along the integrals of $X$, as $x^2$ varies we travel along the integral curves of $Y$, and as $x^3$ varies we travel along the integral curves of $Z$. If we assume without loss of generality that $x(0) = 0$, then on one hand we conclude that $x^{-1}(a,b,0)$ lies in the $xy$ plane, since as we vary the $x^1$ coordinate from $(0,0,0)$ to $(a,0,0)$, we travel along the $x$-axis, and then as we vary $x^2$ from $(a,0,0)$ to $(a,b,0)$, we travel on a line parallel to the $y$ axis. On the other hand, if we vary $x^1$ first from $(0,0,0)$ to $(0,b,0)$, then we travel on the $y$-axis, and then we travel along a parabola as we vary $x^1$ from $(0,b,0)$ to $(a,b,0)$.
\end{example}

\section{Distributions and Integrability Conditions}

There is one way to switch our point of view from vector fields so that global existence results may be obtained. Consider a one-dimensional subbundle $\Delta$ of $TM$. Thus for each point $p \in M$, we associate a one-dimensional subspace $\Delta_p \subset T_p M$ such that around each point $p \in M$, we can find a neighbourhood $U$ and a smooth vector field $X \in \Gamma(TU)$ such that $\Delta_q$ is spanned by $X_q$ for each $q \in U$. Our goal is now to find a one-dimensional immersed submanifold $i: N \to M$ such that $i_*(T_pN) = \Delta_{i(p)}$ for each $p \in N$, known as an integral curve. The existence and uniqueness theory of differential equations gives the local existence of the manifold $N$, but the advantange of this approach is that we can join overlapping solutions. Thus $M$ can be uniquely written as the disjoint union of connected manifolds $N$ forming integral curves. The function $\Delta$ is known as a \emph{one dimensional distribution}, and $N$ is called a \emph{foliation} of $\Delta$. Our goal in this section is to extend this result to higher dimensional results.

A smooth $m$ dimensional subbundle $\Delta$ of $TM$ is known as a \emph{$m$ dimensional distribution} (these have nothing at all to do with the distributions of modern analysis). A useful fact for our purposes are that for each $p \in M$, we can find a neighbourhood $U$ of $p$ and $m$ vector fields $X_1, \dots, X_m \in \Gamma(TU)$ which span $\Delta$ at each point. Our goal, given an $m$ dimensional distribution $\Delta$, is to find immersed $m$ dimensional submanifolds $i: N \to M$ such that $i_*(T_p N) = \Delta_{i(p)}$ for each $p \in N$. We call $N$ an \emph{integral manifold} to $\Delta$.

In general, the problem of finding integral manifolds is not, in general, solvable. The simplest example is given in $\RR^3$ by the distribution $\Delta$ expressed by the equation $dz = y dx$, i.e. such that
%
\[ \Delta_{(x,y,z)} = \left\{ v_{(x,y,z)}: dz(v) = y dx(v) \right\}. \]
%
Thus $\Delta_{(x,y,z)}$ is spanned by $\partial_y$ and $\partial_x + y \partial_z$. If $N$ was an integral surface to $\Delta$ at the origin, then we may locally write $z = f(x,y)$ for some $C^\infty$ function $f$, such that
%
\[ \frac{\partial f}{\partial x} = y \quad\text{and}\quad \frac{\partial f}{\partial y} = 0. \]
%
The second equation implies $f$ is a function solely depending on $x$, but that cannot possibly be true since the partial derivatives of $f$ depend on $y$. Thus $f$, and thus the surface $N$, cannot possibly exist.

To see what causes integrability to fail here, let us consider the more general situation where $\Delta_{(x,y,z)}$ is specified by the equation $dz = f(x,y) dx + g(x,y) dy$, for $C^\infty$ functions $f$ and $g$, i.e. such that
%
\[ \Delta_{(x,y,z)} = \left\{ v_{(x,y,z)}: dz(v) = f(x,y) dx(v) + g(x,y) dy(v) \right\}. \]
%
As in the previous case, we can locally write $z = u(x,y)$ for some $C^\infty$ function $u$, and the constraints of thie function are that $u_x(x,y) = f(x,y)$ and $u_y(x,y) = g(x,y)$. Since mixed partials of smooth functions are equal, we conclude
%
\[ f_y = u_{xy} = u_{yx} = g_x. \]
%
It is well known that if $f$ and $g$ are such functions for which this equation holds, then such a function $u$ can be found, and thus an immersed submanifold exists.

\begin{lemma}
    If $f,g : \RR^2 \to \RR$ are smooth functions such that, in a neighbourhood of 0, $f_y = g_x$. Then for any $z_0 \in \RR$, there exists a function $u: \RR^2 \to \RR$ defined in a neighbourhood of zero such that $u_x = f$, $u_y = g$, and $u(0,0) = z_0$.
\end{lemma}
\begin{proof}
    Let $u(0,0) = z_0$. Then define $u(x,0) = \int_0^x f(t,0)\; dt$, and finally define
    %
    \[ u(x,y) = u(x,0) + \int_0^y g(x,t)\; dt = \int_0^x f(t,0)\; dt + \int_0^y g(x,t)\; dt. \]
    %
    Then
    %
    \begin{align*}
        u_x(x,y) &= f(x,0) + \int_0^y g_x(x,t)\; dt\\
        &= f(x,0) + \int_0^y f_y(x,t)\; dt = f(x,0) + (f(x,y) - f(x,0)) = f(x,y).
    \end{align*}
    %
    and $u_y(x,y) = g(x,y)$ by the fundamental theorem of calculus.
\end{proof}

Let us try and come up with a necessary and sufficient condition for integral curves to the more general distribution $\Delta$ given by the equation
%
\[ dz = f(x,y,z) dx + g(x,y,z) dy. \]
%
Our goal is to write $z = u(x,y)$, where $u$ is a function such that $u_x(x,y) = f(x,y,u(x,y))$ and $u_y(x,y) = g(x,y,u(x,y))$. Trying to apply equality of mixed derivatives, we calculate that
%
\[ u_{xy}(x,y) = f_y(x,y,u) + f_z(x,y,u) u_y(x,y) = f_y(x,y,u) + f_z(x,y,u) g(x,y,u) \]
%
and
%
\[ u_{yx}(x,y) = g_x(x,y,u) + g_z(x,y,u) u_x(x,y,u) = g_x(x,y,u) + g_z(x,y,u) f(x,y,u). \]
%
But this implies that $f_y + f_z g = g_x + g_z f$. This equation turns out to be sufficient to obtain the existence of $u$, but we need not prove this specific case, because we can treat a much more general situation.

We consider $C^\infty$ functions $f_i: \RR^m \times \RR^n \to \RR^n$ for $i \in \{ 1, \dots, m \}$, where we use $t$ to denote points in $\RR^m$ and $x$ to denote points in $\RR^m$. We then fix $x_0 \in \RR^n$, and try and find functions $u: \RR^m \to \RR^n$ such that $u(0,0) = x_0$, and $u_{t^i}(t) = f_i(t,u(t))$ for each $i$. Setting mixed partials in $t$ equal to one another, we find a necessary condition for this result to hold, and this is also a sufficient condition.

\begin{theorem}
    Let $U$ and $V$ be open subsets of $\RR^m$ and $\RR^n$, where $U$ is a neighbourhood of zero, and let $f_i: U \times V \to \RR^n$ be $C^\infty$ functions for $i \in \{ 1, \dots, m \}$. Moreover, suppose that for each $i,j \in \{ 1, \dots, m \}$,
    %
    \[ \frac{\partial f_j}{\partial t^i} - \frac{\partial f_i}{\partial t^j} + \sum_{k = 1}^n \frac{\partial f_j}{\partial x^k} f_i^k - \frac{\partial f_i}{\partial x^k} f_j^k = 0. \]
    %
    Then for each $x_0 \in V$, there exists a unique function $u: W \to V$, where $W$ is a neighbourhood of zero, such that
    %
    \[ \frac{\partial u}{\partial t^i}(t) = f_i(t,u(t)). \]
\end{theorem}
\begin{proof}
    Uniqueness will be obvious from our proof of existence. We define $u(0) = x_0$. Then we define $u(t,0,\dots,0)$ such that
    %
    \[ \frac{\partial u}{\partial t^1}(t,0,\dots,0) = f_1(t,0,\dots,0,u(t,0,\dots,0)). \]
    %
    Such a definition exists, and is unique, since it is an ordinary differential equation. For a fixed $t^1$, we then define $u(t^1,t,0,\dots,0)$ such that
    %
    \[ \frac{\partial u}{\partial t^2}(t^1,t,0,\dots,0) = f_2(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0)). \]
    %
    Now define
    %
    \[ g(t) = \frac{\partial u}{\partial t^1}(t^1,t,0,\dots,0) - f_1(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0)). \]
    %
    We calculate that
    %
    \begin{align*}
        g'(t) &= \frac{\partial^2 u}{\partial t^2 \partial t^1}(t^1,t,0,\dots,0)\\
        &\ \ - \frac{\partial f_1}{\partial t^1}(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0))\\
        &\ \ - \sum_{k = 1}^n \frac{\partial f_1}{\partial x^k}(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0)) \frac{\partial u^k}{\partial t^2}(t^1,t,0,\dots,0)\\
        &= \frac{\partial f_2(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0))}{\partial t^1}\\
        &\ \ - \frac{\partial f_1}{\partial t^1}(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0))\\
        &\ \ - \sum_{k = 1}^n \frac{\partial f_1}{\partial x^k}(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0)) \frac{\partial u^k}{\partial t^2}(t^1,t,0,\dots,0)\\
        &= \frac{\partial f_2}{\partial t^1}(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0))\\
        &\ \ + \sum_{k = 1}^n \frac{\partial f_2}{\partial x^k}(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0)) \frac{\partial u^k}{\partial t^1}(t^1,t,0,\dots,0)\\
        &\ \ - \frac{\partial f_1}{\partial t^1}(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0))\\
        &\ \ - \sum_{k = 1}^n \frac{\partial f_1}{\partial x^k}(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0)) \frac{\partial u^k}{\partial t^2}(t^1,t,0,\dots,0) = 0.
    \end{align*}
    %
    Since $g(0) = 0$, this means that
    %
    \[ \frac{\partial u}{\partial t^1}(t^1,t,0,\dots,0) = f_1(t^1,t,0,\dots,0,u(t^1,t,0,\dots,0)). \]
    %
    By a nasty calculation, we can continue defining $u$ along coordinate lines, using the integrability conditions to verify that $u$ satisfies the required partial differential equations at each step.
\end{proof}

\begin{example}
    Let $f: \CC^2 \to \CC$ be a complex analytic function. This means that $f(z^1,z^2) = u(z^1,z^2) + iv(z^1,z^2)$, where for $k \in \{ 1, 2 \}$, if we set $z^k = x^k + i y^k$, then the Cauchy-Riemann equations
    %
    \[ \frac{\partial u}{\partial x^k} = \frac{\partial v}{\partial y^k} \quad\text{and}\quad \frac{\partial v}{\partial x^k} = - \frac{\partial u}{\partial y^k} \]
    %
    hold. We will show the `complex ordinary differential equation' $\phi'(z) = f(z,\phi(z))$ is locally solvable by a complex analytic function $\phi$. Let $\phi = \phi^1 + i \phi^2$. The Cauchy-Riemann equations cannot be expressed as a partial differential equation like the one above, but we can `cheat' the Cauchy-Riemann equation by using the complex differentiable nature of $f$. Note that if $\phi$ solves the system of four PDEs
    %
    \[ \frac{\partial \phi^1}{\partial x} = u(z,\phi(z)) \quad \frac{\partial \phi^2}{\partial x} = v(z,\phi(z)) \]
    \[ \frac{\partial \phi^1}{\partial y} = -v(z,\phi(z)) \quad \frac{\partial \phi^2}{\partial y} = u(z,\phi(z)), \]
    %
    then $\phi$ automatically solves the Cauchy-Riemann equations, and $\phi'(z) = f(z,\phi(z))$. Setting mixed derivatives equal here, shows integrality conditions hold provide the Cauchy-Riemann equations hold for $f$, and thus a function $\phi$ exists locally in a neighbourhood of any point.
\end{example}

A basic fact about many theorems of differential geometry is that a situation holds if certain `integrality conditions' are satisfied, which involve mixed derivatives being set equal to one another. But there are almost always concise ways to express these integrality conditions without mentioning partial derivatives at all. So we now prove essentially the same formula, but using a more geometric approach.

If $f:M \to N$ is $C^\infty$, then for two vector fields $X \in \Gamma(TM), Y \in \Gamma(TN)$, we say $X$ and $Y$ are \emph{$f$ related} if for each $p \in M$,
%
\[ Y_{f(p)} = f_*(X_p). \]
%
Given a vector field $X \in \Gamma(TM)$, there need not be a vector field $Y \in \Gamma(TN)$ which is $f$ related to $X$, and vice versa, except in one simple case.

\begin{lemma}
    Let $f: M \to N$ be an immersion, and let $Y \in \Gamma(TN)$ be a vector field with $Y_{f(p)} \in f_*|_p(T_pM)$ for each $p \in M$. Then there exists a unique vector field $X \in \Gamma(TM)$ which is $f$ related to $Y$.
\end{lemma}
\begin{proof}
    Clearly, we must define $X_p$ to be the unique vector such that $f_*|_p(X_p) = Y_{f(p)}$. It remains to show that $X$ is smooth. To do this, we work locally, for each $p \in M$ finding coordinate systems $(x,U)$ with $p \in U$ and $(y,V)$ with $U \subset V$ such that
    %
    \[ (y \circ f \circ x^{-1})(a^1,\dots,a^n) = (a^1,\dots,a^n,0,\dots,0). \]
    %
    Then there are functions $\alpha^i \in C^\infty(U)$ such that for each $q \in U$,
    %
    \[ Y_{f(q)} = \sum_{k = 1}^n \alpha^i(q) \left. \frac{\partial}{\partial y^k} \right|_{f(q)}. \]
    %
    But then it is easy to see that for each $q \in U$,
    %
    \[ X_q = \sum_{k = 1}^n \alpha^i(q) \left. \frac{\partial}{\partial x^k} \right|_q, \]
    %
    so $X$ is $C^\infty$.
\end{proof}

For us, the most important property is the relation to the Lie bracket.

\begin{theorem}
    If $X^1$ is $f$-related to $Y^1$, and $X^2$ is $f$-related to $Y^2$, then $[X^1,X^2]$ is $f$-related to $[Y^1,Y^2]$.
\end{theorem}
\begin{proof}
    We calculate that for any $g \in C^\infty(N)$ and any $p \in M$,
    %
    \begin{align*}
        f_*([X^1,X^2]_p)(g) &= [X^1,X^2]_p(g \circ f)\\
        &= X^1_p(X^2_p(g \circ f)) - X^2_p(X^1_p(g \circ f))\\
        &= X^1(f_*(X^2_p)(g)) - X^2_p(f_*(X^1_p(g)))\\
        &= X^1_p(Y^2g \circ f) - X^2_p(Y^1g \circ f)\\
        &= f_*(X^1_p)(Y^2 g) - f_*(X^2_p)(Y^1g)\\
        &= Y^1_{f(p)}(Y^2 g) - Y^2_{f(p)}(Y^1 g)\\
        &= [Y^1,Y^2]_{f(p)}(g).
    \end{align*}
    %
    Since $g$ was arbitrary, we conclude that $f_*[X^1,X^2]_p = [Y^1,Y^2]_{f(p)}$.
\end{proof}

Given a distribution $\Delta$ on $M$, we say a vector field $X \in \Gamma(TM)$ \emph{belongs} to $\Delta$ if $X_p \in \Delta_p$ for each $p \in M$. If $N$ is an integral manifold of $M$ given by an immersion $i: N \to M$. If $Y^1$ and $Y^2$ are two vector fields which belong to $\Delta$, then we have shown there exist vector fields $X^1$ and $X^2$ which are $f$ related to $Y^1$ and $Y^2$. Then $[X^1,X^2]$ is $f$-related to $[Y^1,Y^2]$. But this means that for each $p \in N$, $f_*([X^1,X^2]_p) = [Y^1,Y^2]_{f(p)}$ is an element of $\Delta_{f(p)}$. If there is an integral manifold through each point on $M$, this implies that $[Y^1,Y^2]_q \in \Delta_q$ for each $q \in M$. Thus we conclude that if $Y^1$ and $Y^2$ belong to $\Delta$, then $[Y^1,Y^2]$ belongs to $\Delta$. We say that a distribution $\Delta$ is \emph{integrable}, or \emph{involutive} if the set of vector fields belonging to $\Delta$ is closed under the Lie bracket. This is a necessary condition for integral manifolds to exist, and we will soon show this is sufficient.

\begin{lemma}
    If $X^1, \dots, X^m$ span a distribution $\Delta$ in a neighbourhood $U$, then $\Delta$ is integrable on $U$ if and only if there exists smooth functions $a_{ij}^k$ such that
    %
    \[ [X_i,X_j] = \sum a_{ij}^k X_k. \]
\end{lemma}
\begin{proof}
    Such functions clearly exist if $\Delta$ is integrable. Conversely, if $Y^1$ and $Y^2$ are vector fields belonging to $\Delta$, then we can find smooth functions $\{ \alpha_k \}$ and $\{ \beta_k \}$ such that $Y^1 = \sum_{k = 1}^m \alpha_k X^k$ and $Y^2 = \sum_{k = 1}^m \beta_k X^k$. Since
    %
    \[ [\alpha_i X^i, \beta_j X^j] = \alpha_i \beta_j [X^i,X^j] + \alpha_i X^i(\beta_j) X^j - \beta_j X^j(\alpha_i) X^i, \]
    %
    we can certainly write $[Y^1,Y^2]$ as a linear combination of $X^1, \dots, X^m$.
\end{proof}

\begin{remark}
    If, on $\RR^3$, $X = \partial_x + f \partial_z$, and $Y = \partial_y + g \partial_z$, then
    %
    \[ [X,Y] = \left( g_x - f_y + fg_z - g f_z \right) \partial_z. \]
    %
    If $\Delta_{(x,y,z)}$ is spanned by $\partial_x + f \partial_z$ and $\partial_y + g \partial_z$ at each point, then $\Delta$ is integrable if and only if $g_x - f_y + fg_z - gf_z = 0$, which is the integrality condition we saw before. The next theorem thus addresses the PDE solution we gave before in a geometric setting.
\end{remark}

We now address the `solutions to PDEs' approach we took before in a geometric setting. Our proof is quite different.

\begin{theorem}[Frobenius Integrability Theorem]
    If $\Delta$ is an integrable, $m$ dimensional distribution on a manifold $M$, and $p \in M$, then there is a coordinate system $(x,U)$ with $p \in U$ such that for each $a^{m+1}, \dots, a^n$, the set
    %
    \[ \{ q \in U: x^{m+1}(q) = a^{m+1}, \dots, x^n(q) = a^n \} \]
    %
    is an integral manifold of $\Delta$.
\end{theorem}
\begin{proof}
   Without loss of generality, we may assume $M = \RR^n$, $p = 0$, and that $\Delta_0$ is spanned by $\partial_1|_0, \dots, \partial_m|_0$. Let $\pi: \RR^n \to \RR^m$, and define
   %
   \[ \pi(a) = (a^1, \dots, a^m). \]
   %
   Then $\pi_*$ is locally injective when restricted as a map from $\Delta$ to $T\RR^m$. So in a neighbourhood of the origin in $M$, for each $q$, we can find smooth vector fields $X^1(q), \dots, X^m(q) \in \Delta_q$ such that for each $i \in \{ 1, \dots, m \}$,
   %
   \[ \pi_*(X^i(q)) = \left. \partial_i \right|_{\pi(q)}. \]
   %
   Thus the vector fields $X^i$ and $\partial_i$ are $\pi$-related in a neighbourhood of the origin. But this means that for each $i,j \in \{ 1, \dots, m \}$, $[X^i,X^j]$ are $\pi$-related to $[\partial_i,\partial_j] = 0$. Thus $\pi_*([X^i,X^j]_p) = 0$ for each $p$ in a neighbourhood of the origin. But since $\pi_*$ is injective on $\Delta_p$, this implies $[X^i,X^j]_p = 0$. But this means that $[X^i,X^j] = 0$, so in particular, we find a coordinate system $(x,U)$ such that for each $i \in \{ 1, \dots, m \}$,
   %
   \[ X^i = \frac{\partial}{\partial x^i}. \]
   %
   But this means that integral manifolds are obtained by fixing the coordinates $x^{m+1}, \dots, x^n$, and varying $x^1, \dots, x^m$. If $N$ is any connected integral submanifold obtained by a immersion $i: N \to \RR^n$ and with $i(p) = 0$, then for each $1 \leq k \leq n-m$, and for each tangent vector $X_q$ of $N$,
   %
   \[ d(x^{m+k} \circ i)(X_q) = i_*(X_q)(x^{m+k}) = 0, \]
   %
   since $i_*(X_q)$ is an element of $\Delta_{i(q)}$. So $x^{m+k}$ is constant on $i^{-1}(I) \cap N$, which gives the required uniqueness result.
 \end{proof}

As we mentioned, the advantage of distribution theory is it enables us to obtain global results about differential equations on manifolds. We say a manifold $N$ is an $m$ dimensional \emph{foliation} of $M$ if it is immersed in $M$ by a map $i: N \to M$ which is injective, surjective, and such that around each $p \in M$, there exists a coordinate system $(x,U)$ with $p \in U$ and such that the components of $N \cap i^{-1}(U)$ are given by
%
\[ \{ q \in N: x^{m+1}(i(q)) = a^{m+1}, \dots, x^n(i(q)) = a^n \}, \]
%
for fixed coefficients $a^{m+1}, \dots, a^n$. Each component of $N$ is known as a \emph{leaf}, or \emph{folium} of the foliation.

\begin{theorem}
    Let $\Delta$ be a smooth integrable distribution on $M$. Then $M$ is foliated by an integral manifold $N$ of $\Delta$, each component of $N$ being called a \emph{maximal integral manifold} of $\Delta$. If $M$ is metrizable, then $N$ is metrizable.
\end{theorem}
\begin{proof}
    We can cover $M$ by a family of coordinate systems $(x_i,U_i)$ such that for each of the slices
    %
    \[ S_i(a) = \{ p \in U_i: x_i^{m+1}(p) = a^{m+1}, \dots, x^n(p) = a^n \} \]
    %
    for $a = (a^{m+1}, \dots, a^n)$, are integral manifolds of $\Delta$. We let $N = M$, but with a different topological and smooth structure which we will now define. We let $\{ S_i(a) \}$ be a cover of $N$, and for each $S_i(a)$, we associate the chart $(x_{i,a}, U \cap S_i(a))$ where $x_{ia}(p) = (x_i^1(p), \dots, x_i^m(p))$. If a slice $S_i(a)$ intersects a slice $S_j(b)$, then $x_{ia} \circ x_{jb}^{-1}$ is certainly smooth, so this gives $N$ a topological and smooth manifold structure. It is clear the set theoretic inclusion $i: N \to M$ gives a foliation of $M$ by $N$. It now suffices to show that if $M$ is metrizable, then $N$ is metrizable. We may assume $M$ is connected, and thus that the family of coordinate systems $(x_i,U_i)$ is countable. For each $i,j$, and $a \in \RR^m$, $S_i(a) \cap U_j$ has at most countably many components in $N$, and each component is contained in a slice of $U_j$, so $S_i(a) \cap U_j$ is contained in countably many slices of $U_j$. We say two slices $S_i(a)$ and $S_j(b)$ if there is a sequence $i_0, \dots, i_n$ and $a_0, \dots, a_n$ with $i_0 = i$, $i_n = j$, $a_0 = a$, and $a_n = b$, such that $S_{i_k}(a_k) \cap S_{i_{k+1}}(a_{i_{k+1}}) \neq \emptyset$. The union of slices joined to a particular slice forms a component of $N$, and countably many slices are joined to one another, hence the union is second countable. Thus $N$ is metrizable.
\end{proof}

If $(x,U)$ is a coordinate chart considered in the proof, then two slices of the chart may belong to the same leaf of the foliation. However, only at most countably many slices can belong to the same leaf, for otherwise the manifold $N$ will not be metrizable. Here is a simple consequence.

\begin{theorem}
    Let $M$ be a $C^\infty$ manifold, and let $M_1$ be a leaf of the foliation $i: N \to M$ determined by some distribution $\Delta$. If $f: L \to M$ is smooth and $f(L) \subset i(M_1)$, then $f$ is smooth considered as a map into $M_1$.
\end{theorem}
\begin{proof}
    It suffices to show that $f$ is continuous as a map into $M_1$. Given $p \in L$, choose a coordinate system $(x,U)$ around $f(p)$ such that the slices
    %
    \[ \{ q \in U : x^{m+1}(q) = a^{m+1}, \dots, x^n(q) = a^n \}. \]
    %
    Since $f$ is continuous as a map into $M$, there exists a coordinate system $(y,V)$ around $p$ such that $f(V) \subset U$. Without loss of generality, we may assume that $V$ is connected. Then $f(V)$ is connected. If $f(V)$ was not contained in a single slice of the chart $(x,U)$, then it would pass over uncountably many slices, which is impossible since $f(V) \subset M_1$, and $M_1$ intersects at most countably many slices. Thus $V$ is contained in a single slice, and it is therefore clear that $f$ is continuous restricted as a map from $V$ to $M_1$, and thus continous as a map into $M_1$ at $p$. Since $p$ is arbitrary, this shows $f$ is continuous.
\end{proof}

We can also restate the Frobenius integrability theorem in a `dual form' in terms of differential forms. Given a distribution $\Delta$ of rank $m$, we can consider, for each $k$, the subspace $I^k(\Delta)$ of $\Omega^k(M)$ consisting of $\omega$ such that for any $k$ vector fields $X_1, \dots, X_k$ belonging to $\Delta$, $\omega(X_1, \dots, X_k) = 0$. This are simply sections of the sub-bundle of $\Omega^k(TM)$ consisting of $k$-forms vanishing on sections of $\Delta$. We then let $I(\Delta) = I^0(\Delta) + \dots + I^n(\Delta) \subset \Omega(M)$. Then $I(\Delta)$ is actually an ideal of $\Omega(M)$, because if $\omega \in I^i(\Delta)$ and $\eta \in \Omega^j(M)$, then $\omega \wedge \eta \in I^{i+j}(\Delta)$.

If $\Delta$ is an $m$ dimensional distribution, then for each $p \in M$, there is a chart $(x,U)$ containing $p$ such that, restricted to $U$, the ideal $I(\Delta)$ is locally generated by $n-m$ one-forms $\omega^{m+1}, \dots, \omega^n$. To see why, we note that for a suitably small neighbourhood $U$ there are $m$ vector fields $X_1, \dots, X_m$ which span the distribution $\Delta$ on $U$. We may also choose a coordinate chart $(x,U)$ such that for each $i \in \{ 1, \dots, m \}$,
%
\[ X_i = \left. \frac{\partial}{\partial x^i} \right|_p. \]
%
It then follows that
%
\[ \left( dx^1(p) \wedge \dots \wedge dx^m(p) \right) \left( X_1|_p, \dots, X_m|_p \right) = 1. \]
%
By continuity, $(dx^1 \wedge \dots \wedge dx^m)(X_1, \dots, X_m)$ is non-zero in a neighbourhood of $p$, which, by thinning $U$ if necessary, we may assume to be all of $U$. It therefore follows that $dx^1(q), \dots, dx^m(q)$ are linearly independent linear functionals on $\Delta_q$ for all $q \in U$. Since $\Delta_q$ is $m$ dimensional, this implies
%
\[ dx^1(q), \dots, dx^m(q) \]
%
spans $\Delta_q^*$, and, in particular, $dx^1, \dots, dx^m$ span the $C^\infty(U)$ module $\Gamma(\Delta|_U^*)$. In particular there exist smooth functions $\smash{a_i^j} \in C^\infty(U)$ for each $i \in \{ 1, \dots, m \}$ and $j \in \{ m+1, \dots, n \}$ such that for each such $j$,
%
\[ dx^j = \sum_{i = 1}^m a_i^j dx^i. \]
%
If we define, for each $j \in \{ m+1, \dots, n \}$, the 1-form
%
\[ \omega_j = dx^j - \sum_{i = 1}^m a_i^j dx^i, \]
%
then $\omega_j(X) = 0$ for each $X$ belonging to $\Delta$. Then $dx^1, \dots, dx^m, \omega_{m+1}, \dots, \omega_{m+n}$ form a basis for $\Gamma(T^*U)$, and so if $\omega$ is any 1-form which vanishes on all vector fields belonging to $\Delta$, then we can write it as a $C^\infty(U)$ linear combination of $\omega_{m+1}, \dots, \omega_{m+n}$. The space of all alternating $k$ tensors on $\Delta_p$ has dimension
%
\[ \frac{m!}{k!(m-k)!}. \]
%
Similarily, the space of all alternating $k$ tensors on $T_p M$ has dimension
%
\[ \frac{n!}{k!(n-k)!}. \]
%
The space of $k$ forms in the ideal generated by $\omega_{m+1}, \dots, \omega_{m+n}$ has dimension
%
\[ \frac{n!}{k!(n-k)!} - \frac{m!}{k!(m-k)!}. \]
%
But this means that any alternating $k$ tensor on $T_p M$ which vanishes when restricted to a tensor on $\Delta_p$ is an element of the ideal generated by the forms $\omega_{m+1}, \dots, \omega_n$, so we conclude $I(\Delta|_U) = (\omega_{m+1}, \dots, \omega_n)$.

\begin{remark}
    The last argument also established the fact that, as an ideal of $\Gamma(TM)$, $I(\Delta)$ is locally generated by the elements of $I^1(\Delta)$. A partition of unity argument then shows that $I(\Delta)$ is generated by $I^1(\Delta)$.
\end{remark}

To prove the following restatement of the Frobenius integrability theorem, we recall that for any $k$ form $\omega$, the exterior derivative $d\omega$ satisfies
%
\begin{align*}
    d\omega(X_1, \dots, X_{k+1}) &= \sum_{i=1}^{k+1} (-1)^{i+1} X_i(\omega(X_1, \dots, \widehat{X_i}, \dots, X_{k+1}))\\
    &\ + \sum_{1 \leq i < j \leq k} (-1)^{i+j} \omega([X_i,X_j], X_1, \dots, \widehat{X_i}, \dots, \widehat{X_j}, \dots, X_{k+1}).
\end{align*}
%
The exterior derivative extends to a linear map on $\Gamma(\Omega(TM))$ by linearity.

\begin{theorem}[The Frobenius Integrability Theorem]
    A distribution $\Delta$ on a manifold $M$ is integrable if and only if $d(I(\Delta)) \subset I(\Delta)$.
\end{theorem}
\begin{proof}
    Suppose $\Delta$ is integrable. If $\omega$ is a $k$ form vanishing on $\Delta$, then for any $k+1$ vector fields $X_1, \dots, X_{k+1} \in \Gamma(\Delta)$, we have $[X_i,X_j] \in \Gamma(\Delta)$, and thus $d\omega(X_1, \dots, X_{k+1}) = 0$ by the formula above. On the other hand, if $d(I(\Delta)) \subset I(\Delta)$, then in particular, for any one form $\omega \in I(\Delta)$, and any $X,Y \in \Gamma(\Delta)$,
    %
    \[ d\omega(X,Y) = X(\omega(Y)) - Y(\omega(X)) - \omega([X,Y]) = - \omega([X,Y]) = 0. \]
    %
    Thus $\omega([X,Y]) = 0$. But since $\omega$ was arbitrary, this means $[X,Y] \in \Gamma(\Delta)$, which shows $\Delta$ is integrable. 
\end{proof}

\begin{remark}
    The condition $d(I(\Delta)) \subset I(\Delta)$ can be verified by checking that $d(I^1(\Delta)) \subset I^2(\Delta)$, since $I(\Delta)$ is generated by the one-forms it contains.
\end{remark}

\begin{corollary}
    Let $\omega^{m+1}, \dots, \omega^n$ be linearly independant one-forms. Then there exists a neighbourhood $U$ around each point such that there are smooth functions $f^\alpha_{m+1}, \dots, f^\alpha_n, g^{m+1}, \dots, g^n$ for each $\alpha \in \{ m+1, \dots, n \}$ such that for each $\alpha$, on $U$,
    %
    \[ \omega^\alpha = f^\alpha_{m+1} dg^{m+1} + \dots + f^\alpha_n dg^n, \]
    %
    if and only if there exists 1-forms $\theta^\alpha_\beta$ for $\alpha, \beta \in \{ m+1, \dots, n \}$ such that for each $\alpha$,
    %
    \[ d\omega^\alpha = \theta^\alpha_{m+1} \wedge \omega^{m+1} + \dots + \theta^\alpha_n \wedge \omega^n. \]
\end{corollary}
\begin{proof}
    The one-forms $\omega^{m+1}, \dots, \omega^n$ generate a $m$ dimensional distribution $\Delta$ by considering the intersection of the kernels of the one-forms at each point. Then $I(\Delta)$ is generated by $\omega^{m+1}, \dots, \omega^n$. If we can write
    %
    \[ \omega^\alpha = f^\alpha_{m+1} dg^{m+1} + \dots + f^\alpha_n dg^n, \]
    %
    then $dg^{m+1}, \dots, dg^n$ are linearly independant, hence we may write $dg^\alpha = \sum h^\alpha_\beta \omega^\beta$ for some smooth functions $h^\alpha_\beta$. It follows that for each $\alpha$,
    %
    \begin{align*}
        d\omega^\alpha &= \sum_{i = m+1}^n df^\alpha_i \wedge dg^i = \sum_{j = m+1}^n \sum_{i = m+1}^n h^i_j df^\alpha_i \wedge \omega^j.
    \end{align*}
    %
    Conversely, if
    %
    \[ d\omega^\alpha = \sum_{i = m+1}^n \theta^\alpha_i \wedge \omega^i, \]
    %
    then $d(I(\Delta)) \subset I(\Delta)$. It follows from the Frobenius theorem that $\Delta$ is integrable, so around each point $p$, there exists a coordinate system $(x,U)$ such that the integral manifolds of $\Delta$ on $U$ are given by the slices
    %
    \[ \{ p \in U: x^{m+1}(p) = a^{m+1}, \dots, x^n(p) = x^n \}, \]
    %
    for fixed constants $a^{m+1}, \dots, a^n$. But then $dx^{m+1}, \dots, dx^n$ span $I^1(\Delta)$ on $U$, so there exists $f^\alpha_\beta$ such that on $U$,
    %
    \[ \omega^\alpha = \sum_{\beta = m+1}^n f^\alpha_\beta dx^\beta. \qedhere \]
\end{proof}

\chapter{Differential Forms and Integration}

We wish to define a `coordinate-independant' way to define integration, so that we may extend the notion of integration from open subsets of $\RR^n$ to more general manifolds. Recall the change of variables formula, which says that if $y: U \to V$ is a diffeomorphism, and $f \in L^1(U)$, then
%
\begin{equation} \label{changeofvariablesformula}
    \int_V \left|\text{det} \left( \frac{\partial y^i}{\partial x^j} \right) \right| \cdot (f \circ y)(x)\; dx = \int_U f(y)\; dy.
\end{equation}
%
The objects we integrate on a manifold therefore must satisfy some transformation law. In particular, we cannot integrate scalar functions since the integral of the function in different coordinate systems can give different answers. Of course, there is no natural choice of measure on a manifold, so we should not expect to be able to integrate pure functions. Whatever objects $\omega$ we integrate must contain additional information which substitutes for a choice of measure on the manifold. If, in a certain coordinate system $(x,U)$, the object $\omega$ is associated with a smooth function $\omega_x: U \to \RR$, then in order to integrate $\omega$ in a coordinate independent way based on \eqref{changeofvariablesformula}, we would expect
%
\begin{equation}
    \omega_y = \omega_x \cdot \left| \text{det} \left( \frac{\partial y^i}{\partial x^j} \right) \right|.
\end{equation}
%
Then, for any two coordinate systems $(x,U)$, $(y,V)$, if $\omega_x$ is compactly supported on $U \cap V$, then \eqref{changeofvariablesformula} indicates that
%
\begin{equation} \label{welldefinedintegration}
    \int_{x(U)} (\omega_x \circ x^{-1})\; dx = \int_{y(V)} (\omega_y \circ y^{-1})\; dy.
\end{equation}
%
If $\omega$ is a scalar density which is \emph{compactly supported}, in the sense that there exists a compact set $K \subset M$ such that $\omega(p) = 0$ for $p \not \in K$, then we can cover $K$ by a finite family of coordinate systems $\{ (x_1,U_1), \dots, (x_N,u_N) \}$, as well as considering a smooth partition of unity $\{ \phi_1, \dots, \phi_N \}$ on $U_1 \cup \dots \cup U_N$, with $\phi_i$ compactly supported on some compact set $K_i \subset U_i$. Then it is easy to verify from \eqref{welldefinedintegration} that the quantity
%
\begin{equation} \label{integration}
    \int_M \omega = \sum_{i = 1}^N \int_{x_i(K \cap K_i)} \phi_i \cdot \omega_{x_i} \circ x_i^{-1}
\end{equation}
%
is well defined and independant of the cover and partition of unity chosen. The objects $\omega$ will be known as {\it scalar densities}.

\section{Scalar Densities}

It should be quite clear that we are going to find a vector bundle over $M$ whose sections are precisely scalar densities. Indeed, consider the functor $\text{Vol}$, whose domain is the family of \emph{$n$ dimensional} vector spaces, mapping each vector space to $\RR$. For each $n$ dimensional vector space $V$, we fix an isomorphism $T_V: V \to \RR^n$.  For each linear map $T: V \to W$, we consider the one-dimensional linear map $\text{Vol}(f): \RR \to \RR$ given by multiplication by $\det(T_W \circ f \circ T_V^{-1})$. The functor is certainly smooth, and so given any $n$ dimensional manifold $M$, we can associate a bundle $\text{Vol}(TM)$ (the family of isomorphisms $\{ T_V \}$ chosen is irrelevant since the object $\text{Vol}(TM)$ will be the same up to a natural isomorphism). If $\omega$ is a section of $\text{Vol}(TM)$, and $(x,U)$ induces a trivialization $\text{Vol}(x_*): \text{Vol}(TU) \to U \times \RR$, then we can associate with $\omega$ a function $\omega_x: U \to \RR$ such that $\text{Vol}(x_*)(\omega_p) = (p, \omega_x(p))$. If If $(y,V)$ is another coordinate system, then $(y \circ x^{-1})_*: \varepsilon^n(U \cap V) \to \varepsilon^n(U \cap V)$ is given by $(p,v) \mapsto (p, D(y \circ x^{-1})(x(p)))$. Thus $\text{Vol}((y \circ x^{-1})_*): \varepsilon^1(U \cap V) \to \varepsilon^1(U \cap V)$ is given by
%
\[ t_p \mapsto \Big| \text{det} \big( D(y \circ x^{-1})(x(p)) \big) \Big| \cdot t_p. \]
%
In particular, if $\omega$ is associated the functions $\omega_x$ and $\omega_y$, then
%
\[ \omega_y = \omega_x \cdot \left| \det \left( \frac{\partial y^i}{\partial x^j} \right) \right| \]
%
Thus sections of $\text{Vol}(TM)$ are precisely the scalar densities we needed. We denote $\text{Vol}(TM)$ by $\text{Vol}(M)$.

Given a section $\omega$ of $\text{Vol}(TM)$, we can consider a pointwise-map
%
\[ \omega: \Gamma(TM) \times \dots \times \Gamma(TM) \to C^\infty(M) \]
%
such that if $p \in U$, and $(x,U)$ is a coordinate system, then
%
\[ \omega(X_1, \dots, X_n)(p) = \omega_x(p) \cdot \big|\text{det} \big(x_*(X_1|_p), \dots, x_*(X_n|_p) \big) \big|. \]
%
If $A \in \Gamma(\text{End}(TM))$ is a smooth family of endomorphisms of the tangent bundle, then
%
\begin{equation} \label{parallelogramvol} \omega(A X_1, \dots, A X_n) = |\det(A)| \cdot \omega(X_1, \dots, X_n). \end{equation}
%
Conversely, if $\omega: \Gamma(TM) \times \dots \times \Gamma(TM) \to C^\infty(M)$ is a pointwise-defined map satisfying \eqref{parallelogramvol}, then we can define a section $\omega$ of $\text{Vol}(TM)$ such that for each coordinate system $(x,U)$, and $p \in U$, we have
%
\[ \omega_x(p) = \omega \left( \frac{\partial}{\partial x^1}, \dots, \frac{\partial}{\partial x^n} \right). \]
%
Then \eqref{parallelogramvol} shows that the family of functions $\{ \omega_x \}$ agree with one another in such a way to induce a section of $\text{Vol}(M)$. In particular, we see that a scalar density can be viewed as a way to assign areas to parallelograms in the tangent space of a manifold. Thus a scalar density is a `ruler' which can be used to measure infinitisimal volume on a manifold.

\section{Oriented Integration}

There is a simple trick, that will prove immensely useful, related to the construction of scalar densities on an \emph{oriented} manifold $M$. Instead of considering objects $\omega$ which undergo the transformation law
%
\[ \omega_y = \omega_x \cdot \left| \det \left( \frac{\partial y^i}{\partial x^j} \right) \right|, \]
%
we consider objects which undergo the transformation law
%
\[ \omega_y = \omega_x \cdot \det \left( \frac{\partial y^i}{\partial x^j} \right). \]
%
For the same reasons as before, we can associate with each manifold $M$ a one-dimensional bundle $\Omega^n(M)$ (the notation will become clear shortly) whose sections transform in the manner above, and are known as \emph{signed scalar densities}. To do this, we consider the functor $\Omega^n$ which associates with each vector space $V$ the real numbers, such that if $f: V \to W$ is a linear map, then $\Omega^n(f): \RR \to \RR$ is given by multiplication by
%
\[ \det(T_W \circ f \circ T_V^{-1}) \]
%
rather than multiplication by the absolute value of this determinant.

If the functors $\Omega^n$ and $\text{Vol}$ are considered as functors on the family of \emph{oriented vector spaces}, then they are actually isomorphic. Indeed, if $V$ and $W$ are oriented, and $f: V \to W$ is an oriented isomorphism, then both $\text{Vol}(f)$ and $\Omega^n(f)$ are given by multiplication by $\det(T_W \circ f \circ T_V^{-1})$, since this determinant is positive. If $M$ is an oriented manifold, we can therefore obtain an isomorphism between $\text{Vol}(M)$ and $\Omega^n(M)$ by taking a cover of $M$ by oriented coordinate charts, in which case the correspondence is then trivial. Thus signed and unsigned scalar densities are, in a sense, the same category of objects on a manifold. On non-orientable manifolds, this is not the case. For instance, if $\omega \in \Gamma(\Omega^n(M))$ is an everywhere non-zero section, then it automatically gives an orientation of $M$ by letting those coordinate charts $(x,U)$ be orientable if $\omega_x$ is a positive function. On the other hand, $\text{Vol}(M)$ always has an everywhere non-zero section, since the sign of a scalar density at a point does not depend on the coordinate system, and so we can form a partition of unity to add up positive scalar densities without worrying about cancellation.

\section{Lower Dimensional Integration}

It may appear strange why we introduced the family of signed scalar densities. But it turns out that signed scalar densities are much more amenable than unsigned scalar densities on \emph{lower dimensional submanifolds}. Of course, we can certainly define a $k$-dimensional scalar density; we simply consider a pointwise-defined map
%
\[ \omega: \Gamma(TM) \times \dots \times \Gamma(TM) \to C^\infty(M), \]
%
which takes $k$ vector fields $X_1, \dots, X_k \in \Gamma(TM)$. Restricted to the span of these vector fields, $\omega$ should operate like the scalar densities we previously defined. In particular, this means that for any $a_{ij} \in C^\infty(M)$, for $i,j \in \{ 1, \dots, k \}$,
%
\begin{align*}
    \omega \left( \sum_{i = 1}^k a_{1i} X_i, \dots, \sum_{i = 1}^k a_{ki} X_i \right) = |\det(a_{ij})| \cdot \omega(X_1, \dots, X_k). 
\end{align*}
%
Thus $\omega$ is a way of measuring the area of $k$-dimensional parallelograms. If $N$ is a $k$ dimensional submanifold of $M$, then the restriction of $\omega$ to vectors fields in $\Gamma(TN)$ induces a scalar density on $N$. Thus, if $\omega$ is a $k$-dimensional scalar density and $\text{supp}(\omega) \cap N$ is compact, then the integral
%
\[ \int_N \omega \]
%
is well defined. The only problem with this definition is that the family of all scalar densities are very complex. In particular, the bundle $\xi$ associated with the family of scalar densities is infinite dimensional at each point. For instance, even when $k = 1$, $\xi_p$ would have to contain all the seminorms on $T_p M$, which is certainly an infinite dimensional family.

The advantage of considering \emph{signed $k$ dimensional densities}, is that there is an obvious class of simple signed densities. Instead of focusing on all functions $\omega$ such that
%
\begin{align*}
    \omega \left( \sum a_{1i} X_i, \dots, \sum a_{ki} X_i \right) = \det(a_{ij}) \cdot \omega(X_1, \dots, X_k),
\end{align*}
%
we can focus on those maps $\omega$ which are \emph{multilinear}. The multilinear $k$ dimensional scalar densities will be known as \emph{differential $k$-forms}. Though these densities can now only on \emph{oriented} $k$ dimensional submanifolds, they have an elegant theory which makes them much more useful than their more general counterparts. In particular, the theory of multilinear maps satisfying the signed transformation law have a rich algebraic theory, which we now detail.

\begin{comment}

To begin with, we would like a formula which gives the area of a $k$ dimensional parallelogram spanned by $k$ vectors $v_1, \dots, v_k$ in $\RR^n$. This quantity is
%
\[ \det \left( v_i \cdot v_j \right)^{1/2} \]
%
To see why, we note that this quantity is clearly invariant under rotation, and since we would imagine rotations to preserve rotation, we may assume $v_1, \dots, v_k$ lie in the $k$ dimensional plane in $\RR^n$ spanned by $e_1, \dots, e_k$. If $v_i = w_i \times \{ 0 \}$ for $w_i \in \RR^k$, then
%
\[ \det \left( v_i \cdot v_j \right)^{1/2} = \det \left( w_i \cdot w_j \right)^{1/2} \]
%
That this right hand side is equal to what we would expect the usual volume to be, i.e. $\det(w_1, \dots, w_k)$, is obtained by noting that if $A$ is a $k \times k$ matrix whose $i$'th column is $w_i$, then $(A^T A)_{ij} = w_j \cdot w_i$, and so
%
\begin{align*}
    \det( w_i \cdot w_j) = \det( A^T A ) = \det(A)^2 = \det(w_1, \dots, w_k)^2.
\end{align*}
%
We will refer to the matrix $( v_i \cdot v_j )$ of $k$ vectors as the Grammian, denoted $G(v_1, \dots, v_k)$. Thus the area of the parallelogram spanned by these vectors is equal to $\det(G(v_1, \dots, v_k))^{1/2}$. We note that if $A$ is the matrix with columns $v_1, \dots, v_k$, then $(A^T A)_{ij} = v_i \cdot v_j$, so $G(v_1, \dots, v_k) = A^T A$. 

If $B$ is a $k \times k$ matrix, then $AB$ is a matrix whose $i$'th column is  are now a linear combination of the vectors $v_1, \dots, v_k$. We calculate that
%
\[ G(\sum v_1, \dots, Bv_k) = (BA)^T BA = A^T B^TB A, \]
%
and so $\det(G(Bv_1,\dots,Bv_k))^{1/2} = \det(A^T B^T B A)$.

\end{comment}

\section{Alternating Tensors}

Over the family of multilinear functions $T \in T^k(V)$ on a vector space, we can identify a subset of \emph{alternating} multilinear functions, i.e. those maps such that $T(v_1, \dots, v_k) = 0$ if $v_i = v_j$ for two distinct indices $i$ and $j$. This is equivalent to the tensor being \emph{skew-symmetric}, i.e.
%
\[ T(v_1, \dots, v_i, \dots, v_j, \dots, v_k) = - T(v_1, \dots, v_j, \dots, v_i, \dots, v_k) \]
%
(at least over a field which is of characteristic not equal to two). We let $\Omega^k(V)$ denote the family of all alternating tensors. If $T$ is alternating, then for any coefficients $A = a^{ij}$,
%
\[ T \left(\sum_{j = 1}^k a^{1j} v_j, \dots, \sum_{j = 1}^k a^{kj} v_j \right) = \det(A) \cdot T(v_1, \dots, v_k), \]
%
which is what we want from our signed scalar densities. We let $\Omega^k(V)$ denote the family of all alternating tensors.

In particular, if, for each permutation $\sigma \in S_k$, we let
%
\[ (v_1, \dots, v_k) \cdot \sigma = (v_{\sigma(1)}, \dots, v_{\sigma(k)}), \]
%
then this induces a right representation of $S_k$ on $V^k$. In particular,
%
\[ ((v_1, \dots, v_k) \cdot \sigma) \cdot \tau = (v_1, \dots, v_k) \cdot (\sigma \tau). \]
%
This representation induces a representation of $S_k$ on $T^k(V)$ such that for each $A \in T^k(V)$,
%
\[ (A \cdot \sigma)(v_1, \dots, v_k) = T((v_1, \dots, v_k) \cdot \sigma). \]
%
We note that if $A$ is alternating, then for any $\sigma \in S_k$, $A \cdot \sigma = \text{sgn}(\sigma) \cdot A$. In particular, if we let
%
\[ \text{Alt}(A) = \frac{1}{k!} \sum_{\sigma \in S_k} A \cdot \sigma, \]
%
then $\text{Alt}(A) \in \Omega^k(V)$ for all $A \in T^k(V)$, and if $A \in \Omega^k(V)$, $\text{Alt}(A) = A$. Thus $\text{Alt}$ is a projection map from $T^k(V)$ to $\Omega^k(V)$.

If $\omega \in \Omega^k(V)$, and $\nu \in \Omega^l(V)$, it is not necessarily true that $\omega \otimes \nu \in \Omega^{k+l}(V)$. To obtain a natural product on the space of alternating tensors, we define
%
\begin{align*}
    \omega \wedge \nu &= \frac{(k+l)!}{k! l!} \text{Alt}(\omega \otimes \eta)\\
    &= \frac{1}{k!l!} \sum_{\sigma \in S_{k+l}} \text{sgn}(\sigma) (\omega \otimes \eta) \cdot \sigma,
\end{align*}
%
where the funny coefficient is not essential, but makes some things work more nicely. For instance, if we let $G$ be the subgroup of $S_{k+l}$ which maps $\{ 1, \dots, k \}$ to $\{ 1, \dots, k \}$ and maps $\{ k+1, \dots, k+l \}$ to $\{ k+1, \dots, k+l \}$, which has order $k! l!$, and we choose a family of representations $\sigma_1, \dots, \sigma_N$ from $G \backslash S_{k+l}$, then
%
\[ \omega \wedge \nu = \text{sgn}(\sigma_1) \cdot (\omega \otimes \eta) \cdot \sigma_1 + \dots + \text{sgn}(\sigma_N) \cdot (\omega \otimes \nu) \cdot \sigma_N. \]
%
We note that $\wedge$ is bilinear, and `anti-commutative', i.e. $\omega \wedge \nu = (-1)^{kl} \cdot (\nu \wedge \omega)$. Furthermore, if $f: W \to V$ is a linear map, then $f^*(\omega \wedge \nu) = f^*(\omega) \wedge f^*(\nu)$.

\begin{example}
    Let $\omega_1$ and $\omega_2$ be linear functionals on $V$. Then for any two vector $v \in V$, we can associate the vector $\omega(v) = (\omega_1(v), \omega_2(v)) \in \RR^2$. Then for $v,w \in V$,
    %
    \[ (\omega_1 \wedge \omega_2)(v,w) = \omega_1(v) \omega_2(w) - \omega_1(w) \omega_2(v) = \det(\omega(v), \omega(w)), \]
    %
    which is the signed area of the parallelogram spanned by $\omega(v)$ and $\omega(w)$.
\end{example}

It takes some work, but we will also find that the wedge product is associative.

\begin{lemma}
    If $S \in T^k(V)$, $T \in T^l(V)$, and $\text{Alt}(S) = 0$, then
    %
    \[ \text{Alt}(S \otimes T) = \text{Alt}(T \otimes S) = 0. \]
\end{lemma}
\begin{proof}
    We calculate
    %
    \[ (k+l)! \cdot \text{Alt}(S \otimes T) = \sum_{\sigma \in S_{k+l}} \text{sgn}(\sigma) \cdot (S \otimes T) \cdot \sigma. \]
    %
    If we let $G$ be the subgroup of $S_{k+l}$ consisting of permutations which fix $\{ k+1, \dots, k+l \}$. Then
    %
    \[ \sum_{\sigma \in G} (S \otimes T) \cdot \sigma = k! \cdot \text{Alt}(S) \otimes T = 0. \]
    %
    If we let $\{ \sigma_1, \dots, \sigma_N \}$ denote representatives of the cosets $G \backslash S_{k+l}$, then
    %
    \[ \sum_{\sigma \sigma_i \in G \sigma_i} (S \otimes T) \cdot \sigma \cdot \sigma_i = \left( \sum_{\sigma \in G} (S \otimes T) \cdot \sigma \right) \cdot \sigma_i = 0 \cdot \sigma_i = 0. \]
    %
    Thus summing over each representative completes the calculation in the case $\text{Alt}(S \otimes T)$. The calculation of $\text{Alt}(T \otimes S)$ is treated similarily.
\end{proof}

\begin{lemma}
    For any alternating tensors $\omega \in \Omega^k(V)$, $\nu \in \Omega^l(V)$,
    %
    \[ \text{Alt}(\text{Alt}(\omega \otimes \eta) \otimes \theta) = \text{Alt}(\omega \otimes \eta \otimes \theta) = \text{Alt}(\omega \otimes \text{Alt}(\eta \otimes \theta)). \]
\end{lemma}
\begin{proof}
    Clearly
    %
    \[ \text{Alt}(\text{Alt}(\omega \otimes \eta) - \omega \otimes \eta) = 0. \]
    %
    Thus the last lemma implies
    %
    \[ \text{Alt}(\text{Alt}(\omega \otimes \eta) \otimes \theta - \omega \otimes \eta \otimes \theta) = 0. \]
    %
    But rearranging completes the proof of one side of the equality to be proved. The other side is treated similarily.
\end{proof}

\begin{theorem}
    If $\omega \in \Omega^k(V)$, $\eta \in \Omega^l(V)$, and $\theta \in \Omega^m(V)$, then
    %
    \[ (\omega \wedge \eta) \wedge \theta = \omega \wedge (\eta \wedge \theta) = \frac{(k + l + m)!}{k! l! m!} \text{Alt}(\omega \otimes \eta \otimes \theta). \]
\end{theorem}
\begin{proof}
    We calculate
    %
    \begin{align*}
        (\omega \wedge \eta) \wedge \theta &= \frac{(k + l + m)!}{(k + l)! m!} \text{Alt}((\omega \wedge \eta) \otimes \theta)\\
        &= \frac{(k+l+m)!}{k! l! m!} \text{Alt}(\text{Alt}(\omega \otimes \eta) \otimes \theta)\\
        &= \frac{(k+l+m)!}{k! l! m!} \text{Alt}(\omega \otimes \eta \otimes \theta).
    \end{align*}
    %
    A similar argument shows that the same is true for $\omega \wedge (\eta \wedge \theta)$, so we have proved associativity.
\end{proof}

\begin{remark}
    More generally, we find that if $\omega_i \in \Omega^{k_i}(V)$, then
    %
    \[ \omega_1 \wedge \dots \wedge \omega_n = \frac{(k_1 + \dots + k_n)!}{k_1! \dots k_n!} \text{Alt}(\omega_1 \otimes \dots \otimes \omega_n). \]
    %
    If we choose representatives $\sigma_1, \dots, \sigma_N$ from $G \backslash S_{k_1 + \dots + k_n}$, where $G$ is the subgroup consisting of permutations which fix $\{ 1, \dots, k_1 \}$, $\{ k_1 + 1, \dots, k_1 + k_2 \}, \dots, \{ k_1 + \dots + k_{n-1} + 1, \dots, k_1 + \dots + k_n \}$, then
    %
    \begin{align*}
        \omega_1 \wedge& \dots \wedge \omega_n\\
        &= \text{sgn}(\sigma_1) \cdot (\omega_1 \otimes \dots \otimes \omega_n) \cdot \sigma_1 + \dots + \text{sgn}(\sigma_N) \cdot (\omega_1 \otimes \dots \otimes \omega_n) \cdot \sigma_N.
    \end{align*}
\end{remark}

\begin{remark}
    If $\omega_1, \dots, \omega_k \in V^*$, and for $v_1, \dots, v_k$, $(\omega_1 \wedge \dots \wedge \omega_k)(v_1, \dots,v_k)$ is the area of the paralleliped spanned by $\omega(v_1), \dots, \omega(v_k) \in \RR^k$, i.e.
    %
    \[ (\omega_1 \wedge \dots \wedge \omega_k)(v_1, \dots,v_k) = \det(\omega(v_1), \dots, \omega(v_k)). \]
    %
    where $\omega(v) = (\omega_1(v), \dots, \omega_k(v))$ for each $v \in V$.
\end{remark}

The wedge product therefore turns $\oplus_k \Omega^k(V)$ into a graded algebra, which has an identity if we let $\Omega^0(V)$ denote the real numbers, $a \wedge \omega = a \omega$. Unlike $T(V)$, $\Omega(V)$ is finite dimensional if $V$ is finite dimensional, since we will soon show that $\Omega^k(V) = (0)$ if $k > \dim(V)$.

\begin{remark}
    We note that even without the $(k+l)! / k! l!$, the wedge product would still be associative. The reason the coefficients have been included is so that $v_1, \dots, v_n$ are a basis for $V$, and $\phi_1, \dots, \phi_n$ are the dual basis, then $(\phi_1 \wedge \dots \wedge \phi_n)(v_1, \dots, v_n) = 1$.
\end{remark}

\begin{theorem}
    If $\{ v_1, \dots, v_n \}$ is a basis for $V$, and $\{ \phi_1, \dots, \phi_n \}$ the dual basis for $V^*$, then the set of all
    %
    \[ \phi_I = \phi_{i_1} \wedge \dots \wedge \phi_{i_k} \]
    %
    where $I = \{ 1 \leq i_1 < \dots < i_k \leq n \}$, forms a basis for $\Omega^k(V)$.
\end{theorem}
\begin{proof}
    If $\omega = \sum a_{i_1 \dots i_k} \phi_{i_1} \otimes \dots \otimes \phi_{i_k}$, then
    %
    \begin{align*}
        \omega &= \text{Alt}(\omega)\\
        &= \sum a_{i_1 \dots i_k} \text{Alt}(\phi_{i_1} \otimes \dots \otimes \phi_{i_k})\\
        &= \sum \frac{a_{i_1 \dots i_k}}{k!} (\phi_{i_1} \wedge \dots \wedge \phi_{i_k}).
    \end{align*}
    %
    Thus $\Omega^k(V)$ is spanned by $\{ \phi_I \}$. But if $j_1 < \dots < j_n$, then $\phi_I(v_{j_1}, \dots, v_{j_k}) = 1$ if and only if $i_r = j_r$ for each $k \in \{ 1, \dots, k \}$, so the $\phi_I$ must be linearly independant.
\end{proof}

\begin{corollary}
    Covectors $\omega_1, \dots, \omega_k \in \Omega^1(V)$ are linearly independant if and only if $\omega_1 \wedge \dots \wedge \omega_k \neq 0$.
\end{corollary}
\begin{proof}
    If the vectors are linearly dependant, then certainly $\omega_1 \wedge \dots \wedge \omega_k = 0$. If the vectors are linearly independent, they can be completed to a dual basis, and then $\omega_1 \wedge \dots \wedge \omega_k$ is an element of the basis.
\end{proof}

\section{Differential Forms}

We now return to the study of differential forms from an algebraic perspective. Given a linear function $f: V \to W$, we find that for the induced map $f^*: T^k(W) \to T^k(V)$, $f^*(\Omega^k(W)) \subset \Omega^k(V)$. Thus this functor is smooth, and so given any smooth vector bundle $\xi$, we can associate another vector bundle $\Omega^k(\xi)$. We will of course want to focus on the alternating tensor bundle $\Omega^k(TM)$, which is a subbundle of $T^k(TM)$, and which we also denote by $\Omega^k(M)$. A section of this bundle is known as a \emph{differential form}, the space of all such sections forming a $C^\infty(M)$ module. By taking wedge products locally, we can take the direct sum of bundles $\Omega(M) = \bigoplus \Omega^k(M)$ into a bundle of algebras, and the sections of this space form a $C^\infty(M)$ algebra, denoted $\Omega^k(M)$. As we have seen in the last section, if $(x,U)$ is a coordinate system, then any $\omega \in \Gamma(TM)$ can locally be written as
%
\[ \sum_I a_I dx^I, \]
%
where $I$ ranges over all monotonic multi-indexes.

\begin{theorem}
    A manifold $M^n$ is orientable precisely when $\Omega^n(TM)$ has a nowhere vanishing section.
\end{theorem}
\begin{proof}
    If $\omega$ is a nowhere zero differential $n$ form, we can define an orientation $\mu$ by letting $(e_1, \dots, e_n) \in \mu_p$ if $\omega(p)(e_1, \dots, e_n) > 0$. In coordinates $(x,U)$, $\omega = f dx^1 \wedge \dots \wedge dx^n$, and we must either have $f > 0$ everywhere, or $f < 0$ everywhere, since $\omega$ doesn't vanish. This shows the choice of orientation is locally trivial. Conversely, if $M^n$ is orientable, we can cover $M$ by a family of orientated coordinate charts $(x_\alpha, U_\alpha)$. If $\varphi_\alpha$ is a partition of unity subordinate to these charts, and we choose a nonvanishing differential form $\omega_\alpha$ on $U_\alpha$, then we can consider the form
    %
    \[ \omega = \sum \varphi_\alpha \omega_\alpha \]
    %
    and then $\omega(p) \neq 0$ naywhere, since for a given oriented basis $e_1, \dots, e_n$ of $M_p$, $\omega_\alpha(p)(e_1, \dots, e_n) > 0$, hence $\omega(p)(e_1, \dots, e_n) > 0$.
\end{proof}

Before we move on, we note that if $f: M \to N$ is a smooth map between manifolds, then we can pull back covariant tensor fields under the dual map $f^*$. We note that if $\omega$ is a $k$-form on $N$, then $f^* \omega$ is a $k$-form on $M$, i.e. $f^* \omega$ is alternating if $\omega$ is alternating. The dual map is particularly amenable to the operations on forms. In particular, $f^*(\omega \wedge \eta) = f^*\omega \wedge f^* \eta$. One can thus feasible compute the action of $f^*$ in coordinates. In particular if we consider coordinates $(x,U)$ on $M$ and $(y,V)$ on $N$, then
%
\[ f^*(dy^i)(X) = dy^i(f_* X) = (f_* X)(y^i) = X(y^i \circ f), \]
%
so in particular,
%
\[ f^*(dy^i) = \sum_j \frac{\partial (y^i \circ f)}{\partial x_j} dx^j. \]
%
Taking wedge products will allow us to compute the complete action of $f^*$ on $k$-forms for any $k$. We perform a complete calculation only on $n$-forms.

\begin{lemma}
    Let $f: M \to N$ be a smooth map between $n$ dimensional manifolds. Then if $(x,U)$ and $(y,V)$ are coordinate systems on $M$ and $N$ respectively, then
    %
    \[ f^*(a dy^1 \wedge \dots \wedge dy^n) = (a \circ f) \cdot \det \left( \frac{\partial y^i \circ f}{\partial x_j} \right) dx^1 \wedge \dots \wedge dx^n. \]
\end{lemma}
\begin{proof}
    We calculate that $f^*(a dy^1 \wedge \dots \wedge dy^n) = b dx^1 \wedge \dots \wedge dx^n$ for some smooth function $b$, where
    %
    \begin{align*}
        b(p) &= f^*(a dy^1 \wedge \dots \wedge dy^n)_p \left( \left. \frac{\partial}{\partial x^1} \right|_p, \dots, \left. \frac{\partial}{\partial x^n} \right|_p \right)\\
        &= (a dy^1 \wedge \dots \wedge dy^n)_{f(p)} \left(f_*\left( \left. \frac{\partial}{\partial x^1} \right|_p \right), \dots, f_* \left( \left. \frac{\partial}{\partial x^n} \right|_p \right) \right)\\
        &= (a \circ f)(p) \cdot (dy^1 \wedge \dots \wedge dy^n)(f(p)) \\
        &\ \ \ \cdot \left( \sum \left. \frac{\partial y^i \circ f}{\partial x^1} \right|_p \left. \frac{\partial}{\partial y^i} \right|_{f(p)}, \dots, \sum \left. \frac{\partial y^i \circ f}{\partial x^n} \right|_p \left. \frac{\partial}{\partial y^i} \right|_{f(p)} \right)\\
        &= (a \circ f)(p) \cdot \det \left( \left. \frac{\partial y^i \circ f}{\partial x^j} \right|_p \right). \qedhere
    \end{align*}
\end{proof}

\section{Integration of Differential Forms}

As we have seen in the case of $n$-forms, if $N$ is an oriented $k$-dimensional submanifold of a manifold $M$, and $\omega$ is a $k$-form on $M$, then $\omega$ induces a scalar density on $N$. Thus if $\text{supp}(\omega) \cap N$ is compact, then
%
\[ \int_N \omega \]
%
is well defined. Of course, we are being as restrictive as possible here to ensure everything is concretely defined. One can weaken compactness assumptions given additional criteria about the form $\omega$, or extend the definition of $N$ to more general domains. For instance, it is relatively easy to define the integral of a $k$-form on an oriented $k$-dimensional manifold with boundary $N$ since the boundary of the manifold is a `set' of measure zero, so we can set
%
\[ \int_N \omega = \int_{N^\circ} \omega. \]
%
We also want to consider the integral of differential forms over shapes like triangles and cubes, which are neither manifolds nor manifold with boundary, since they have `corners'. So we define a \emph{manifold with corners} to be a topological space $M$ such that each point $p \in M$ has a neighbourhood homeomorphic either to an open subset of $\RR^n$, an open subset of a half space $\HH^n$ with the image of $p$ on the boundary of $\HH^n$, or an open subset of
%
\[ \RR^n_+ = \{ x \in \RR^n: x_1, \dots, x_n \geq 0 \}, \]
%
with the image of $p$ equal to $0$. Topologically, a manifold with corners is the same as a manifold with boundary, but we can consider smooth structures on a manifold with corners which distinguish the two categories, and we will of course consider smooth manifolds with corners. Unlike smooth manifolds with boundary, the boundary points $\partial M$ formed by points which do not have charts diffeomorphic to open subsets of $\RR^n$ does not form a smooth manifold with boundary, though the interior $M^\circ$ is a smooth manifold. This is because $\partial M$ still has corners. On the other hand, we can define $(\partial M)^\circ$ to be the set of points $p$ with charts diffeomorphic to $\HH^n$, and this set will form a smooth manifold. Of course, on manifolds with corners we can also defined tangent bundles, cotangent bundles, tensor bundles, and so on. In particular, if $\omega$ is a $k$-form on an oriented smooth manifold with corners $M$, we will define
%
\[ \int_M \omega = \int_{M^\circ} \omega. \]
%
If $M$ is an oriented manifold, then $(\partial M)^\circ$ has a natural orientation obtained by outward pointing vectors, and so for a $k-1$ form on $M$, we define
%
\[ \int_{\partial M} \omega = \int_{(\partial M)^\circ} \omega. \]
%
Other generalizations are obvious to the imagination, but hard to specify precisely, so we leave these for another time.

\section{The Differential Operator}

A section of $\Omega^0(TM)$, or a differential form of order 0, is just a smooth function on $M$. If $f$ is such a function, we defined its differential as
%
\[ df = \sum \frac{\partial f}{\partial x^\alpha} dx^\alpha \]
%
Notice that we can now say that `the differential of a zero form is a one form'. In this section we extend this definition, defining an operator $d$ taking $k$ forms to $k+1$ forms. Our main goal thereafter will be to prove Stoke's theorem, which says that for a $k$-form $\omega$ on a $k+1$ dimensional manifold $M$ with boundary,
%
\[ \int_M d\omega = \int_{\partial M} \omega. \]
%
The operator $d$ is defined such that $d\omega$ satisfies this equation pointwise on an `infinitisimal surface'.

The idea of extending a signed-area on $k$-dimensional shapes to a signed-area on $k+1$-dimensional shapes is quite sneaky. For a fixed $p \in M$, after switching to an oriented coordinate system $(x,U)$ with $x(p) = 0$, we consider $k+1$ tangent vectors $X_1|_p, \dots, X_{k+1}|_p \in T_pM$. We consider vectors $a_i \in \RR^n$ such that
%
\[ X_i|_p = \sum_{j = 1}^n a_i^j \left. \frac{\partial}{\partial x^i} \right|_p. \]
%
Then for suitably small $\varepsilon > 0$, we can consider a `curvilinear paralleliped' $\Sigma(\varepsilon) \subset M$ which is the inverse image under $x$ of the paralleliped generated by the vectors $\varepsilon a_1, \dots, \varepsilon a_{k+1}$. We also extend the vectors $X_1|_p, \dots, X_{k+1}|_p$ to vector fields $X_1, \dots, X_{k+1}$ on $U$ such that $x_*(X_i|_q) = (a_i)_{x(q)}$ for each $q \in U$. Then $\Sigma(\varepsilon)$ has a natural orientation such that $X_1, \dots, X_{k+1}$ is oriented. In particular, $\partial \Sigma(\varepsilon)$ also has a natural orientation. The $k+1$ form $d\omega$ is then defined such that as $\varepsilon \to 0$,
%
\[ \int_{\partial \Sigma(\varepsilon)} \omega = \varepsilon^{k+1} \cdot d\omega_p(X_1, \dots, X_k) + o(\varepsilon^{k+1}) = \int_{\Sigma(\varepsilon v_1, \dots, \varepsilon v_k)} d\omega + o(\varepsilon^{k+1}). \]
%
Thus $d\omega$ is defined so that Stoke's theorem holds `infinitisimally'.

To show such a limit exists, we note that $\partial \Sigma(\varepsilon)$ is the union of $2k$ faces, which can be paired together and identified with elements of $i \in \{1, \dots, k \}$. Thus we consider the pair of faces $\Sigma_i(\varepsilon)^-$ and $\Sigma_i(\varepsilon)^+$, the first being the inverse image of the $k$-dimensional paralleliped spanned by $\{ \varepsilon v_1, \dots, \widehat{\varepsilon v_i}, \dots, \varepsilon v_{k+1} \}$, and the second being the inverse image of the same paralleliped shifted by $\varepsilon v_i$. Now $X_i$ is an outward pointing vector field to $\Sigma_i(\varepsilon)^+$, and $-X_i$ is an outward pointing vector to $\Sigma_i(\varepsilon)^-$. Thus the vector fields $X_1, \dots, \widehat{X_i}, \dots, X_{k+1}$ are oriented on $\Sigma_i(\varepsilon)^-$ if and only if the basis $-X_i,X_1, \dots, \widehat{X_i}, \dots, X_{k+1}$ are oriented, i.e. if $i$ if even, and $X_1, \dots \widehat{X_i}, \dots, X_{k+1}$ is an oriented basis on $\Sigma_i(\varepsilon)^+$ if and only if $X_i,X_1, \dots, \widehat{X_i}, \dots, X_k$ is oriented, i.e. if $i$ is odd. Thus expanding definitions, and then applying the mean value theorem, we conclude
%
\begin{align*}
    &\int_{\Sigma_i^+(\varepsilon)} \omega + \int_{\Sigma_i^-(\varepsilon)} \omega\\
    &\ \ \ \ \ \ = (-1)^{i+1} \int_{[0,\varepsilon]^k} dy^1 \dots \widehat{dy^i} \dots dy^{k+1}\\
    &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \Bigg( \omega(X_1, \dots, \widehat{X_i},\dots, X_{k+1})(x^{-1}(\varepsilon a_i + y_1 a_1 + \dots + y_{k+1} a_{k+1}))\\
    &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ - \omega(X_1,\dots,\widehat{X_i},\dots,X_{k+1})(x^{-1}(y^1a_1 + \dots + y^{k+1}a_{k+1})) \Bigg)\\
    &\ \ \ \ \ \ = (-1)^{i+1} \varepsilon \cdot \int_{[0,\varepsilon]^k} \left( X_i(\omega(X_1, \dots, \widehat{X_i}, \dots, X_{k+1}))(p) + o(1) \right)\; dy\\
    &\ \ \ \ \ \ = (-1)^{i+1} \varepsilon^{k+1} X_i(\omega(X_1, \dots, \widehat{X_i}, \dots, X_{k+1}))(p) + o(\varepsilon^{k+1}).
\end{align*}
%
Thus we have computed that we should expect
%
\[ d\omega_p(X_1|_p, \dots, X_{k+1}|_p) = \sum_{i = 1}^{k+1} (-1)^{i+1} \cdot X_i(\omega(X_1, \dots, \widehat{X_i}, \dots, X_{k+1}))(p). \]
%
Our only problem is that the right hand side depends on the extension of the vector fields in a neighbourhood of $p$, and thus may not locally depend on the vector fields at the point (we also do not even know if this extension is independent of the coordinate system we chose at $p$). We will show this \emph{does not} depend on the extension of the vector field, and thus not on the coordinate system, by giving a slightly more invariant definition of the differential operator, which works when applied to vector fields $X_1, \dots, X_{k+1}$ that may not necessarily commute, unlike those given above, by introducing some Lie brackets.

\begin{theorem}
    Let $\omega$ be a $k$-form on a manifold $M$, then there exists a $k+1$-form $d\omega$ on $M$ such that for any vector fields $X_1, \dots, X_{k+1} \in \Gamma(TM)$,
    %
    \begin{align*}
        d\omega(X_1, \dots, X_{k+1}) &= \sum_{i = 1}^{k+1} (-1)^{i+1} X_i(\omega(X_1, \dots, \widehat{X_i}, \dots, X_{k+1}))\\
        &\ \ \ + \sum_{1 \leq i < j \leq k+1} (-1)^{i+j} \omega([X_i,X_j],X_1, \dots, \widehat{X_i}, \dots, \widehat{X_j}, \dots, X_{k+1}).
    \end{align*}
\end{theorem}
\begin{proof}
    It suffices to show that the definition is $C^\infty(M)$ linear in the vector fields $X_1, \dots, X_{k+1}$. Write the expression above as $\Sigma_1(X_1, \dots, X_{k+1}) + \Sigma_2(X_1, \dots, X_{k+1})$. It is obvious that $d\omega$ is $\RR$ linear in the vector fields. And for any function $f \in C^\infty(M)$ and $j \in \{ 1, \dots, k+1 \}$, we find by the product rule that
    %
    \begin{align*}
        \Sigma_1(X_1, \dots, fX_j, \dots, X_{k+1}) &= \sum_{i \neq j} (-1)^{i+1} X_i(\omega(X_1,\dots, \widehat{X_i}, \dots, fX_j, \dots, X_{k+1}))\\
        &\ \ \ \ \ + (-1)^{j+1} \cdot f \cdot X_j(\omega(X_1, \dots, \widehat{fX_j}, \dots, X_{k+1}))\\
        &= f \cdot \Sigma_1(X_1, \dots, X_{k+1})\\
        &\ \ \ \ \ + \sum_{i \neq j} (-1)^{i+1} X_i(f) \cdot \omega(X_1, \dots, \widehat{X_i}, \dots, X_j).
    \end{align*}
    %
    \begin{align*}
        \Sigma_2(X_1, \dots, fX_j, \dots, X_{k+1}) &= f \cdot \Sigma_2(X_1, \dots, X_{k+1})\\
        &\ \ \ \ \ + \sum_{i < j} (-1)^{i+j} \cdot X_i(f) \cdot \omega(X_j, X_1, \dots, \widehat{X_i}, \dots, \widehat{X_j}, \dots, X_{k+1})\\
        &\ \ \ \ \ + \sum_{i > j} (-1)^{i+j + 1} \cdot X_i(f) \cdot \omega(X_j, X_1, \dots, \widehat{X_j}, \dots, \widehat{X_i}, \dots, X_{k+1})\\
        &= f \cdot \Sigma_2(X_1, \dots, X_{k+1})\\
        &\ \ \ + \sum_{i \neq j} (-1)^i X_i(f) \cdot \omega(X_1, \dots, \widehat{X_i}, \dots, X_j).
    \end{align*}
    %
    Thus summing up, we conclude
    %
    \[ d\omega(X_1, \dots, fX_j, \dots, X_{k+1}) = f d\omega(X_1, \dots, X_{k+1}), \]
    %
    hence the operator is $C^\infty(M)$ linear.
\end{proof}

It is a bit of magic to define the differential in terms of vector fields, and then find that this definition is really pointwise by proving $C^\infty(M)$ linearity. But it is necessary magic; even though $d\omega$ does not depend on any properties of the vector fields except those at a point, it certainly depends on the behaviour of $\omega$ in a neighbourhood of a point, and this must enter the picture somehow.

\begin{remark}
    We note that $d$ is not $C^\infty(M)$ linear, only $\RR$ linear, which reflects the fact that $d$ is not defined pointwise; really only very basic operators can be defined pointwise, since in coordinates, they only are obtained by linear combinations of the values defining the tensor.
\end{remark}

We note that the exterior derivative defined above does generalize the definition $df$ defined for smooth functions, which we think of as 0-forms on the manifold. The definition above is precisely how we defined $df$, namely
%
\[ df(X) = X(f). \]
%
In coordinates, we have
%
\[ df = \sum_{i = 1}^n \frac{\partial f}{\partial x^i}\; dx^i. \]
%
It is incredibly convenient to work with the higher order exterior derivative in coordinates, which we now do, after studying the differential.

If $X_1, \dots, X_{k+1}$ are vector fields with $[X_i,X_j] = 0$ for each $i,j$, we find
%
\begin{align*}
    d(a \omega)(X_1, \dots, X_{k+1}) &= \sum_{i = 1}^{k+1} (-1)^{i+1} X_i \left( (a \omega)(X_1, \dots, \widehat{X_i}, \dots, X_{k+1}) \right)\\
    &= a d\omega(X_1, \dots, X_{k+1})\\
    &\ \ \ + \sum_{i = 1}^{k+1} (-1)^{i+1} X_i(a) \cdot \omega(X_1, \dots, \widehat{X_i}, \dots, X_{k+1})\\
    &= a d\omega(X_1, \dots, X_{k+1})\\
    &\ \ \ + \sum_{i = 1}^{k+1} (-1)^{i+1} da(X_i) \cdot \omega(X_1, \dots, \widehat{X_i}, \dots, X_{k+1})\\
    &= a d\omega(X_1, \dots, X_{k+1}) + (da \wedge \omega)(X_1, \dots, X_{k+1}).
\end{align*}
%
Next, we calculate that for any index set $I$, $d(dx^I) = 0$. We find
%
\begin{align*}
    d(dx^I)\left( \frac{\partial}{\partial x^{j_1}}, \dots, \frac{\partial}{\partial x^{j_k}} \right) &= \sum_{i = 1}^{k+1} (-1)^{i+1} \frac{\partial}{\partial x^{j_i}} \left( dx^I \left( \frac{\partial}{\partial x^{j_1}}, \dots, \frac{\partial}{\partial x^{j_i}} , \dots, \frac{\partial}{\partial x^{j_k}} \right) \right)\\
    &= 0.
\end{align*}
%
Thus, for any index set $I$ and smooth function $a$, $d(a dx^I) = da \wedge dx^I$. Thus
%
\[ d(a dx^I) = \sum_{i = 1}^n \frac{\partial a}{\partial x^i} dx^i \wedge dx^I, \]
%
which gives a simple calculation to compute the exterior derivative.

\begin{theorem}
    For any $k$-form $\omega$ and $l$-form $\eta$,
    %
    \[ d(\omega \wedge \eta) = d\omega \wedge \eta + (-1)^k \omega \wedge d\eta. \]
\end{theorem}
\begin{proof}
    We work locally in coordinates. By linearity, it suffices to deal with the case $\omega = a dx^I$ and $\eta = b dx^J$. Since there is $\alpha$ such that $dx^I \wedge dx^J = (-1)^\alpha dx^{I \cup J}$, so $d(dx^I \wedge dx^J) = 0$. Since $\omega \wedge \eta = ab dx^I \wedge dx^J$, we find
    %
    \[ d(\omega \wedge \eta) = d(ab) \wedge dx^I \wedge dx^J = (a db + b da) \wedge dx^I \wedge dx^J. \]
    %
    On the other hand,
    %
    \[ d\omega \wedge \eta + (-1)^k \omega \wedge d\eta = b da \wedge dx^I \wedge dx^J + (-1)^k a dx^I \wedge db \wedge dx^J = (a db + b da) \wedge dx^I \wedge dx^J. \qedhere \]
\end{proof}

\begin{theorem}
    For any $k$ form $\omega$, $d(d\omega) = 0$. In short, $d^2 = 0$.
\end{theorem}
\begin{proof}
    If $\omega = a dx^I$, then
    %
    \begin{align*}
        d(d\omega) &= d(da \wedge dx^I)\\
        &= \sum_{i = 1}^n d \left( \frac{\partial a}{\partial x^i} dx^i \wedge dx^I \right)\\
        &= \sum_{i = 1}^n \frac{\partial a}{\partial x^i} d(dx^i \wedge dx^I) + \sum_{i = 1}^n \sum_{j = 1}^n \frac{\partial^2 a}{\partial x^j \partial x^i} dx^j \wedge dx^i \wedge dx^I.
    \end{align*}
    %
    The first sum is zero because $d(dx^i \wedge dx^I) = 0$ for all $i$ and $I$. The second sum is zero because if $i = j$, $dx^j \wedge dx^i \wedge dx^I = 0$, and if $i \neq j$ we can pair up the term with the term obtained by swapping $i$ and $j$, and since mixed partials are equal, and $dx^j \wedge dx^i = - dx^i \wedge dx^j$, the two terms cancel one another out.
\end{proof}

The exterior derivative is, in some senses, uniquely defined. It is the unique linear operator $d$ such that $d^2 = 0$, $df$ is defined as usual for functions, and such that $d(\omega \wedge \eta) = d\omega \wedge \eta + (-1)^k \omega \wedge d\eta$. This is easy to see by working in a coordinate system, as we saw before. But the derivative is some sense, even more natural, since it satisfies a naturality condition.

\begin{theorem}
    Given $f: M \to N$ and a $k$-form $\omega$ on $N$, $d(f^*\omega) = f^*(d\omega)$.
\end{theorem}
\begin{proof}
    We prove the result by induction. For the case $k = 0$, and a smooth function $g \in C^\infty(M)$, if $(x,U)$ is a coordinate system on $M$, and $(y,V)$ a coordinate system on $N$,
    %
    \begin{align*}
        f^*(dg) &= f^* \left( \sum_{i = 1}^m \frac{\partial g}{\partial y^i} dy^i \right)\\
        &= \sum_{i = 1}^n \sum_{j = 1}^n \left( \frac{\partial g}{\partial y^i} \circ f \right) \frac{\partial y^i \circ f}{\partial x^j} \cdot dx^j.
    \end{align*}
    %
    On the other hand, by the chain rule
    %
    \begin{align*}
        d(f^* g) &= d(g \circ f)\\
        &= \sum_{j = 1}^m \frac{\partial (g \circ f)}{\partial x^j} dx^j\\
        &= \sum_{j = 1}^m \sum_{i = 1}^n \left( \frac{\partial g}{\partial y^i} \circ f \right) \frac{\partial y^i \circ f}{\partial x^j} \cdot dx^j.
    \end{align*}
    %
    Thus the result is true for $0$-forms. Next, we consider the case of 1-forms, but only when $\omega = dy^j$ for some $j$. Then clearly $f^*(d^2y^j) = 0$. On the other hand,
    %
    \[ f^*(dy^j) = \sum_{\alpha = 1}^n \frac{\partial y^j \circ f}{\partial x^\alpha} dx^\alpha. \]
    %
    Thus
    %
    \[ d(f^*(dy^j))(X) = \sum_{\alpha = 1}^n \sum_{\beta = 1}^n \left( \frac{\partial^2 y^j \circ f}{\partial x^\alpha \partial x^\beta} \right) dx^\alpha \wedge dx^\beta. \]
    %
    But this is equal to zero since mixed partial cancel out. Now we consider the general case by induction. Assuming the formula for $k-1$ forms, if $\omega = a dx^i \wedge dx^I$, we calculate that for any index $i$ and size $k-1$ index set $I$, by induction we find
    %
    \begin{align*}
        d(f^* \omega) &= d(f^*(a dx^I))\\
        &= d(f^*(dx^i) \wedge f^*(a dx^I))\\
        &= d(f^*(dx^i)) \wedge f^*(a dx^I) - f^*(dx^i) \wedge d(f^*(a dx^I))\\
        &= f^*(d^2 x^i) \wedge f^*(a dx^I) - f^*(dx^i) \wedge f^*(d(a dx^I))\\
        &= - f^*(dx^i \wedge d(a dx^I))\\
        &= f^*(a dx^i \wedge dx^I). \qedhere
    \end{align*}
\end{proof}

For each fixed $k$, the operator $d$ is the unique linear map, up to scalar multiplication, which takes $k$ forms to $k+1$ forms, such that for each $f: M \to N$, $f^*(d\omega) = d(f^* \omega)$. This is a result of R.S Palais (Natural Operations on Differential Forms, 1959). This might explain why the differential operator turns out to be so important.

\begin{example}
    One nice thing about the differential is it generalizes the differential operators found in the three dimensional vector calculus found in a standard multivariate class. If $f$ is a function, then
    %
    \[ df = \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy + \frac{\partial f}{\partial z} dz. \]
    %
    Thus $df(X)(p) = X_p \cdot (\nabla f)$, and so $\nabla f$ carries the same information as the gradient of $f$. If $\omega = a_1 dx + a_2 dy + a_3 dz$, then
    %
    \[ d\omega = \left( \frac{\partial a_2}{\partial x} - \frac{\partial a_1}{\partial y} \right) dx \wedge dy + \left( \frac{\partial a_3}{\partial y} - \frac{\partial a_2}{\partial z} \right) dy \wedge dz + \left( \frac{\partial a_1}{\partial z} - \frac{\partial a_3}{\partial x} \right) dz \wedge dx. \]
    %
    Thus if we write $Z = (a_1, a_2, a_3)$, then we verify
    %
    \[ d\omega(X,Y)(p) = (\nabla \times Z) \cdot (X \times Y). \]
    %
    Thus $d\omega$ carries the same information as the curl of the vector field $Z$. If $\omega = a_1 dy \wedge dz + a_2 dz \wedge dx + a_3 dx \wedge dy$ is a two form, then
    %
    \[ d\omega = \left(\frac{\partial a_1}{\partial x} + \frac{\partial a_2}{\partial y} + \frac{\partial a_3}{\partial z} \right) (dx \wedge dy \wedge dz). \]
    %
    Thus if $W = (a_1, a_2, a_3)$, then $d\omega(X,Y,Z) = (\nabla \cdot W) ((X \times Y) \cdot Z)$. So $d\omega$ carries the same information as the divergence of the vector field $W$. In particular, the equation $d^2 = 0$ implies the three classical equations
    %
    \[ \nabla \times \nabla f = \nabla \cdot (\nabla \times X) = 0 \]
    %
    for any smooth functions $f$ and vector fields $X$. In the next section, we prove a generalization of Stoke's theorem which encompasses Green's theorem, Gauss' theorem, and Stoke's theorem.
\end{example}

%\begin{example}
%    If the values $a^i$ form the component of a contravariant tensor field with respect to coordinates $x$, then the values $\partial_j a^i$ do {\it not} form the components of a tensor, since if $b^i$ are components with respect to $y$, then
    %
%    \[ b^i = \frac{\partial y^i}{\partial x^j} a^j \]
    %
%    and so
    %
%    \[ \partial_j b^i = \partial_j \left( \frac{\partial y^i}{\partial x^k} a^k \right) = \frac{\partial y^i}{\partial x^k} \partial_j a^k + \frac{\partial^2 y^i}{\partial x_j x_k} a^k \]
    %
%    This shows that contravariant vector fields cannot be differentiated in the same way that we can with exterior forms. Note also that if $a_i$ are components of a covariant tensor, then $\partial_j a_i - \partial_i a_j$ are components of a covariant tensor, which is the differential.
%\end{example}

%\begin{theorem}
%    If $d'$ is another linear map taking $m$ forms to $m+1$ forms, satisfying $d(\omega \wedge \nu) = d\omega \wedge \nu + (-1)^n \omega \wedge d\nu$, and $(d')^2 = 0$, with $d'f = df$ when $f$ is a function, then $d' = d$.
%\end{theorem}
%\begin{proof}
%    It suffices to prove this if $\omega = f dx^I$. We calculate
    %
%    \[ d'(f dx^I) = d'(f \wedge dx^I) = d'f \wedge dx^I - f \wedge d'(d'(x^I)) \]
    %
%    and since $d'f = df$, we now only need to prove $d'(dx^I) = d(dx^I)$, but these two terms both vanish.
%\end{proof}

%This theorem implies that the $d$ operator is coordinate independant, and thus extends to globally defined differential forms. Another way to see this is to give a coordinate independant definition of the operator.

\section{Stoke's Theorem}

We now show that Stoke's theorem holds for manifolds. The proof, after switching to simple coordinate systems, is a simple application of the fundamental theorem of calculus.

\begin{theorem}
    If $M$ is an oriented $n$ dimensional manifold, and $\omega$ is an $n-1$ form with compact support, then
    %
    \[ \int_{\partial M} \omega = \int_M d\omega. \]
\end{theorem}
\begin{proof}
    Suppose first that $\omega$ is supported on $[0,1]^n$, but compactly supported on the interior. Write
    %
    \[ \omega = \sum_{i = 1}^n a_i \cdot dx^1 \wedge \dots \wedge \widehat{dx^i} \wedge \dots \wedge dx^n. \]
    %
    Then
    %
    \[ d\omega = \sum_{i = 1}^n (-1)^{i+1} \frac{\partial a_i}{\partial x^i} \cdot (dx^1 \wedge \dots \wedge dx^n). \]
    %
    Thus we calculate from Fubini's theorem and the fundamental theorem of calculus that
    %
    \begin{align*}
        \sum_{i = 1}^n& (-1)^{i+1} \int_{[0,1]^n} \frac{\partial a_i}{\partial x^i}(x) dx^1 \dots dx^n\\
        &= \sum_{i = 1}^n (-1)^{i+1} \int_{[0,1]^{n-1}} [a_i(x_1, \dots,1, \dots, x_n) - a_i(x_1, \dots, 0, \dots, x_n)] dx^1 \dots \widehat{dx^i} \dots dx^n\\
        &= 0.
    \end{align*}
    %
    Thus the theorem is obvious in this case. Next, we adress the case where $\omega$ is supported on a compact subset of $[0,1]^n$, but $\supp(\omega) \cap \partial [0,1]^n \subset \{ x \in [0,1]^n : x_1 = 0 \}$. Again, the fundamental theorem of calculus and Fubini's theorem enables us to conclude that
    %
    \begin{align*}
        \int_M d\omega &= - \int_{[0,1]^{n-1}} a_1(0, x_2, \dots, x_n) \cdot dx^2 \dots dx^n.
    \end{align*}
    %
    On the other hand, because $\{ e_2, \dots, e_n \}$ is \emph{not oriented correctly} on $\partial M$,
    %
    \[ \int_{\partial M} \omega = - \int_{[0,1]^{n-1}} a_1(0, x_2, \dots, x_n) \cdot dx^2 \dots dx^n, \]
    %
    thus Stoke's theorem is, again, obvious. But now if $M$ is an arbitrary manifold, and $\omega$ is compactly supported, then $\text{supp}$ can be covered by finitely many coordinate systems $(x_1,U_1), \dots, (x_N, U_N)$ like the one above, and a partition of unity $\phi_1, \dots, \phi_N$, such that $(x^{-1})^*(\phi_i \omega)$ fits into one of the two cases above. Then we see that for each $i$,
    %
    \[ \int_M d(\phi_i \omega) = \int_{\partial M} \omega. \]
    %
    We also know
    %
    \[ \int_M d\omega = \sum_{i = 1}^N \int_M \phi_i d\omega. \]
    %
    We calculate that $d(\phi_i \omega) = d(\phi_i) \wedge \omega + \phi_i d\omega$. Since $\phi_1 + \dots + \phi_N = 1$, $d\phi_1 + \dots + d\phi_N = 0$. Thus
    %
    \[ \sum_{i = 1}^N d(\phi_i \omega) = \sum_{i = 1}^N \phi_i d\omega. \]
    %
    Thus
    %
    \begin{align*}
        \sum_{i = 1}^N \int_M \phi_i d\omega &= \sum_{i = 1}^N \int_M d(\phi_i \omega) = \sum_{i = 1}^N \int_{\partial M} \phi_i \omega = \int_{\partial M} \omega. \qedhere
    \end{align*}
\end{proof}

Just as the fundamental theorem of calculus is essential to understand analysis, Stokes' theorem is essential to modern manifold theory. As we shall see in the next chapter, it provides a connection between the analysis of the `boundaries' of manifolds to the analysis of differential forms, which continues the connection between the topology of a manifold and it's smooth structure.

\section{Basic De-Rham Cohomology}

Our goal in this chapter will be study when a $k$-form $\omega$ on a manifold $M$ is \emph{exact}, that is, when there exists a $k-1$ form $\nu$ such that $\omega = d\nu$. In some senses, this is like forming an `antiderivative' to the form $\omega$. In particular, this is exactly the case when considering a $1$-form $f\; dx$ on $\RR$, in which case if $dF = f$, then $F$ is an antiderivative to $f$. We will find that the ability to find such antiderivatives depends largely on the `shape' of the manifold $M$, i.e. it's homology class. The most fundamental result in this regard is the Poincare lemma. We recall that a smooth manifold $M$ is \emph{smoothly contractible} if there exists a smooth map $H: [0,1] \times M \to M$ such that $H(0,p) = p$ for all $p \in M$, and there exists $p_0 \in M$ such that $H(1,p) = p_0$ for all $p \in M$. If $\omega$ is an exact $k$-form, then certainly $\omega$ is \emph{closed}, i.e. $d\omega = 0$. 12e manifold, this is also sufficient.

\begin{lemma}[Poincare Lemma]
    If $M$ is a smoothly contractible manifold and $k > 0$, all closed $k$ forms on $M$ are exact.
\end{lemma}

The trick to the proof of this statement is to analyze $[0,1] \times M$. For each $t \in [0,1]$ we define $i_t: M \to [0,1] \times M$ by setting $i_t(p) = (t,p)$. If $\omega$ is a form on $[0,1] \times M$. We begin by analyzing the one forms on $[0,1] \times M$.

\begin{lemma}
    If $\omega$ is a closed $1$-form on $[0,1] \times M$, then $i_1^* \omega - i_0^* \omega$ is an exact one-form on $M$.
\end{lemma}
\begin{proof}
    Let us begin by working in coordinates. Consider the function $t: [0,1] \times M \to [0,1]$ given by projection onto the first factor. If $(x,U)$ is a coordinate system on $M$, then $dx^1, \dots, dx^n, dt \in \Gamma(T^*([0,1] \times U))$ form a frame, and $(x^1, \dots, x^n, t)$ form a coordinate system in a neighbourhood of each point. Thus given a closed 1-form $\omega$, there are smooth functions $a_1, \dots, a_n, f \in C^\infty([0,1] \times U)$ such that on $[0,1] \times U$,
    %
    \[ \omega = a_1 dx^1 + \dots + a_n dx^n + f dt. \]
    %
    For each $\alpha \in [0,1]$ and $p \in M$,
    %
    \[ i_\alpha^*(\omega)(p) = a_1(\alpha,p) dx^1 + \dots + a_n(\alpha,p) dx^n. \]
    %
    Now $d\omega = 0$ implies that for each $i \in \{ 1, \dots, n \}$,
    %
    \[ \frac{\partial a_i}{\partial t} = \frac{\partial f}{\partial x^i}. \]
    %
    Thus
    %
    \[ a_i(1,p) - a_i(0,p) = \int_0^1 \frac{\partial a_i}{\partial t}(s,p)\; ds = \int_0^1 \frac{\partial f}{\partial x^i}(s,p)\; ds. \]
    %
    In particular, if we define a smooth function $g$ on $U$ by setting
    %
    \[ g(p) = \int_0^1 f(s,p)\; ds, \]
    %
    then
    %
    \[ \frac{\partial g}{\partial x^i} = \int_0^1 \frac{\partial f}{\partial x^i}(s,p)\; ds = a_i(1,p) - a_i(0,p). \]
    %
    It thus follows that $dg = i_1^* \omega - i_0^* \omega$. This does not complete the proof, since we worked only locally in a coordinate system. But the function $f$ does not really depend on the coordinate system, since the vector field
    %
    \[ X = \frac{\partial}{\partial t}, \] 
    %
    is globally defined on $[0,1] \times M$, and $f = \omega(X)$. Thus the local definition of $g$ extends globally to all of $M$.
\end{proof}

The general case of $k$-forms works in essentially the same way for any $k > 0$.

\begin{lemma}
    If $\omega$ is a closed $k$-form, then $i_1^* \omega - i_0^* \omega$ is exact.
\end{lemma}
\begin{proof}
    Write $\pi(t,p) = p$ for the smooth projection map $\pi: [0,1] \times M \to M$. We can write $\omega = \omega_1 + (dt \wedge \eta)$, when $\eta$ is a $k-1$ form, and for each $(t,p) \in [0,1] \times M$ and $X_1, \dots, X_k \in T_{(t,p)}([0,1] \times M)$, we have $\omega_1(X_1, \dots, X_k) = 0$ if there exists $i \in \{ 1, \dots, k \}$ such that $\pi_*(X_i) = 0$, with $\eta$ also satisfying the analogous property for $k-1$ forms. We define a $k-1$ form $I\omega$ on $M$ such that for $X_1, \dots, X_{k-1} \in T_{(t,p)}([0,1] \times M)$,
    %
    \[ (I\omega)_p(X_1, \dots, X_{k-1}) = \int_0^1 \eta(s,p)((i_s)_* X_1, \dots, (i_s)_* X_{k-1})\; ds \]
    %
    We claim that for any (not necessarily closed) $k$-form $\omega$,
    %
    \[ i_1^* \omega - i_0^* \omega = d(I\omega) + I(d \omega), \]
    %
    which would complete the proof in general. The advantage of this formula is that both sides of the equation are linear in $\omega$, and so we can work in coordinates, and moreover, assume $\omega$ is a monomial. We consider two cases:
    %
    \begin{itemize}
        \item $\omega = f dx^I$ for some index set $I$. Then
        %
        \[ d\omega = * + \frac{\partial f}{\partial t} dt \wedge dx^I. \]
        %
        It is easy to see that
        %
        \[ I(d\omega)_p = \int_0^1 \frac{\partial f}{\partial t}(p,s)\; ds = (i^*_1 \omega)_p - (i^*_0 \omega)_p. \]
        %
        On the other hand, $I \omega = 0$, so the proof is complete.

        \item $\omega = f dt \wedge dx^I$. Then $i_1^* \omega = i_0^* \omega = 0$. Now
        %
        \begin{align*}
            I(d \omega)_p &= I \left( - \sum_{\alpha = 1}^n \frac{\partial f}{\partial x^\alpha}\; dt \wedge dx^\alpha \wedge dx^I \right)_p\\
            &= - \sum_{\alpha = 1}^n \left( \int_0^1 \frac{\partial f}{\partial x^\alpha}(s,p)\; ds \right) dx^\alpha \wedge dx^I.
        \end{align*}
        %
        On the other hand, we have
        %
        \[ (I \omega)_p = \left( \int_0^1 f(s,p)\; ds \right) dx^I. \]
        %
        And then
        %
        \begin{align*}
            d(I \omega) &= \sum_{\alpha = 1}^n \frac{\partial}{\partial x^\alpha} \left( \int_0^1 f(s,p)\; ds \right) dx^\alpha \wedge dx^I\\
            &= \sum_{\alpha = 1}^n \left( \int_0^1 \frac{\partial f}{\partial x^\alpha}(s,p)\; ds \right) dx^\alpha \wedge dx^I.
        \end{align*}
        %
        Adding the two terms completes this argument.
    \end{itemize}
    %
    Thus if $\omega$ is closed, then $i_1^* \omega - i_0^* \omega = d(I\omega)$ is exact.
\end{proof}

\begin{proof}
    To complete the proof of the Poincare lemma, we consider a smooth contraction $H: [0,1] \times M \to M$. Given any $k$-form $\omega$ on $M$, we consider the $k$-form $H^* \omega$ on $[0,1] \times M$. If $\omega$ is closed, then $H^* \omega$ is closed. Thus $i_1^*(H^* \omega) - i_0^*(H^* \omega)$ is exact. But $i_0^*(H^* \omega) = 0$, and $i_1^*(H^* \omega) = \omega$.
\end{proof}

In the special case where $U$ is an open subset of $\RR^n$ which is \emph{star shaped}, in the sense that there is a point $x_0 \in U$ such that if $x \in U$, then any point on the line between $x$ and $x_0$ lies in $U$,  there exists a $k$-form $\omega$ we can find a \emph{explicit} formula for a $k-1$ form $\eta$ such that $d\eta = \omega$.

\begin{theorem}
    If $U$ is star-shaped around the origin, define $H(t,x) = tx$. If
    %
    \[ \omega = \sum_{i_1 < \dots , i_k} a_{i_1 \dots i_k} dx^{i_1} \wedge \dots \wedge dx^{i_k}, \]
    %
    then
    %
    \begin{align*}
        I(H^*& \omega)\\
        &= \sum_{i_1 < \dots < i_k} \sum_{\alpha = 1}^k (-1)^{\alpha - 1}) \left( \int_0^1 t^{k-1} a_{i_1 \dots i_k}(tx)\; dt \right)\\
        &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ x^{i_\alpha} dx^{i-1} \wedge \dots \wedge \widehat{dx^{i_\alpha}} \wedge \dots \wedge dx^{i_k}.
    \end{align*}
    %
    In particular, if $\omega$ is closed, then the formula above gives a form $\eta$ such that $\omega = d\eta$.
\end{theorem}
\begin{proof}
    We calculate that
    %
    \begin{align*}
        (H^* \omega)(t,x) &= \sum_{i_1 < \dots < i_k} a_{i_1 \dots i_k}(tx) \bigwedge_{\alpha = 1}^k (x^{i_\alpha} dt + t dx^\alpha)\\
        &= \sum_{i_1 < \dots < i_k} a_{i_1 \dots i_k}(tx) \Big( t^k dx^{i_1} \wedge \dots \wedge dx^{i_k}\\
        &\ \ \ \ \ \ + \sum_{\alpha = 1}^k (-1)^{\alpha - 1} t^{k-1} x^{i_\alpha} \cdot dt \wedge (dx^{i_1} \wedge \dots \wedge \widehat{dx^{i_\alpha}} \wedge \dots \wedge dx^{i_k}) \Big).
    \end{align*}
    %
    Thus
    %
    \begin{align*}
        I&(H^* \omega)_x
        \\ &= \sum_{i_1 < \dots < i_k} \sum_{\alpha = 1}^k (-1)^{\alpha - 1} \left( \int_0^1 a_{i_1 \dots i_k}(tx) t^k \right) x^{i_\alpha}\; (dx^{i_1} \wedge \dots \wedge \widehat{dx^{i_\alpha}} \wedge \dots \wedge dx^{i_k}).
    \end{align*}
    %
    This completes the calculation.
\end{proof}

Just as topological information about a manifold indicates information about antidifferentiation, the Poincare lemma, combined with Stokes' theorem, also allows us to transfer information about integration into topological information.

\begin{theorem}
    Any compact oriented manifold is not smoothly contractible.
\end{theorem}
\begin{proof}
    Let $M$ be a compact oriented manifold of dimension $n$. By working locally, we can certainly find an $n$-form $\omega$ such that
    %
    \[ \int_M \omega \neq 0. \]
    %
    Since $d\omega$ is an $n+1$ form, it is trivial that $d\omega = 0$. If there existed $\eta$ such that $d\eta = \omega$, then $\eta$ is trivially compactly supported, so we may apply Stokes' theorem to conclude
    %
    \[ \int_M \omega = \int_M d\eta = \int_{\partial M} \eta = 0. \]
    %
    Thus there exists a closed form on $M$ which is not exact, so $M$ cannot be smoothly contractible.
\end{proof}

We now want to form the ability to `count' how many closed forms there are on a manifold which are not exact. This will make it much easier for us to characterize the behaviour of integrals of differential forms. If $\omega$ is a non-exact closed $k$-form on a manifold $M$, then $\omega + d \eta$ is a non-exact closed $k$-form for any $k-1$ form $\eta$. Thus it is natural to consider these forms as equivalent when counting the non-exact forms. This is also natural from the point of integration; If $N$ is an oriented $k$ dimensional compact submanifold of $M$ without boundary, then Stoke's theorem implies that
%
\[ \int_N d\eta = \int_{\partial N} \eta = 0, \]
%
so
%
\[ \int_N \omega + d \eta = \int_N \omega. \]
%
This means that the `induced action' of $\omega$ on closed submanifolds is the same as those that differ by an exact differential. If we let $Z^k(M) \subset \Omega^k(M)$ denote the class of closed $k$-forms on $M$, and $B^k(M)$ the class of exact $k$ forms, then we can form the quotient space $H^k(M) = Z^k(M)/B^k(M)$, known as the \emph{$k$'th De Rham Cohomology Vector Space}. The dimension of $H^k(M)$ then precisely counts the maximal number of linearly independant non-exact closed $k$ forms on $M$. We use the computation of De Rham groups as a chance to hone our techniques for integrating functions on manifolds.

\begin{example}
    If $M$ is smoothly contractible, then $H^k(M)$ is trivial for $k \geq 0$. Thus there are no interesting ways to integrate closed submanifolds on Euclidean space.
\end{example}

\begin{example}
    The space $B^0(M)$ is trivial, so $H^0(M)$ is really just the space of all smooth real-valued functions $f$ on $M$ such that $df = 0$. Another way of describing this is the space of all functions on $M$ which vanish on each component of $M$, so the dimension of $H^0(M)$ gives the number of connected components of $M$.
\end{example}

\begin{example}
    We have shown that if $M$ is oriented, compact, and $n$-dimensional, then $H^n(M)$ is nontrivial.
\end{example}

The fact that we don't have techniques to calculate homology of anything beyond contractible examples. The next simplest example for us will be obtained from introducing a hole to a contractible example, i.e. $\RR^n - \{ 0 \}$. To calculate this De-Rham cohomology, we introduce a generalized system of polar coordinates on $\RR^n - \{ 0 \}$.

On $S^{n-1}$, there is a natural choice of an `orientation form' on $S^{n-1}$. We define an $n-1$ form $\sigma$ on $\RR^n - \{ 0 \}$ by setting
%
\[ \sigma_x(X_1, \dots, X_{n-1}) = \det(x,X_1, \dots, X_{n-1}). \]
%
Then $\sigma$ restricts to a form on $S^{n-1}$ such that for any oriented vectors $X_1, \dots, X_{n-1} \in T_x M$, $\sigma_x(X_1, \dots, X_{n-1}) > 0$. In standard coordinates,
%
\[ \sigma = \sum_{i = 1}^n (-1)^{i-1} x^i dx^1 \wedge \dots \wedge \widehat{dx^i} \wedge \dots \wedge dx^n. \]
%
This form is not closed, i.e. $d\sigma = n \cdot dx^1 \wedge \dots \wedge dx^n$, but we can use it to find a form which is closed, but not exact. We consider the retraction map $r: \RR^n - \{ 0 \} \to S^{n-1}$ given by setting $r(x) = x |x|^{-1}$. We then consider the form $\eta = r^*( \sigma|_{S^{n-1}} )$. Then $\eta$ is closed, since $\sigma|_{S^{n-1}}$ is closed on $S^{n-1}$, and $d\eta = r^*( d\sigma|_{S^{n-1}}) = r^*(0) = 0$. But $\eta$ is certainly not exact, because if we had $\eta = d\psi$, then since $i^*(\eta) = \sigma|_{S^{n-1}}$, we would have
%
\[ d(i^*(\psi)) = i^*(d\psi) = i^*(\eta) = \sigma|_{S^{n-1}}, \]
%
which is impossible since $\int_{S^{n-1}} \sigma > 0$.

It will be useful for us to calculate $\eta$ is cartesian coordinates. But this is simple, since we find that for each $x \in \RR^n - \{ 0 \}$,
%
\[ \eta_x = |x|^{-n} \sigma_x. \]
%
To prove this claim, we consider $n-1$ vectors $v_1, \dots, v_{n-1} \in \RR^n$, and show that for each $x \in \RR^n - \{ 0 \}$,
%
\[ \sigma_{r(x)}(r_*((v_1)_x), \dots, r_*((v_{n-1})_x)) = \sigma_x((v_1)_x, \dots, (v_{n-1})_x). \]
%
We note that $r_*(x_x) = 0$, since the curve passing through $x$ and travelling in the direction of $x$ becomes constant in the image of the retraction map $r$. This implies the left hand side of the equation vanishes if $v_i$ is a multiple of $x$, for any $i$. Thus to prove this claim, we may assume that the $n-1$ vectors $v_1, \dots, v_{n-1}$ all lie in the plane perpendicular to $x$. The proof will be complete if we can show for such vectors, $r_*((v_i)_x) = |x|^{-1} v_i$. We may assume $v_i$ is a unit vector, but then this is obvious, since by Pythagoras' theorem,
%
\begin{align*}
    r(x + tv_i) &= \frac{x + tv_i}{|x + tv_i|}\\
    &= \frac{x + tv_i}{\sqrt{|x|^2 + t^2}}\\
    &= (x + tv_i) \left( \frac{1}{|x|} + O(t^2) \right)\\
    &= r(x) + t (v_i/|x|) + O(t^2),
\end{align*}
%
which implies that $D_{v_i}(r)(x) = v_i/|x|$. Thus we conclude
%
\[ \eta_x = \frac{1}{|x|^n} \sum_{i = 1}^n (-1)^{i-1} x^i (dx^1 \wedge \dots \wedge \widehat{dx^i} \wedge \dots \wedge dx^n). \]
%
As a simple corollary, we have proved a form of `polar coordinate integration' in $\RR^n$.

\begin{theorem}
    Let $f: B \to \RR$, where $B$ is the closed unit ball in $\RR^n$. Define a function $g: S^{n-1} \to \RR$ by setting
    %
    \[ g(x) = \int_0^1 r^{n-1} f(rx)\; dr. \]
    %
    Then
    %
    \[ \int_B f\; dx^1 \wedge \dots \wedge dx^n = \int_{S^{n-1}} g \sigma. \]
\end{theorem}
\begin{proof}
    If $s: B - \{ 0 \} \to (0,\infty)$ is given by setting $s(x) = |x|$, then we claim that
    %
    \[ ds \wedge \eta = |x|^{-(n-1)} \cdot dx^1 \wedge \dots \wedge dx^n. \]
    %
    Indeed, we have
    %
    \[ ds = |x|^{-1} \sum_{i = 1}^n x^i dx^i \]
    %
    so
    %
    \begin{align*}
        ds \wedge \eta &= \frac{1}{|x|^{n+1}} \sum_{i = 1}^n (-1)^{i-1} (x^i)^2 dx^i \wedge (dx^1 \wedge \dots \wedge \widehat{dx^i} \wedge \dots \wedge dx^n)\\
        &= \frac{dx^1 \wedge \dots \wedge dx^n}{|x|^{n-1}}.
    \end{align*}
    %
    If we consider the diffeomorphism $g: (0,1) \times S^{n-1} \to B - \{ 0 \}$ given by $g(s,x) = sx$, then $g^*(ds) = ds$, and $g^*(\eta) = \sigma|_{S^{n-1}}$ since $(r \circ g)(s,x) = x$, and $\eta = r^* \sigma|_{S^{n-1}}$. Thus we conclude that if $f$ is compactly supported on the interior of $B$, away from the origin, then
    %
    \begin{align*}
        \int_B f\; dx^1 \wedge \dots \wedge dx^n &= \int_B f\; s^{n-1} ds \wedge \eta\\
        &= \int_{S^{n-1}} \left( \int_0^1 f(sx) s^{n-1}\; ds \right) \sigma.
    \end{align*}
    %
    An approximation argument then establishes this result in general.
\end{proof}

We would like to reduce our computations to coordinate neighbourhoods, but in order to do this, we must introduce another quotient class of forms. We restrict our knowledge to the class $\Omega^k_c(M)$ of differential forms with compact support, with the corresponding spaces $Z^k_c(M)$ and $B^k_c(M)$, which is the class of forms which is the differential of a form {\it with compact support}. We therefore get an induced cohomology $H^k_c(M)$, known as the \emph{De Rham cohomology with compact support}. Of course, if $M$ is compact, then $H_c(M) = H(M)$.

\begin{example}
    The space $B^k_c(M)$ is {\it not} the same as the class of all exact differentials with compact support. On $\mathbf{R}^n$, consider a compactly supported function $f$ with $f(x) \geq 0$, and $f(x_0) > 0$ at some point $x_0$. Now the form $\omega = f\; dx^1 \wedge \dots \wedge dx^n$ is exact, but it is not a differential of an $n-1$ form $\eta$ with compact support, because if this were true, then by Stoke's theorem
    %
    \[ 0 < \int_{\mathbf{R}^n} f(x)\; dx = \int_{\mathbf{R}^n} d\eta = 0 \]
    %
    Thus $H^n_c(\mathbf{R}^n)$ is non trivial, even though $H^n(\mathbf{R}^n)$ is trivial. A similar argument shows that if $M$ is any oriented $n$ manifold, then $H^n_c(M)$ is nontrivial.
\end{example}

In fact, we now prove $H^n_c(M)$ is always one dimensional.

\begin{theorem}
    If $M$ is oriented and connected, the map
    %
    \[ \omega \mapsto \int_M \omega \]
    %
    induces an isomorphism of $H^n_c(M)$ to $\mathbf{R}$.
\end{theorem}
\begin{proof}
    First, take $M = \mathbf{R}$. If $\omega$ is a closed one form on $\mathbf{R}$, there exists a function $f$ such that $df = \omega$. If $\omega$ vanishes outside of $[a,b]$, then $f$ must be constant on $(-\infty,a]$ and $[b,\infty)$. Since
    %
    \[ f(b) - f(a) = \int_a^b \omega = \int_{\mathbf{R}} \omega, \]
    %
    if $\int_{\mathbf{R}} \omega = 0$, then $f(b) = f(a)$, and so if $g(x) = f(x) - f(a)$, then $g$ has compact support and $dg = \omega$. Thus we have proved the theorem in this special case.

    Now we prove that if the theorem is true for all $n-1$ manifolds, then it must be true for $\mathbf{R}^n$. Consider a closed $n$ form $\omega$ with compact support on $\mathbf{R}^n$. For simplicity we may assume $\omega$ is supported on the interior of the unit ball centered at the origin. We claim that there exists an $n-1$ form $\eta$ such that $d\eta = \omega$. In particular, if $\omega = f dx^1 \wedge \dots \wedge dx^n$, we can actually define $\eta$ by the formula
    %
    \begin{align*}
        \eta(x) &= \sum_{i = 1}^n (-1)^{i-1} \left( \int_0^1 t^{n-1} f(tx)\; dt \right) x^i \cdot (dx^1 \wedge \dots \wedge \widehat{dx^i} \wedge \dots \wedge dx^n)\\
        &= \left( \int_0^{|x|} t^{n-1} f(t \cdot r(x))\; dt \right) \cdot |x|^{-n} \sum_{i = 1}^n (-1)^{i-1} x^i \cdot (dx^1 \wedge \dots \wedge \widehat{dx^i} \wedge \dots \wedge dx^n)\\
        &= \left( \int_0^{|x|} t^{n-1} f(t \cdot r(x))\; dt \right) \cdot r^*(\sigma|_{S^{n-1}}).
    \end{align*}
    %
    However, $\eta$ does not have compact support, which we will fix. Define $g: S^{n-1} \to \RR$ by setting
    %
    \[ g(x) = \int_0^1 t^{n-1} f(tx)\; dt. \]
    %
    If $|x| > 1$, $f(x) = 0$. Thus for such $x$ we have
    %
    \[ \eta(x) = g(r(x)) \cdot r^*(\sigma|_{S^{n-1}}) = r^*(g \sigma|_{S^{n-1}}). \]
    %
    By the polar coordinates formula,
    %
    \[ \int_{S^{n-1}} g \sigma = \int f\; dx^1 \wedge \dots \wedge dx^n = \int_{\RR^n} \omega = 0. \]
    %
    By induction, we know that this implies $g \sigma|_{S^{n-1}}$ is exact, so we can write $g \sigma|_{S^{n-1}} = d\lambda$ for some $n-2$ form $\lambda$ on $S^{n-1}$. But this means that
    %
    \[ \eta = r^*(d\lambda) = d(r^* \lambda) \]
    %
    If we choose $h \in C^\infty(M)$ such that $h(x) = 1$ for $|x| \geq 1$ and $h(x) = 0$ in a neighbourhood of the origin, then
    %
    \[ \omega = d\eta = d(\eta - d(h r^* \lambda)), \]
    %
    The form $\eta - d(h r^* \lambda)$ has compact support since for $|x| > 1$,
    %
    \[ \eta - d(h r^* \lambda) = \eta - d(r^* \lambda) = 0. \]
    %
    Thus we have established the result in this case.

    Finally we show that if the theorem is true for $\RR^n$, then the theorem is true for all connected oriented $n$ dimensional manifolds. Let $M$ be an $n$ dimensional manifold, and consider some $n$-form $\omega$ on $M$ with
    %
    \[ \int_M \omega \neq 0, \]
    %
    and such that $\omega$ is compactly supported on some open set $U \subset M$ diffeomorphic to $\RR^n$. We must show that for any other compactly supported $n$ form $\omega'$, there exists a constant $c$ and a compactly supported $n-1$ form $\lambda$ such that $\omega = \omega' + d\lambda$. Using a partition of unity argument, we may assume without loss of generality that $\omega'$ is also compactly supported on an open set $V$ diffeomorphic to $\RR^n$. Since $M$ is connected, there is a sequence of open sets $U_0, \dots, U_N$ with $U_i \cap U_{i+1} \neq \emptyset$ for each $i$, with $U_0 = U$ and $U_N = V$. We may then choose forms $\omega_i$ supported on $V_i \cap V_{i+1}$ with $\int_{V_i} \omega_i \neq 0$. Then since we are assuming the theorem holds on $\RR^n$, we can find constants $c_i$ such that
    %
    \[ \omega_0 - c_0 \omega = d\eta_0 \]
    %
    for $i \in \{ 1, \dots, N \}$,
    %
    \[ \omega_i - c_i \omega_{i-1} = d\eta_i, \]
    %
    and such that $\omega' - c_{N+1} \omega_N = d\eta_{N+1}$. Adding up these inequalities gives the desired result.
\end{proof}

One can use the chain of sets argument at the end of the last proof to establish another result.

\begin{theorem}
    If $M$ is a connected non-orientable $n$ manifold, $H^n_c(M) = 0$.
\end{theorem}
\begin{proof}
    Let $\omega$ be a compactly supported $n$-form on $M$, supported on an open set $U$ diffeomorphic to $\RR^n$ by a map $x_0: U \to \RR^n$, and such that
    %
    \[ \int_U \omega > 0; \]
    %
    the integral being defined after choosing an orientation for $U$ induced by $x_0$. It suffices to show that $\omega = d\eta$ for some form $\eta$ with compact support. If $M$ is non-orientable, there is a sequence $U_0, \dots, U_N$ with $U_0 = U_N = U$, $U_i \cap U_{i+1} \neq \emptyset$ for each $i \in \{ 1, \dots, N-1 \}$, and each $U_i$ is diffeomorphic to $\RR^n$ by a map $x_i: U_i \to \RR^n$ such that $x_{i+1} \circ x_i^{-1}$ is orientation preserving for each $i \in \{ 0, \dots, N-1 \}$, and such that $x_1 \circ x_N^{-1}$ is orientation \emph{reversing}. Without loss of generality, we may assume $\omega$ is supported on $U_0 \cap U_1$, and that
    %
    \[ \int_{U_0} \omega > 0 \]
    %
    We set $\omega_0 = \omega$, set $\omega_N = -\omega$, and for each $i \in \{ 1, \dots, N-1 \}$, we pick an $n$-form $\omega_i$ supported on $U_i \cap U_{i+1}$ such that
    %
    \[ \int_{U_i} \omega_i > 0, \]
    %
    where $U_i$ is given the orientation induced by $x_i$. Since $x_{i+1} \cap x_i^{-1}$ is orientation preserving for each $i \in \{ 0, \dots, N \}$, we find
    %
    \[ \int_{U_{i+1}} \omega_i = \int_{U_i} \omega_i > 0. \]
    %
    For each $i \in \{ 0, \dots, N-1 \}$, since $H^n_c(U_i) = H^n_c(\RR^n) = 0$, we can find $c_i \in \RR$ and a compactly supported $n-1$ form $\eta_i$ such that $\omega_{i+1} = c_i \omega_i + d\eta_i$, and
    %
    \[ c_i = \frac{\int_{U_{i+1}} \omega_{i+1}}{\int_{U_{i+1}} \omega_i} = \frac{\int_{U_{i+1}} \omega_{i+1}}{\int_{U_i} \omega_i} > 0. \]
    %
    Thus we conclude that there is $c > 0$ and a compactly supported $n-1$ form $\eta$ such that
    %
    \[ -\omega = c\omega + d\eta \]
    %
    But this means that $\omega$ is exact, because $\omega = d(-\eta/(c + 1))$.
\end{proof}

\begin{theorem}
    If $M$ is a connected non-compact $n$-manifold, $H^n(M) = 0$.
\end{theorem}
\begin{proof}
    Let $\omega$ be an $n$-form. We begin by assuming the support of $\omega$ is compactly contained in an open set $U$ diffeomorphic to $\RR^n$. We consider a special cover of $M$, consisting of a sequence of open sets $\{ U_i \}$ such that each $U_i$ is diffeomorphic to $\RR^n$, $U_i \cap U_{i+1} \neq \emptyset$ for each $i$, and for any compact set $K$, it is eventually true that $K \cap U_i = \emptyset$. We set $\omega_0 = \omega$, and then choose $n$-forms $\omega_i$ compactly supported on $U_i \cap U_{i+1}$, with $\int_{U_i} \omega_i \neq 0$. Then there are constants $c_i$ and $n-1$ forms $\eta_i$ each compactly supported in $U_i$ for each $i$ such that $\omega_i = c_i \omega_{i+1} + d\eta_i$. By induction, it therefore follows that
    %
    \[ \omega = \sum_{i = 0}^\infty c_0 \dots c_{i-1} d\eta_i = d \left( \sum_{i = 0}^\infty c_0 \dots c_{i-1} \eta_i \right), \]
    %
    which is well defined by local compactness, since $d\eta_i$ eventually vanishes on every compact set. But now a partition of unity argument shows that this type of argument works for all forms, not necessarily just compactly supported forms.
\end{proof}

We have thus computed the De Rham cohomology in quite a few useful situations:
%
\begin{itemize}
    \item If $M$ is a connected, smoothly contractible manifold, then $H^k(M)$ is trivial for $k > 0$, and $H^0(M)$ is one dimensional.
    \item If $M$ is a connected $n$-dimensional manifold, then $H^0(M)$ is one dimensional, and
    %
    \[ \dim(H^n(M)) = \begin{cases} 1 &: \text{if}\ M\ \text{is compact and orientable,} \\ 0 &: \text{if}\ M\ \text{is non-compact or non-orientable}. \end{cases} \]
    %
    and
    %
    \[ \dim(H^n_c(M)) = \begin{cases} 1 &: \text{if}\ M\ \text{is orientable,} \\ 0 &: \text{if}\ M\ \text{is non-orientable.} \end{cases} \]
\end{itemize}
%
Since one only ever integrates $n$ dimensional forms over a manifold, these results are the most useful for applying De-Rham cohomology to integration theory. In the next section, we apply this result to obtain a useful formula for integration of pullbacks of differential forms, known as the \emph{degree formula}.

\section{Pullbacks}

A smooth map $f: M \to N$ between two manifolds induces a map $f^*: \Gamma(\Omega^k(TN)) \to \Gamma(\Omega^k(TM))$ for each $k$. Since $f^*(d\eta) = d(f^* \eta)$, $f^*(Z^k(N)) \subset Z^k(M)$, and $f^*(B^k(N)) \subset B^k(M)$. In particular, $f^*$ descends to a map from $H^k(N)$ to $H^k(M)$.

\begin{example}
    Consider the simplest example where $k = 0$. Then $H^0(N)$ can be identified with the family of functions on $N$ which are constant on each component. If $f: M \to N$ is smooth, and $u \in C^\infty(N)$ is constant on each fibre, then $f^* u = u \circ f$ is constant on each fibre, and is therefore an element of $H^0(M)$. Knowledge of the behaviour of $f^*$ from $H^0(M)$ to $H^0(N)$ thus gives us the description of which components of $M$ map to which components of $N$.
\end{example}

The other case we are able to analyze presently is where $M$ and $N$ are both compact, connected, oriented $n$-manifolds, and we consider the map $f^*: H^n(N) \to H^n(M)$. Since $H^n(N)$ and $H^n(M)$ are both one dimensional, and $H^n(M)$ and $H^m(N)$ can be naturally identified with $\RR$ under the maps
%
\[ \omega \mapsto \int_M \omega \quad\text{and}\quad \omega \mapsto \int_N \omega, \]
%
there must exist a constant $\alpha$, depending only on the map $f$ such that for each closed $n$-form $\omega$ on $N$,
%
\[ \int_M f^*(\omega) = \alpha \int_N \omega. \]
%
This constant $\alpha$ is known as the \emph{degree} of the map $f$, denoted $\deg(f)$. Thus for any $n$-form $\omega$ on $N$,
%
\[ \int_M f^*(\omega) = (\deg f) \int_N \omega. \]
%
We now show that, perhaps surprisingly, $\deg(f)$ is \emph{always} an integer.

\begin{theorem}
    Let $f: M \to N$ be a proper map between two compact, connected, oriented $n$ manifolds, and let $q \in N$ be a regular value of $f$. For each $p \in f^{-1}(q)$, let
    %
    \[ \text{sgn}_p(f) = \begin{cases} 1 &: f_*|_p: T_p M \to T_p N\ \text{is orientation preserving} \\ -1 &: f_*|_p: T_p M \to T_p N\ \text{is orientation reversing}. \end{cases} \]
    %
    Then
    %
    \[ \deg(f) = \sum_{p \in f^{-1}(q)} \text{sgn}_p(f), \]
    %
    where the right hand side is equal to zero if $f^{-1}(q) = \emptyset$, and is well defined since $f^{-1}(q)$ is finite because $M$ is compact. In particular, this quantity is invariant of the regular value chosen (and by Sard's theorem, regular values always exist).
\end{theorem}
\begin{proof}
    Let $f^{-1}(q) = \{ p_1, \dots, p_N \}$. Since $q$ is a regular value, we may choosen $N$ disjoint coordinate charts $(x_1,U_1), \dots, (x_N,U_N)$ with $p_i \in U_i$ for each $i$, and a chart $(y,V)$ containing $q$ such that $y \circ f \circ x_i^{-1}: x_i(U_i) \to y(V)$ is a diffeomorphism for each $i$. Set $\omega = g dy^1 \wedge \dots \wedge dy^n$, where $g$ is a non-negative function with compact support contained in $V$. Then
    %
    \[ \int_M f^* \omega = \sum_{i = 1}^k \int_{U_i} f^*\omega. \]
    %
    But for each $i$, because $f$ is a diffeomorphism,
    %
    \[ \int_{U_i} f^* \omega = \begin{cases} \int_V \omega &: \text{$f$ is orientation preserving} \\ - \int_V \omega &: \text{$f$ is orientation reversing} \end{cases}. \qedhere \]
\end{proof}

\begin{example}
    Consider the antipodal map $f: S^n \to S^n$ given by setting $f(x) = -x$. Then $f$ is a diffeomorphism, and hence either orientation preserving or orientation reversing. Since $f_*(v_x) = (-v)_{-x}$, and so $f_*$ takes the oriented basis $\{ (e_2)_{e_1}, \dots, (e_{n+1})_{e_1} \}$ to the basis $\{ (-e_2)_{-e_1}, \dots, (-e_{n+1})_{-e_1} \}$. Since
    %
    \[ \det(-e_1,-e_2, \dots, -e_{n+1}) = (-1)^{n+1}, \]
    %
    we conclude $f$ is orientation preserving if $n$ is odd, and orientation reversing if $n$ is even. Since $f^{-1}(p)$ consists of a single point for each $p \in S^n$, this means that the degree of $f$ is -1 if $n$ is even, and $1$ if $n$ is odd.
\end{example}

Two maps $f,g: M \to N$ are \emph{smoothly homotopic} if there exists a smooth map $H: [0,1] \times M \to N$ such that $H(0,p) = f(p)$, for each $p \in M$, and $H(1,p) = g(p)$ for each $p \in N$. Notice that $M$ is \emph{smoothly} contractible if and only if the identity map $i: M \to M$ is smoothly homotopic to a constant map. Recall that for each $k$-form $\omega$ on $[0,1] \times M$, we constructed a $k-1$ form $I\omega$ on $M$ such that
%
\[ i_1^* \omega - i_0^* \omega = d(I\omega) + I(d\omega). \]
%
This result implies a more general result.

\begin{theorem}
    If $f,g$ are smoothly homotopic, then for each $k$, the two maps $f^*, g^*: H^k(N) \to H^k(M)$ are equal.
\end{theorem}
\begin{proof}
    Then if $\omega$ is any closed $k$-form on $N$,
    %
    \[ g^* \omega - f^* \omega = d(I \omega), \]
    %
    so $g^* \omega$ is equal to $f^* \omega$ in $H^k(M)$.
\end{proof}

\begin{corollary}
    If $M$ and $N$ are compact oriented $n$ manifolds, and $f,g: M \to N$ are smoothly homotopic, then $\deg(f) = \deg(g)$.
\end{corollary}

\begin{corollary}
    If $n$ is even, there are no globally non-zero vector fields on $S^n$.
\end{corollary}
\begin{proof}
    Since the degree of the identity map on $S^n$ is equal to one, and the degree of the antipodal map $x \mapsto -x$ is equal to $-1$, this implies the identity map is not smoothly homotopic to the antipodal map. If $X$ is an everywhere non-zero vector field on $S^n$, then we can construct a homotopy $H: [0,1] \to M \to M$ between the identity map and the antipodal map by setting $H(t,p)$ to be the point $\pi t$ radians along the unique great circle connecting $p$ to $-p$ which travels in the same direction as $X_p$. This gives a contradiction.
\end{proof}

\begin{remark}
    If $n$ is odd, we can construct an everywhere non-zero vector field $X$ on $S^n$ such that if $p = (x_1, \dots, x_{n+1})$,
    %
    \[ X_p = (-x_2,x_1,-x_4,x_3, \dots, -x_{n+1},x_n). \]
    %
    In particular, the antipodal map \emph{is} homotopic to the identtiy map if $n$ is odd.
\end{remark}

As another application, consider the retraction $r: \RR^n - \{ 0 \} \to S^{n-1}$ given by setting $r(x) = x/|x|$. Then if $i: S^{n-1} \to \RR^n - \{ 0 \}$ is the inclusion, then $r \circ i$ is the identity map in $S^{n-1}$. Conversely, $i \circ r$ is \emph{not} the identity map on $\RR^n - \{ 0 \}$. On the other hand, it \emph{is} homotopic to the identity, which we can define by setting
%
\[ H(p,t) = tp + (1 - t) r(p). \]
%
A retraction $r$ such that $i \circ r$ is homotopic to the identity map is known as a \emph{deformation retraction}. Thus $(r \circ i)^*$ and $(i \circ r)^*$ act as the identity maps on $H^k(S^{n-1})$ and $H^k(\RR^n - \{ 0 \})$ for each $k$. Thus $r^*$ and $i^*$ are inverses of one another, so in particular, $H^k(S^{n-1})$ and $H^k(\RR^n - \{ 0 \})$ are \emph{isomorphic} to one another. In particular, this implies $H^{n-1}(\RR^n - \{ 0 \})$ is 1-dimensional - a generator for this space being the closed form $r^*\sigma$ defined earlier in this chapter.

\begin{theorem}
    For $0 < k < n-1$, $H^k(\RR^n - \{ 0 \}) = H^k(S^{n-1}) = 0$.
\end{theorem}
\begin{proof}
    We prove the theorem by induction on $n$, the claim being obvious for $n < 3$. For $n = 3$, we claim $H^1(\RR^3 - \{ 0 \})$ is trivial. Let $\omega$ be a closed one-form on $\RR^3 - \{ 0 \}$. Let
    %
    \[ A = \RR^3 - \{ (0,0) \times (-\infty,0] \} \quad\text{and}\quad B = \RR^3 - \{ (0,0) \times [0,\infty) \}. \]
    %
    Then $A$ and $B$ are both contractible (they are both star shaped), so there exists smooth functions $f_A$ and $f_B$ on $A$ and $B$ respectively such that $\omega = df_A$ on $A$, and $\omega = df_B$ on $B$. In particular, $df_A - df_B = 0$ on $A \cap B$. Since $A \cap B = (\RR^2 - \{ 0 \}) \times \RR$, which is connected, this implies $f_A - f_B$ is a constant $c$ on $A \cap B$. But this means that $\omega$ is exact, for $\omega = d(f_A - c)$ on $A$, and $\omega = df_B$ on $B$, and $f_A - c$ and $f_B$ agree on $A \cap B$.

    In general, consider $\RR^n - \{ 0 \}$. The case of one-forms is similar. We define
    %
    \[  A = \RR^n - \{ \{ 0 \} \times (-\infty,0] \} \quad\text{and}\quad B = \RR^n - \{ \{ 0 \} \times [0,\infty) \}. \]
    %
    Use contractibility to show closed one-form are the differentials of functions on $A$ and $B$, and then shift by a constant. If $k \geq 2$, and $\omega$ is a $k$-form on $\RR^n - \{ 0 \}$, since $A$ and $B$ are star-shaped, there exists two $k-1$ forms $\eta_A$ and $\eta_B$ on $A$ and $B$ such that $\omega = d\eta_A$ on $A$, and $\omega = d\eta_B$ on $B$. Since $A \cap B = (\RR^{n-1} - \{ 0 \}) \times \RR)$, which is homotopic to $\RR^{n-1} - \{ 0 \}$, or to $S^{n-2}$, we have by induction that $H^{k-2}(S^{n-2})$ is zero dimensional. Since $d(\eta_A - \eta_B) = 0$ on $A \cap B$, there exists a $k-2$ form $\lambda$ on $A \cap B$ such that $\eta_A - \eta_B = d\lambda$.

    Unlike the previous case, we cannot extend $\eta_A - d\lambda$ to a $k-2$ form on $A$, since $\lambda$ is only defined on $A \cap B$. To circumvent this, we consider a partition of unity $\{ \phi_A, \phi_B \}$ subordinate to $\{ A, B \}$. Then the form $\phi_A \lambda$ and $\phi_B \lambda$ extend to forms on $A$ and $B$. On $A \cap B$, we have
    %
    \begin{align*}
        \eta_A - d(\phi_B \lambda) &= \eta_A - d\phi_B \cdot \lambda - \phi_B d\lambda\\
            &= (\eta_B + d\lambda) + d\phi_A \cdot \lambda - (1 - \phi_A) d\lambda\\
            &= \eta_B + d\phi_A \cdot \lambda + \phi_A \cdot d\lambda\\
            &= \eta_B + d(\phi_A \lambda).
    \end{align*}
    %
    Thus we can define a form $\eta$ by setting it equal to $\eta_A - d(\phi_B \lambda)$ on $A$, and equal to $\eta_B + d(\phi_A \lambda)$ on $B$. Then $d\eta = d\eta_A = \omega$ on $A$, and $d\eta = d\eta_B = \omega$ on $B$. So $\omega$ is exact, and so we conclude that $H^k(\RR^n - \{ 0 \})$ is trivial.
\end{proof}

We end this chapter by finishing our calculation of the homology groups $H^k_c(\RR^n)$.

\begin{theorem}
    For $0 \leq k < n$, we have $H^k_c(\RR^n) = 0$.
\end{theorem}
\begin{proof}
    We begin by showing $H^0_c(\RR^n) = 0$. Indeed, if $f$ is a compactly supported function on $\RR^n$ with $df = 0$, then $f$ is constant, and since the function is compactly supported, $f = 0$. So there are no nontrivial compactly supported one-forms. For $0 < k < n$, we consider a closed compactly supported $k$-form $\omega$ on $\RR^n$. There certainly exists a $k-1$ form $\eta$ such that $d\eta = \omega$. We just do not know if we can find a compactly-supported $k-1$ form with this property. Let $B$ be a closed ball containing $\omega$. Then we know $d\eta = 0$ on $\RR^n - B$, which is homotopic to $\RR^n - \{ 0 \}$. This means that on $\RR^n - B$, $\eta = d\lambda$ for some $k-2$ form $\lambda$. If $f: \RR^n \to [0,1]$ is a smooth function with $f(x) = 0$ on a neighbourhood of $B$, and $f(x) = 1$ on $\RR^n - 2B$, where $2B$ is the ball with the same centre of $B$ but twice the radius, then $f \lambda$ is a well-defined form on $\RR^n$, and on $\RR^n - 3B$, $d(f\lambda) = d(\lambda) = \eta$. Thus if we define $\eta' = \eta - d(f\lambda)$, then $\eta'$ is compactly supported on $3B$, and $d(\eta') = d\eta = \omega$.
\end{proof}




\chapter{Riemannian Manifold}

Perhaps the only basic structure on vector spaces we have not yet adapted to tangent bundles is an \emph{inner product}. Recall that






\chapter{Lie Groups}

A \emph{Lie group} is a group whose multiplication and inversion operations are smooth. Examples include the group $\mathbf{R}^n$ under addition, the circle group $\mathbf{T} = \mathbf{R}/\mathbf{Z}$, and more generally, the toral groups $\mathbf{T}^n = \mathbf{R}^n / \mathbf{Z}^n$. The main noncommutative exmamples occur as matrix groups, like $GL_n(\mathbf{R})$, $SL_n(\mathbf{R})$, $O_n(\mathbf{R})$ and $SU_n(\mathbf{R})$.

An important fact about $M(n)$ is that matrix multiplication is a differentiable operation in the entries of the matrices (it is a polynomial in the entries), hence continuous. By Cramer's rule, the operation mapping $M$ to $M^{-1}$ is also continuous and differentiable function in the entries of the matrix, because it is a rational function of the matrix elements, which doesn't have any singularities in $GL(n)$.

\section{A Basic Example: $SO(3)$}

In classical physics, the most important Lie group is the rotation group $SO(3)$, the space of $3 \times 3$ matrices $A$ such that $A^T = A^{-1}$. The tangent spaces of this function have an interesting algebraic structure. Consider a curve $A(t) \in SO(3)$. Then, differentiating the identity $A(t) A(t)^T = I$, we conclude that $\dot{A}(t) A(t)^T + A(t) \dot{A}(t)^T = 0$. Thus $\dot{A}(t) = - A(t) \dot{A}(t)^T A(t)$, and furthermore, the differentiation identity gives that $A(t) \dot{A}(t)^T$ is skew symmetric. In particular, the tangent space $T(SO(3))_I$ at the identity is equal to the space of skew symmetric matrices, which we therefore denote by $\mathfrak{so}(3)$. More generally, $T(SO(3))_A$ is naturally equivalent to $\mathfrak{so}(3) \cdot A$.

Now suppose that the path $A(t)$ is a solution to a `constant coefficient' differential equation, i.e. there exists $B \in \mathfrak{so}(3)$ such that $\dot{A}(t) = B A(t)$. Then we can explicitly calculate that $A(t) = e^{Bt} A(0)$, and one can check that $e^{Bt}$ is in $SO(3)$ because
%
\[ (e^{Bt})^T = \sum \frac{[(Bt)^k]^T}{k!} = \sum \frac{(-Bt)^k}{k!} = e^{-Bt} \]
%
The matrix has determinant one since the map $t \mapsto e^{Bt}$ is continuous, and $\text{det}(e^{Bt}) = 1$.

The space $\mathfrak{so}(3)$ is no longer a group. But the tangent space structure gives it a vector space structure. It is three dimensional, and therefore isomorphic to $\mathbf{R}^3$. A natural isomorphism is given, for each 

\section{General Theory}

Any subgroup of a Lie group which is also a submanifold is a Lie group, because the group structure on the subgroup is just the restriction of the group structure on the entire group, which is differentiable. This is essentially the argument we used to show that $SL_n(\mathbf{R})$, $O_n(\mathbf{R})$, and $SU_n(\mathbf{R})$ are Lie groups. We could have also used the fact that $S^1$ is a subgroup of the multiplicative group of non-zero complex numbers to show it was a Lie group. $S^3$ is a Lie group, because it is a subgroup of the Lie group of quaternions, consisting of elements of norm one. More generally, we define a \emph{Lie subgroup} of a Lie group to be a subgroup, with some $C^\infty$ structure making the operations of the subgroup differentiable, and such that the $C^\infty$ structure makes the inclusion of the subgroup an immersion. As an example of a subgroup that is not an imbedded submanifold, consider the set of points $(x,cx) \in \mathbf{T}^2$, where $c$ is an irrational number.

Lie groups are incredibly useful in geometry, because we often want to consider some symmetries which occur in a problem, which often turn out to be a group with some Lie structure. As an example, suppose we are discussing the metric structure of $\mathbf{R}^n$. In this situation, it is natural to discuss the Euclidean group $E_n$, which is the collection of all isometries of $\mathbf{R}^n$. The easiest case to analyze is the group $E_1$. For each $x \in \mathbf{R}$, and $z \in \{ -1, 1 \}$, define $T_{xz}(t) = x + zt$. Every $T \in E_1$ can be written as $T_{xz}$ for some $x$ and some $z$. To see this, let $x = T(0)$. Since $T$ is an isometry, either $T(1) = x + 1$ or $T(1) = x - 1$, because $|T(1) - x| = 1$. If $T(1) = x + 1$, then $T(y) = x + y$, because $x + y$ is the only number satisfying
%
\[ |T(y) - x| = |y|\ \ \ \ \ |T(y) - (x + 1)| = |y - 1|  \]
%
Similarily, if $T(1) = y - 1$, then $T(x) = y - x$. Since
%
\[ T_{x_0z_0} \circ T_{x_1z_1} = T_{(x_0 + z_0x_1)(z_0z_1)} \]
%
and so $E_1$ can be described as the {\it semidirect product} of the multiplicative group $\{ -1, 1 \}$ and $\mathbf{R}$ under the representation $\rho: \{ -1, 1 \} \to \text{Aut}(\mathbf{R})$ defined by $\rho(x)(y) = -y$. For $E_2$, we note that if $T: \mathbf{C} \to \mathbf{C}$ is an isometry such that $T(0) = 0$, and $T(1) = 1$, then $|T(i)| = 1$ and $|T(i) - 1| = \sqrt{2}$, so either
%
\begin{itemize}
    \item $T(i) = 1$, which implies $T(z) = z$ for all $z \in \mathbf{C}$, because $z$ is uniquely specified by the values $|z|$, $|z - 1|$, and $|z - i|$.
    \item $T(i) = -1$, which implies $T(z) = \overline{z}$ for all $z \in \mathbf{C}$, because $\overline{z}$ is the unique point with $|\overline{z}| = |z|$, $|\overline{z} - 1| = |z - 1|$, and $|\overline{z} - (-i)| = |z - i|$.
\end{itemize}
%
If $z \in \mathbf{C}$, and $w \in \mathbf{T}$, we define $T_{zw}$ to be the isometry $T(u) = z + wu$. If $T \in E_2$ is arbitrary, and if $T(0) = u$, $T(1) = w$, then $T_{uw}^{-1} \circ T$ maps 0 to 0, and 1 to 1, hence either $T = T_{uw}$ or $T = \overline{T_{uw}}$. Note that $\overline{T_{uw}}(z) = T_{\overline{uw}}(\overline{z})$, so that the set of all $T_{uw}$ is a normal subgroup of $E_2$. Since the group of all $T_{uw}$ is isomorphic to the semidirect product $\mathbf{C} \rtimes \mathbf{T}$, because $T_{u_0w_0} \circ T_{u_1w_1} = T_{(u_0 + w_0u_1)(w_0w_1)}$, and therefore $E_2$ is isomorphic to the semidirect product $(\mathbf{C} \rtimes \mathbf{T}) \rtimes \{ -1, 1 \}$ with multiplication law
%
\[ (z_0,w_0,t_0)(z_1,w_1,t_1) = \begin{cases} (z_0 + \overline{w_0z_1},w_0\overline{w_1},t_0t_1) & t_0 = -1 \\ (z_0 + w_0z_1,w_0w_1, t_0t_1) & t_0 = 1 \end{cases} \]
%
The fact that $\{ -1, 1 \}$ is isomorphic to $O_1$, and $\{ -1, 1 \} \rtimes \mathbf{T}$ is isomorphic to $O_2$ hints at a more general fact about the structure of the Euclidean groups, but we need to know some structure of the metric of $\mathbf{R}^n$ first. Say a point $x$ lies {\it between} two points $y$ and $z$ if $x = \lambda y + (1 - \lambda)z$ for $0 \leq \lambda \leq 1$. This holds if and only if $\| y - x \| + \| x - z \| = \| y - z \|$, because if $x = \lambda y + (1 - \lambda) z$, then
%
\[ \| y - x \| + \| x - z \| = \|(1 - \lambda)y - (1 - \lambda)z \| + \| \lambda y - \lambda z \| = \| y - z \| \]
%
and the Cauchy Schwartz inequality implies that this inequality occurs only when $y - x = \lambda (x - z)$ for some $\lambda > 0$, in which case we find
%
\[ x = \left( \frac{1}{1 + \lambda} \right) y + \left( \frac{\lambda}{\lambda + 1} \right) z \]
%
We say $x,y,z$ are colinear if one point lies between the other pair of points. This occurs if and only if $y - x$ and $z - x$ are linearly dependant, because if $\lambda (y - x) = (z - x)$, then
%
\begin{itemize}
    \item If $\lambda < 0$, then $\| y - x \| + \| x - z \| = \| y - z \|$, so $x$ lies between $y$ and $z$.
    \item If $0 < \lambda < 1$, then $z$ lies between $x$ and $y$, because $z = \lambda y + (1 - \lambda) x$.
    \item If $\lambda > 1$, then $y$ lies between $x$ and $z$, because
    %
    \[ y = \frac{1}{\lambda} z + \frac{\lambda - 1}{\lambda} x \]
\end{itemize}
%
This tells us that an isometry maps straight lines to straight lines, because betweenness and colinearity are purely metric conditions, hence preserved by an isometry, and a line can be described by a set of points such that any triple of points is colinear. Furthermore, an isometry maps planes to planes, because a plane can be described as the smallest set containing a triple of non-colinear points $x,y,z$, and also containing the line generated by any points in the set. We claim that this plane is the set of points
%
\[ \{ x + \lambda (y - x) + \gamma (z - x) : \lambda, \gamma \in \mathbf{R} \} \]
%
If $x + \lambda_0 (y - x) + \gamma_0 (z - x)$ and $x + \lambda_1 (y - x) + \gamma_1 (z - x)$ are two points in this plane, then the set of points on the line between these two points is exactly $x + (\lambda_0 + t \lambda_1) (y - x) + (\gamma_0 + t \gamma_1) (z - x)$, as $t$ ranges over all real numbers, and these points all lie in the set above. Conversely, if $X$ is any colinearily closed set containing $x$, $y$, and $z$, then $x + t_0 (y - x)$ and $x + t_1 (z - x)$ are elements of $x$, for all $t \in \mathbf{R}$, and therefore
%
\[ x + t_0 (y - x) + t_2 (t_1 (z - x) - t_0 (y - x)) = x + t_0 (1 - t_2) (y - x) + t_2 t_1 (z - x) \]
%
are also points in $X$, for all $t_0,t_1,t_2 \in \mathbf{R}$. This implies that the set of all points $x + \lambda (y - x) + \gamma (z - x)$ are contained in $X$. Since colinearity is a metric notion, an isometry maps planes to planes.

Now suppose $T: \mathbf{R}^n \to \mathbf{R}^n$ is an isometry, with $T(0) = 0$. Our discussion implies that $T$ maps lines through the origin to lines through the origin. Thus $T(cx) = cT(x)$, because $cT(x)$ is the only point on the line through the origin and $x$ which lies at a distance $|c|\|x\|$ from the origin and a distance $|c - 1|\|x\|$ from $x$. Similarily, for a fixed $x,y \in \mathbf{R}^n$, if we assume that $T$ maps the plane generated by $x$ and $y$ to itself, then since $T(0) = 0$ we find $T(x + y) = T(x) + T(y)$. Otherwise, we consider a linear isometry $S$ which projects the plane generated by $T(x)$ and $T(y)$ to the plane generated by $x$ and $y$, and then it follows that $(S \circ T)(x + y) = S(Tx + Ty)$, hence $T(x + y) = Tx + Ty$, because $S$ is a bijection. It follows that $T(0) = 0$ holds if and only if $T$ is an element of the orthogonal group of isometric linear transformations $O_n$. If $T \in E_n$ is any linear transformation, and if $T(0) = x$, then the isometry $T_x^{-1} \circ T$ maps zero to zero, hence $T_x^{-1} \circ T \in O_n$, and we find that we can write any Euclidean transformation as a rotation and a translation, and by normality we find the Euclidean group is actually the semidirect product of $O_n$ and $\mathbf{R}^n$, since if $M,N \in O_n$,
%
\[ (T_x \circ M) \circ (T_y \circ N) = T_{x + My} \circ MN \]
%
$E_n$ can be given the structure of a Lie group if we take the topology corresponding to $\mathbf{R}^n \times O_n$, in which case
%
\[ (x,M)(y,N)^{-1} = (x,M)(-Ny,N^{-1}) = (x - MNx,MN^{-1}) \]
%
which is differentiable, since the multiplication map on $O_n$ is differentiable, and the map $x - MNx$ is differentiable since the action of $M_n$ on $\mathbf{R}^n$ defined by $(M,x) \mapsto Mx$ is differentiable.

The left and right translation maps $L_x(y) = xy$ and $R_x(y) = yx$ are diffeomorphisms on any Lie group $G$, so they induce bundle equivalences $(L_x)_*: TG \to TG$ and $(R_x)_*: TG \to TG$. We say a vector field $X$ is \emph{left-invariant} if $(L_x)_* X = L_x \circ X$ for all $x \in G$, i.e. if $(L_x)_*(X_y) = X_{xy}$. It suffices to show that $(L_x)_*(X_e) = X_x$, because then
%
\[ (L_x)_*(X_y) = (L_x \circ L_y)_*(X_e) = (L_{xy})_*(X_e) = X_{xy} \]
%
Given any $v \in G_e$, we can define a unique left invariant vector field $X$ with $X_e = v$ by setting $X_x = (L_x)_*(v)$.

\begin{theorem}
    Any left-invariant vector field is automatically $C^\infty$.
\end{theorem}
\begin{proof}
    We need only verify that the map $X_p = (L_p)_*(v)$ is $C^\infty$ for any $v \in G_e$, and it suffices to prove this in a neighbourhood of the origin. Let $(x,U)$ be a chart around a neighbourhood of the origin. Let $V \subset U$ be a neighbourhood chosen such that $ab^{-1} \in U$ if $a,b \in U$. Then
    %
    \[ Xx_i \]
\end{proof}

\begin{corollary}
    A Lie group always has trivial tangent bundle.
\end{corollary}

Since a Lie group is differentiable, we should be able to `linearly approximate' the group multiplication action. 

\newpage
















To remedy this fact, we are required to introduce some complicated machinery, which can be skimmed at a first reading. First, the elementary theory of Lie groups tells us that every tangent vector at the identity can be extended to a left-invariant smooth vector field on the Lie group. Given a vector $X_e \in \mathfrak{g}$, we can consider it as a base-point of a left-invariant vector field $X$, and use the field to generate a unique curve $\phi: \mathbf{R} \to G$ satisfying $\phi_0 = e$ and $\smash{d\phi_t/dt = X_{\phi_t}}$. It turns out that $\phi_{t + u} = \phi_t \phi_u$, so that $\phi$ is actually a {\it homomorphism} from $\mathbf{R}$ to $G$ (a `one-parameter subgroup of $G$'), and we let the \emph{exponential map} from $\mathfrak{g}$ to $G$ by letting $e^X = \phi_1$. It turns out that $e^{tX} = \phi_t$, so the exponential map models all curves emerging from infinitisimals at the identity.

\begin{example}
    Over the multiplicative group $\mathbf{C}^\times$ of non-zero complex numbers, we can identify the tangent bundle $T\mathbf{C}^\times$ with $\mathbf{C}^\times \times \mathbf{C}$, and the left-invariant vector fields take the form $X_z = wz$ for some $w \in \mathbf{C}$. This tells us that the exponential on this space is the unique solution to the differential equation
    %
    \[ \frac{dz}{dt} = wz \]
    %
    and this is just the standard exponential map $z(t) = e^{tw}$, so that the exponential on $\mathbf{C}^\times$ is just the normal exponential function. Since the exponential descends consistently to Lie subgroups, this tells us that the exponential map on the multiplicative group $\mathbf{R}^\times$ is just the standard exponential as well.
\end{example}

\begin{example}
    Over the group $GL_n(\mathbf{R})$, we can identify the tangent space at each point $M \in GL_n(\mathbf{R})$ with the space $M_n(\mathbf{R})$ of all $n \times n$ matrices. The left-invariant vector fields on $GL_n(\mathbf{R})$ are of the form $X_M = NM$ for some $N \in M_n(\mathbf{R})$, and the unique solution to the system of linear equations
    %
    \[ \frac{dM}{dt} = NM \]
    %
    is the matrix exponential
    %
    \[ e^M = \sum_{n = 0}^\infty \frac{m^n}{n!} \]
    %
    hence the exponential of Lie groups generalizes many versions of the exponential defined in analysis.
\end{example}

\begin{example}
    Over the additive group of real numbers $\mathbf{R}$, $T\mathbf{R}$ is just the trivial tangent bundle $\mathbf{R} \times \mathbf{R}$, so elements of the tangent space at the identity can be identified with real numbers, and the left-invariant vector fields are just the vector fields with a constant velocity. Recalling the unique solutions to the differential equation
    %
    \[ \frac{dx}{dt} = c \]
    %
    We find that the for a tangent vector $t \in \mathbf{R}$ at the identity, $e^t = t$ is just the identity map. More generally, the exponential related to the additive group $\mathbf{R}^n$ is just the identity map under a suitable idenfication of the tangent bundle.
\end{example}

Some elementary facts about the exponential, proved in an elementary introduction to differentiable manifolds are that
%
\begin{itemize}
    \item For any tangent vector $X$,
    %
    \[ \left. \frac{de^{tX}}{dt} \right|_{t = 0} = X \]
    %
    so as $t$ ranges over all real-numbers, $e^{tX}$ is just the one-parameter subgroup of $G$ corresponding to the map $\phi$ used to construct the exponential. Thus $e^{(t + u)X} = e^{tX} e^{uX}$.

    \item If $\phi: G \to H$ is a homomorphism, then $e^{\phi_*(X)} = \phi(e^X)$.

    \item $\exp: \mathfrak{g} \to G$ is a diffeomorphism in a neighbourhood of the identity. It is not always surjective, even if $G$ is connected, but it is surjective for the group $GL_n(\mathbf{C})$, or any compact, connected Lie group.
\end{itemize}
%
Returning to our discussion of constructing homomorphisms of Lie groups from operations on the tangent space, we see that $e^{\phi_*(X)} = \phi(e^X)$ gives a much stronger condition on the linear map $\phi_*$ restricting which linear maps can be differentials of homomorphisms. In particular, since the image of $\exp$ always contains a neighbourhood of the identity, given any linear map $\phi_*$ such that $\phi_*(X) = \phi_*(Y)$ if $e^X = e^Y$, we can define a map $\phi$ on a neighbourhood of the identity of $G$ by letting $\phi(e^X) = e^{\phi_*(X)}$. Provided that $\phi(gh) = \phi(g)\phi(h)$ where defined, we can try to extend $\phi$ to a homomorphism on the entire space by using the fact that a neighbourhood of the identity in the Lie group generates the entire space. The sufficient condition for this method to work is that the Lie group is simply connected, and this is not too much of a problem because we can always swap a connected Lie group $G$ with its simply connected cover $\tilde{G}$, and the underlying space of infinitisimals will be the same.

Thus we are left with determining the conditions on $\phi_*$ such that $\phi(gh) = \phi(g) \phi(h)$. This is where the Lie bracket enters the picture. If $X$ and $Y$ are arbitrary smooth vector fields on $G$, then they induce integral curves $\phi: \mathbf{R} \times G \to G$ and $\psi: \mathbf{R} \times G \to G$ respectively, and the Lie bracket operation $[X,Y]$ (turning the space of smooth vector fields into an infinite dimensional Lie algebra) defines a smooth vector field with
%
\[ [X,Y]_p = (1/2) \left. \frac{d^2 (\psi_{-t} \circ \phi_{-t} \circ \psi_t \circ \phi_t)}{dt} \right|_p \]
%
which effectively means that for any $C^\infty$ function $f$,
%
\[ [X,Y]_p(f) = (1/2) \left. \frac{d^2 (f \circ \psi_{-t} \circ \phi_{-t} \circ \psi_t \circ \phi_t)}{dt} \right|_p \]
%
The Lie bracket essentially measures the commutivity of $X$ and $Y$.

It turns out that if $X$ and $Y$ are both left-invariant vector fields on a Lie group $G$, then $[X,Y]$ is also a left-invariant vector field, hence the Lie bracket descends to an operation on the tangent space $\mathfrak{g}$, where if $X,Y \in \mathfrak{g}$, then $[X,Y] \in \mathfrak{g}$ satisfies
%
\[ [X,Y](f) = (1/2) \frac{d^2}{dt} f(e^{tX} e^{tY} e^{-tX} e^{-tY}) \]
%
Viewing $\mathfrak{g}$ as the space of curves through the origin identified up to first order, this implies that for any two curves $\lambda, \gamma$,
%
\[ f(\lambda_t \gamma_t \lambda^{-1}_t \gamma^{-1}_t) = f(e) + t^2 [\lambda, \gamma](f) + o(t^3) \]
%
Thus the Lie bracket expresses the second order coefficients of conjugation on the Lie group. Surprisingly, the second order terms are sufficient to characterize the Lie group operation. First note that if $\phi: G \to H$ is a group homomorphism, then in two different ways, we calculate that
%
\begin{align*}
     f(\phi(\lambda_t \gamma_t \lambda^{-1}_t \gamma^{-1}_t)) &= f(e) + t^2[\lambda, \gamma](f \circ \phi) + o(t^3)\\
     &= f(e) + t^2 (\phi_*[\lambda, \gamma])(f) + o(t^3)\\
    f(\phi(\lambda_t \gamma_t \lambda^{-1}_t \gamma^{-1}_t)) &= f((\phi \circ \lambda)_t (\phi \circ \gamma)_t (\phi \circ \lambda)_{-t} (\phi \circ \gamma)_{-t})\\
     &= f(e) + t^2 [\phi_*(\lambda), \phi_*(\gamma)](f) + o(t^3)
\end{align*}
%
It follows that $\phi_*[\lambda, \gamma] = [\phi_*(\lambda), \phi_*(\gamma)]$, so all linear maps on infinitisimals induced by homomorphisms of groups preserve the Lie bracket operation. We shall find that we can `almost' always find a Lie group homomorphism from a linear map on the space of infinitisimals preserving the Lie bracket.

Since $\exp$ is locally a diffeomorphism in a neighbourhood of both identities, we can find a differentiable inverse $\log$, and the Baker-Hausdorff formula implies that there is a neighbourhood of the origin in $\mathfrak{g}$ and universal constants $a_\alpha$ such that
%
\[ \log(e^X e^Y) = X + Y + \sum_{|\alpha| > 1} a_\alpha (X,Y)^\alpha \]
%
where $\alpha$ are multi-indexes, and if $\alpha = (\alpha_1, \dots, \alpha_n)$, then
%
\[ (X,Y)^\alpha = \underbrace{[X, [X, [\dots, [X}_{\alpha_1\ \text{times}}, \underbrace{[Y, [\dots, [Y}_{\alpha_2\ \text{times}}, \dots]]]]]]] \]
%
A nasty formula which implies that given a linear map $\phi_*: \mathfrak{g} \to \mathfrak{h}$, the sufficient condition for the map $\phi: G \to H$ defined by $\phi(e^X) = e^{\phi_*(X)}$ to satisfy $\phi(gh) = \phi(g) \phi(h)$ where defined, a sufficient condition for this to hold is that
%
\[ \log(e^{\phi_*(X)} e^{\phi_*(Y)}) = \phi_*(\log(e^X e^Y)) \]
%
and in terms of the Baker-Hausdorff formula, we find that this means
%
\[ \phi_*(X) + \phi_*(Y) + \sum a_\alpha (\phi_*(X), \phi_*(Y))^\alpha = \phi_*(X) + \phi_*(Y) + \sum a_\alpha \phi_*((X,Y)^\alpha) \]
%
It can be proved by induction that for any map $\phi_*$ with $\phi_*[X,Y] = [\phi_*X, \phi_*Y]$, $\phi_*((X,Y)^\alpha) = (\phi_*(X), \phi_*(Y))^\alpha$, so a sufficient condition for the homomorphism condition $\phi(gh) = \phi(g)\phi(h)$ is that $\phi_*$ preserves the Lie bracket.

Now that this property holds, if $U$ is the neighbourhood upon which the homomorphism property holds, then any $g \in G$ can be written as $h_1 h_2 \dots h_n$ for some $h_i \in U$, and if $\phi$ can be extended to a homomorphism on all of $G$, then we can let $\phi(g) = \phi(h_1) \dots \phi(h_n)$, and provided this is well defined, $\phi$ will be a homomorphism that is differentiable at the identity, hence differentiable everywhere, and $\phi_*$ is the differential of $\phi$. The only problem is that this extension of $\phi$ won't necessarily be well defined everywhere, and in order for this to work, we will need to switch to studying simply connected Lie groups.

\begin{example}
    Recall the Grassmanian space $G(k,n)$ of $k$ dimensional subspaces of $\mathbf{R}^n$. There is another way to view the topology of $G(k,n)$ which is very useful for constructing continuous maps on the space. Consider the family $M(n,k;k)$ of full rank $n$ by $k$ matrices, which may be identified by separating columns with the family of linearly independant tuples $(v_1, \dots, v_k)$ of vectors in $\mathbf{R}^n$. We obtain a surjective map $f: M(n,k;k) \to G(k,n)$ by mapping a matrix corresponding to the tuples $(v_1, \dots, v_k)$ to the vector space $\text{span}(v_1, \dots, v_k)$. We claim that this map is a submersion, hence it is a continuous open map identifying the topological structure of $G(k,n)$ as a quotient space of $M(n,k;k)$, by identifying vector tuples which generate the same subspace. The utility of this is that continuous map $g$ with domain $G(k,n)$ can be constructed as continuous maps with domain $M(n,k;k)$ which have the same value on vector tuples that generate the same subspace. To prove that the map is a submersion, we consider the open set
    %
    \[ U = \left\{ \begin{pmatrix} A \\ B \end{pmatrix}: A \in GL_k(\mathbf{R}) \right\} \]
    %
    If we let $V = \text{span}(e_1, \dots, e_k)$, and $V' = \text{span}(e_{k+1}, \dots, e_n)$, then $f(U)$ is a subset of $A_{V'}$, and if we let $(y,A_{V'})$ be the standard coordinates corresponding to the set $A_{V'}$, identifying $L(V,V')$ with $M(k,n-k)$, then
    %
    \[ (y \circ f) \begin{pmatrix} A \\ B \end{pmatrix} = BA^{-1} \]
    %
    which is the linear transformation mapping the $i$'th column of $A$ to the $i$'th column of $B$. The map is now easily seen to be differentiable. For a fixed $A$, the map $B \mapsto BA^{-1}$ is an invertible linear map, and therefore the map is a submersion everywhere, because we can always permute the coordinates so that a given matrix is in $U$ (this corresponds to a diffeomorphism of $M(n,k;k)$), and when we take the span of this vectors, the way to get back to the original span is to unpermute the coordinates, so we can always assume our matrices have the nice form above. In other words, the group $GL_n(\mathbf{R})$ acts transitively on $M(n,k;k)$ by left multiplication, and transitively on $G(k,n)$ by the action $M \cdot V = \{ Mv : v \in V \}$, and we find that $f$ is a $GL_n(\mathbf{R})$ morphism, because $f(MN) = Mf(N)$.
\end{example}




\chapter{Riemannian Manifolds}

On $\mathbf{R}^n$, an inner product enables us to discuss distances and angles. In 1854, Bernard Riemann figured out how to generalize this concept to a smooth manifold. We have exploited almost every tool of finite dimension linear algebra on manifolds, except for the theory of bilinear forms. Recall that a symmetric bilinear form $\beta$ on a real vector space $V$ is a bilinear map satisfying $\beta(v,w) = \beta(w,v)$. This form is called positive-definite if $\beta(v,v) > 0$ for $v \neq 0$, and {\it nondegenerate} if, for every $v \neq 0$, there is $w$ with $\beta(v,w) \neq 0$ (every positive-definite form is automatically nondegenerate). Given a non-degenerate form, we can define a map $v \mapsto \nu$ from $V$ to $V^*$ by defining $\nu(w) = \beta(v,w)$. This map is injective precisely when $\beta$ is non-degenerate, and thus naturally identifies $V$ with $V^*$ if $V$ is finite dimensional. Given a basis $e_1, \dots, e_n$ for $V$, there are coefficients $g_{ij} = \beta(e_i,e_j)$ such that $\beta(a^ie_i, b^je_j) = a^i b^j g_{ij}$. A \emph{Riemannian metric} on a manifold $M$ is a smooth assignment of a positive-definite bilinear form $\langle \cdot, \cdot \rangle_p$ on the tangent spaces $T_p M$, for each point $p \in M$. This smoothness is characterized either by talking in the language of smooth tensor fields, so that a Riemannian metric is a smooth $(0,2)$ tensor field, in more basic terms, for any smooth vector fields $X$ and $Y$, $\langle X, Y \rangle$ is a smooth function, or through coordinates, using a coordinate system $x$ to locally write
%
\[ \langle \cdot, \cdot \rangle_p = \sum a_{ij}(p) dx^i \otimes dx^j \]
%
and then the field is smooth if the $a_{ij}$ are smooth. If the choice bilinear form is merely non-degenerate, the object is called a \emph{psuedo-Riemannian metric}. A \emph{(psuedo) Riemannian manifold} is just a smooth manifold with a (psuedo) Riemannian metric.

\begin{example}
    $\mathbf{R}^n$ is a Riemannian manifold, with the Riemannian tensor field
    %
    \[ \sum dx^i \otimes dx^i = \sum (dx^i)^2 \]
    %
    Often, the field is just denoting by $\delta$.
\end{example}

\begin{example}
    A surface in $\mathbf{R}^3$ inherits a Riemannian metric from $\mathbf{R}^3$ by constricting the restricted inner product on each tangent space to the surface. More generally, a metric is induced on submanifolds of a Riemannian manifold; if we have an immersion $i: M \to N$, and $N$ has a Riemannian metric $\langle \cdot, \cdot \rangle_N$, then we can define a Riemannian metric by the equation
    %
    \[ \langle X, Y \rangle_M = \langle i_*X, i_*Y \rangle_N \]
    %
    so submanifolds of Riemannian manifolds are naturally given the structure of a Riemannian submanifold. Note that since every smooth manifold can be immersed in Euclidean space, every smooth manifold has a Riemannian metric (alternatively, we can construct a Riemannian metric by locally defining a positive definite form, and then extending it using a partition of unity).
\end{example}

\begin{example}
    The hyperbolic plane $\mathbf{H}^2$ can be modelled as the upper half plane, where the metric is given by $g = \delta/y^2$, or
    %
    \[ \langle X, Y \rangle_{(x,y)} = \frac{X^1Y^1 + X^2Y^2}{y^2} \]
    %
    This metric pushes the $x$ axis `off to infinity' by stretching the distance between points near the axis. This is known as the Poincare model of the hyperbolic plane. More generally, we can define the higher dimensional hyperbolic spaces $\mathbf{H}^n = \{ (x,y): x \in \mathbf{R}^{n-1}, y > 0 \}$ with a metric $\delta/y^2$.
\end{example}

An \emph{isometry} between two Riemannian manifolds is a diffeomorphism which pullsback one metric on one manifold to a metric on the other. The study of Riemannian geometry can be considered the study of properties of Riemannian manifolds which are invariant under isometries.

Psuedo-Riemannian metrics most often occur in the physical theories employing differential geometry. Since the eigenvalues corresponding to the symmetric operator change smoothly, and cannot be zero by non-degeneracy, the number of negative and positive eigenvalues do not change across a connected psuedo-Riemannian manifold. The number of negative eigenvalues for a pseudometric is known as the index. An index 0 metric is a Riemannian metric, and an index 1 metric is known as a Lorentz metric.

\begin{example}
    The classical example of a Lorentz metric occurs in special relativity, known as the Minkowski metric on $\mathbf{R}^n \times \mathbf{R}$, which is given by
    %
    \[ \eta(X,Y) = X^1Y^1 + \dots + X^nY^n - X^{n+1}Y^{n+1} \]
    %
    In general relativity, we replace $\mathbf{R}^n \times \mathbf{R}$ by a general $n+1$ dimensional manifold, and the Minkowski metric by an arbitrary Lorentz metric, satisfying certain physical equations known as the Einstein equations, which control the geometry of space. The Lorentz transformations are precisely the isometries of the Riemannian manifold with respect to the Lorentz metric.
\end{example}

\section{The Gradient Vector}

Since a form enables us to identify a vector space $V$ with its dual, a metric enables us to identify $TM$ and $(TM)^*$ in a natural way. Given a scalar valued function $f: M \to \mathbf{R}$, we obtain a section $df$ of $(TM)^*$. By taking the dual, we obtain the \emph{gradient} $\nabla f$, which is a section of $TM$, defined by
%
\[ \langle \nabla f, X \rangle = df(X) \]
%
In local coordinates, by taking $X = \partial/\partial x^j$, this equation says
%
\[ dx^i(\nabla f) g_{ij} = \frac{\partial f}{\partial x^j} \]
%
If we let $g^{ij}$ denote the components of the inverse of $g_{ij}$, so $g^{ij} g_{jk} = g_{ij} g^{jk} = \delta^i_k$,  then
%
\[ \nabla f = \frac{\partial f}{\partial x^j} g^{ji} \frac{\partial}{\partial x^i} \]
%
Note that then
%
\[ |\nabla f|^2 = \langle \nabla f, \nabla f \rangle = df(\nabla f) = \frac{\partial f}{\partial x^i} g^{ij} \frac{\partial f}{\partial x^j} \]
%
Over Euclidean space, this is precisely the definition of the gradient.

\begin{example}
    On the sphere $S^2$, we have spherical coordinates $x = r \cos \phi \cos \theta$, $y = r \sin \phi \cos \theta$, and $z = r \sin \theta$. We then have
    %
    \[ dx = \cos \phi \cos \theta dr - r \sin \phi \cos \theta d \phi - r \cos \phi \sin \theta d\theta \]
    \[ dy = \sin \phi \cos \theta dr + r \cos \phi \cos \theta d \phi - r \sin \phi \sin \theta d \theta \]
    \[ dz = \sin \theta dr + r \cos \theta d \theta \]
    %
    and so the metric is
    %
    \begin{align*}
        g &= dx^2 + dy^2 + dz^2 = dr^2 + (r^2 \cos^2 \theta) d\phi^2 + (r^2) d\theta^2
    \end{align*}
    %
    Thus the basis is orthogonal, but nor orthonormal, with
    %
    \[ \left|\frac{\partial}{\partial r} \right| = 1\ \ \ \ \ \left|\frac{\partial}{\partial \phi}\right| = r |\cos \theta|\ \ \ \ \ \left|\frac{\partial}{\partial \theta}\right| = r \]
    %
    If we normalize these vectors, letting
    %
    \[ \widehat{\frac{\partial}{\partial \phi}} = \frac{1}{r \cos \theta} \frac{\partial}{\partial \phi}\ \ \ \ \ \widehat{\frac{\partial}{\partial \theta}} = \frac{1}{r} \frac{\partial}{\partial \theta} \]
    %
    then
    %
    \[ \nabla f = \frac{\partial f}{\partial r} + \frac{1}{r \cos \theta} \frac{\partial f}{\partial \phi} \widehat{\frac{\partial}{\partial \phi}} + \frac{1}{r} \frac{\partial f}{\partial \theta} \widehat{\frac{\partial}{\partial \theta}} \]
    %
    These are known as the {\it physical components} of the gradient with respect to spherical coordinates.
\end{example}

\begin{example}
    In Minkowski space, we have a time coordinate $t$, and three spatial coordinates $x$, $y$, and $z$. If $t$ is chosen as the `zeroeth' coordinate, the Minkowski metric is chosen such that
    %
    \[ g_{ij} = \begin{cases} 1 & i = j > 0 \\ -c^2 & i = j = 0 \\ 0 & \text{otherwise} \end{cases} \]
    %
    Then if $f$ is a function on Minkowski space, we find
    %
    \[ \nabla f = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} + \frac{\partial f}{\partial z} - \frac{1}{c^2} \frac{\partial f}{\partial t} \]
\end{example}

The gradient $\nabla f$ has the same interpretation as in Euclidean space as the direction of steepest ascent on the manifold. If $v \in M_p$ is any vector with $|v| = 1$, then the Schwarz inequality implies
%
\[ |v(f)| = |df(v)| = |\langle v, \nabla f \rangle| \leq |\nabla f| \]
%
If $f$ is real valued, then the tangent space of the level sets are precisely the vectors orthogonal to the gradient vector.

\section{Unit Tangent Bundle}

If $M$ is a Riemannian manifold, we can obtain an interesting submanifold of $TM$ by considering the subspace of {\it unit} vectors $v$ with $|v| = 1$. The equation $g(v,v) = 1$ is given in local coordinates by
%
\[ \sum_{ij} \dot{x}^i g_{ij} \dot{x}^j = 1 \]
%
If we let $f(v) = g(v,v)$. Then
%
\[ \frac{\partial f}{\partial \dot{x}^i} = 2 \sum_j g_{ij} \dot{x}^j \]
%
Since $g$ is non-degenerate, these partial derivatives cannot all simulatenously vanish unless $\dot{x} = 0$. Thus the space of $v$ with $|v| = 1$ is a submanifold of $TM$ of dimension $2n - 1$, if $M$ has dimension $n$.

\begin{example}
    If $w$ is a unit tangent vector to $S^2$ at some point $v \in S^2$, then $(v,w,v \times w)$ is a right handed orthonormal coordinate frame. Thus we find that the space of unit tangent vectors is diffeomorphic to $SO(3)$.
\end{example}

\section{The Volume Form}

In $\mathbf{R}^n$, the length of a smooth curve $c$ parameterized on $[a,b]$ is given by
%
\[ \int_a^b |c'(t)|\ dt \]
%
In a Riemannian manifold, we can measure the lengths of tangent vectors $v$ by the equation $|v|^2 = \langle v, v \rangle$, and so we can also measure the lengths of curves on Riemannian manifolds. We also have the existence of a \emph{volume forms}, which enable one to integrate smooth functions on the manifold in a natural way.

\section{Connections}

On $\mathbf{R}^n$, given two vector fields $X$ and $Y$, we can define the `derivative' of the vector field $Y$ along $X$ to be the vector field
%
\[ (D_X Y)_p = \lim_{t \to 0} \frac{Y_{p + tX} - Y_p}{t} \]
%
On a manifold, there is no canonical way to differentiate vector fields; the Lie derivative $L_XY$ isn't a staisfying definition because it measures how `independent' two vector fields are, rather than measuring the change of $Y$ along $X$. The main problem is that we are unable to `connect' tangent spaces on a manifold close to one another, like we can in $\mathbf{R}^n$. On a Riemannian manifold, there is a canonical way to connect the fibres of a tangent bundle at points close to one another, and this is best explained through the concept of a connection.

Recall the definition of a smooth vector bundle $\pi: E \to M$, and smooth sections $s: M \to E$. We denote the $C^\infty(M)$ module of all smooth sections by $\Gamma(M)$. A \emph{connection} is a map $\nabla: \Gamma(TM) \times \Gamma(E) \to \Gamma(E)$ which maps $(X,s) \to \nabla_X(s)$, which is $C^\infty(M)$ linear in $X$, $\mathbf{R}$ linear in $s$, and satisfies the product rule
%
\[ \nabla_X(fs) = X(f)s + f\nabla_X(s) \]
%
for any $f \in C^\infty(M)$, so that $\nabla_X$ operates `like a derivative' on $s$. An \emph{affine connection} is a connection where $E = TB$.

\begin{example}
    The definition
    %
    \[ (\nabla_X Y)_p = D_X(Y)_p = \lim_{t \to 0} \frac{Y_{p + tX} - Y_p}{t} \]
    %
    is an affine connection on $\mathbf{R}^n$.
\end{example}

\begin{example}
    The Lie derivative is {\it not} an affine connection on a manifold, because it isn't $C^\infty(M)$ linear in the first variable.
\end{example}

The fact that $\nabla$ is $C^\infty(M)$ linear in the 1st variable tells us that for a fixed section $s$, the map $X \mapsto \nabla_X s$ acts `like a tensor' in $X$. An argument analogous to the tensor characterization lemma reveals that $(\nabla_X s)(p)$ depends only on the value $X_p$ of $X$ at $p$, not the entire vector field. On the other hand, for the sections $s$ we only have $\mathbf{R}$ linearity, so $\nabla_X s$ is allowed to depend on more of the behaviour of $s$. A bump function type argument, analogous to the proof that derivations on $C^\infty(M)$ are local, shows that $(\nabla_X s)(p)$ depends only on the behaviour of $s$ in a neighbourhood of $p$. The introduction of Christoffel symbols will show that, if fact, $(\nabla_X s)(p)$ depends only on the behaviour of $s$ on a curve tangent to $X$.

Suppose that we consider a trivialization $U$ from $\pi^{-1}(U) \to U \times \mathbf{R}^n$, some local frame $s_1, \dots, s_n$ on $U$, and a coordinate chart $(x,U)$. We know that we can define $\nabla_X(s_k)$, even though the sections $s_k$ are not defined globally, because $\nabla_X$ depends only on the behaviour of $s_k$ locally.  The \emph{Christoffel symbols} with respect to this setup are functions $\Gamma_{i \beta}^\alpha$ such that
%
\[ \nabla_{e_i}(s_\beta) = \sum \Gamma_{i \beta}^\alpha s_\alpha \]
%
where $e_i = \partial/\partial_{x_i}$. Then if $X = \sum a^i e_i$, and $s = \sum b^\beta s_\beta$
%
\begin{align*}
    \nabla_X(s) &= \sum \nabla_X(b^\beta s_\beta) = \sum X(b^\beta) s_\beta + b^\beta \nabla_X(s_\beta)\\
    &= \sum X(b^\beta) s_\beta + b^\beta a^i \nabla_{e_i}(s_\beta)\\
    &= \sum X(b^\beta) s_\beta + b^\beta a^i \Gamma_{i \beta}^\alpha s_\alpha\\
    &= \sum \left( X(b^\alpha) + b^\beta a^i \Gamma_{i \beta}^\alpha \right) s_\alpha
\end{align*}
%
The Christoffel symbols and the $a^i$ at $p$ do not depend on $s$, and $b^\beta(p)$ depends only on the values of $s$ at $p$. $X(b^\alpha)$ depends only on the behaviour of $s$ tangent to $X$, and therefore $\nabla_X(s)$ depends only on the bahviour of $s$ locally on a curve tangent to $X$.

There are infinitely many degrees of freedom for defining an affine connection on a manifold. Any smooth family of Christoffel symbols gives rise to a connection on $\mathbf{R}^n$, and using a partition of unity type decomposition, one can patch together local connections to a global connection. The only thing to note here is that linear combinations of connections need not be a connection, but convex combinations are.

To introduce the natural choice of connection on a Riemannian manifold, we need to discuss how parallel transports arise from a particular connection. It turns out that if we have a connection on a smooth bundle $\pi: E \to M$, and $\gamma$ is a curve on $M$ from a point $p$ to a point $q$, then there arises a linear transformation $P_p^q: E_p \to E_q$ arising from the curve $\gamma$, called a parallel transport. Conversely, a family of parallel transports give rise to a connection.

\section{Riemannian Submanifolds}

Recall that if $M$ is a Riemannian manifold with metric $g$, and $N$ is an immersed submanifold with immersion $i: M \to N$, then we can give $i^*(TM)$ a metric structure by the pullback metric $i^*(g)$. We can of course embed $TN$ in $i^*(TM)$, and obtain a Riemannian metric structure on $TN$ by restricting $i^*(g)$, but we can also define another interesting bundle from this embedding. We define the \emph{normal bundle} of $N$ with respect to the embedding to be the orthogonal complement of $TN$ in $i^*(TM)$.

\begin{theorem}
    The Levi-Civita connection $\nabla$ relative to the induced metric on $N$ is given by
    %
    \[ \nabla^N_X Y = P(\nabla^M_X Y) \]
    %
    For $X,Y \in \Gamma(TM)$, where $P$ is the orthogonal projection of $i^*(TM)$ onto $TN$.
\end{theorem}
\begin{proof}
    It is straightforward to check $\nabla^N$ defines an affine connection on $TN$. It suffices to sheck that the connection is metric compatible and torsion free. We find $\nabla^N_X Y - \nabla^N_Y X = P(\nabla^M_X Y - \nabla^M_Y X) = P([X,Y]) = [X,Y]$, since we know that if $X$ and $Y$ lie in a subbundle of the tangent bundle, then $[X,Y]$ also lies in this subbundle. To check metric compatibility, we find
    %
    \[ X_p (i^*g)(Y_p,Z_p) = X_p g(i_* Y_p, i_* Z_p) = g(\nabla^M_X (i_* Y_p), Z_p) + g(Y, \nabla^M_X Z) = g(\nabla^N_X Y, Z) + g(Y, \nabla^N_X Z) \]
    %
    and the uniqueness of the Levi-Cevita connection gives our result.
\end{proof}

The \emph{second fundamental form} of a pair $N \subset M$ of Riemannian manifold is defined to be $\mathbf{II}(X,Y) = Q(\nabla^M_X Y)$, where $Q$ is the orthogonal projection onto $TN^\perp$. It is $C^\infty(M)$ bilinear and symmetric, and so corresponds to a smooth covariant two tensor field on $i^*(TM)$. The equation
%
\[ \nabla^M_X Y = \nabla^N_X Y + \mathbf{II}(X,Y) \]
%
The manifold $N$ is \emph{totally geodesic} if $\mathbf{II} = 0$. The \emph{mean curvature vector} $H$ is the trace of $\mathbf{II}$, and is the gradient of the volume function on the manifold. A manifold is \emph{minimal} if the mean curvature vector vanishes. Given a unit vector $\nu \in N_p M$ normal to $M$, set $A^\nu: T_p M \to T_p M$ by $A^\nu(X) = -Q(\nabla^M_X \nu)$, known as the \emph{shape operator}. Then $A^\nu$ is self-adjoint, and $\langle \mathbf{II}(X,Y), \nu \rangle = \langle A^\nu(X), Y \rangle$.

The Gauss equation says that
%
\[ \langle R^N(X,Y)Z,W \rangle = \langle R^M(X,Y)Z,W \rangle + \langle \mathbf{II}(X,W), \mathbf{I}(Y,Z) \rangle - \langle \mathbf{II}(X,Z), \mathbf{II}(Y,W) \rangle \]


\section{Jacobi Fields}

Let $\gamma$ be a geodesic. Then $\gamma$ is locally minimizing if and only if $\gamma$ has no conjugate points. Recall that $I(V,V)$ is the index form
%
\[ I(V,V) = \int_0^t \left| \nabla_{\partial_t} V \right|^2 - \langle R(V,\gamma') \gamma', V \rangle \]
%
A pair of \emph{conjugate points} along a geodesic if there is a nonvanishing Jacobi field between the two points.

\begin{theorem}
    If $\gamma$ has no conjugate points then $I(\cdot,\cdot)$ is positive definite.
\end{theorem}

We proved this theorem this morning, in the lecture I missed. This lecture we focus on the converse. This means that past a conjugate point pair, geodesics can fail to be locally minimizing.

\begin{theorem}[Index Inequality]
    Let $\gamma$ be a geodesic with no conjugate points, and let $J$ be a Jacobi field along $\gamma$. If $V$ is a vector field along $\gamma$ with $V(0) = J(0)$ and $V(l) = J(l)$ then $I(J,J) \leq I(V,V)$, with equality if and only if $J = V$.
\end{theorem}
\begin{proof}
    Since $\gamma$ contains no conjugate points, $I(V,V) = 0$ for all $V$ with $V(0) = V(l) = 0$, with $I(V,V) = 0$ if and only if $V = 0$. In particular, $I(J-V,J-V) = I(J,J-V) - I(V,J-V) = I(V,J-V) \geq 0$, since $I(J,J-V) = 0$. But then $I(J,J) = I(J,V)$, which shows $I(J,J) \leq I(V,V)$.
\end{proof}

\begin{corollary}
    A geodesic containing an interior point which is conjugate to $\gamma(0)$ is not locally minimizing.
\end{corollary}
\begin{proof}
    Let $\gamma(t_0)$ be the first conjugate point to $\gamma(0)$. Then there is a nonzero Jacobi field $J$ on $\gamma_{[0,t_0]}$ with $J(0) = J(t_0)$, which can be extended to a vector field $X$ on all of $\gamma$ by making the vector field vanish elsewhere. Then $I(X,X) = 0$ on $[0,t_0]$. Note $X$ is not smooth at $t_0$, since $(\nabla_{\partial_t} J)(t_0) = 0$, because this would imply $J$ is zero everywhere. However, if $\delta$ is sufficiently smlal, then $\gamma$ contains no pairs of conjugate points around $t_0$, so there exists a local Jacobi field $W$ with $W(t_0 - \delta) = J(t_0 - \delta)$ and $W(t_0 + \delta) = 0$. The index inequality implies $I(V,V) < I(X,X)$ on $[t_0 - \delta, t_0 + \delta]$. But $X = V$ outside of the $[t_0 - \delta, t_0 + \delta]$, so $I(V,V) < I(X,X) = 0$ on $[0,l]$, and so there is a variation of $\gamma$ decreasing the length.
\end{proof}

\begin{remark}
    A quantitative generalization of this theorem is given by the Morse index theorem. The Morse index  is the maximal dimension of a subspace of variation fields on which $I$ is negative definite. We just proved that if there is a conjugate point, the index is at least one. The morse index theorem says the index is finite and equals the \# of interior cojugate points, counted with multiplicity.
\end{remark}

\section{Nonpositive Curvature}

Recall that give $p \in M$ and a 2-plane $\Pi \subset T_p M$, we define the {\it sectional curvature}
%
\[ K(\Pi) = \frac{\langle R(V,W)W, V \rangle}{|V|^2|W|^2 - \langle V ,W \rangle^2} \]
%
We set $K_M > k$ if $K(\Pi) > k$ for all 2 planes $\Pi \subset T_p M$.

\begin{theorem}
    If $K_M \leq 0$ then $M$ has no conjugate points.
\end{theorem}
\begin{proof}
    Let $\gamma$ be a unit speed geodesic on the manifold, and let $V$ be a normal vector field along $\gamma$ with $V(0) = V(l) = 0$. Then
    %
    \[ \left. \frac{d^2 L(\gamma_s)}{ds^2} \right|_{s = 0} = \int_0^l \left[ |\nabla_{\partial_t} V|^2 - \langle R(V,\gamma', V \rangle) \right] \geq \int_0^l \left| \nabla_{\partial_t} V \right|^2\; dt > 0 \]
    %
    except when $\nabla_{\partial_t} V = 0$, but this only occurs if $V = 0$ (uniqueness of initial conditions).
\end{proof}

\begin{theorem}[Cartan-Hadamard]
    If $M$ is a complete Riemannian manifold, and $K_M \leq 0$, then the exponential is a covering map.
\end{theorem}
\begin{proof}
    It suffices to note that if $M$ is a complete Riemannian manifold, and $F$ is a local isometry, thne $F$ is a covering map. We know that since $K_M \leq 0$, $M$ has no conjugate points, which is the set of points where the differential of the exponential map is non-invertible.
\end{proof}

We shall find that if $K_M$ is stictly negative, then geodesics are unique.

\section{March 22nd}

Now we address problems related to positive curvature. Recall that the Ricci tensor is a symmetric quadratic form which is obtained by contracting the Riemann curvature tensor, i.e.
%
\[ \text{Ric}(X,Y)  = \sum_{n = 1}^N \langle R(X,e_n) e_n, Y \rangle \]
%
It is the directional average of sectional curvatures. We say $\text{Ric}_M \geq K$ if $\text{Ric}_M(X,X) \geq K$ for all unit vectors $X$.

\begin{theorem}[Bonnet-Myers]
    If $(M,g)$ is complete, and $\text{Ric}_M \geq (n-1)K > 0$, then ever geodesic of length $\geq \pi/\sqrt{K}$ contains conjugate points. THus the diameter of the manifold is less than or equal to $\pi/\sqrt{K}$.
\end{theorem}
\begin{proof}
    Let $\gamma: [0,l] \to M$ be a unit length geodesic. Suppose $L(\gamma) = l > \pi/\sqrt{K}$. We will now show $\gamma$ is not locally minimizing. Choose an orthonormal basis $\{ E_1, \dots, E_n \}$ of $T_{\gamma(0)} M$, and extend the basis by parallle transport. Set $V_i = \varphi E_i$ where $\varphi(0) = \varphi(l) = 0$. We calculate
    %
    \begin{align*}
        \sum I(V_i,V_i) &= - \sum \int_0^l \langle \nabla_{\partial t} \nabla_{\partial t} V_i, V_i \rangle + \langle R(V_i, \varphi') \varphi', V_i \rangle\; dt\\
        &= - \int_0^l (n-1) \varphi'' \varphi + \varphi^2 \text{Ric}(\varphi', \varphi')\\
        &\leq -(n-1) \int_0^l (\varphi + k\varphi) \varphi\; dt
    \end{align*}
    %
    Setting $\varphi(t) = \sin(\pi t/l)$, which is the first laplacian eigenfunction on $[0,l]$, this causes the integral to be bounded by
    %
    \[ -(n-1) \int_0^l [-(\pi/l)^2 + k] \varphi^2 < 0 \]
    %
    provided $l > \pi/\sqrt{K}$.
\end{proof}

\begin{corollary}
    Let $(M,g)$ be a complete manifold, with $\text{Ric}_M \geq (n-1)k > ?$. Then $M$ is compact with a finite fundamental group.
\end{corollary}
\begin{proof}
    Every point in the manifold can be connected to be a geodesic of length at most $\pi/\sqrt{K}$, so it follows that the exponential map on the closure of a ball maps surjectively onto the whole space, hence the image is compact. To obtain that the image is a finite fundamental group, the universal cover of this manifold also satisfies the complete curvature condition, hence it is compact, and so the number of sheets is finite, hence the fundamental group is finite.
\end{proof}

\begin{example}
    SInce the fundamental group of $S^1 \times S^2$ is $\mathbf{Z}$, which is infinite, for every metric there is a plane with zero curvature. The Hopf conjecture is whether $S^2 \times S^2$ has a product metric with $K \geq 0$.
\end{example}

\begin{theorem}[Synge]
    A compact orientable even dimensional manifold with positive sectional curvature
\end{theorem}
\begin{proof}
    Suppose $M$ is not simply connected. Every nontrivial free homotopy class has a minimizing geodesic
\end{proof}










\part{Differential Topology}

Differential Topology is a subject closely aligned with differential geometry. The goal of differential geometry is to study various geometric structures that can be added to a manifold, i.e. a Riemannian metric, Lie group structure, and so forth. The goal of differential topology is to determine what geometric structures tell us about the topological structure of the underlying manifold.

\chapter{Intersection Theory}

It is natural, given two manifolds $M$ and $N$ lying in an ambient space, to try and understand the geometric structure of the intersection $M \cap N$. In general, it is not possible to study this intersection from the perspective of differential geometry, since $M \cap N$ can become quite pathological.

\begin{example}
    Consider the plane $\Sigma_0 = \RR^k \times \{ 0 \}$ in $\RR^d$. We note the fact that if $C \subset \RR^k$ is any closed set, there exists a scalar-valued function $f \in C^\infty(\RR^k)$ such that $f^{-1}(0) = C$. The graph of $f$ can be identified with a smooth submanifold of $\RR^d$, given by
    %
    \[ \Sigma_1 = \{ (x,f(x)) \times \{ 0 \} : x \in \RR^k \}. \]
    %
    It follows that $\Sigma_0 \cap \Sigma_1 = C \times \{ 0 \}$. Since all submanifolds of some ambient space locally look like this picture, and closed sets can be quite strange, we can expect arbitrary intersections of manifolds to behave quite pathologically.
\end{example}

To fix this discussion, we rely on the rank theorems we have established in Chapter 2. Given two manifolds $M^n$ and $N^m$ contained in some ambient manifold $E^d$. Given $p \in M \cap N$, there exists a chart $(y,U)$ in $E$ containing $p$ such that
%
\[ N \cap U = \{ p \in U : y^{m+1}(p) = \dots = y^d(p) = 0 \}. \]
%
If $i: M \to E$ is the inclusion map, and if the map $g = (y^{m+1}, \dots, y^d) \circ i$ has constant rank, then the rank theorem implies that $(U \cap M) \cap N = g^{-1}(0)$ is a smooth submanifold of $M$. This seems like a tricky condition to verify, but we can find a simple geometric condition in the special case where the map $g$ is a \emph{submersion}. We note that for each $p \in M \cap N$, $T_p M$ and $T_p N$ can be viewed as subspaces of $T_p E$. We note that $g$ is a submersion at $p \in M \cap N$ precisely when
%
\[ \left. \frac{\partial}{\partial y^{m+1}} \right|_p, \dots, \left. \frac{\partial}{\partial y^d} \right|_p \in T_p M. \]
%
But since
%
\[ T_p N = \text{span} \left( \left. \frac{\partial}{\partial y^1} \right|_p, \dots, \left. \frac{\partial}{\partial y^m} \right|_p \right), \]
%
this occurs if and only if $T_p M + T_p N = T_p E$. In particular, given two manifolds $M$ and $N$, we say $M$ and $N$ are \emph{transverse} to one another if for each $p \in M \cap N$, $T_p M + T_p N = T_p E$. It then follows that $M \cap N$ is a smooth submanifold of $M$ and $N$. 

\begin{remark}
    If we define the codimension $\codim(L) = \dim(E) - \dim(L)$ for any submanifold $L$ of $E$, we find
    %
    \[ \codim(M \cap N) = \codim(M) + \codim(N), \]
    %
    at least if $\codim(M) + \codim(N) \leq d$ (if $\codim(M) + \codim(N) > d$, then $M$ and $N$ can only intersect transversally if $M \cap N = \emptyset$). We write $\codim_E(L)$ to make the fact that $L$ is a submanifold of $E$ explicit.
\end{remark}

\begin{remark}
    If we define two subspaces $V,W \subset \RR^d$ to be transversal if $V + W = \RR^d$, then two manifolds $M$ and $N$ are transversal if $T_p M$ and $T_p N$ are transversal at each $p \in M \cap N$.
\end{remark}

\begin{example}
    Let $\Sigma_0 \subset \RR^3$ be the hyperboloid defined by the equation $x^2 + y^2 - z^2 = 1$, and for each $a > 0$, let $\Sigma_1 \subset \RR^3$ be the sphere of radius $a$ specified by the equation $x^2 + y^2 + z^2 = a^2$. We now determine when $\Sigma_0$ is transversal to $\Sigma_1$. If we set $f(x,y,z) = x^2 + y^2 - z^2$, and $g(x,y,z) = x^2 + y^2 + z^2$, At each point $p = (x,y,z) \in \RR^3$, we let $V(p) = \{ v \in T_p \RR^3 : df_p(v) = 0 \}$ and $W(p) = \{ w \in T_p \RR^3 : dg_p(w) = 0 \}$. Then $V(p)$ and $W(p)$ are transversal precisely when $V(p) \cap W(p)$ is one dimensional, which occurs precisely when $\text{span}(df_p,dg_p)$ is two dimensional. We calculate that
    %
    \[ \text{span}(df_p,dg_p) = \text{span}(x \cdot dx_p + y \cdot dy_p, z \cdot dz_p). \]
    %
    Thus $V(p)$ and $W(p)$ fail to be transversal if $x = y = 0$, or $z = 0$. If $p \in \Sigma_0 \cap \Sigma_1$, then $T_p \Sigma_0 = V(p)$ and $T_p \Sigma_1 = W(p)$. Thus $\Sigma_0$ and $\Sigma_1$ are transversal precisely when $\Sigma_0 \cap \Sigma_1$ contains no points $p = (x,y,z)$ with $x = y = 0$, or $z = 0$. But $\Sigma_0$ contains no points with $x = y = 0$, so this situation is not a problem. If $\Sigma_0 \cap \Sigma_1$ contains a point $(x,y,0)$, then $x^2 + y^2 = 1$, and $x^2 + y^2 = a^2$, so $a^2 = 1$. Thus $\Sigma_0$ intersects $\Sigma_1$ transversally whenever $a \neq 1$. We find that if $a < 1$, $\Sigma_0 \cap \Sigma_1 = \emptyset$, if $a = 1$, then $\Sigma_0 \cap \Sigma_1$ is the unit circle in the $xy$ axis, and if $a > 1$, then $\Sigma_0 \cap \Sigma_1$ is the union of two circles above and below the $xy$ axis.
\end{example}

\begin{comment}
In many respects, we should expect `generic' submanifolds to intersect transversally.

\begin{example}
    Two subspaces $V,W \subset \RR^d$ intersect transversally precisely when $V + W = \RR^d$. Fix $n,m < d$ with $n + m \geq d$, and consider
    %
    \[ \Sigma = \{ (V,W) \in G(n,d) \times G(m,d) : V + W \neq \RR^d \}. \]
    %
    If we identify $G(n,d) \times G(m,d)$ with a quotient of
    %
    \[ \{ (v,w) \in (\RR^d)^n \times (\RR^d)^m : (v_1, \dots, v_n)\ \text{and}\ (w_1, \dots, w_m)\ \text{are linearly independent} \}, \]
    %
    We can write $\Sigma$ as the set of all $[v,w]$ such that for each $i_1, \dots, i_\alpha \in \{ 1, \dots, n \}$ and $j_1, \dots, j_{d-\alpha} \in \{ 1, \dots, m \}$,
    %
    \[ v_{i_1} \wedge \dots \wedge v_{i_\alpha} \wedge w_{j_1} \wedge \dots \wedge w_{j_{d-\alpha}} = 0. \]
    %
    Thus $\Sigma$ is a proper projective subvariety of $GL(n) \times GL(m)$, which means that for a `algebraically generic' pair $(V,W) \in GL(n,d) \times GL(m,d)$, $V + W = \RR^d$.
\end{example}
\end{comment}

We can also generalize transversality to talk about `intersections' between the image of some map $f: M \to E$ and a manifold $N \subset E$, by considering $f^{-1}(N)$. We say $f$ is \emph{transverse} to $N$ if for each point $q \in L$ and each $p \in M$ with $f(p) = q$, $f_*(T_p M) + T_q N = T_q E$. It then follows that $f^{-1}(N)$ is a submanifold, and $\codim_M(f^{-1}(N)) = \codim_E(N)$.












\part{Differential Geometry}

Our goal is now to apply the foundational tools we have developed to analyze Riemannian manifolds in order to study the classical methods of differential geometry, most importantly, the properties of \emph{curvature}, as initially developed alongside the calculus by Newton, Leibnitz, and Huygens, and then later revolutioned by Gauss in the late 19th century.

\chapter{Curves}

\section{Planar Curves}

We begin our study by understanding the \emph{curvature} of differentiable curves in the plane. In particular, we want to study $C^2$ curves, parameterized by $C^2$ maps $c: [a,b] \to \RR^2$ such that $c'(t) \neq 0$ for all $t \in [a,b]$. In particular, this means that the arclength function
%
\[ s(t) = \int_a^t |c'(u)|\; du \]
%
is $C^2$ diffeomorphism from $[a,b]$ onto $[0,L]$, where $L = \int_a^b |c'(u)|\; du$ is the length of the curve. We now find that
%
\[ \frac{dc}{ds} = \frac{dc}{dt} \bigg/ \frac{ds}{dt} = \frac{c'(t)}{|c'(t)|}. \]
%
Thus once our curve is reparameterized by arclength, we may assume the tangent to the curve is always a unit vector. Annoyingly, $s$ is also used canonically denote an arbitrary point in $[a,b]$, even for a curve that is not parameterized by arclength. We set
%
\[ t(s) = \frac{c'(s)}{|c'(s)|} \]
%
to be the unit tangent vector in this context.

The concept of curvature is vague, but nonetheless, we might expect a straight line to have less curvature than a circle, and a circle with a smaller radius should have more curvature than a circle with a larger radius. Thus we might define a circle of radius $R$ to have curvature $1/R$ at each point. A straight line has curvature $0$, which makes sense if we view a straight line as a circle of radius $\infty$.

To extend this concept to arbitrary curves, we try and apply the same intuitions that brought us the tangent line. Recall that the tangent line 




\chapter{Moving Frames}

Let's start by considering frames in $\mathbf{R}^3$. Let $e_1, e_2, e_3$ be a right handed smooth orthonormal frame. Since we are working in Euclidean space, we can think of the $e_n$ as vector fields. Because $e_1, e_2, e_3$ are a basis, we can write
%
\[ e_i' = \omega_{i1} e_1 + \omega_{i2} e_2 + \omega_{i3} e_3 \]
%
Since $e_i \cdot e_j = \delta_{ij}$, we can differentiate on both sides to conclude that $e_i' \cdot e_j + e_i \cdot e_j' = 0$. This implies
%
\[ \omega_{i1} = e_i' \cdot e_1 = - e_1' \cdot e_i = - \omega_{1i} \]
%
Thus the matrix $\Omega = (\omega_{ij})$ is skew symmetric. In particular, $e_i' \cdot e_i = 0$. If we consider the one forms $\sigma_k = dx(e_k) dx + dy(e_k) dy + dz(e_k) dz$, then











\chapter{Differential Topology Notes}

A \emph{connection} $d_A$ on a bundle $E$ is an $\mathbf{R}$ linear map $d_A: \Omega^0(M,E) \to \Omega^1(M,E)$ satisfies $d_A(fs) = df \wedge s + f d_A s$ for all $f \in C^\infty(M)$, $s \in \Gamma(E)$. Note that the support of $d_A s$ must be contained in the support of $s$. The operator $d_A$ is local, and for local frames $s^1, \dots, s^n$, we have $d_A s^i = \sum a_j^i s^j$. This is the connected matrix, or matrix valued one forms, or gauge fields.


\section{November 2nd}

Recall that the space of endomorphisms on a bundle $E$ is isomorphic naturally to $E \otimes E^*$. Given a connection $A$ on $E$, we get an induced connection on $E \otimes E^*$, also denoted by $A$ (though really we should denote it by $A \otimes A^*$). If $g$ is a section of $E \otimes E^*$, then $g$ acts on a section $s$ of $E$ to give another section $gs$. We let $(d_A g)(s) = d_A(gs) - g(d_A s) = [d_A, g] s$. We can check that $(d_A g)(fs) = f(d_A g)(s)$ so $d_A g$ is a one form on $E \otimes E^*$, and also $d_A(fg) = (df) g + f (d_A g)$, so $d_A$ is a connection on $E \otimes E$. More generally, $d_A(g \circ h) = (d_A g) \circ h + g \circ (d_A h)$, which can be verified by a quick calculation. If $\theta$ is a $p$ form on $\text{End}(E)$, we let $(d_A \theta)(s) = d_A(\theta s) - (-1)^p \theta(d_A s)$.

\begin{theorem}[Bianchi Identity]
    If $A$ is a connection on $E$, inducing a curvature $F_A$, then $d_A F_A = 0$.
\end{theorem}
\begin{proof}
    \[ d_A F = [d_A,F_A] = d_A \circ F_A - F_A \circ d_A = d_A^3 - d_A^3 = 0 \]
\end{proof}

\section{The Variation of Curvature Formula}

The space $A(E)$ is affine moddled on $\Omega^1(M, \text{End}(E))$. Given $A_0 \in A(E)$, every connection is of the form $A_0 + a$, for some one form $a$. Then $d_A = d_{A_0} + a$, and
%
\[ F_{A_0 + a} = (d_{A_0} + a)^2 = d_{A_0}^2 + d_{A_0} a + a d_{A_0} + a^2 = F_{A_0} + (d_{A_0} a) + a^2 \]
%
Given any element $a$ of $\Omega^1(M, \text{End}(E))$, the trace $\text{tr}$ gives us a one form $\text{tr}(a)$ on $E$.

\begin{lemma}
    For any $\eta \in \Omega^p(M,\text{End}(E))$, and $\theta \in \Omega^q(M,\text{End}(E))$,
    %
    \[ \text{Tr}(\theta \eta) = \text{Tr}((-1)^{pq} \eta \theta) \]
    %
    and for any connection $A \in A(E)$,
    %
    \[ d \text{Tr}(\theta) = \text{Tr}(d_A \theta) \]
\end{lemma}
\begin{proof}
    The first statement is immediate when written in any local frame. For the second property,
    %
    \[ d_{A + a} \theta = [d_A + a, \theta] = [d_A, \theta] + [a, \theta] = (d_A \theta) + [a,\theta] \]
    %
    Taking traces on both sides, we conclude
    %
    \[ \text{Tr}(d_{A + a} \theta) = \text{Tr}(d_A \theta) + \text{tr}(a \theta - (-1)^q a \theta) \]
    %
    But the right-most term is zero, which gives that the trace is independant of $A$. In particular, if we take a local frame and a trivial connection, we get the required formula.
\end{proof}

\begin{theorem}
    If $E$ is a complex vector bundle, and $A$ is a connection on $E$, then $i \text{Tr}(F_A)/2\pi$ is a closed two form, and the class $c_1(E) = [i \text{tr}(F_A)/2 \pi]$ is invariant of $A$.
\end{theorem}
\begin{proof}
    For any $A$, $d(\text{Tr}(F_A)) = \text{Tr}(d_A F_A) = 0$, which gives closure. Now
    %
    \[ \text{Tr}(F_{A + a}) - \text{Tr}(F_A) = \text{Tr}(F_A + d_A a + a^2 - F_A) = \text{Tr}(d_A a) + \text{Tr}(a^2) = d \text{Tr}(a) \]
    %
    because $\text{Tr}(a^2) = 0$.
\end{proof}















\chapter{Homology of Vector Bundles}

The theory of characteristic classes enables us to assign invariants to smooth bundles.

Let's use this construction to obtain an interesting $K$ vector bundle on projective space. A point in projective space $\mathbf{RP}^n$ can be viewed as a line through the origin in $\mathbf{R}^{n+1}$. We thus assign a vector bundle $L$ known as the \emph{Hopf line bundle} to $\mathbf{RP}^n$ by taking the subbundle of $\varepsilon^{n+1}(\mathbf{RP}^n)$ consisting of all vectors $v_l$, for $v \in \mathbf{R}^{n+1}$ and $l \in \mathbf{RP}^n$, such that $v \in l$. To see that this is truly a line bundle, we consider the continuous sections
%
\[ l \mapsto \frac{l_i e_j - l_j e_i}{l_k} \]
%
which are continuous where $l_k \neq 0$, and nonzero where $l_i$ and $l_j$ are not both vanishing. We can also consider complex projective space $\mathbf{CP}^n$, and the resultant complex Hopf line bundle, which we shall also denote by $L$

One interesting application of the Hopf line bundle is to understand the tangent bundle to projective space. If we consider $L$ as a subbundle of $\varepsilon^{n+1}(\mathbf{R}^{n+1})$, with the trivial Riemannian metric, then we can consider the normal bundle $L^\perp$. The immersion $\pi: S^n \to \mathbf{RP}^n$ induces $\pi_*: TS^n \to T\mathbf{RP}^n$, and for a given $v \in T\mathbf{RP}^n_l$, $\pi_*^{-1}(v)$ consists of two vectors $w_x$ and $-w_{-x}$, where $x,-x \in l$. Thus $v$ corresponds to the linear map $f_v: l \to l^\perp$ mapping $x$ to $w$ and $-x$ to $-w$. Conversely if $f: l \to l^\perp$ is given, if we take $x,-x \in S^n \cap l$ and consider their images $f(x) = w$, $f(-x) = -w$, then $\pi_*(w_x) = \pi_*(-w_{-x})$ is some vector $v$, and $f = f_v$. It is easy to see this action is smooth, and so we obtain a bundle equivalence between $T\mathbf{RP}^n$ and $\text{Hom}(L,L^\perp)$. The same is true of $T\mathbf{CP}^n$, where the homomorphisms are complex linear.

A simple corollary of this is that $T\mathbf{RP}^n \oplus \varepsilon^1(\mathbf{RP}^n)$ is isomorphic $L^{\oplus (n+1)}$ to the $n+1$ fold sum $L \oplus \dots \oplus L$. First, we notice that for any line bundle $\xi$, $\text{Hom}(\xi,\xi)$ is trivial (the identity homomorphism constitutes a global section). Thus we find
%
\begin{align*}
    T\mathbf{RP}^n \oplus \varepsilon^1(\mathbf{RP}^n) &\cong \text{Hom}(L,L^\perp) \oplus \text{Hom}(L,L) \cong \text{Hom}(L,L^\perp \oplus L)\\
    &\cong \text{Hom}(L,\varepsilon^{n+1}(\mathbf{RP}^n)) \cong (L^*)^{\bigoplus (n+1)}
\end{align*}
%
The same argument justifies that $T\mathbf{CP}^n$ is isomorphic to the complex dual $(L^*)^{\bigoplus (n+1)}$. In the case of a real vector bundle, by taking a Riemannian metric on a real dual bundle we obtain an isomorphism between it's dual and itself, so $L^*$ is isomorphic to $L$. This is no longer true in the complex space. But we do find that $T^* \mathbf{CP}^n \oplus \varepsilon^1(\mathbf{CP}^n)$ is isomorphic to $L^{\bigoplus (n+1)}$.

\section{Vector Valued Differential Forms}

We have been considering the differential forms $\omega \in \Omega^k(TM)$, which assign to each point $p$ an alternating map $\omega(p)$ in $k$ variables into $\mathbf{R}$. If we replace $\mathbf{R}$ by any other vector space $V$, then we obtain a \emph{vector valued differential form}. We can view these forms as sections of $\text{Hom} \left( \bigwedge^k(TM) ,V \right)$. In view of the natural isomorphisms
%
\[ \text{Hom} \left( \bigwedge^k(W) ,V \right) \cong (\bigwedge^k(W))^* \otimes V \cong \bigwedge^k(W^*) \otimes V \]
%
The space of vector valued differential forms is viewed as sections of $\bigwedge^k(TM^*) \otimes V$. We denote the space of smooth $V$ valued $k$ forms as $\Omega^k(M,V)$.

If $V$ has a basis $e_1, \dots, e_n$, then for any $\omega \in \Omega^k(M,V)$, we have coefficients $a_i$ such that $\omega(p)(v_1, \dots, v_n) = \sum a^i(v_1, \dots, v_n) e_i$. The functions $a_i$ are alternating, and therefore they are normal $k$ forms in $\Omega^k(M)$. But this means that $\omega = \sum a^i \otimes e_i$. In particular, we see that $\Omega^k(M,V)$ is isomorphic to the direct sum of $n$ copies of $\Omega^k(M)$.

If we allow $V$ to vary from point to point, we obtain the notion of a differential form taking values in a vector bundle. If $E$ is a vector bundle, then an $E$ valued $k$ forms assigns to each point $p$ an alternating map from $M_p^k$ to $E_p$. Arguing as before, such forms are sections of the bundle $\smash{\bigwedge^k T^* M \otimes E}$. We denote the space of sections as $\Omega^k(M,E)$. We can introduce coefficients just as in $\Omega^k(M,V)$, but only in coordinate systems which trivialize $E$.

\section{Algebraic Understand of Connections}

Recall that a connection on a smooth bundle $(\xi,E)$ over a manifold $M$ is a map $\nabla: \Gamma(M) \times \Gamma(E) \to \Gamma(E)$, whose image for $X \in \Gamma(M)$ and $s \in \Gamma(E)$ is denoted $\nabla_X(s)$, which is $C^\infty(M)$ linear in $X$ and satisfies the Leibnitz rule $\nabla_X(fs) = X(f) s + f \nabla_X(s)$. Since a connection is surely bilinear in $X$ and $s$, we can consider $\nabla$ as a map from $\Gamma(E)$ to $\Omega^1(M,E)$. The Leibnitz rule $\nabla$ then takes the form $\nabla(fs) = df \otimes s + f + \nabla(s)$. A section $s$ of a vector bundle is called \emph{flat} if $\nabla(s) = 0$.

\begin{theorem}
    If $\nabla, \nabla': \Gamma(E) \to A^1(E)$ are connections, then $\nabla - \nabla'$ is $C^\infty(M)$ linear, and so can be identified as an element of $A^1(\text{End}(E))$. Conversely, if $\nabla$ is a connection, and $a \in A^1(\text{End}(E))$, then $\nabla + a$ is a connection.
\end{theorem}
\begin{proof}
    We calculate that
    %
    \[ (\nabla - \nabla')(fs) = (df \otimes s + f \nabla(s)) - (df \otimes s + f \nabla'(s)) = f (\nabla - \nabla')(s) \]
    %
    which gives the $C^\infty(M)$ linearity. Thus for each $X$, $(\nabla - \nabla')_X$ is a map from $\Gamma(E)$ to itself, which by $C^\infty$ linearity localizes to an element of $\text{End}(E)$. Conversely, if $a \in A^1(\text{End}(E))$, then we have
    %
    \[ (\nabla + a)(f s) = f \nabla s + df \otimes s + f a(s) = f(\nabla + a)(s) + df \otimes s \]
    %
    so the map satisfies the Leibnitz rule.
\end{proof}

\begin{remark}
    The family of all connections on a vector bundle is therefore an affine space over $A^1(\text{End}(E))$, and once we have found a single connection $\nabla$, all other connections can be written as $\nabla + a$, where $a: M \to M_n(\mathbf{C})$ is a {\it matrix valued} one form, at least locally in coordinates.
\end{remark}

Since $\Gamma(E)$ is equal to $A^0(E)$, $\nabla$ maps $A^0(E)$ to $A^1(E)$. We can actually extend $\nabla$ so it maps $A^n(E)$ to $A^{n+1}(E)$ for all $n$. To do this, for $\alpha \in \Omega^n(M)$, and $s \in \Gamma(E)$, we write $\nabla(\alpha \otimes s) = d\alpha \otimes s + (-1)^n \alpha \wedge \nabla s$. This is a well defined bilinear operation by the Leibnitz theorem in the base situation, so $\nabla(\alpha \otimes (fs)) = \nabla(f (\alpha \otimes s))$. And now we have the more generalized Leibnitz rule, which for every $\alpha \in \Omega^k(M)$ and $\beta \in A^l(E)$ we have
%
\[ \nabla(\alpha \wedge \beta) = (d\alpha \wedge \beta) + (-1)^k (\beta \wedge \nabla \alpha) \]
%
In particular, this enables us to define the \emph{curvature} $F_\nabla = \nabla \circ \nabla$, mapping $\Gamma(E)$ to $A^2(E)$. It is really an element of $A^2(\text{End}(E))$, because it is $C^\infty(M)$ linear.

\begin{theorem}
    The curvature map $F_\nabla$ is $C^\infty(M)$ linear, so we can consider the curvature as an element of $A^2(\text{End}(E))$.
\end{theorem}
\begin{proof}
    We compute
    %
    \begin{align*}
        \nabla(\nabla(fs)) &= \nabla(df \otimes s + f \nabla s)\\
        &= (d^2f \otimes s - df \wedge \nabla s) + df \otimes \nabla(s) + f F_\nabla(s)\\
        &= f F_\nabla(s)
    \end{align*}
    %
    Thus for two vector fields $X$ and $Y$ we can consider $F_\nabla(X,Y)$ as an endomorphism on $E$ of each fibre.
\end{proof}

\begin{example}
    On $\mathbf{R}^n$, with the canonical tangent bundle, we have the trivial connection $\nabla_X Y = X(Y)$. This has vanishing curvature $F_\nabla = 0$. Any other connection is of the form $\nabla + A$, where $A$ is a matrix of one forms. We obtain that
    %
    \begin{align*}
        F_{\nabla + A}(X) &= (\nabla + A)(\nabla + A)(X) = (\nabla + A)(\nabla X + A(X))\\
        &= A(\nabla X) + \nabla(A(X)) + A^2(X) = \nabla(A)(X) + (A \wedge A)(X)
    \end{align*}
\end{example}






\begin{thebibliography}{10}
    \bibitem{intro} Michael Spivak,
    \emph{A Concise Introduction to Differential Geometry: Vol. One}

    \bibitem{leesmooth} James Lee,
    \emph{An Introduction to Smooth Manifolds}

    \bibitem{halm} Paul Halmos,
    \emph{Naive Set Theory}

    \bibitem{wiki} Wikipedia,
    \emph{Lie Groups}
\end{thebibliography}

\end{document}











\section{* A Non Metrizable Manifold}

In this chapter, we will, for completeness, provide an example of a non-metrizable manifold. Recall that a \emph{well-ordered set} is a set $X$ together with a linear ordering such that every subset has a least element. A subset $Y$ of a well-ordered segment is an \emph{initial segment} if $y \in Y$ and $x < y$ imply $x \in Y$.

\begin{definition}
    An \emph{order morphism} between two well-ordered sets $X$ and $Y$ is a map $f:X \to Y$ such that if $x < y$, $f(x) < f(y)$. A bijective order morphism is called an \emph{order isomorphism}, and all order morphisms are order isomorphisms onto their codomains. An \emph{ordinal} is an equivalence class of order isomorphic well ordered sets.
\end{definition}

It is helpful to visualize ordinals as the well-ordered set they represent, since we need no further properties of well ordered sets other than the ordering they possess. We will often (to our convenience) confuse the two. One key feature of ordinals is that they allow us to measure the size of infinite sets. It should come as no surprise then, that ordinals will allow us to construct a manifold too large to be metrizable.

The most well known ordinals are the natural numbers. 0 can be considered the equivalence class containing the empty set. 1 can be considered the equivalence class of well ordered sets consisting of a single element (which obviously must be order isomorphic). In general, the number $n$ can be considered the equivalence class of well ordered sets consisting of $n$ elements (which, less obviously, must be order isomorphic). It doesn't stop here though, for we can consider the equivalence class containing $\mathbf{N}$ of all natural numbers, which is also a well ordered set. By custom, this ordinal is denoted $\omega$. Then we may consider $\omega + 1$, the equivalence class of the well ordered set obtained by taking $\mathbf{N}$ and popping a greatest element on the end, and so on and so forth. There's many more ordinals in this magnificant menagerie, and they form a beautiful transfinite chain:

\[ 0, 1, 2, 3, \dots, \omega, \omega + 1, \dots, \omega 2, \omega 2 + 1, \dots, \omega 3, \dots, \omega^2, \dots \omega^\omega, \dots  \]

\begin{lemma}
    If $X$ and $Y$ are well ordered sets, and if for $A \subset B \subset X$ there are two order morphisms $f:A \to Y$ and $g:B \to Y$ whose ranges are initial segments of $Y$, then $g|_A = f$.
\end{lemma}
\begin{proof}
    Consider the set of all elements in $B$ that do not agree on $f$ and $g$. If this set is non-empty, there must be a least such element $b$, so either $f(b) < g(b)$, or $g(b) < f(a)$. In the first case, there must be $b'$ such that $g(b') = f(b)$ (since $g$ maps onto an initial segment). We also must have $b' < b$, and so $f(b') = g(b') = f(b)$. All order isomorphisms are injective, so we reach a contradiction. The latter case is similar, and shows by contradiction that there can be no elements that disagree on the domains of the functions.
\end{proof}

\begin{corollary}
    There is at most one map $f:X \to Y$ which maps onto an initial segment of $Y$.
\end{corollary}

\begin{lemma}
    If $X$ and $Y$ are well ordered sets, there either exists a unique order morphism from $X$ to an initial segment of $Y$, or a unique order morphism from $Y$ to an initial segment of $X$. What's more, this map is unique.
\end{lemma}
\begin{proof}
    Consider the set $A$ of all initial segments of $X$ which have order morphisms $f_A$ (which are necessarily unique) onto initial segments of $Y$. If we have a linear chain $\{A_k\}$ of such sets, we may by the last corollary take the union $\bigcup f_A$ of order morphisms to form an order morphism on $\bigcup A_k$. By Zorn's lemma, we must have a maximal initial segment $A$. If $A = X$, we are done. If $A \neq X$, and $f_A(A) = Y$, then we may invert the domain of $f_A$ to obtain an order morphism from $Y$ to $A$, and initial segment of $X$. These are all of the possibilities, since if $f_A(A) \neq Y$, we may consider the least element $y$ in $f_A(A)^c$ and $x$ in $A^c$, and extend the map $f_A$ by defining $f_A(x) = y$, contradicting the fact that $A$ is maximal.
\end{proof}

We say $X \leq Y$ if there is an order morphism from $X$ to an initial segment of $Y$. Because of the above theorem, we can visualize any ordinal as an initial segment of an ordinal of a larger size. In fact, with the above ordering, any ordinal is the equivalence class of the set of ordinals less than itself. From this, we can also see than any set of ordinals is well ordered, and that any set of ordinals is contained within an ordinal.

\begin{lemma}
    If $A$ is an initial segment which is a proper subset of a well ordered set $B$, there is no order isomorphism from $B$ to $A$.
\end{lemma}
\begin{proof}
    Let $f:A \to B$ be an order isomorphism from $A$ to $B$. Consider the smallest element $a \in A$ such that $f(a) \neq a$. There must be one such $a$, since $f$ is surjective, and there are some $b \in B$ which are not in $A$. We cannot have $f(a) < a$, since $f$ is injective, and this would imply $f(f(a)) \neq f(a)$, and $f(a)$ an element of $A$ since $A$ is an initial segment. We also cannot have $f(a) > a$, since there is $a' \in A$ such that $f(a') = a$, and since $f(a') < f(a)$, we have $a' < a$. By contradiction, there cannot be an order isomorphism $f$.
\end{proof}

If two well-ordered sets are order isomorpic, they have the same cardinality, and therefore it makes sense to discuss the cardinality of an ordinal. The well ordering theorem stipulates that any set can be well ordered. Therefore, taking the equivalence class of a well-ordering of $\mathbf{R}$, we obtain an uncountable ordinal. All countable ordinals can be considered initial segments of $X$, and we may therefore consider the set $\Omega$ of all countable ordinals.

\begin{theorem}
    $\Omega$ is uncountable.
\end{theorem}
\begin{proof}
    Suppose $\Omega$ is countable, Then $\Omega$ itself represents a countable ordinal $\alpha \in \Omega$. But $\alpha$ is order isomorphic to the set of ordinals less than $\alpha$, and so $\Omega$ is order isomorphic to a proper initial segment of itself, contradicting the above lemma.
\end{proof}

After this development, we can now release our non-metrizable manifolds.

\begin{example}[The Long Line]
    Take the set $\Omega$ of all countable ordinals. Then $\Omega$ is itself an ordinal, and we may consider the space $L = \Omega \times [0,1)$ together with the dictionary order. The order topology established forms a space, the long ray. Now take two copies of the long ray, and attach them at the smallest elements. This create a one-manifold -- the long line. Obviously, the space isn't metrizable -- it contains an uncountable discrete subset, so none of the other nice properties that we considered above hold.
\end{example}

\begin{example}[Long 2-Manifolds]
    The two-manifold $L \times S^1$ is called the long cylinder, and is also non-metrizable, and the long plane $L \times L$ is the same. A 2-manifold that is long only in one direction is the long strip $L \times \mathbf{R}$.
\end{example}

We'll encounter more unmetrizable manifolds in later chapters.




















