\input{../../style.tex}

\title{Statistics}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents
\pagenumbering{arabic}

\chapter{Statistical Models}

\emph{Statistic inference} is the theory of obtaining reasonably reliable information about a population in the prescence of uncertainty. That population can be a \emph{population of individuals}, or a \emph{population of outcomes}. We want to learn certain features of a probability distribution via various samples taken from that probability distribution, and to investigate causality between different random phenomena, the latter being the most difficult, but also the most essential to the use of statistics in the sciences. In general, the samples we observe vastly underdetermine the entire population, and so our methods are about inferring larger features of the population \emph{to a reasonable degree of precision}. On the other hand, we often have too much data for the human mind to comprehend individually, and so we wish to simplify the data to a few relevant features.

The problem of statistics is threefold:
%
\begin{itemize}
	\item \emph{Specification}: Forming a mathematical model for the population.
	\item \emph{Estimation} Calculating, from a sample of a population, statistics of that population.
	\item \emph{Misspecification Testing and Respecification}: Testing whether the assumptions made in the formation of our model fit the data that we observed, and suggesting an alternate model if the assumptions do not seem to fit such data.
\end{itemize}

In general, it is very difficult to determine properties of $\mu$, so we often work with some assumptions on the distribution $\mu$, a \emph{statistical model}. More precisely, a statistic model $\mathcal{F}$ is a subset of the space of all distributions on a set $S$. We might write such a set as
%
\[ \mathcal{F} = \{ \mu_\theta : \theta \in \Theta \}, \]
%
where $\Theta$ is the \emph{parameter space} of the model. If $\Theta$ can be seen naturally as a subset of $\RR^n$ for some $n$, we say this is a \emph{parametric model}.

\begin{example}
    In many statistical problems, we commonly assume the distribution is normal. That is, we work over the statistical model
    %
    \[ \{ N(\mu,\Sigma): \mu \in \RR^n, \Sigma \in \RR^{n \times n} \}. \]
    %
    Thus a normal distribution in $\RR^n$ is specifiable by $n^2 + n$ parameters.
\end{example}

\begin{example}
	A standard class in nonparametric statistics is the class of \emph{all distributions}. But some methods might assume some minor regularity about the distribution. For instance, we might work in the model $\mathcal{F}_{\text{ABS}}$, which consists of all probability measures $\mu$ which are absolutely continuous (i.e. that have a density function). Or perhaps some additional regularity, i.e. that $\mu$ lies in some function space, i.e. a Sobolev space.
\end{example}

A \emph{statistical functional} is any function of the distribution $\mu$. Examples include the \emph{expected value} of the distribution, the \emph{variance}, or the \emph{median} (only defined for probability measures without atoms). A \emph{statistic} is a function of the data drawn from this distribution. We can now formally describe the problem of statistics: given a statistical functional, find a statistic which best approximates this functional. This is the problem of \emph{point estimation}. The methods involved depend on the statistical functional employed.

For notational purposes, in statistics we often consider a multitude of distributions $\mu$ over the same set $S$. We therefore introduce the notation $\PP_\mu$, $\EE_\mu$, and $\VV_\mu$, to denote the probabilistic quantities obtained by using the distribution $\mu$. For parametric problems, we might also use the notation $\PP_\theta$, $\EE_\theta$, and $\VV_\theta$.

One of the most important point estimation problem is the problem of \emph{regression}, \emph{prediction}, and \emph{classification}, we are given pairs
%
\[ (X_1,Y_1),\dots,(X_N,Y_N) \]
%
drawn independently from some product distribution $\mu$. Given $\mu$ and some \emph{error}, or \emph{loss function}, we have a given \emph{regression function} $Y = R(X)$, which best approximates the relationship between $X$ and $Y$ given by the distribution $\mu$. Finding a good approximation of $R$ via the data is called \emph{regression}, or \emph{curve estimation}. Given a new input $X$ that has not yet been observed, we can then \emph{predict} from the approximate regression function what the best estimate for the output $Y$ should be.

Related to point estimation is the computation of \emph{confidence intervals}. Given a statistical functional $\theta \in \RR$, the problem is to compute an interval $C$ from observed data $X_1,\dots,X_N$ such that for each $\mu$,
%
\[ \PP_\mu( \theta \in C ) \geq 1 - \alpha \]
%
The interval $C$ is then called an $\alpha$-\emph{confidence interval}. More generally, if $\theta$ is a vector we can consider a \emph{confidence set}. It is standard, but not necessary, to take $\alpha = 0.05$, and unless specified we will take this as the choice of $\alpha$.

\begin{remark}
	Under the frequentist interpretation of probability, a confidence interval \emph{does not} give a probability associated with the quantity $\theta$, since we assign no probability distribution to the distributions $\mu$ we consider. Instead, one can interpret a confidence interval as follows: each time we perform a confidence interval computation, we should be expected to be right $95\%$ of the time.
\end{remark}

Another problem is \emph{hypothesis testing}. Here we have a particular fact about a distribution, called the \emph{null hypothesis}. Given some data, the goal of hypothesis testing is to determine whether we should reject the null hypothesis, or if we have grounds to still believe in the null hypothesis.

\chapter{Hypothesis Testing}

Suppose we collect some data about some situation. The problem of \emph{hypothesis testing} is determining whether the data we generate provide evidence for or against some \emph{hypothesis}. When sufficient evidence is provided, we say the data is \emph{statistically significant}. Classically, such a situation arises in testing whether there is a difference between data generated with respect to two different conditions, and we want to determine if the conditions generate such data differently. Hypothesis testing helps us determine if this proposition is true or false, but \emph{does not quantify the size of the effects between the two conditions}.

It is important for scientists to follow a methodology in order to determine the statistical significance of data. It is fairly easy to become subject to cognitive bias otherwise. In 2015, in response to the replication crisis and the percieved misuse of $p$-values in psychological literature, the Journal \emph{Basic and Applied Social Psychology} banned the use of inferential statistics. In 2019, Fricker et. al published an article analyzing 31 articles published by the journal in 2016, and find multiple instances of scientists overstating claims relative to what could be claimed if statistical significance was considered.

\section{Rejecting the Null Hypothesis}

Let us begin with the first approach to hypothesis testing, first advocated by British statistician and geneticist Ronald Fischer. In this approach, one does not use data to provide evidence \emph{for} a given hypothesis. Instead, perhaps unintuitively, data is used to provide evidence \emph{against} a given hypothesis (called the \emph{null hypothesis}).

Mathematically, we model such hypothesis testing by considering a family of probability distributions on some space $\mathcal{X}$, parameterized by some set $\Theta$. In an experiment, we observe some $X \in \mathcal{X}$. A hypothesis corresponds to some assumption on what generated the data. Mathematically, we identify a hypothesis with a subset $\Theta_0 \subset \Theta$. To perform a hypothesis test, we specify in advance a set $\mathcal{R} \subset \mathcal{X}$, called the \emph{rejection region}. If $X$ takes some value in $\mathcal{R}$, we consider this evidence to reject the null hypothesis, and otherwise we do not reject the null hypothesis. Usually, we define $\mathcal{R} = \{ x \in \mathcal{X} : T(x) > T_0 \}$ for a \emph{test statistic} $T: \mathcal{X} \to \RR$ and a \emph{critical value $T_0 \in \RR$}.

Note that if a hypothesis does not reject the null hypothesis, then in Fischerian approach we do not view this as evidence that the hypothesis is true. Our goal in this approach is thus only to minimize the number of \emph{Type $I$ errors} we make, where we reject the null hypothesis, despite it being true. If we are able to minimize these errors, then when a given data leads us to reject the null hypothesis, we should treat that as significant evidence that the null hypothesis is false.

This leads us to define the \emph{size} of a test. First, define the \emph{power function} $\beta$ of the test, given by $\beta(\theta) = \PP_\theta(X \in \mathcal{R})$. The \emph{size} of a test is $\alpha = \sup_{\theta \in \Theta_0} \beta(\theta)$, which is precisely the least upper bound on the probability of the test making a Type $I$ error. We say a test has \emph{significance level} $\alpha$ if it has size at most $\alpha$.

Suppose that for a given null hypothesis, we have an increasing family of rejection regions $\mathcal{R}_\alpha$ for each $\alpha \in (0,1)$, which is increasing in $\alpha$, and for each $\alpha$, $\mathcal{R}_\alpha$ corresponds to a hypothesis test of significance level $\alpha$. Given this setup, we define the \emph{$p$-value} to be the random quantity $p = \inf \left\{ \alpha: X \in \mathcal{R}_\alpha \right\}$. For each $\alpha \in [0,1]$, $\alpha = \sup\nolimits_{\theta \in \Theta_0} \PP_\theta( X \in \mathcal{R}_\alpha )$, so
%
\[ p = \sup\nolimits_{\theta \in \Theta_0} \PP_\theta( T(X) \geq \mathcal{R}_p ). \]
%
Thus the $p$-value can be thought of as the least upper bound of the probability of observing something at least as extreme as the data observed, from distributions in the null hypothesis.

The use of $p$-values as a methodology of checking data for statistical significance was popularized by Ronald Fischer in his 1925 book \emph{Statistical Methods for Research Workers}. Fischer interpreted the p-value of a hypothesis test as a continuous measure of how compatible data observed was with a null hypothesis. Fischer's approach interprets $p$-values not as probabilistic statement about the world, but as a continuous measure of how reluctant we should be to believe a null hypothesis. The lower the $p$-value, the more reluctant we should be to believe a null hypothesis; either a rare result occured, or the null hypothesis does not explain the data observed.

The main problem with Fischer's approach is the lack of an explicit \emph{alternate hypothesis}. Neyman and Pearson expanded upon Fischer's philosophy to allow for such hypotheses, though the data is still only tested under the main hypothesis.

 however, is not tested, but used as a measure of the expected effect size, a measure of how difficult .

Neyman and Pearson (1933) expanded upon Fischer's philosophy of hypothesis testing. Rather than simply testing whether to reject the null hypothesis, in the Neyman Pearson approach one instead sets a fixed level at which evidence from an experiment is statistically significant to provide evidence against the null hypothesis: The orthodox approach in 20th century science was to reject the null hypothesis when $p \leq 0.05$, and to accept the null hypothesis when $p > 0.05$. Unlike the Fischerian approach, in the Neyman Pearson approach the particular value of $p$ does not indicate that the null hypothesis is more or less believable.

\begin{example}
	Let $X = (X_1,\dots,X_n)$ be a vector with components being i.i.d. normally distributed random variables with some unknown mean $\mu$ and a known variance $\sigma$. Suppose we wish to test the null hypothesis that the mean of these variables is negative. We can parameterize the possible probability distributions that fit this null hypothesis by the mean of these distributions. Thus we can set $\Theta_0 = (-\infty,0]$. Consider the test where we reject the null hypothesis if $T(\overline{X}) > T_0$, for some  $T_0 \in \RR$. Then one can calculate that
	%
	\[ \beta(\mu) = 1 - \Phi \left( \frac{n^{1/2}(T_0 - \mu)}{\sigma} \right), \]
	%
	and thus the size of the test is
	%
	\[ 1 - \Phi \left( \frac{n^{1/2} T_0}{\sigma} \right) \sim \left( \frac{\sigma}{n^{1/2} T_0} \right) e^{- ( n^{1/2} T_0/\sigma)^2}. \]
	%
	For a given $\alpha$, we get a test of size $\alpha$ by taking
	%
	\[ T_0 = \sigma n^{-1/2} \Phi^{-1}(1 - \alpha) \sim \sigma n^{-1/2} \ln \left( 1/\alpha \right) . \]
	%
	For a given observation $X$, the $p$-value associated with this family of hypothesis tests is thus precisely obtained by the equation
	%
	\[ X = \sigma n^{-1/2} \Phi^{-1}(1 - \alpha), \]
	%
	i.e. we have
	%
	\[ p = 1 - \Phi(n^{1/2} X / \sigma). \]
	%
	The larger $X$ is, the closer the $p$-value is to $0$. For a given mean $\mu$, the CDF of the p-value $p$ is thus
	% 1 - Phi(n^{1/2} X / sigma) <= t
	% Phi(n^{1/2} X / sigma) >= 1 - t
	% n^{1/2} X / sigma >= Phi^{-1}(1 - t)
	% X >= n^{-1/2} sigma Phi^{-1}(1 - t)
	% 1 - Phi
	\[ t \mapsto 1 - \Phi( \sigma n^{-1/2} \Phi^{-1}(1 - t) - \mu ). \]
	%
	For $\mu \ll 0$, the distribution of the $p$-value is concentrated near large values of $p$, but as $\mu \to 0$, the distribution of the $p$-value becomes uniformly distributed on $[0,1]$.
\end{example}

The last example should cause us to treat a particular $p$-value with some skepticism. It is a random quantity, highly dependent on the data we observe, and thus will vary as we replicate a given experiment. The only guarantee we have on the distributions of the $p$-values is that, if the null hypothesis is true, then a $p$-value less than $\alpha$ will only be observed at most a fraction $\alpha$ of the time. Thus observing the $p$-value of one experiment will not necessarily lead to good predictions of $p$-values of replications. This contrasts with something with more descriptive power, like a confidence interval: Given a $95\%$ confidence interval, 19 out of 20 replications will have confidence intervals overlapping with the confidence interval computed on the first experiment (see Geoff Cumming's \href{https://www.youtube.com/watch?v=5OL1RqHrZQ8&ab_channel=GeoffCumming}{Dance of the $p$-values} for more detail).

This behaviour also leads to \emph{Lindlay's Paradox} (Lindsay, 1957). For $\mu \gg 0$ the $p$-values we will observe from the above experiment will be very low, guaranteed with very high probability to have $p$-values smaller than $0.01$. For $\mu = 0$, the $p$-values will be approximately normally distributed. It thus follows that, if we observe a $p$-value of $0.04$, then from a Bayesian maximum likelihood perspective (we should favor hypothesis that are more likely to producea given data), we are more likely to be in the situation $\mu = 0$ than in the situation $\mu \gg 0$, despite having a $p$-value less than $0.05$ (and thus favoring a rejection of the null hypothesis from the approach of Fischer). Thus different approaches to statistical inference lead to different interpretations of a given data set. One approach which prevents this paradox is decreasing the $p$-value required for a test to be statistically significant as the sample size increases (see Good, 1992; Leamer, 1978; Maier \& Lakens, 2022).

%Fischer tried to develop his philosophy into an approach he called ‘fiducial inference’, but this was not as well adopted as other philosophies of thought, such as decision theory and Bayesian inference. Zabell 1992 writes that fiducial inference was ‘Fischer’s one great failure’, though others believe a consistence theory may soon be developed (Schweder and Hjort, 2016).

Even if you are using the Neyman-Pearson method for using $p$-values to refute a null hypothesis, in a paper you should always report the precise $p$-values obtained from your experiment. This is because these $p$-values are useful for secondary analysis, and allows researchers to compare $p$-values if they wish to use another $p$-value as a baseline.

On the other hand, defined purely in terms of levels, unless we have some information about the power of the tests we are using, the $p$-value says \emph{nothing} about probabilities associated with the hypothesis. Small $p$-values indicate that it is unlikely that we could have observed given data if we are observing data from distributions given by the null hypothesis, but \emph{large $p$-values do not imply that we should reject a given hypothesis}. Large $p$-values just indicate that, if the null hypothesis is true, it is not unlikely that we have observed data at least as extreme as what was observed. One should perform an \emph{equivalence test} or \emph{minimum effect test} to obtain more information outside of the null hypothesis.



\section{Sources}

https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2015.00223/full





\newpage





 A \emph{hypothesis} is then a statement as to whether a distribution is in some subset $\Theta_1$ of $\Theta$. We call the complement of $\Theta_1$ in $\Theta$ the \emph{null hypothesis}, and denote it by $\Theta_0$. Our goal is to find a method which distinguishes between the two situations. The orthodox philosophy of scientists, in order to obtain greater evidence as to whether a given hypothesis is true, is to instead perform experiments that cause us to believe that the \emph{null hypothesis}, the statement that the hypothesis fails, is highly improbable. The goal of hypothesis testing is to come up with a methodology that allows us to take data, and determine whether this allows us to make a statistically significant statement providing evidence against a given null hypothesis. We want to minimize the number of errors we make: either Type $I$ errors, where we reject the null hypothesis, despite it being true, or Type $II$ errors, where we retain the null hypothesis when the null hypothesis is false.

For each $\theta \in \Theta$, the hypothesis $\{ \theta \}$ is called a \emph{simple hypothesis}. If $\Theta$ is a subset of $\RR$, then we call the hypotheses $\{ \theta > \theta_0 \}$ and $\{ \theta < \theta_0 \}$ \emph{composite hypotheses}. A \emph{two-sided} test is a test where the null hypothesis is a simple hypothesis, and a \emph{one-sided} test is a test where the null hypothesis is a composite hypothesis.

Mathematically, how does hypothesis testing work? We consider a family of probability distributions over some set $\mathcal{X}$, indexed by some set $\Theta$, and consider an $\mathcal{X}$ valued random variable $X$ which can be assigned probabilities that fit each distribution in a family parameterized by $\Theta$. A test of the null hypothesis can be given by considering a region $\mathcal{R} \subset \mathcal{X}$, the \emph{rejection region}. If $X$ takes some value $x \in \mathcal{R}$, we reject the null hypothesis, and if $X$ takes some value $x \not \in \mathcal{R}$, we do not reject the null hypothesis. 

Given a hypothesis test, we define the \emph{power function} of the test is $\beta(\theta) = \PP_\theta(X \in \mathcal{R})$. The \emph{size} of the test is $\alpha = \sup_{\theta \in \Theta_0} \beta(\theta)$, and the \emph{power} of the test is $\inf_{\theta \in \Theta_1} \beta(\theta)$. A test has \emph{level $\alpha$} if it's size is less than or equal to $\alpha$. A test with low size is unlikely to commit Type $I$ errors, but the size of a test does not tell us anything about the probability of committing Type $II$ errors. Similarily, a test with high power is unlikely to commit Type $II$ errors, but the power of a test does not tell us anything about the probability of committing Type $I$ errors.



In 1933, Neyman and Pearson, inspired by insights into p-values due to Gosset and Fischer, developed an approach called \emph{statistical hypothesis testing}. In this framework, the goal of statistical tests is to guide researchers with respect to two different hypothesis, a null hypothesis, and an alternate hypothesis. Given a particular value $\alpha$, classically equal to $0.05$, one computes the p-value of a null hypothesis. If the value is smaller than $\alpha$, one should be more prone to believing in the alternate hypothesis of the null hypothesis. Thus the exact quantitative p-value is not viewed as a measure of how reluctant we should be to accept the null hypothesis.









\chapter{Point Estimation}

Let us consider point estimation. We consider a statistical functional $\theta$ of some distribution $\mu$, from which we observe data $X_1,\dots,X_N$. We must now find a statistic $\widehat{\theta}_N$, called the \emph{estimator}, which is our best guess of $\theta$. We say the estimator $\widehat{\theta}$ is \emph{unbiased} if $\EE_\mu[\widehat{\theta} - \theta_N] = 0$ for any distribution $\mu$ in our statistical model. It is consistant if $\widehat{\theta}_N$ converges to $\theta$ in probaibility as $N \to \infty$, for any $\mu$.

As a function of random data, the estimator $\widehat{\theta}$ is a random variable. It's distribution is called the \emph{sampling distribution}. The standard deviation of the estimator is called the \emph{standard error} $\text{SE}_N$. Often, this error will depend on $\mu$, and will thus be unknown. But we can often find an \emph{estimator} for the standard error, which will be denoted by $\widehat{\text{SE}}_N$.

To determine how good an estimator is, we produce a \emph{loss function} $L$, and then to determine the \emph{expected loss}
%
\[ \EE_\mu[ L(\theta, \widehat{\theta}_N) ]. \]
%
The function $L$ is normally selected so that if $\widehat{\theta}_N = \theta$ then no loss in incurred, and more loss is incurred the `further away' $\widehat{\theta}_N$ is from being correct. There are several natural choices for the loss function:
%
\begin{itemize}
	\item The most common is the loss function
	%
	\[ L(\theta_1,\theta_2) = (\theta_1 - \theta_2)^2. \]
	%
	The resulting expected loss is the \emph{mean square error}, denoted $\text{MSE}$. A nice feature of this formula is the \emph{bias} \emph{variance} decomposition, i.e. for any estimator, if we define
	%
	\[ \text{BIAS} = \EE[ \widehat{\theta}_N - \Theta_N ], \]
	%
	then
	%
	\[ \text{MSE} = \text{BIAS}^2 + \text{SE}^2. \]

	\item Another common choice is the $L^1$ loss function
	%
	\[ L(\theta_1,\theta_2) = |\theta_1 - \theta_2|. \]

	\item For discrete problems, another choice is the $0-1$ loss function
	%
	\[ L(\theta_1,\theta_2) = \mathbf{I}(\theta_1 \neq \theta_2). \]
\end{itemize}
%
Given a loss function, our goal is often therefore to find an estimator which minimizes the expected loss over all other possible estimators.

Expanding out the mean square error, we conclude that
%
\[ \text{MSE} = \text{BIAS}^2 + \text{SE}^2, \]
%
where $\text{BIAS} = \EE_\mu[ \widehat{\theta}_N ] - \theta$. In particular, we conclude that if $\text{BIAS} \to 0$ and $\text{SE} \to 0$ as $N \to \infty$, then $\widehat{\theta}_N$ is a consistent estimator, since then $\widehat{\theta}_N$ converges in the $L^2$ norm to $\theta$ for any distribution $\mu$, and convergence in $L^2$ implies convergence in probability.

\begin{example}
	Consider $X_1,\dots,X_N \sim \text{Bernoulli}(p)$, where $p \in [0,1]$ is unknown. A natural estimator to use is
	%
	\[ \widehat{p}_N = \frac{X_1 + \dots + X_N}{N}. \]
	%
	It is simple to check via linearity of expectation that $\widehat{p}_N$ is unbiased. It's standard error is
	%
	\[ \text{SE} = \sqrt{\frac{p(1-p)}{N}}. \]
	%
	A natural estimator for this standard error is therefore
	%
	\[ \widehat{\text{SE}}_N = \sqrt{\frac{\widehat{p}_N (1 - \widehat{p}_N)}{N}}. \]
	%
	For any value $p$, the standard error converges to zero, so this is a consistent estimator.
\end{example}

A common method for point estimation, especially in parametric problems where the point we are estimating determines the distribution under study, is the \emph{maximum likelihood estimator}. We begin by assuming the statistical model we are working in is contained in the space of all distributions which are either continuous or are discrete. Given data $X_1,\dots,X_N$, we define the \emph{likelihood function}
%
\[ L(\mu) = f(X_1) \cdots f(X_N) \]
%
where $f$ is the density function of $\mu$. To find the maximum likelihood estimator, we find
%
\[ \mu_* = \argmax_{\mu \in \mathcal{F}}(L(\mu)), \]
%
the `most likely' distribution to generate a given dataset. We then set $\widehat{\theta}_N = \theta^*$, where $\theta^*$ is the value of the statistical function $\theta$ we are estimating which is associated with the distribution $\mu^*$. To compute $\mu^*$, it is often easier to work with the \emph{log likelihood}
%
\[ l(\mu) = \log f(X_1) + \log f(X_2) + \dots + \log f(X_N), \]
%
since the distribution maximizing likelihood also maximizes the log-likelihood.

\begin{example}
	Consider $X_1,\dots,X_N \sim \text{Bernoulli}(p)$, where $p$ is unknown, and suppose we want to estimate $p$. We will find that the estimator we saw above is the maximum likelihood estimator for this example. Indeed, we have
	%
	\begin{align*}
		l(p) &= \sum_{i = 1}^N \log \PP_p(X_i)\\
		&= \sum_{i = 1}^N \log \left( p^{X_i} (1 - p)^{1 - X_i} \right)\\
		&= \sum_{i = 1}^N X_i \log(p) + (1 - X_i) \log(1 - p).
	\end{align*}
	%
	The derivative of $l$ as a function of $p$ is equal to
	%
	\[ \sum_{i = 1}^N \frac{X_i}{p} - \frac{1 - X_i}{1 - p} = \frac{1}{1 - p} \left( \frac{S}{p} - N \right), \]
	%
	where $S = \sum X_i$. Setting this quantity equal to zero gives that we should set $\widehat{p}_N = S/N$, which was the estimator we considered before.
\end{example}

\begin{example}
	Consider $X_1,\dots,X_N \sim N(\mu,\sigma^2)$, where $\mu$ and $\sigma$ are unknown. The problem is to estimate $\theta = (\mu,\sigma)$. The likelihood function is
	%
	\[ L(\theta) = \frac{1}{\sigma^N (2 \pi)^{N/2}} \exp \left( - \frac{1}{2} \sum_{i = 1}^N \left( \frac{X_i - \mu}{\sigma} \right)^2 \right), \]
	%
	and thus the log likelihood is proportional to
	%
	\[ l(\theta) = -N \log(\sigma) - \frac{1}{2} \sum_{i = 1}^N \left( \frac{X_i - \mu}{\sigma} \right)^2. \]
	%
	Taking partial derivatives in $\sigma$ and $\mu$ gives that the maximum likelihood parameters $\mu^*$ and $\sigma^*$ satisfy
	%
	%
	\[ \mu^* = \frac{X_1 + \dots + X_N}{N} \]
	%
	and
	%
	\[ \sigma^* = \left( \frac{(X_1 - \mu)^2 + \dots + (X_N - \mu)^2}{N} \right)^{1/2}, \]
	%
	which are the natural estimators we might choose given the observed data.
\end{example}

\begin{example}
	Consider $X_1,\dots,X_N \sim \text{Uniform}([0,\theta])$, for some $\theta \in [0,\infty)$. What is the maximum likelihood estimator of $\theta$? The likelihood function is
	%
	\[ L(\theta) = \theta^{-N} \mathbf{I}(\max(X_1,\dots,X_N) \leq \theta). \]
	%
	This function is equal to zero for $\theta \in [0,\max(X_1,\dots,X_N))$, and decreases for values bigger than $\max(X_1,\dots,X_N)$. Thus we conclude that the maximum likelihood estimator for $\theta$ is
	%
	\[ \widehat{\theta}_N = \max(X_1,\dots,X_N). \]
\end{example}

\begin{example}
	Consider a practical example. Let us suppose that there are $N$ fish in a lake, a quantity unknown to us, and which we would like to estimate. We capture $A$ fish, paint them with red spots, and then release them back into the wild. We then capture $B$ fish, and observe that $X$ of them are red. Let us use this information to compute the maximum likelihood estimate of $N$. The only random quantity here is $X$, the other quantities $A$ and $B$ determining the statistical model we use. Given $N$, the distribution of $X$ is a hypergeometric distribution, and so the likelihood function is
	%
	\[ L(N) = \frac{{A \choose X} { N-A \choose n-X }}{{N \choose B}}, \]
	%
	To determine the maximum quantity here, we compute there for $N > A$,
	%
	\[ \frac{L(N)}{L(N-1)} = \frac{N-A}{N-A-B+X} \frac{N-B}{N}. \]
	%
	This quantity is greater than one if $N < AB / X$, and smaller than one if $N > AB/X$. The maximum likelihood estimator for $N$ is therefore the greatest integer less than or equal to $AB/X$, i.e.
	%
	\[ \widehat{N} = \left\lfloor \frac{AB}{X} \right\rfloor. \]
	%
	As an example, if we capture 1000 fish and mark them with red spots, capture another set of 1000 fish, and observe that 100 have red spots, we should guess that there are 10000 fish in the lake.
\end{example}

\chapter{Regression}

The most basic task of statistics is regression. Given a sequence of independent experiments $(X_1, Y_1), \dots, (X_N, Y_N)$ drawn from some distribution $(X,Y)$, we want to determine the a function $f$ such that $f(X)$ approximates $Y$. To measure the approximations success, for each value $(x,y)$, we consider a \emph{loss function} $L(y,f(x)) \geq 0$. The \emph{estimated prediction error} is then defined to be $\text{EPE}(f) = \EE(L(Y,f(X)))$. The goal of \emph{regression} is to find a function $f$ which minimizes the estimated prediction error. In parametric problems, the goal is to find the function $f$ lying in a given finite dimensional class of candidate functions minimized estimated prediction error. In non parametric problems, we must find the function $f$ from an infinite dimensional class of candidate functions.

\begin{example}
	In most cases, the most analytically convenient loss functions is the \emph{square error loss}
	%
	\[ L(y,f(x)) = (y - f(x))^2. \]
	%
	We then have
	%
	\[ \text{EPE}(f) = \EE(L(Y,f(X))) = \EE((Y - f(X))^2) = \EE(\EE(Y - f(X))^2 | X)). \]
	%
	In this case, the expected prediction error is known as the \emph{mean square error}, denoted $\text{MSE}(f)$. Thus regression reduces to minizing $(Y - f(X))^2$ pointwise, given $X$. The minimizer of this quantity is the conditional expectation
	%
	\[ f(X) = \EE(Y|X). \]
	%
	This is because if $f$ is any function, then $\EE(f(X)|X) = f(X)$, and so
	%
	\[ \EE((Y - \EE(Y|X))f(X)|X) = \EE(Y|X) f(X) - \EE(Y|X) f(X) = 0. \]
	%
	Thus $Y - \EE(Y|X)$ is orthogonal to the subspace of random variables measurable with respect to the sigma algebra generated by $X$. Thus we can apply the Pythagorean theorem to conclude that
	%
	\begin{align*}
		\text{MSE}(f) = \EE \left( (Y - f(X))^2 | \right) &= \EE \left((Y - \EE(Y|X))^2 \right) + \EE \left( (\EE(Y|X) - f(X))^2 \right)\\
		&\geq \EE \left( (Y - \EE(Y|X))^2 \right) = \text{MSE}(\EE(Y|X)).
	\end{align*}
	%
	Thus regression with respect to squared loss is equivalent to estimating the conditional expectation of one variable with respect to one another. The decomposition above using Pythagoras' theorem is very useful, which is really what makes the square error loss most useful in a given situation.
\end{example}

\begin{example}
	Another standard loss function is the $L^1$ loss function, given by
	%
	\[ L(y,f(x)) = |y - f(x)|. \]
	%
	As with the squared loss it suffices to choose $f(X)$ which pointwise minimizes
	%
	\[ \EE\left. \Big( |Y - f(X)| \big| X \right). \]
	%
	Fix $y$. We note that if $\varepsilon > 0$, then
	%
	\[ |Y - y + \varepsilon| - |Y - y| = \begin{cases} \varepsilon &: Y \geq y\\ -\varepsilon &: Y \leq y - \varepsilon \\
	2(Y - y) + \varepsilon &: y - \varepsilon < Y < y \end{cases} \]
	%
	Thus
	%
	\begin{align*}
		&\left| \EE \left( |Y - y + \varepsilon| - |Y - y| | X \right) - \varepsilon [\PP(Y \geq y | X) - \PP(Y \leq y - \varepsilon | X) ] \right|\\
		&\ \ \ \ \ \leq \varepsilon \PP(y - \varepsilon < Y < y | X).
	\end{align*}
	%
	Provided we are working with a regular probability measure, this means that for each $\omega$, the function $y \mapsto \EE(|Y - y| | X = X(\omega))$ is right differentiable, with derivative $\PP(Y \geq y | X = X(\omega)) - \PP(Y < y | X = X(\omega))$. In particular, a choice of $y$ which minimizes $\EE(|Y - y| | X = X(\omega))$ must satisfy
	%
	\[ \PP(Y \geq y | X = X(\omega)) = \PP(Y < y | X = X(\omega)) = 0.5. \]
	%
	If $f(X)$ is a function such that almost surely,
	%
	\[ \PP(Y \geq f(X) | X) = \PP(Y < f(X) | X) = 0.5 \]
	%
	then we say it is a \emph{conditional median}, and it will be a pointwise minimizer of the expected loss. We normally denote a conditional median by $\MM(Y | X)$. Unlike the conditional expectation, the conditional median need not be unique if the underlying distribution of $X$ is not continuous.
\end{example}

\begin{example}
	Suppose that the values of $Y$ lie in some discrete set of values. The problem of regression in this setting is normally called \emph{classification}. A natural loss function to use here is the 0-1 loss function
	%
	\[ L(y,f(x)) = \mathbf{I}(y \neq f(x)). \]
	%
	As with the previous examples, the regression function in this setting can be easily proved to be
	%
	\[ \mathbf{B}(Y|X) = \argmin \PP(Y = y | X). \]
	%
	This is known as the \emph{Bayes classifier}. The value $\text{EPE}(\mathbf{B}(Y|X))$ is known as the \emph{Bayes rate}.
\end{example}

Even in these examples, where we can calculate an explicit formula for the regression function, in practice we cannot compute the regression function from sample data. Thus we must come up with an approximation $\widehat{f}$ of the regression function $f(x)$, where for each $x$, $\widehat{f}(x)$ is a random variable determined by the data $(X_1,Y_1), \dots, (X_N,Y_N)$. Such a random variable is known as a \emph{statistic}.

There is a very useful decomposition result for the expected prediction error of $\widehat{f}$, where $f(x) = \EE(Y|X=x)$. We can write $Y = f(X) + \varepsilon$, where $\varepsilon$ has mean zero and variance $\sigma^2$. If we define the \emph{bias}
%
\[ \text{Bias} \left( x \right) = \EE \left( \widehat{f}(x) \right) - f(x). \]
%
then
%
\begin{align*}
	\text{MSE}(f) &= \EE[(Y - f(X))^2] + \EE[(f(X) - \widehat{f}(X)]^2)\\
	&= \sigma^2 + \mathbf{V} [\widehat{f}] + \EE \left[ \text{Bias}(X)^2 \right].
\end{align*}
%
This is referred to as the {\it Bias-Variance decomposition}. The error $\sigma^2$ is unavoidable for any function $\widehat{f}$, whereas to minimize the mean square error, we must make a tradeoff between making the variance of our estimator small, and the bias small.

\section{Parametric Regression}

In parametric statistics, we are given independent and identically distributed data $\{ (X_1,Y_1), \dots, (X_N,Y_N) \}$. We do not know the distribution they are drawn from, but we \emph{do} know the distribution lies in some

\section{Linear Regression}

In some case, we assume our regression functions take the form $\beta^* X$, where $\beta^* \in (\mathbf{R}^n)^*$. Given the data $(X_1, Y_1), \dots, (X_k, Y_k)$, we determine the best estimate $\widehat{\beta}$ of $\beta^*$ by evaluating it against the loss function $\mathcal{L}(\beta) = \mathbf{E}[(Y - \beta X)^2]$. Of course, we cannot calculate $\mathcal{L}$ directly, but we may estimate it with our samples. Because the loss function is a differentiable function of $\beta$, we may take derivatives to determine $\beta$:
%
\[ \nabla \mathcal{L} (\beta) = 2 \mathbf{E}[(Y - \beta X)X^T] = 2 \mathbf{E}(YX^T) - 2\beta \mathbf{E}(XX^T) \]
%
This is optimized when the derivative of this function is zero. i.e., when
%
\[ \beta \mathbf{E}(XX^T) = \mathbf{E}(YX^T) \]
%
Assuming $\mathbf{E}(XX^T)$ is invertible, we may invert, and determine that the optimal value $\beta^*$ can be calculated as
%
\[ \beta^* = \mathbf{E}(YX^T) \mathbf{E}(XX^T)^{-1} \]
%
Now if we only have the samples $(X_i, Y_i)$, we may approximate this value by forming the conglomerate matrices $\mathbf{X} = (X_1 | X_2 | \dots | X_n)$ and $\mathbf{Y} = (Y_1, \dots, Y_n)$, and calculating $\widehat{\beta} = \mathbf{Y} \mathbf{X}^T (\mathbf{X} \mathbf{X}^T)^{-1}$. This minimizes the error over the training data set $\sum (Y_i - \beta X_i)^2 = \|\mathbf{y} - \beta \mathbf{X}\|^2$.

How do we estimate how accurate our prediction is. First, assume that each $Y_i$ is independant, with the same variance $\sigma^2$. Then
%
\[ \mathbf{V}(\widehat{\beta}) = \mathbf{V}(\mathbf{Y} \mathbf{X}^T (\mathbf{X} \mathbf{X}^T)^{-1}) \]

INSERT THEORETICAL ESTIMATES, Gauss Markov theorem, etc.





\section{Additive Models}

A generalized additive model has a regression function of the form
%
\[ \mathbf{E}(Y|X) = \alpha + f_1(X^1) + \dots + f_n(X^n) \]
%
where the $f_i$ are unspecified smooth ($C^\infty$) functions, and $X = (X^1, \dots, X^n)$ is a random vector. To fit an additive model, given a sample $(X_1, Y_1), \dots, (X_m, Y_m)$, we take as a cost function the penalized sum of squares to find the constant $\alpha$ and functions $f_i$,
%
\[ \sum_{i = 1}^m \left( Y_i - \alpha - \sum_{j = 1}^n f_j(X_i^j) \right)^2 + \sum_{j = 1}^m \lambda_j \int (f''_j)^2 \]
%
Where the $\lambda_j \geq 0$ are arbitrary parameters. The minimizer of this cost function is not unique -- it is standard convention to require that $\sum_{i,j} f_i(X_i^j) = 0$. One can apply an iterative cubic smoothing spline solution to find this minimum.

\section{Tree-Based Models}

Tree based methods partition the feature space , and then fit a simple model (normally a constant) into each one. If $\mathcal{S}$ is such a space, and we partition it into $S_1, \dots, S_n$, each with a model $f_1, \dots, f_n$, then are model is
%
\[ \mathbf{E}(Y|X) = \sum_{i = 1}^n f_i(X) [X \in S_i] \]
%
If $S_i$ has already been decided, and we are using constants for the $f_i$, then the best choice of constants (according to the least squares cost function) given a sample $(X_1, Y_1), \dots, (X_n, Y_n)$ are just the mean values of the $Y_j$ with $X_i \in S_i$. The questions remains, however, on how to choose our partitions.

To find optimal partitions, we assume our feature space is $\mathbf{R}^n$, and our regions formed by `binary splits'. We start with the whole space, pick a `splitting coordinate' $i$ and `splitting point' $t \in \mathbf{R}$, and partition our region into two sets $A = \{ x \in \mathbf{R}^n : x_i < t \}$ and $B = \{ x \in \mathbf{R}^n : x_i \geq t \}$. We then recursively partition $A$ and $B$ up in this manner, until we are satisfied with our splits.

Finding the best choice of partition using the method above is generally computationally infeasible. We shall proceed with a greedy approximation. Given a region $A$ containing features $X_1, \dots, X_n$, we seek to find a splitting variable $i$ and split point $t$ which minimize the cost function
%
\[ \argmin_{i,t} \min_a \sum_{X_j^i \leq t} (Y_j - a)^2 + \min_b \sum_{X_j^i > t} (Y_j - b)^2 \]
%
Given $i$ and $t$, the minimum values of $a$ and $b$ are just obtained by taking the mean of the results $Y_j$. By doing a linear scan on each coordinate, it is fairly simple to find $i$ and $t$. Then we recursively perform this greedy approach on the subpartitions.

Now when do we stop splitting? If we split far enough, then we will only have very few examples in each subregion, and we will have overfitted our training data! Furthermore, it will be very difficult to interpret the model we have created.

\chapter{Neural Networks}

Neural Networks arise from the solution of a certain model, known as the Projection Pursuit Regression model. Assume we have an input vector $X \in \mathbf{R}^n$, with target $Y$. The projection pursuit regression model has the form
%
\[ f(X) = \sum_{i = 1}^M g_i(\beta_i X) \]
%
Where the $g_i$ are unspecified, and $\beta_i \in (\mathbf{R}^n)^*$. This is an additive model, but in the features $V_i = \beta_i X$. Each $g_i(\beta_i X)$ is called a ridge function in $\mathbf{R}^n$.

This model is really general. For instance, the product of the coordinates can be written
%
\[ X_1X_2 = \frac{(X_1 + X_2)^2 - (X_1 - X_2)^2}{4} \]
%
In fact, if we let $M$ be large enough, for appropriate choices of $g_i$ can approximate arbitrary continuous functions on $\mathbf{R}^n$ (this model is a universal approximator). Unfortunately, this means this model will be hard to fit exactly, and thus the model is better for estimating data rather than obtaining an understandable model.

Given some data $(X_1, Y_1), \dots, (X_n, Y_n)$ from the regression model, we thus seek the minimize the error
%
\[ \sum_{i = 1}^n \left(y_i - \sum_{j = 1}^M g_j(\beta_j x_i) \right) \]
%
as a choice of $g_j$ and $\beta_j$. We need to impose constraints on $g_j$ to prevent overfitting.

Suppose we have $M = 1$, and that $g = g_1$ is differentiable, and $\beta = \beta_i$ is the linear functional. Then, taking the initial terms around the taylor series,
%
\[ g(\beta x_i) \approx g(\alpha x_i) + g'(\alpha x_i) (\alpha - \beta)x_i \]
%
\begin{align*}
    \sum_{i = 1}^n [y_i - g(\beta x_i)]^2 &\approx \sum_{i = 1}^n [y_i - g(\alpha x_i) - g'(\alpha x_i)(\alpha - \beta)x_i]^2\\
    &= \sum_{i = 1}^n g'(\alpha x_i)^2 \left[ \alpha x_i +  \frac{y_i - g(\alpha x_i)}{g'(\alpha x_i)} - \beta x_i \right]^2
\end{align*}
%
We can minimize the right-hand side by carrying out a least squares regression with target
%
\[ \alpha x_i + \frac{y_i - g(\alpha x_i)}{g'(\alpha x_i)} \]
%
We can then iterate this regression until convergence. With more than one term in the model, we just perform forward stage-wise reggresion.

The projection pursuit regression model has not been widely used in the field of statistics, possibly due to the lack of computational resources when it was created. Nonetheless, it leads to the field of neural networks, which are much more useful.

\section{Nets}

There is a lot of mysticism surrounding neural networks (perhaps for the same reason `the god particle' is so controversial) but they are really just non-linear statistical models. Here we will discuss the most basic kind of neural nets, the single hidden layer back-propogation network.

Suppose we are given a set of inputs $X = (X_1, \dots, X_n)$. A neural net creates layers of derived features $Z = (Z_1, \dots, Z_m)$ as affine transformations of the $X_i$, `flattened' by some activation function $\sigma$. In the single layer approach, we have one layer of these derived features, and then these derived features are used to generate the target $Y = (Y_1, \dots, Y_k)$ as a function of the $Z_i$, again modified by an output function. In terms of formulas, our mathematical model is
%
\[ Z = \sigma(\Lambda X + v)\ \ \ \ \ W = \Delta Z + w\ \ \ \ \ f(X) = g(W) \]
%
where $\Lambda$ and $\Delta$ are linear transformations, and our regression function is $f$.

For regression, we normally choose not to modify our outputs via an output function (that is, we let $g = \mathbf{1}$). For classification, we need to choose an output function which results in reasonable results. The sigmoid function is often chosen as the activation, $\sigma(t) = (1 + e^{-t})^{-1}$. Sometime Gaussian radial basis functions are used, producing a radial basis function network. Note that if we let $\sigma$ and the output regularization function be the identity, we obtain a linear model. Thus in this way, a neural network is a generalization of the linear model.

\chapter{Bayesian Networks}

Let $X,Y,Z$ be random variables. We say that $X$ and $Y$ are \emph{conditionally independent} given $Z$, if for any measurable $A,B \subset \mathbf{R}$,
%
\[ \mathbf{P}(X \in A,Y \in B|Y,Z) = \mathbf{P}(X \in A|Z)\mathbf{P}(Y \in B|Z) \]
%
This just means that once you know $Z$, you can gain no information about $X$ from information about $Y$. Bayesian networks are a model of information which allow us to measure the conditional independence of random variables.

Given a set $X_1, \dots, X_n$ of random variables, suppose we form a directed, acyclic graph whose certices are the random variables. We say the variables are \emph{Markov} with respect to the graph if for any random variable $X_i$,
%
\[ \mathbf{P}(X_1 \in A_1, \dots, X_n \in A_n) = \prod_{i = 1}^n \mathbf{P}(X_i \in A_i | \text{parents}(X_i)) \]
%
In other words, this means exactly that $X_i$ is independent of all variables once we condition on the parents of $X_i$.















\part{Computational Learning Theory}

Computational Learning Theory is the study of Machine Learning, from the perspective of theoretical computing science. In the study of computational learning theory, we specify learning models, which limit the \emph{complexity} of the kinds of algorithms one can use to study data in machine learning. Since we can often obtain huge inputs in statistics, we often limit our study to data with low \emph{sample complexity}, i.e. the minimum number of samples required to solve an algorithm, up to a certain degree of error. We might also naturally limit algorithms in \emph{time complexity}, since, despite a problem having low sample complexity, there might not exist an efficient algorithm to analyze a given set of data. We might also limit the \emph{hypothesis complexity}.

\begin{comment}

Common Machine Learning Tasks:
	- Classification / Clustering (e.g. in Text Categorization, Fraud Detection, Web Search)

	- Prediction: (e.g. in Weather, Financial Market Prediction, Earthquake Prediction, Handwriting Recognition)

Overview of CLT:

		E.g. specifying the assumptions about the input data. We could consider randomly chosen input, or adversarial learning, or a teacher giving you the best answers, or a query based system.

		- Learner makes queries (Active Learning)
			- Learner chooses x, get f(x) (Membership Query)
			- Learner poses hypotheses, and ges counterexamples

		- Learner recieves data (passive Learning)


		Is the data every noisy?

	- Performance Criterion:
		- Depends on whether we are doing Batch / Offline Learning, or Online Learning.
		- How do we measure accuracy?

	We will focus on:
		- Online Mistake-Bound Model
		- PAC Learning
		- Query Models (SQ, MQ, EQ)

	Analyze Specific Learning Algorithms:
		- Necessary + Sufficient Conditions for Learning
		- General theory on necessary conditions on sample complexity, ad hoc methods on time complexity of algoriths.
		- Learning from Noisy Data
		- Boosting (Repeating an algorithm with 51\% accuracy to get an algorithm with 99\% accuracy)
		- Comparing Learning Models

Let X be a set, and C a *concept class* of functions c: X -> {0,1}.

Examples:
	- Set of all monotone conjuctions c(x) = x_{i_1} ^ ... ^ x_{i_k}. Then C has cardinality 2^n. We assume the conjunction corresponding to the empty set is one everywhere.

	- C is the set of all literals (a variable or it's negation). Then C has cardinality 3^n.

	- C is the set of all DNF formulas with at most n^2 terms (replace n^2 with 2^n and you can model any function).

	- C is the set of all k-DNF formulas (DNF where each term has at most k variables).

	- C is the set of all linear threshold functions ( c(x) = 1 if and only if w * x >= theta for some fixed theta and w ).
		E.g. Majority function Maj: { 0,1 }^n -> { 0,1}

	- C is the set of all initial intervals of X = { 0, ..., 2^n - 1 }, i.e. indicators of all intervals [0,n].

We describe the Online Mistake Bound Learning Model. This came about in the 80s, whereas the more modern and general *regret* bounded learning model was developed later.
	- At each stage of the algorithm, a learning maintains a hypothesis function h: X -> {0,1} (which does not need to be in the concept class C).
	- Learner is given x in X. If h(x) does not equal c(x), they lose a point.
	- Learner can now update h given this information.
The goal is to minimize the # of mistakes, *in the worst case*.



For any finite set X, one can always choose a mistake bound of |X| (a much more sophisticated argument shows that at most log |X| mistakes can be made)
Also we can always choose a strategy to make at most |C| - 1 mistakes.

If C is the set of all initial intervals of { 0, ..., n }, then despite the fact that C has n elements, there is an algorithm that in the worst case incurs at most log n mistakes. Every time we observe a 1, we know there are 1s to the left, and if we observe a 0, there are 0s to the right. Whenever we observe x, we return the answer we know to be correct, or if we do not know the answer, we choose the answer that, if wrong, allows us to learn more 0s and 1s. That way, each time we are wrong, we half the amount of unknown values we do not know.

If X = [0,1], then in the worst case, we make infinitely many mistakes. We have to change the model to discuss learning in here.




Now let's talk about the *elimination* algorithm for the online learning of (monotone) disjunctions. We maintain a hypothesis of which variables are in the disjunction, starting with the hypothesis h that *all* variables are in the disjunction. For each observation x_0, we see the following result:

	- If h(x) = 1 and c(x) = 0:
		Remove from h all variables that are equal to one in x_0.

	- If h(x) = c(x), don't change anything.

	We will *never* see that h(x) = 0 and c(x) = 1.

The mistake bound here is precisely n

A similar algorithm works for all (not necessarily monotone) disjunctions. We start with the hypothesis

x_1 v -x_1 v x_2 v -x_2 ... x_n v -x_n

Then remove all variables we can when we make a mistake. The mistake bound for this algorithm is n+1, since after a single mistake, we can cross off n literals in the disjunction, and we essentially have the same problem as the monotone disjunctions.




Decision List: Decision Tree which can only branch to the right (branches to the left always terminate)

Algorithm to learn length r decision lists with mitsake bound O(nr). This is non-optimal: The most optimal is O(log n * r). But it is unknown whether an algorithm can do this in *polynomial time*.

To determine the algorithm: we maintain a *generalized Decision List*. Each level can have multiple possible if / else statements, so that the list doesn't even define a Boolean function. We start with a generalized decision list that has only one level, and returns both zero and one for each variable observed. Each time we see an observation, we pick an arbitrary decision list that the generalized decision list contains. If we make a mistake, we take all possible variables that could be used to make the mistake, and shift then to the right. Since our generalized decision list always has n terms, After nr mistakes, we would observe that all possible mistakes have been shifted as far to the right as needed to see that the generalized decision list represents a unique decision list.











Next Time: First Interesting Algorithm to Learn Conjunctions

\end{comment}














\end{document}