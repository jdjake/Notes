\input{../../style.tex}

\title{High Dimensional Probability}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents
\pagenumbering{arabic}

\chapter{Introduction}

In these notes, we study the problems and phenomena that arise when studying random phenomena in high dimensional spaces. These phenomena arise from numerous situations, including when studying large random graphs, large random matrices, or doing statistics with large data sizes. A few informal principles guide our exploration of the subject
%
\begin{itemize}
    \item (Concentration): The law of large numbers gives the asymptotic result that if $\{ X_k \}$ is a sequence of i.i.d random variables, then
    %
    \[ \frac{1}{n} \sum_{k = 1}^n X_k - \mathbf{E} \left( \frac{1}{n} \sum_{k = 1}^n X_k \right) \to 0 \]
    %
    almost surely. But in many cases in analysis we need non-asymptotic results, which replace this limit theorem with precise {\it deviation bounds} which provide upper bounds on
    %
    \[ \mathbf{P} \left[ \frac{1}{n} \sum_{k = 1}^n X_k - \mathbf{E} \left( \frac{1}{n} \sum_{k = 1}^n X_k \right) \geq t \right] \]
    %
    which decay fast with respect to $t$. For the general class of {\it subgaussian} random variables, one can obtain very fast decaying bounds on this limit process. Similar results hold if the $X_k$ are only weakly dependant on one another. More generally, if $f: \mathbf{R}^n \to \mathbf{R}$ is not `too sensitive' with respect to any of it's coordinates, then we should be able to obtain sharp bounds on
    %
    \[ \mathbf{P} \left(|f(X_1, \dots, X_n) - \mathbf{E} f(X_1, \dots, X_n) \right| \geq t) \]
    %
    when the $X_k$ are subgaussian. We note that concentration estimates the fluctuations of $f$, but not it's magnitude. We require other tools to compute $\mathbf{E} f(X_1, \dots, X_n)$. Though concentration holds for very general functions $f$, we cannot hope to find general methods of calculating $\mathbf{E} f(X_1, \dots, X_n)$ for general functions.

    \item (Controlling Suprema) It is often natural to control the expected magnitude of a family of random variables, i.e. we wish to control $\mathbf{E} \sup_{t \in T} X_t$, where $t$ is some index set.  A natural principle is that if the random field $\{ X_t \}$ is `sufficiently continuous', the magnitude is controlled by the `complexity' of the index set $T$.

    \item (Universality) The central limit theorem says that for large $n$, if the $X_i$ are independant then the CDF of the random variable
    %
    \[ \frac{1}{\sqrt{n}} \sum_{k = 1}^n (X_k - \mathbf{E}(X_k)) \]
    %
    behaves like the CDF of a Gaussian distribution. The fact that this is true irrespective of the distribution of the components $X_k$ is known as {\it universality}. In general, universality refers to the features of the components of a distribution becoming irrelevant when $n$ is large. Another way to state this is that if $f$ is a `sufficiently smooth function' and $n$ is large, then $\mathbf{E}(f(X_1, \dots, X_n))$ is insensitive to the distributions of the $X_k$. This means that high dimensional phenomena we study are robust to the precise details of the model we approximate them with. Universality is very useful because it allows us to replace $X_k$ with very well behaved distributions, i.e. Gaussian distributions. We note that universality is not necessarily related to a Gaussian distribution, but Gaussian distributions do tend to show up with high dimensional phenomena.

    \item (Sharp Transitions) The last understood principle is given by sharp transitions. In high dimensional models, as we vary parameters there tends to be abrupt changes in the qualitative phenomena. As an example, if $\{ X_k \}$ is a sequence of $\{ 0, 1 \}$ valued Bernoulli random variables with parameter $p$, and $Z_n$ is the majority function of $X_1, \dots, X_n$, then $\mathbf{E}(Z_n) \to 0$ if $p < 1/2$, and $\mathbf{E}(Z_n) \to 1$ if $p > 1/2$. As $n \to \infty$, there is an abrupt change in the behaviour of the $Z_n$ as we vary $p$. In some cases, this can be explained by concentration phenomena. But this occurs even in cases that cannot be explained using concentration. In general, if $f(E_1, \dots, E_n)$ is `sufficiently symmetric' and `sufficiently monotone', with $\{ E_k \}$ events depending on a probability $p$, then $f(E_1, \dots, E_n)$ undergoes a `sharp transition'.
\end{itemize}

We often use asymptotic notation. We write $A \lesssim B$, for two positive quantities $A$ and $B$ which depend on various parameters, if $A \leq C \cdot B$ for some positive constant $C$. All constants involved in our results are {\it effective}, in the sense that one can feasibly calculate them by following through our arguments with some patience, and none are of exceedingly large magnitude.











\chapter{Concentration In High Dimensional Spaces}

A basic instance of high dimensional probability occurs when studying random vectors $X \in \mathbf{R}^n$, where $n$ is a very large number. The exponential increase in room in high dimensions leads to concentration of the vector in unlikely places. If $X$ is a random standard Gaussian vector in $\mathbf{R}^n$, then
%
\[ \EE |X|^2 = \sum \mathbf{E} X_i^2 = n \]
%
Since $|X|$ is formed from $n$ independant random variables, each having equal contribution to the magnitude of $|X|$, we could guess that when $n$ is large, $|X|$ is close to $\sqrt{n}$ with high probability. And this is certainly the case. Indeed, since $|X|^2$ is a sum of independant subexponential random variables, and it has mean $n$ and standard deviation $O(\sqrt{n})$, then we should expect $|X|^2 = n + O(\sqrt{n})$ with high probability, and if this is true then
%
\[ |X| = \sqrt{n + O \left(\sqrt{n}\right) } = \sqrt{n} + O(1). \]
%
Thus $|X|$ should deviate from $\sqrt{n}$ by a constant distance, independant of $n$. This is precisely the content of the next theorem.

\begin{theorem} \label{concentrationNorm}
    Let $X$ be a random vector in $\RR^n$ with independant coordinates, and with $\| X_i \|_{\psi_2} \leq K$ and $\mathbf{E}(X_i^2) = 1$ for each $i$. Then $\| |X| - \sqrt{n} \|_{\psi_2} \lesssim K^2$.
\end{theorem}
\begin{proof}
	Without loss of generality, we assume $K \geq 1$. In general, since
    %
    \[ 1 = \mathbf{E}(X_i^2) \lesssim \| X_i \|_{\psi_2}^2 \leq K^2, \]
    %
    we know $K \gtrsim 1$. Thus if $K \leq 1$, we can apply the theorem with $K = 1$ to obtain that
    %
    \[ \| |X| - \sqrt{n} \|_{\psi_2} \lesssim 1 \lesssim K^2, \]
    %
    so the theorem is obtained for free in this case.

    The random variables $X_i^2$ are subexponential, with
    %
    \[ \| X_i^2 \|_{\psi_1} = \| X_i \|_{\psi_2}^2 \leq K^2. \]
    %
    By centering, we know $\| X_i^2 - 1 \|_{\psi_1} \lesssim \| X_i^2 \|_{\psi_1}$. Thus we can apply Bernstein's inequality. This gives a universal constant $c$ such that
    %
    \begin{align*}
        \PP \left( \big||X|^2 - n \big| \geq t \right) &\leq 2 \exp \left( -c \cdot \min \left( \frac{t^2}{\sum \| X_i \|_{\psi_2}^4}, \frac{t}{\max \| X_i \|_{\psi_2}^2} \right) \right)\\
        &\leq 2 \exp \left( -c \cdot \min \left( t^2 / K^4 , t/K^2 \right) \right)\\
        &\leq 2 \exp(- c/K^4 \cdot \min(t^2,t)).
    \end{align*}
    %
    Here we used the fact that $K \geq 1$, so that $1/K^2 \geq 1/K^4$. The inequality above is a good concentration bound for $|X|^2$, and we now need to turn it into a concentration bound for $|X|$.

    Given any $t \geq 0$, if $u = \min(t,t^2)^{1/2}$, then $t = \max(u,u^2)$. Thus we have shown
    %
    \[ \PP \left( \big||X|^2 - n \big| \geq \max(u,u^2) \right) \leq 2 \exp \left( - cu^2/K^4 \right). \]
    %
    If $u$ is fixed, and $||X| - n^{1/2}| \geq u$, then we conclude
    %
    \[ ||X|^2 - n| = ||X| - n^{1/2}|||X| + n^{1/2}| \geq u \cdot (|X| + n^{1/2}). \]
    %
    We either have $|X| \geq n^{1/2} + u$, or $|X| \leq n^{1/2} - u$. The former case implies
    %
    \[ ||X|^2 - n| \geq u \cdot (2n^{1/2} + u) \geq \max(u,u^2). \]
    %
    If the latter case holds, we must have $u \leq n^{1/2}$, so
    %
    \[ ||X|^2 - n| \geq u \cdot n^{1/2} \geq \max(u,u^2). \]
    %
    In both cases, $||X|^2 - n| \geq \max(u,u^2)$. Thus for any $u \geq 0$,
    %
    \begin{align*}
        \PP \left( \big| |X| - n^{1/2} \big| \geq u \right) &\leq \PP \left( \big| |X|^2 - n \big| \geq \max(u,u^2) \right) \leq 2 \exp(-cu^2/K^4).
    \end{align*}
    %
    Since this holds for all $u \geq 0$, we have shown $\| |X| - n^{1/2} \|_{\psi_2} \lesssim K^2$.
\end{proof}

This theorem contradicts our intuitions from low dimensional probability. We would expect a standard normal distribution in $\RR^n$ to lie near the origin, since that is where the density function is largest. But the volume near the origin is negible in high dimensions, which means that in fact, a normal distribution is not likely to lie near the origin at all, and for most purposes acts distributionally the same as a uniformly random vector chosen on the sphere of radius $n^{1/2}$.

\begin{corollary}
    If $X$ is as in Theorem \ref{concentrationNorm}, then
    %
    \[ \EE |X| = n^{1/2} + O(K^2)\quad\text{and}\quad\Var |X| = O(K^4). \]
\end{corollary}
\begin{proof}
    To prove the expectation bound, we first apply centering to the random variable $|X| - n^{1/2}$. Thus we know
    %
    \[ \left\| |X| - \EE |X| \right\|_{\psi_2} \lesssim \| |X| - n^{1/2} \|_{\psi_2} \lesssim K^2. \]
    %
    Applying the triangle inequality now shows that
    %
    \[ \| \mathbf{E} |X| - n^{1/2} \|_{\psi_2} \leq \| \mathbf{E} |X| - |X| \|_{\psi_2} + \| |X| - n^{1/2} \|_{\psi_2} \lesssim K^2, \]
    %
    and the left hand side is proportional to $| \mathbf{E} |X| - n^{1/2} |$, which gives the result. The variance bound then follows easily, because
    %
    \begin{align*}
        \Var |X| &= \Var(|X| - n^{1/2})\\
        &= \EE((|X| - n^{1/2})^2) - \EE(|X| - n^{1/2})^2\\
        &\lesssim \| |X| - n^{1/2} \|_{\psi_2}^2 - O(K^4) \lesssim K^4. \qedhere
    \end{align*}
\end{proof}

Later on, we will show that if $A$ is an $m \times n$ matrix, and $X$ is as in Theorem 2.1, then
%
\[ \left\| |AX| - \| A \|_F \right\|_{\psi_2} \lesssim K^2 \cdot \| A \| \]
%
where $\| A \|_F$ is the Frobenius norm of $A$, and $\| A \|$ it's operator norm. Taking $A$ as the identity transformation yields Theorem 2.1 as a special case.

\section{Isotropic Vectors}

We say a random vector $X$ in $\RR^n$ is \textbf{isotropic} if it's second moment matrix $\Sigma(X) = \EE(XX^T)$ is the identity matrix. Note that for any $x \in \RR^n$,
%
\[ \EE((X \cdot x)^2) = \EE((x^T X)(X^T x)) = x^T \EE(XX^T) x = x^T \Sigma(X) x \]
%
Thus being isotropic is equivalent to saying $\EE((X \cdot x)^2) = |x|^2$. Thus the vector $X$ is on average extended evenly in all directions.

It is often natural to assume random vectors under analysis are centered, and isotropic. And often one can reduce to this case. For any random vector $X$ with mean $\mu$, the random vector $Y = \text{Cov}(X)^{-1/2}(X - \mu)$ is centered and isotropic. To see this, we may assume without loss of generality that $\mu = 0$. Then for any $y \in \RR^n$,
%
\begin{align*}
	\Sigma(Y) = \EE(Y Y^T) &= \text{Cov}(X)^{-1/2} \cdot \EE(X X^T) \cdot \text{Cov}(X)^{-1/2}\\
	&= \text{Cov}(X)^{-1/2} \cdot \text{Cov}(X) \cdot \text{Cov}(X)^{-1/2} = I_n.
\end{align*}
%
If $\text{Cov}(X)$ is degenerate, then $X$ almost surely lies on a lower dimensional subspace of $\RR^n$, and we can then reduce our analysis to this lower dimensional subspace, where the corresponding covariance matrix is non-degenerate.

%We recall some basic facts about random vectors. Given a centrally distributed random vector $X$ in $\mathbf{R}^n$, we define the $n \times n$ covariance matrix $\Sigma(X)$ by the formula $\Sigma(X)_{ij} = \mathbf{E}(X_iX_j)$. This is a symmetric, positive semi-definite matrix, since
%
%\[ x^T \Sigma(X) x = \mathbf{E}((x \cdot X)^2) \geq 0. \]
%
%The spectral decomposition theorem implies that there is a basis of normalized eigenvectors $u_1, \dots, u_n$ for $\Sigma(X)$, with non-negative eigenvalues $\lambda_1, \dots, \lambda_n$. We assume that they have been arrange so that the eigenvalues are placed in decreasing order. If $Y_i = u_i \cdot X$, then this means $\mathbf{E}(Y_i^2) = \lambda_i$, and $\mathbf{E}(Y_iY_j) = 0$ if $i \neq j$. Thus we can always rotate a distribution so it's coordinates are independant `up to second order'.

\begin{lemma}
    If $X$ is isotropic, then $\mathbf{E} |X|^2 = n$. More generally, if $X$ and $Y$ are independant and isotropic, then $\mathbf{E}(X \cdot Y)^2 = n$.
\end{lemma}
\begin{proof}
    We write
    %
    \begin{align*}
        \mathbf{E} |X|^2 &= \mathbf{E} X^T X = \mathbf{E} \left( \text{tr}(X^T X) \right)\\
        &= \mathbf{E} \left( \text{tr}(XX^T) \right) = \text{tr}(\mathbf{E}(XX^T)) = \text{tr}(I) = n.
    \end{align*}
    %
    Next, given $Y$, we find
    %
    \begin{align*}
        \mathbf{E}((X \cdot Y)^2 | Y) = \sum Y_iY_j \mathbf{E}(X_iX_j|Y) = \sum Y_iY_j \mathbf{E}(X_iX_j) = \sum Y_i^2 = |Y|^2.
    \end{align*}
    %
    But this means that
    %
    \[ \mathbf{E}((X \cdot Y)^2) = \mathbf{E}(\mathbf{E}((X \cdot Y)^2 | Y)) = \mathbf{E} |Y|^2 = n. \qedhere \]
\end{proof}

\begin{remark}
	Lemma 2.3 implies that for any two independant isotropic random vectors $X$ and $Y$,
    %
    \[ \EE(X \cdot Y) = \sum \EE(X_i) \EE(Y_i) = 0\quad\text{and}\quad \Var(X \cdot Y) = \EE((X \cdot Y)^2) = n, \]
    %
    we can expect that $X \cdot Y = O(n^{1/2})$ with high probability. But combined with the fact that $|X|$ and $|Y|$ are $O(n^{1/2})$ with high probability, this means that with high probability
    %
    \[ \frac{X \cdot Y}{|X| |Y|} = \frac{O(n^{1/2})}{O(n^{1/2}) O(n^{1/2})} = O \left(1/n^{1/2} \right). \]
    %
    Thus independant isotropic vectors in high dimensional spaces tend to lie almost at right angles to one another. This is very different from the intuitive, two dimensional cases, where two independant unit vectors chosen uniformly at random on the unit circle are on average $45^\circ$ from one another.
\end{remark}

\begin{example}
    Let $X$ be a vector chosen uniformly at random on the sphere of radius $\sqrt{n}$ in $\RR^n$. For $i \neq j$, $(X_i,X_j)$ is identically distributed to $(X_i,-X_j)$, so $\mathbf{E}(X_iX_j) = - \mathbf{E}(X_iX_j)$. This implies $\mathbf{E}(X_iX_j) = 0$. Since $\mathbf{E} |X|^2 = n$, and the $\mathbf{E} |X_i|^2$ are independant of $i$, this implies $\mathbf{E} |X_i|^2 = 1$ for each $i$. Thus $X$ is isotropic.
\end{example}

It is good to remember that the coordinates of an isotropic vector need not be independant. A uniformly random point $X$ on the radius $\sqrt{n}$ sphere need not be independant, because the points must satisfy $X_1^2 + \dots + X_n^2 = 1$.

\begin{example}
    Let $X$ be a random vector with independent, symmetric Bernoulli distributions as coordinates. Then $\EE(X_iX_j) = \EE(X_i) \EE(X_j) = 0$ for $i \neq j$, and $\EE(X_i^2) = 1$. More generally, any random vector with independant, mean zero, unit variance coordinates are isotropic. This includes the example of a random vector $X \sim N(0,I_n)$ with the standard, normal distribution.
\end{example}

Isotropic random variable need not be centered.

\begin{example}
    Pick a random vector uniformly at random from $\{ n^{1/2} \cdot e_1, \dots, n^{1/2} \cdot e_n \}$. Then for each index $i$, $X_i^2$ is $\{ 0,n \}$ valued, with $\mathbf{P}(X_i^2 = n) = 1/n$. This gives $\mathbf{E}(X_i^2) = 1$. On the other hand, $X_iX_j = 0$ for $i \neq j$, so $\mathbf{E}(X_iX_j) = 0$. Note that the mean of $\mathbf{E}(X_i)$ is non-zero; it is actually equal to $1/\sqrt{n}$.
\end{example}

We obtain a family of discrete isotropic random vectors by considering uniformly distributions over discrete families of vectors used most notably in signal processing, known as {\bf frames}. A {\bf frame} in $\RR^n$ is a set $\{ v_1, \dots, v_m \}$ for which there are positive constants $A$ and $B$ for which the approximate Parseval's identity $A |x|^2 \leq \sum (v_i \cdot x)^2 \leq B |x|^2$ holds for all vectors $x \in \RR^n$. if $A = B$, the frame is called {\bf tight}. A frame is tight if and only if $\sum v_i^T v_i = A I_n$, and more generally, it is a frame with constants $A$ and $B$ if and only if $A I_n \preceq \sum v_i^T v_i \preceq B I_n$.

\begin{example}
    An example of a tight frame which isn't an orthonormal basis is the `Mercedes Benz' frame, three uniformly separated points on the unit circle in the plane. If
    %
    \[ v_1 = (1,0),\quad v_2 = \left( -1/2, \sqrt{3}/2 \right)\quad\text{and}\quad v_3 = \left( -1/2, -\sqrt{3}/2 \right), \]
    %
    then
    %
    \[ v_1^T v_1 = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix},\quad v_2^T v_2 = \begin{pmatrix} 1/4 & -\sqrt{3}/4 \\ - \sqrt{3}/4 & 3/4 \end{pmatrix} \]
    \[ \text{and}\quad v_3^T v_3 = \begin{pmatrix} 1/4 & \sqrt{3}/4 \\ \sqrt{3}/4 & 3/4 \end{pmatrix}. \]
    %
    This means $v_1^T v_1 + v_2^T v_2 + v_3^T v_3 = 3/2 \cdot I_2$, so the frame is tight.
\end{example}

\begin{theorem}
    If $\{ u_1, \dots, u_m \}$ is a tight frame, then a uniformly random choice of a frame element, scaled by $(m/A)^{1/2}$ is isotropic. Conversely, if $X$ is isotropic, and takes only finitely many values $\{ u_1, \dots, u_n \}$ with $\PP(X = u_i) = p_i$, then $\sqrt{p_i} \cdot u_i$ is a tight frame with $A = 1$.
\end{theorem}
\begin{proof}
    If $X = (m/A)^{1/2} \cdot \text{Uniform}(u_1, \dots, u_n)$, then
    %
    \[ \mathbf{E} (X \cdot x)^2 = \frac{(m/A) \sum (u_i \cdot x)^2}{n} = (1/A) A|x|^2 = |x|^2. \]
    %
    which means precisely that the frame is isotropic. To prove the converse, we find that
    %
    \[ \sum (p_i^{1/2} u_i \cdot x)^2 = \sum p_i (u_i \cdot x)^2 = \mathbf{E}((X \cdot x)^2) = |x|^2, \]
    %
    which is precisely the definition of a tight frame.
\end{proof}

\begin{example}
    Let $K$ be a convex set with nonempty interior in $\mathbf{R}^n$. If $X$ is a uniformly chosen point in $K$, which by translation we may assume to have mean zero, and covariance matrix $\Sigma$, then $\Sigma$ is positive definite, because if $\Sigma$ has a zero eigenvalue, there would be a vector $a$ such that $X$ is almost surely orthogonal to $a$, which is impossible since $K$ has non-empty interior. Thus $\Sigma^{-1/2} X$ is isotropic, and thus $\Sigma^{-1/2} K$ can be seen as a convex set `uniformly extended in each direction'. This is often useful as a preprocessing step before applying algorithms on convex sets.
\end{example}

\section{Uniformly Subgaussian Vectors}

We say a random vector $X \in \RR^n$ is \textbf{subgaussian} if $X \cdot x$ is subgaussian for all $x$, or equivalently, if all coordinates of $X$ are subgaussian. Then there is a smallest value $\| X \|_{\psi_2} < \infty$ such that $\| X \cdot x \|_{\psi_2} \leq \| X \|_{\psi_2} |x|$. We think of $\| X \|_{\psi_2}$ as a generalization of the subgaussian norm to random vectors.

\begin{example}
    Suppose $X$ has independant, subgaussian coordinate $X_1, \dots, X_n$. If $x = (x_1, \dots, x_n)$ satisfies $|x| = 1$, then
    %
    \[ \| X \cdot x \|_{\psi_2}^2 = \left\| \sum x_i X_i \right\|_{\psi_2}^2 \lesssim \sum x_i^2 \| X_i \|_{\psi_2}^2 \leq \max \| X_i \|_{\psi_2}^2. \]
    %
    Thus $\| X \|_{\psi_2} \lesssim \max \| X_i \|_{\psi_2}$. On the other hand, we know $\| X \|_{\psi_2} \geq \max \| X_i \|_{\psi_2}$, so in this case the subgaussian norm of the vector is essentially equal to the maximum subgaussian norm of it's coordinates. On the other hand, even if the coordinates of a random vector $X$ are individually subgaussian, if independence is not satisfied then we may have $\| X \|_{\psi_2} \gg \max \| X_i \|_{\psi_2}$. For instance, if $X_i = X_j$ for all $i = j$, then
    %
    \[ \| X \|_{\psi_2} = \sqrt{n} \cdot \max \| X_i \|_{\psi_2} \]
    %
    This is the maximal difference, since if $|x| = 1$, then
    %
    \[ \left\| \sum x_i X_i \right\|_{\psi_2} \leq \sum |x_i| \| X_i \|_{\psi_2} \leq \left( \sum |x_i| \right) \cdot \max \| X_i \|_{\psi_2} \leq \sqrt{n} \cdot \max \| X_i \|_{\psi_2}. \]
    %
    Since we often want bounds which are independant of dimension, this is not a useful bound in practice.
\end{example}

\begin{example}
    The isotropic random vector $X$ chosen uniformly randomly from $\{ \sqrt{n} \cdot e_k \}$ is subgaussian, but not \emph{quantitatively} subgaussian. Since
    %
    \[ \exp \left( X_k^2/t^2 \right) = \exp(n/t^2)/n, \]
    %
    we find
    %
    \[ \| X \|_{\psi_2} \geq \| X_k \|_{\psi_2} = \left( \frac{n}{\log 2n} \right)^{1/2} \gtrsim \left( \frac{n}{\log n} \right)^{1/2}. \]
    %
    This large norm makes the subgaussian property fairly useless in practice.
    %
    %We will also show that $\| X \|_{\psi_2} \lesssim (n/\log n)^{1/2}$. To do this, we must show there is a small constant $c$ such that for any $a_i$ with $\sum a_i^2 = 1$,
    %
    %\[ \mathbf{P} \left(\sum a_i X_i \geq \sqrt{n}t \right) \leq 2n^{-c t^2} \]
    %
    %If we assume $a_1 \geq a_2 \geq \dots \geq a_n$, then the discreteness of the random variable we are working with makes it sufficient to prove that
    %
    %\[ \mathbf{P} \left(\sum a_i X_i \geq \sqrt{n} \cdot a_k \right) = k/n \leq 2 n^{-ca_k^2} = 2 n^{-ca_k^2} \]
    %
    %This is equivalent to showing that $k \leq 2n^{1-ca_k^2}$. Since $1 \geq a_1^2 + \dots + a_k^2 \geq ka_k^2$, $a_k^2 \geq 1/k$, then it suffices to prove that $k \leq 2n^{1-c/k}$, or $\log k / (1 - c/k) \leq \log 2 + \log n$. But as $k \to \infty$,
    %
    %\[ \frac{\log k}{1 - c/k} = \log k (1 + O(c/k)) \leq \log n + c \cdot o(1) \]
    %
    %Thus there is $N$, independant of $n$ and $c$, such that the for $k \geq N$, $\log k (1 - c/k)^{-1} \leq \log n + c \log 2$. Choosing $c \leq 1$ completes the argument in this case. For $k = \{ 1, \dots, N \}$, we can just choose $c$ small enough that the inequality holds here, and becase $N$ is independant of $n$ and $c$, the choice of $c$ made here is independant of $n$. Thus the general inequality is established.
\end{example}

In fact, if $X \in \RR^n$ is an isotropic, discrete random vector with $\| X \|_{\psi_2} \leq 1$, then it must be supported on at least $e^{cn}$ points. Thus subgaussian random vectors are not quantifiably discrete.

\begin{theorem}
	There exists a universal constant $c$ such that if $X$ is a discrete, isotopic random vector in $\RR^n$ with support $S$, and $\| X \|_{\psi_2} \leq 1$, then $|S| \geq \exp(cn)$.
\end{theorem}
\begin{proof}
	Suppose that $|X| \leq A \cdot n^{1/2}$ for some constant $A$. Note that
	%
	\[ |X|^2 \leq \sup_{s \in S} |X \cdot s|. \]
	%
	Since $\| X \cdot s \|_{\psi_2} \leq A n^{1/2}$, we can use the expectation bound on the supremum of random variables to conclude
	%
	\[ n = \EE |X|^2 \leq \EE \sup_{s \in S} |X \cdot s| \leq A \cdot \sqrt{n \log |S|}. \]
	%
	Thus $|S| \geq \exp(n/A^2)$.

	It suffices to reduce the general case to the last case with a constant $A$ independant of $n$. Because $\| X \|_{\psi_2} \leq 1$, there is a universal constant $C$ such that for any $x$, $\EE (X \cdot x)^4 \leq C$. Let
	%
	\[ Y = X \cdot \mathbf{I}(|X|^2 \leq 4C n)\quad\text{and}\quad Y' = X \cdot \mathbf{I}(|X|^2 > 4 C n). \]
	%
	By Cauchy-Schwartz,
	%
	\begin{align*}
		\EE((Y' \cdot x)^2) &\leq (\EE((X,x)^4) \PP(|X|^2 > 4 C\cdot n) )^{1/2} \leq 1/2.
	\end{align*}
	%
	Thus $|x|^2/2 \leq \EE((Y \cdot x)^2) \leq |x|^2$, and so $I_n / 2 \preceq \Sigma(Y) \preceq I_n$. In particular, this means that
	%
	\[ I_n \preceq \Sigma(Y)^{-1} \preceq 2 \cdot I_n. \]
	%
	The vector $\Sigma(Y)^{-1/2} Y$ is isotropic, and $|\Sigma(Y)^{-1/2} Y|^2$ is upper bounded by $8 C \cdot n$. Thus we can set $A = \sqrt{8C}$.
\end{proof}

The uniform distribution on the sphere is an example of a well behaved subgaussian random variable for which the coordinates are not independant of one another.

\begin{theorem}
    If $X$ is chosen uniformly at random on $S^{n-1}$, $\| X \|_{\psi_2} \lesssim n^{-1/2}$.
\end{theorem}
\begin{proof}
    By rotation invariance, to bound $\| x \cdot X \|_{\psi_2}$, it suffices to bound $\| X_1 \|_{\psi_2}$. We also only need tail bounds for $X_1$ if $t < 1$, for they are trivial for $t \geq 1$. If we let $Z$ be a Gaussian vector, then $Z/|Z|$ is identically distributed to $X$. We know by the concentration of norm that there exists a small universal constant $c$ such that
    %
    \[ \PP(||Z| - \sqrt{n}| \geq t/2) \leq 2\exp(-ct^2)\quad\text{and}\quad \PP(Z_1 \geq t) \leq \exp(-ct^2). \]
    %
    Applying a union bound shows that
    %
    \begin{align*}
        \PP(X_1 \geq t/\sqrt{n}) &= \PP(Z_1/|Z| \geq t/\sqrt{n})\\
        &\leq \PP(||Z| - \sqrt{n}| \geq t/2) + \PP(Z_1/|Z| \geq t/\sqrt{n}, ||Z| - \sqrt{n}| \leq t/2)\\
        &\leq \PP(||Z| - \sqrt{n}| \geq t/2) + \mathbf{P}(Z_1 \geq t)\\
        &= 2 \exp(-ct^2) + \exp(-ct^2) = 3\exp(-ct^2).
    \end{align*}
    %
    This gives the subgaussian bound required.
\end{proof}

Note that the uniform distribution on the sphere is not isotropic. But if we scale by a factor of $n^{1/2}$, it becomes an isotropic distribution, and the resulting distribution has a subgaussian norm independant of $n$.

\begin{corollary}
    If $X$ is uniformly chosen on $\sqrt{n} \cdot S^{n-1}$, then $\| X \|_{\psi_2} \lesssim 1$.
\end{corollary}

If $x_1, \dots, x_m$ are values with $x_1^2 + \dots + x_m^2 = 1$, and $X$ is uniformly distribution on the sphere of radius $n^{1/2}$ in $\RR^n$, then $x_1 X_1 + \dots + x_n X_n$ looks like a $N(0,1)$ distribution when $n \gg m$. This observation is known as the \emph{projective} central limit theorem. The last corollary gives the tail decay aspect of this theorem.

One may conjecture that a uniformly random vector lying on an isotropic convex body is subgaussian, independant of the body and the dimension $n$. But this not be the case.

\begin{example}
    Let $K$ be the ball of radius $t$ with respect to the $l^1$ norm in $\RR^n$, i.e.
    %
    \[ K = \{ x \in \RR^n: |x_1| + \dots + |x_n| \leq 1 \}. \]
    %
    %If $X$ is uniformly chosen on $K$, and $x_1^2 + \dots + x_n^2 = 1$, if $S_k = x_1X_1 + \dots + x_k X_k$, then $\{ S_k \}$ is a martingale, and $|S_{i+1} - S_i| \leq x_{i+1}$. Azuma's inequality implies that
    %
    %\[ \PP(x \cdot X \geq t) \leq \exp(-t^2/2) \]
    %
    %Thus $\| X \|_{\psi_2} \lesssim 1$.
    Since the $l^1$ ball has volume $2^n/n!$, and the intersection of $K$ with any plane $\{ x_1 = s \}$ is equal to an $l^1$ ball of radius $1 - s$, if $t < 0.1$,
    %
    \begin{align*}
        \PP(X_1 \geq t) &= \frac{1}{|K|} \frac{2^{n-1}}{(n-1)!} \int_t^1 (1-s)^{n-1}\; ds\\
        &= \frac{(1 - t)^n}{2} = \frac{\exp(n \log(1-t))}{2} \geq \exp(-nt)/2.
    \end{align*}
    %
    Thus $\| X \|_{\psi_2} \gtrsim 1$. Since $K = -K$, $\EE(X_iX_j) = 0$ if $i \neq j$. But the coordinates of $X$ are all identically distributed, some scalar multiple of $X$ is isotropic. One can calculate that $\VV(X_i^2) = 2/(n+1)(n+2)$, so $tX$ is isotropic, where $t = [(n+1)(n+2)/2]^{1/2}$. But now $tX$ is certainly not \emph{uniformly} subgaussian in $n$, because $\| tX \|_{\psi_2}$ is proportional to $n$.
\end{example}

Nonetheless, it is possible to prove that if $K$ is an arbitrary isotropic convex body, and $X$ is uniformly distributed on $K$, then $X$ is uniformly \emph{subexponential}, i.e. $\| X \|_{\psi_1} \lesssim 1$, uniformly in $n$. This follows C. Borell's lemma.









\section{Concentration For Lipschitz Functions}

Let $X$ be a subgaussian vector, and $f$ a real valued function. A natural question to ask is when $f(X)$ concentrates about it's mean $\mathbf{E} f(X)$. For linear functions $f$, this question is easy. And if $f$ does not oscillate too much under small pertubations of the input, the theorem remains true. Here we consider the situation when the values of $X$ lie in some metric space $M$ satisfying an `isoperimetric blowup' phenomenon with respect to the distribution of $X$. In such spaces, our result gives concentration bounds for Lipschitz functions $f: M \to \RR$. Our main result is for Lipschitz functions on the sphere, but we also indicate concentration results using this method on other spaces.

First, we state, without proof, the isoperimetry phenomenon for the sphere. Recall that if $E$ is a subset of a metric space, we let
%
\[ E_\delta = \left\{ x : d(x,E) < \delta \right\} \]
%
denote the $\delta$ thickening of $E$. We let $\sigma$ denote the normalized surface area measure on $S^{n-1}$. Now let $C$ denote a spherical cap with $\sigma(C) = A$. Isoperimetry says that if $E \subset S^{n-1}$ is \emph{any} set with $\sigma(E) = A$, then $\sigma(E_\delta) \geq \sigma(C_\delta)$. In other words, spherical caps minimize volume expansion on the sphere. A simple corollary is a blow-up phenomenon for the thickenings of sets on spheres.

\begin{lemma}
    Let $E \subset S^{n-1}$. There exists a universal constant $c$ such that if $\sigma(E) \geq 1/2$, then for any $t \geq 0$, $\sigma(E_t) \geq 1 - 2\exp(-cnt^2)$.
\end{lemma}
\begin{proof}
    Let $H$ denote the lower hemisphere of $S^{n-1}$, i.e.
    %
    \[ H = \{ x \in S^{n-1} : x_1 \leq 0 \}. \]
    %
    By assumption, $\sigma(E) \geq 1/2 = \sigma(H)$. Thus the isoperimetric inequality implies that $\sigma(E_t) \geq \sigma(H_t)$. Thus we have reduced lower bounding the surface area of $E_t$ to lower bounding the surface area of $H_t$.

    Consider $x \in S^{n-1}$ with $x_1 \leq 2^{-1/2} \cdot t$. Set $a = (x_2^2 + \dots + x_n^2)^{1/2}$. Then $\sqrt{1 - t^2/2} \leq a \leq 1$. If we set $x' = \left( 0, x_2/a, \dots, x_n/a \right) \in H$, then
    %
    \begin{align*}
        |x - x'|^2 &= |x_1|^2 + (1 - a)^2 \leq t^2/2 + \left(1 - (1 - t^2/2)^{1/2} \right)^2\\
        &= 2 \left(1 - (1 - t^2/2)^{1/2} \right) = \frac{2t^2}{1 + (1 - t^2/2)^{1/2}} \leq t^2.
    \end{align*}
    %
    So $|x - x'| \leq t$. In particular, this means we have proved
    %
    \[ H_t \supset \left\{ x \in S^{n-1} : x_1 \leq t 2^{-1/2} \right\} \]
    %
    If $X$ is uniformly distributed on the unit sphere, then $\| X \|_{\psi_2} \lesssim 1$, which means there exists a small constant $c$ such that
    %
    \[ \sigma(H_t) \geq 1 - \PP \left( X_1 \geq t 2^{-1/2} \right) \geq 1 - 2 \exp(-cn t^2). \qedhere \]
\end{proof}

\begin{lemma}
    Let $E$ be a subset of $S^{n-1}$ with $\sigma(E) > 2 \exp(-cns^2)$. Then for any $t \geq s$, $\sigma(E_{2t}) \geq 1 - \exp(cnt^2)$.
\end{lemma}
\begin{proof}
    First, we argue that $\sigma(E_s) > 1/2$. If not, then $\sigma(E_s^c) \geq 1/2$. So we can then apply the last lemma to conclude that for any $t$,
    %
    \[ \sigma((E_s^c)_t) \geq 1 - 2 \exp(-cn t^2). \]
    %
    In particular, we can select some $t < s$ such that $\sigma((E_s^c)_t) + \sigma(E) > 1$, so $(E_s^c)_t \cap E$ is non-empty. But this mean that $d(E,E_s^c) \leq t < s$, which is impossible. Thus $\sigma(E_s) > 1/2$, so we can apply the last lemma to $E_s$ to yield the required inequality.
\end{proof}

Using isoperimetry and blow-up, we can now prove a concentration result for Lipschitz functions on the sphere. Given a Lipschitz function $f$, we let $\| f \|_{\text{Lip}}$ denote the minimum value with $|f(x) - f(y)| \leq \| f \|_{\text{Lip}} |x - y|$.

\begin{theorem}
    If $X$ is uniformly distributed on $S^{n-1}$, and $f: S^{n-1} \to \RR$, then
    %
    \[ \| f(X) - \EE f(X) \|_{\psi_2} \lesssim \| f \|_{\text{Lip}} \cdot n^{-1/2}. \]
\end{theorem}
\begin{proof}
    Let $M$ be a medium for $f(X)$, i.e. a value such that
    %
    \[ \PP(f(X) \geq M) \geq 1/2\quad\text{and}\quad \PP(f(X) \leq M) \geq 1/2 \]
    %
    Let $E = \{ f(X) \leq M \}$ denote a level set of $f$. Then
    %
    \[ E_t \subset \{ f(X) \leq M + \| f \|_{\text{Lip}} \cdot t \}. \]
    %
    This means that
    %
    \[ \PP(f(X) \leq M + \| f \|_{\text{Lip}} \cdot t ) \geq \PP(E_t) \geq 1 - \exp(-cn t^2 ). \]
    %
    Similarily, we can show
    %
    \[ \PP(f(X) \geq M - \| f \|_{\text{Lip}} \cdot t) \geq 1 - \exp(-cn t^2). \]
    %
    A union bounds then shows
    %
    \[ \PP(|f(X) - M| \geq \| f \|_{\text{Lip}} \cdot t) \leq 2 \exp(-cn t^2). \]
    %
    This gives that $\| f(X) - M \|_{\psi_2} \lesssim \| f \|_{\text{Lip}} \cdot n^{-1/2}$. But we can now apply centering to show
    %
    \[ \| f(X) - \EE f(X) \|_{\psi_2} \lesssim \| f(X) - M \|_{\psi_2} \lesssim \| f \|_{\text{Lip}} \cdot n^{-1/2}. \qedhere \]
\end{proof}

\begin{remark}
    Concentration around the expectation and concentration around the medium are essentially equivalent facts. Centering tells us that if $M$ is a median, then for any random variable $X$, $\| X - \EE X \|_{\psi_2} \lesssim \| X - M \|_{\psi_2}$. On the other hand,
    %
    \[ \PP(X \geq \EE X + t) \leq 2 \exp \left( -\frac{c t^2}{\| X - \EE X \|_{\psi_2}^2} \right). \]
    %
    In particular, if $C = (\log(4))^{1/2}/c$, and $t \geq C \| X - \EE X \|_{\psi_2}$, then $\PP(|X - \EE X| \geq t) \leq 1/2$, which means that $|M - \EE X| \leq C \| X - \EE X \|_{\psi_2}$, and so
    %
    \[ \| X - M \|_{\psi_2} \lesssim |\EE X - M| + \| X - \EE X \|_{\psi_2} \leq (1 + C) \| X - \EE X \|_{\psi_2}. \]
    %
    Thus $\| X - M \|_{\psi_2}$ and $\| X - \EE X \|_{\psi_2}$ are comparable to one another.
\end{remark}

\begin{example}
    For a geometric application of this claim, we show that while there are at most $n$ orthogonal vectors in $\mathbf{R}^n$, we can have exponentially many \emph{almost} orthogonal vectors. Two unit vectors $x$ and $y$ are almost orthogonal if $|x \cdot y| \leq \varepsilon$. We construct a set of exponentially many almost orthogonal vectors inductively. Consider unit vectors $e_1, \dots, e_N$, which are almost orthogonal. For each $k$, we can consider $E_k = \{ x \in S^{n-1} : |(x \cdot e_k)| \leq \varepsilon \}$. Then $\sigma(E_k) \geq 1 - 2\exp(-cn\varepsilon^2)$, and so $\sigma(E_1 \cap \dots \cap E_N) \geq 1 - 2N \exp(-cn \varepsilon^2)$. If $N < 0.5 \cdot \exp(cn\varepsilon^2)$, this is positive, so there certainly exists a unit vector simultaneously orthogonal to all other vectors. Adding this to the list and continuing, we can continue adding almost orthogonal vectors up to the point where $N \geq 0.5 \cdot \exp(cn \varepsilon^2)$.
\end{example}

There is nothing really special to the sphere here. Given any other measure space with a metric, we can consider the minimizers of volume expansion, and thus achieve isoperimetric inequalities in this domain. If the minimizers of the isoperimetry problem have a mass blow up, we can obtain the same result.

\begin{example}
    Consider $\mathbf{R}^n$ equipped with the Gaussian measure, which has the Gaussian distribution as a density function. It is non-obvious, but the minimizers of measure expansion are achieved by half planes. From this, we can calculate the precise constants of the blow up phenomenon, and then deduce that if $X \in \RR^n$ is Gaussian, and $f$ is Lipschitz, then $\| f(X) - \mathbf{E} f(X) \|_{\psi_2} \lesssim \| f \|_{\text{Lip}}$. We should expect the Gaussian result to look essentially the same as the result based on the uniform distribution on spheres, because in high dimensions, the two distributions are essentially the same.
\end{example}

\begin{example}
    A similar phenomenon is obtained over $\{ -1, 1 \}^n$, where the measure is the uniform probability distribution, and the metric is the Hamming distance, i.e. for $x,y \in \{ -1, 1 \}^n$, $d(x,y) = \# \{ i: x_i \neq y_i \}$ in this domain are balls with respect to the Hamming distance. We can conclude from this that
    %
    \[ \| f(X) - \mathbf{E} f(X) \|_{\psi_2} \lesssim \| f \|_{\text{Lip}} \cdot n^{-1/2}. \]
    %
    for Lipschitz functions on this domain.
\end{example}

\begin{example}
	If we consider the Hamming distance on $S_n$, i.e. for two permutations $\pi$ and $\eta$, we let $d(\pi,\eta) = \# \{ i: \pi(i) \neq \eta(i) \}$, and consider the uniform distribution on $S_n$, then the minimizers of volume expansion in this domain are given by balls, and so we also conclude that
	%
    \[ \| f(X) - \mathbf{E} f(X) \|_{\psi_2} \lesssim \| f \|_{\text{Lip}} \cdot n^{-1/2}. \]
    %
    for Lipschitz functions on this domain.
\end{example}

Using other tools, we can establish semigroup results in other spaces.

\begin{example}
    If $M$ is a Riemannian manifold, we can consider the arclength distance, as well as a natural normalized volume of $M$ inducing a probability distribution $X$ chosen uniformly at random on $M$. If $c(M)$ denotes the infinum of the Ricci curvature tensor over all points, and $c(M) > 0$, then semigroup tools enable us to establish a concentration bound
    %
    \[ \| f(X) - \mathbf{E} f(X) \|_{\psi_2} \lesssim \frac{\| f \|_{\text{Lip}}}{c(M)^{1/2}}. \]
    %
    For instance, $c(S^n) = n$, which gives the concentration inequality for the sphere. Other examples include the matrix group $SO(n)$, with the metric induced by the Frobenius norm, which gives
    %
    \[ \| f(X) - \EE f(X) \|_{\psi_2} \lesssim \frac{\| f \|_{\text{Lip}}}{n^{1/2}}. \]
    %
    Another important example is the Grassmanian space $G_{nm}$ consisting of $m$ dimensional subspaces of $\RR^n$, with the distance metric between two vectors spaces $V$ and $W$ given by operator norm of $\| P_V - P_W \|$, where $P_V$ and $P_W$ are orthogonal projections. Here, we also obtain that
    %
    \[ \| f(X) - \EE f(X) \|_{\psi_2} \lesssim \frac{\| f \|_{\text{Lip}}}{n^{1/2}}. \]
    %
    We obtain the same concentration as for $SO(n)$. We note that the measure given in $SO(n)$ is the \emph{Haar measure}, and the measure on $G_{nm}$ is the Haar measure induced by the action of $SO(n)$ on the space, i.e.
    %
    \[ A \cdot V = A \cdot V \cdot A^{-1} \]
    %
    Since this action is transitive, we can actually realize $G_{nm}$ as a quotient of $SO(n)$. The stabilizer of the action is $SO(n) \times SO(n-m)$, and we find
    %
    \[ G_{nm} \equiv \frac{SO(n)}{SO(m) \times SO(n-m)}. \]
    %
    Using this, we can also deduce concentration bounds on the Grassmanian from concentration bounds on the orthogonal group.
\end{example}

\begin{example}
    Let $\Phi(x)$ denote the cumulative distribution function of a normal distribution. If $Z \sim N(0,I_n)$, then $\phi(Z) = (\Phi(Z_1), \dots, \Phi(Z_n))$ is uniformly distributed on $[0,1]^n$. To see why, it suffices to show $\Phi(Z_1)$ is uniformly distributed on $[0,1]$, and we calculate that for $t \in [0,1]$,
    %
    \[ \PP(\Phi(Z_1) \leq t) = \PP(Z_1 \leq \Phi^{-1}(t)) = \Phi(\Phi^{-1}(t)) = t. \]
    %
    Given a Lipschitz function $f: [0,1]^n \to \RR$, consider $f \circ \phi: \RR^n \to \RR$. Then $\| f \circ \phi \|_{\text{Lip}} \leq \| f \|_{\text{Lip}} \| \phi \|_{\text{Lip}}$. Since
    %
    \[ | \nabla \Phi(x)| = \frac{|x| e^{-|x|^2/2}}{(2\pi)^{n/2}} \lesssim 1. \]
    %
    Thus $\| \Phi \|_{\text{Lip}} \lesssim 1$, and so
    %
    \[ |\phi(x-y)| \leq \sqrt{ \sum \Phi(x_i - y_i)^2 } \lesssim \sqrt{ \sum |x_i - y_i|^2 } = |x - y|, \]
    %
    which implies $\| \phi \|_{\text{Lip}} \lesssim 1$. Thus we can apply concentration in Gaussian space to conclude that if $X = \phi(Z)$, then $\| f(X) - \EE(f(X)) \|_{\psi_2} \lesssim \| f \|_{\text{Lip}}$. Thus we have Lipschitz concentration for the uniform distribution on $[0,1]^n$.
\end{example}

\begin{example}
    If the density of a random vector $X$ in $\RR^n$ is of the form $\exp(-U(x))$, where $U \to \RR^n \to \RR$. Assume there is $\kappa$ such that $H(U) \succeq \kappa$. Then for any Lipschitz function $f: \RR^n \to \RR$, $\| f(X) - \EE(f(X)) \|_{\psi_2} \lesssim \| f \|_{\text{Lip}} \cdot \kappa^{-1/2}$.
\end{example}

\begin{example}
    Let $X$ be a random vector whose coordinates are independant and $|X_i| \leq 1$ almost surely. Then \emph{Talagrand's concentration inequality} implies $\| f(X) - \EE(f(X)) \|_{\psi_2} \lesssim \| f \|_{\text{Lip}}$.
\end{example}

There is some results which are distribution dependant, but slightly more stringent conditions need to be satisfied.

\begin{theorem}[Talagrand's Concentration Inequality]
	Let $X$ be a random vector in $\RR^n$, with independant coordinates $\{ X_i \}$, with $|X_i| \leq 1$ almost surely. Then for any {\it convex} Lipschitz function $f: [-1,1]^n \to \RR$,
	%
	\[ \| f(X) - \EE f(X) \|_{\psi_2} \lesssim \| f \|_{\text{Lip}}. \]
\end{theorem}

\section{The Johnson-Lindenstrauss Lemma}

Suppose we have $N$ data points in $\mathbf{R}^n$, where $n$ is very large. We would like to reduce the dimension of the data, while still preserving the geometric properties of the data points. The simplest data reduction is to project the data points onto a lower dimensional subspace. A natural question is the smallest dimension we can project the points, while still approximately preserving the distance between points. The Johnson-Lindenstrauss lemma says the distances will be approximately preserved when projecting into a space with dimension $\log N$. Given $V \in G_{nm}$, let $Q_V = (n/m)^{1/2} \cdot P_V$.

\begin{lemma}
    Let $V$ be a randomly chosen projection onto an $m$ dimensional subspace of $G_{n,m}$. If $z \in \mathbf{R}^n$ is fixed, and $\varepsilon > 0$, then $\mathbf{E} |Q_V(z)|^2 \leq |z|^2$, and with probability greater than $1 - 2\exp(-c\varepsilon^2m)$,
    %
    \[ (1 - \varepsilon) |z| \leq |Q_V(z)| \leq (1 + \varepsilon) |z| \]
\end{lemma}
\begin{proof}
    Without loss of generality, assume that $|z| = 1$. Then, instead of considering a random subspace $V$, we can consider a fixed space $V$ acting on a random unit vector $z$, since the distribution of $Q_V(z)$ will be the same. Using rotation invariance, we may assume that $P_V$ is the projection onto the first $m$ coordinates. Since $\EE(z_i^2) = 1/n$ for each $i$,
    %
    \[ \mathbf{E}|Q_V(z)|^2 = (n/m) \cdot \sum_{i = 1}^m \mathbf{E} z_i^2 = 1. \]
    %
    Thus the first part of the lemma is proven. Next, we apply the concentration result for Lipschitz functions on a sphere. if $f(x) = |Q_V(x)|$, then $\| f \|_{\text{Lip}} = (n/m)^{1/2}$. Thus
    %
    \[ \| Q_V(X) - (n/m)^{1/2} \|_{\psi_2} \lesssim 1/m^{1/2}, \]
    %
    so
    %
    \[ \mathbf{P}\left(\left| |Q_V(z)| - 1 \right| \geq t \right) \leq 2 \exp(-cmt^2). \qedhere \]
\end{proof}

\begin{theorem}
    Let $V \in G_{nm}$ be uniformly chosen. Then there exists constants $c$ and $C$ such that if $X$ is a set of $N$ points in $\mathbf{R}^n$, $\varepsilon > 0$, and $m \geq C \log N / \varepsilon^2$, then with probability $1 - 2 \exp(-c m \varepsilon^2)$, the projection $Q_V$ of $X$ onto $E$ satisfies
    %
    \[ (1 - \varepsilon)|x-y| \leq | Q_V(x) - Q_V(y) | \leq (1 + \varepsilon) |x - y| \]
    %
    for all $x,y \in X$.
\end{theorem}
\begin{proof}
    We can apply the last lemma, for each $x,y \in X$, to $z = x-y$ and then take a union bound over all possible $N^2$ pairs of points. Combined with the fact that there is a constant $C$ with $N \leq \exp(C \varepsilon^2 m)$, this gives that the inequality is satisfied for all $x,y$ with probability
    %
    \begin{align*}
        1 - 2 N^2 \exp(-c \varepsilon^2 m) \geq 1 - 2 \exp((2C - c) \varepsilon^2 m)
    \end{align*}
    %
    if $C$ is sufficiently small, depending on $c$, this is bounded by $1 - 2 \exp(-c\varepsilon^2 m)$ for a slightly smaller constant $c$.
\end{proof}

\begin{remark}
    It is an important, and incredible fact that the random choice of projections depends in no way on the incoming data. Furthermore, the dimension $n$ of the ambient space is not featured in the lemma at all. We also remark that the theorem remains true if we consider a random matrix whose rows are independant, mean zero, subgaussian random vectors, and we normalize by $1/m^{1/2}$.
\end{remark}

The Johnson Lindenstrauss lemma is tight for general data. For instance, if $X$ is an orthonormal basis in $\RR^n$, and $Q: \RR^n \to \RR^m$ is an almost isometry on this set of points, then for each $x,y \in X$,
%
\[ \sqrt{2} \cdot (1 - \varepsilon) \leq |Qx - Qy| \leq \sqrt{2} \cdot (1 + \varepsilon)  \]
%
Thus $\{ Q(x): x \in X \}$ is a $\sqrt{2} \cdot (1 - \varepsilon)$ packing in the ball $B$ of radius $\sqrt{2} \cdot (1 + \varepsilon)$ in $\RR^n$. A volumetric argument shows that
%
\begin{align*}
	n \leq P(Q(X), \sqrt{2} \cdot (1 - \varepsilon)) &\leq P \left( B, \sqrt{2} \cdot (1 - \varepsilon) \right)\\
	&\leq \frac{(1 + \varepsilon)^m [2^{1/2} + 1/2^{1/2}]^m}{2^{-m/2} \cdot (1 - \varepsilon)^m} = \left( 1 + 2 \cdot \frac{\varepsilon}{1 - \varepsilon} \right) 3^m,
\end{align*}
%
where $P(T,\varepsilon)$ is the $\varepsilon$ \emph{packing number} of $T$, discussed later on in these notes. We conclude $m \gtrsim_\varepsilon \log n$.










\chapter{Useful Techniques}

Here we discuss some very useful techniques for reducing the analysis of certain distributions to other situations. Decoupling enables us to reduce the analysis of a random quadratic form to a random bilinear form, which is much easier to understand. Symmetrization enables us to reduce the study of a certain distribution to the study of a random distribution.

\section{Decoupling}

In this section, we study the distribution of quadratic forms
%
\[ X^T A X = \sum A_{ij} X_i X_j, \]
%
where $\{ X_i \}$ are independant random variables, and $A_{ij}$ are arbitrary constants. The expectation is easy to describe. If $X_i$ has variance $\sigma_i^2$, then
%
\[ \EE(X^T A X) = \sum A_{ii} \sigma_i^2 \]
%
But establishing concentration bounds is much harder -- one cannot use a Lipschitz bound here unless that variables $\{ X_i \}$ are bounded, and this probably won't give a good concentration bound regardless. Decoupling is a technique to replace the random variable $X^T AX$ with $X^T A X'$, where $X'$ is an independant copy of $X$. Since bilinear forms are much easier to analyze than quadratic forms, this is often a useful reduction.

\begin{lemma}
    Let $Y$ and $Z$ be independant random variables with $\EE Z = 0$. Then for any convex function $F: \RR^n \to \RR$, $\EE F(Y) \leq \EE F(Y+Z)$.
\end{lemma}
\begin{proof}
    We apply Jensen's inequality. Since $\EE Z = 0$,
    %
    \begin{align*}
        \EE F(Y) = \EE F(Y + \EE Z) &= \EE F(\EE(Y + Z|Y))\\
        &\leq \EE(\EE(F(Y+Z)|Y)) = \EE(F(Y+Z)). \qedhere
    \end{align*}
\end{proof}

We now use this lemma to establish a decoupling inequality.

\begin{theorem}
    Let $A$ be a diagonal free matrix. Let $X$ be a random vector with independant, mean zero coordinates. Then for any convex function $F$, $\EE(F(X^T A X)) \leq \EE(F(4 X^T A X'))$, where $X'$ is an independant copy of $X$.
\end{theorem}
\begin{proof}
    Let $\delta_1, \dots, \delta_n \in \{ 0, 1 \}$ be independant symmetric Bernoulli random variables, and define a random subset $I = \{ k : \delta_k = 1 \}$ of $\{ 1, \dots, n \}$. Since $\EE(\delta_i(1 - \delta_j)) = 1/4$ for $i \neq j$,
    %
    \[ \EE \left( \sum_{(i,j) \in I \times I^c} A_{ij} X_i X_j \right) = \EE \left( \sum_{ij} A_{ij} \delta_i (1 - \delta_j) X_i X_j \right) = (1/4) \EE(X^TAX). \]
    %
    We now apply the function $F$ to both sides, calculating
    %
    \[ \EE(F(X^T A X)) \leq \EE \left(F \left( 4 \sum_{(i,j) \in I \times I^c} A_{ij} X_i X_j' \right) \right). \]
    %
    In particular, this means that we may fix a \emph{non random} choice of $I$ for which this equation still remains true, which we do for the remainder of the proof. Note that $\sum_{(i,j) \in I} A_{ij} X_i X_j$ is identically distributed to $\sum_{(i,j) \in I} A_{ij} X_i X_j'$, where $X'$ is an independant copy of $X$. Write
    %
    \[ Y = \sum_{(i,j) \in I \times I^c} A_{ij} X_i X_j'\quad Z_1 = \sum_{(i,j) \in I \times I} A_{ij} X_i X_j'\quad \text{and}\quad Z_2 = \sum_{(i,j) \in I^c \times [n]} A_{ij} X_i X_j'. \]
    %
    Let $\EE'$ denote conditional expectations with respect to all random variables \emph{except} $\{ X_i \}_{i \in I^c}$ and $\{ X_j' \}_{j \in I}$. Then $\EE'(Y) = Y$, and $\EE'(Z_1) = \EE'(Z_2) = 0$. If we apply the last lemma, we conclude
    %
    \[ F(4Y) \leq \EE'(F(4Y + 4Z_1 + 4Z_2)). \]
    %
    Taking expectations on both sides of this inequality concludes the argument, since $Y + Z_1 + Z_2 = \sum A_{ij} X_i X_j$.
\end{proof}

We can use this fact to get bounds on moment generating functions of quadratic forms, which yields deviation inequalities. We first show how to replace the question of random variables $X$, $X'$ with arbitrary distributions with Gaussian distributions. This is quite often a useful technique when upper bounding certain monotonic quantities which depend on subgaussian variables.

\begin{lemma}
    If $X,X' \in \RR^n$ are mean zero independant subgaussian random vectors, with $\| X \|_{\psi_2}, \| X' \|_{\psi_2} \leq K$. If $g,g' \sim N(0,I_n)$ are independant normal random vectors, and $A$ is an $n \times n$ matrix, then
    %
    \[ \EE \exp(\lambda X^T A X') \leq \EE \exp(C K^2 \lambda g^T A g'). \]
\end{lemma}
\begin{proof}
    We let $\EE_X$ denote conditioning with respect to $X'$, averaging over $X$. When $X'$ is fixed, $X^T A X' = X \cdot AX'$ is subgaussian, with $\| X^T A X' \|_{\psi_2} \leq K |AX'|$. Thus
    %
    \[ \EE_X \exp(\lambda X^T A X') \leq \exp(C \lambda^2 K^2 |AX'|^2). \]
    %
    Note that if $\EE_g$ is obtained by averaging over $g$,
    %
    \[ \EE_g \exp(\gamma g^T A X') = \exp(\gamma^2 |AX'|^2/2). \]
    %
    If $\gamma^2 = 2C \lambda^2 K^2$, then we conclude
    %
    \[ \EE_X \exp(\lambda X^T A X') \leq \EE_g \exp \left( (2C)^{1/2} \lambda K g^T A X' \right). \]
    %
    Taking expectations on both sides shows that we can replace $X$ with $g$ with the cost of $(2C)^{1/2} K$. A similar argument replaces $X'$ with $g'$ at the cost of an additional $(2C)^{1/2} K$ factor.
\end{proof}

\begin{lemma}
    Let $X,X'$ be mean zero subgaussian random vectors. Then
    %
    \[ \EE \exp(\lambda X^TAX') \leq \exp(C K^4 \lambda^2 \| A \|_F^2) \]
    %
    for all $\lambda$ satisfying $|\lambda| \leq c/\|A\|$.
\end{lemma}
\begin{proof}
    Consider the singular value decomposition of $A$, i.e. write
    %
    \[ A = \sum s_i u_i v_i^T. \]
    %
    Consider first the case of two Gaussian random vectors $g,g'$. Then $g^T Ag' = \sum s_i (g \cdot u_i)(g' \cdot v_i)$. Since the $u_i$ and $v_i$ are orthonormal, $\sum s_i (g \cdot u_i)(g' \cdot v_i)$ is identically distributed to $\sum s_i g_i g'_i$. By independence,
    %
    \[ \EE(\exp(\lambda g^T A g')) = \prod \EE(\exp(\lambda s_i g_i g'_i)), \]
    %
    and if $\lambda^2 s_i^2 \leq c$,
    %
    \begin{align*}
        \EE(\exp(\lambda s_i g_i g'_i)) &= \EE(\EE(\exp(\lambda s_i g_i g'_i | g_i)))\\
        &\leq \EE(\exp(\lambda^2 s_i^2 g_i^2/2)) \leq \exp(C \lambda^2 s_i^2),
    \end{align*}
    %
    where we used the fact that $g_i^2$ is subexponential. This means that provided $\lambda^2 \leq c / \max s_i = c / \| A \|$,
    %
    \[ \EE(\exp(\lambda g^T A g')) \leq \exp \left( C \lambda^2 \sum s_i^2 \right) = \exp(C \lambda^2 \| A \|_F). \]
    %
    In general, we apply the comparison inequality. If $\| X \|_{\psi_2}, \| X' \|_{\psi_2} \leq K$, then
    %
    \[ \EE(\exp(\lambda X^T A X')) \leq \EE \exp(C K^2 \lambda g^T A g') \leq \exp(C K^4 \lambda^2 \| A \|_F). \qedhere \]
\end{proof}

We can now prove a concentration inequality for quadratic forms, which should be viewed as analogoue to Bernstein's inequality in the linear case.

\begin{theorem}[Hanson-Wright]
    Let $X$ be a random vector with independant, mean zero, subgaussian coordinates. Then for $t \geq 0$,
    %
    \[ \PP \left( |X^T A X - \EE X^T A X| \geq t \right) \leq 2 \exp\left( -c \min \left( \frac{t^2}{K^4 \| A \|_F^2}, \frac{t}{K^2 \| A \|} \right) \right). \]
\end{theorem}
\begin{proof}
    Without loss of generality, assume $\| X \|_{\psi_2}, \| X' \|_{\psi_2} \lesssim 1$. We note that $\EE X^T A X = \sum a_{ii} \EE(X_i^2)$. Thus
    %
    \begin{align*}
        \PP(X^T A X - &\EE X^T A X \geq t)\\
        &\leq \PP \left(\sum a_{ii}(X_i^2 - \EE X_i^2) \geq t/2 \right) + \PP \left(\sum_{i \neq j} a_{ij} X_i X_j \geq t/2 \right).
    \end{align*}
    %
    We note that
    %
    \[ \| X_i^2 - \EE X_i^2 \|_{\psi_1} \lesssim \| X_i^2 \|_{\psi_1} \lesssim \| X_i \|_{\psi_2}^2 \lesssim 1. \]
    %
    Bernstein's inequality implies
    %
    \begin{align*}
        \PP \left( \sum a_{ii}(X_i^2 - \EE X_i^2) \geq t/2 \right) &\leq \exp \left( -c \min \left( t^2 / \sum a_{ii}^2, t/\max |a_{ii}| \right) \right)\\
        &\leq \exp \left( -c \min \left(t^2/ \|A \|_F^2, t/\|A\| \right) \right).
    \end{align*}
    %
    We use our moment generating bound for the non-diagonal elements. We note that if $S = \sum_{i \neq j} a_{ij} X_i X_j$, then our decoupling bound implies that provided $\lambda \leq c / \| A \|$.
    %
    \begin{align*}
        \PP \left( S \geq t/2 \right) &\leq \exp(-\lambda t / 2) \EE \exp(\lambda S)\\
        &\leq \exp(\lambda t/2) \exp(C \lambda^2 \| A \|_F^2).
    \end{align*}
    %
    Optimizing the choice of $\lambda$, we conclude
    %
    \[ \PP(S \geq t/2) \leq \exp(-c \min(t^2/ \| A \|_F, t/\|A\|)). \]
    %
    Putting the non-diagonal bound with the diagonal bound together, we conclude that
    %
    \[ \PP (X^T A X - \EE X^T A X \geq t) \leq 2 \exp \left(-c \min \left(t^2/\|A \|_F, t/\|A\| \right) \right). \qedhere \]
\end{proof}

As a consequence, we can obtain concentration bounds for the norms of more general subgaussian distributions.

\begin{theorem}
    Let $B$ be an $n \times n$ matrix, and $X$ a random variable with independance, mean zero, unit variance sub-gaussian coordinates. Then if $K = \max \| X_i \|_{\psi_2}$,
    %
    \[ \| |BX| - \| B \|_F \|_{\psi_2} \lesssim K^2 \| B \|. \]
\end{theorem}
\begin{proof}
    We apply the Hanson-Wright inequality for $A = B^TB$. Then $X^TAX = |BX|^2$, and $\EE X^TAX = \| B \|_F^2$. Note that $\| A \| = \| B \|^2$, and
    %
    \[ \| B^T B \|_F \leq \| B^T \| \| B \|_F = \| B \| \| B \|_F. \]
    %
    Since $K^4 \geq K^2$,
    %
    \[ \PP(| |BX|^2 - \| B \|_F^2 | \geq u) \leq 2 \exp(- (c/K^4) \cdot \min(u^2/\| B \|^2 \| B \|_F, u / \| B \|^2)). \]
    %
    Setting $u = \varepsilon \| B \|_F^2$, we conclude
    %
    \[ \PP(| |BX|^2 - \| B \|_F^2 | \geq \varepsilon \| B \|_F^2) \leq 2 \exp(-c \min(\varepsilon,\varepsilon^2) \| B \|_F^2 / K^4 \| B \|^2). \]
    %
    Observe that if $\varepsilon = \max(\delta, \delta^2)$, i.e. $\delta^2 = \min(\varepsilon, \varepsilon^2)$, then
    %
    \[ \PP(||BX| - \| B \|_F| \geq \delta \| B \|_F) \leq 2 \exp(-c\delta^2 \| B \|_F^2 / K^4 \| B \|^2), \]
    %
    But this means that
    %
    \[ \PP(|BX| - \| B \|_F| \geq t) \leq 2 \exp(-c t^2 / K^4 \| B \|^2). \qedhere \]
\end{proof}

\begin{example}
	Let $E$ be a subspace of $\RR^n$ of dimension $m$. let $X$ be a random vector in $\RR^n$ with independant, mean zero, unit variance, sub-gaussian coordinates. If we let $P$ denote orthgononal projection onto $E$, then $d(X,E) = |X - P(X)|$. Since the singular value decomposition of $I - P$ consists of $n - m$ copies of 1, and $m$ copies of 0, we conclude $\| I - P \| = 1$, and $\| I - P \|_F = \sqrt{n - m}$. Thus the Hardy-Wright inequality implies $\| d(X,E) - \sqrt{n-m} \|_{\psi_2} \lesssim K^2$, where $K = \max \| X_i \|_{\psi_2}$.
\end{example}

We can also use decoupling to obtain tail bounds on the norms of subgaussian vectors which may not necessarily have independant coordinates.

\begin{lemma}
	There exists a universal constant $C$ and $c$ such that if $X$ is a mean zero subgaussian vector, with $\| X \|_{\psi_2} \leq K$, then
	%
	\[ \EE \exp(\lambda^2 |BX|^2) \leq \exp(C K^2 \lambda^2 \| B \|_F^2)\quad\text{provided}\quad |\lambda| \leq \frac{c}{K \| B \|}. \]
\end{lemma}
\begin{proof}
	For each $y \in \RR^m$, $y \cdot BX = B^Ty \cdot X$. Thus we find $\| y \cdot BX \|_{\psi_2} \leq K |B^Ty|$, and so there exists a universal constant $C'$ such that
	%
	\[ \EE(\exp(\lambda y \cdot BX)) \leq \exp(C' \lambda^2 K^2 |B^T y|^2). \]
	%
	If $g \sim N(0,I_m)$ is independant of $X$, then
	%
	\[ \EE_X(\exp(\lambda g \cdot BX)) \leq \exp(C' \lambda^2 K^2 |B^Tg|^2). \]
	%
	Taking expectations on both sides yields that
	%
	\[ \EE(\exp(\lambda g \cdot BX)) \leq \EE(\exp(C' \lambda^2 K^2 |B^T g|^2)). \]
	%
	On the other hand,
	%
	\[ \EE_g(\exp(\lambda g \cdot BX)) = \exp(\lambda^2 |BX|^2/2) \]
	%
	Taking expectations on both sides of this equality, and combining it with the previous inequality yields that
	%
	\[ \EE \exp(\lambda^2 |BX|^2) \leq \EE(2C' \lambda^2 K^2 |B^T g|^2) \]
	%
	Thus we have reduced our analysis to the understanding of a normal distribution.

	Now consider the singular value decomposition of $B^T$, i.e. we can write $B^T = \sum s_i u_i v_i^T$. Thus for any $\gamma > 0$,
	%
	\begin{align*}
		\EE(\exp(\gamma^2 |B^T g|^2)) &= \prod \EE(\exp(\gamma^2 s_i^2 (g \cdot v_i)^2))\\
		&= \prod (1 - 2 \gamma^2 s_i^2)^{-1/2}
	\end{align*}
	%
	Provided $\gamma^2 s_i^2 < 1/2$ for all $i$, which is equivalent to saying $\gamma \leq 2^{-1/2} \| B \|^{-1}$. Since $\log(1-x) \geq -2x$ if $x \leq 1/2$, we conclude that if $\gamma^2 s_i^2 < 1/4$,
	%
	\begin{align*}
		(1 - 2 \gamma^2 s_i^2)^{-1/2} &= \exp(- \log(1 - 2\gamma^2s_i^2)/2) \leq \exp(2 \gamma^2 s_i^2)
	\end{align*}
	%
	and so
	%
	\[ \EE(\exp(\gamma^2 |B^T g|^2)) \leq \prod \exp(2 \gamma^2 s_i^2) = \prod \exp(2 \gamma^2 \sum s_i^2) = \exp(2 \gamma^2 \| B \|_F^2). \]
	%
	This completes the proof.
\end{proof}

\begin{theorem}
	Let $B$ be an $m \times n$ matrix, and $X$ a mean zero sub-gaussian random variable with $\| X \|_{\psi_2} \leq K$. Then there exists a universal constant $C$ such that
	%
	\[ \PP \left( |BX| \geq C K \| B \|_F + t  \right) \leq \exp \left( \frac{-ct^2}{K^2 \| B \|^2} \right). \]
\end{theorem}
\begin{proof}
	We know that, if $C$ and $c$ are defined as in the last lemma, then
	%
	\[ \EE \left( \exp \left( \frac{c^2 |BX|^2}{K^2 \| B \|^2} \right) \right) \leq \exp \left( \frac{c^2 C K^2 \| B \|_F^2}{K^2 \| B \|^2} \right). \]
	%
	Thus
	%
	\[ \PP( |BX|^2 - C K^2 \| B \|_F^2 \geq t^2) \leq \exp \left( \frac{-c^2 t^2}{K^2 \|B\|^2} \right). \]
	%
	But now we find
	%
	\begin{align*}
		\PP(|BX| - C^{1/2} K \| B \|_F \geq t) &\leq \PP(|BX|^2 - CK^2 \| B \|_F^2 \geq t^2) \leq \exp \left( \frac{-c^2 t^2}{K^2 \| B \|^2} \right). \qedhere
	\end{align*}
\end{proof}

\begin{remark}
	This is roughly as good as we can get without assuming the coordinates of $X$ are independant. Consider the random vector $X = 2^{1/2} \cdot \phi g$, where $g \sim N(0,I_n)$ is Gaussian, and $\phi \in \{ 0, 1 \}$ is a Bernoulli distribution, and these distributions are both independant of one another. Then it is easy to verify that $X$ is isotropic, mean zero, with $\| X \|_{\psi_2} \lesssim 1$, yet $\PP(|X| = 0) = 1/2$ and
	%
	\begin{align*}
		\PP \left(|X| \geq \sqrt{2n} \right) = \PP(|g|^2 \geq n)/2 \geq 1/2 - o(1).
	\end{align*}
	%
	Thus $|X|$ does not concentrate near $\sqrt{n}$, as suggested by the independant coordinate result.
\end{remark}




\section{Symmetrization}

Another technique often used in high dimensional probability is to replace random variables with symmetric random variables, i.e. variables $X$ for which $X$ is distributed identically to $-X$. For instance, mean zero normal random variables are symmetric, as are symmetric Bernoulli random variables. To obtain a symmetric version of any random variable $X$, we can take an independant Bernoulli random variable $\varepsilon$, and consider $\varepsilon X$, or we can take an independant copy $X'$ of $X$, and we then consider $X - X'$. Throughout this section, we let $\varepsilon_i$ stand for a family of Bernoulli random variables, independant of each other and any other random variable considered in the argument.

\begin{lemma}
    Let $X_1, \dots, X_n$ be independant mean zero random variables in some normed space. Then
    %
    \[ 0.5 \EE \left\| \sum \varepsilon_i X_i \right\| \leq \EE \left\| \sum X_i \right\| \leq 2 \EE \left\| \sum \varepsilon_i X_i \right\|. \]
\end{lemma}
\begin{proof}
    If $X_1', \dots, X_n'$ are independant copies of $X_1, \dots, X_n$, then since $\EE(X_i') = 0$,
    %
    \[ \EE \left\| \sum X_i \right\| \leq \EE \left\| \sum X_i - \sum X_i' \right\| = \EE \left\| \sum (X_i - X_i') \right\| \]
    %
    Now note that since $X_i - X_i'$ is symmetric, it has the same distribution as $\varepsilon_i(X_i - X_i')$. Thus
    %
    \begin{align*}
        \EE \left\| \sum (X_i - X_i') \right\| &= \EE \left\| \sum \varepsilon_i(X_i - X_i') \right\|\\
        &\leq \EE \left\| \sum \varepsilon_i X_i \right\| + \EE \left\| \sum \varepsilon_i X_i' \right\|\\
        &\leq 2 \EE \left\| \sum \varepsilon_i X_i \right\|.
    \end{align*}
    %
    Conversely,
    %
    \begin{align*}
        \EE \left\| \sum \varepsilon_i X_i \right\| &\leq \EE \left\| \sum \varepsilon_i (X_i - X_i') \right\| = \EE \left\| \sum (X_i - X_i') \right\|\\
        &\leq \EE \left\| \sum X_i \right\| + \EE \left\| \sum X_i' \right\| \leq 2 \EE \left\| \sum X_i \right\|. \qedhere
    \end{align*}
\end{proof}

A common use of the symmetrization technique is obtained by introducing the random variables $\varepsilon_i$, then conditioning on $X_i$. This reduces the problem to a statement purely about Bernoulli random variables, which are often simpler to reason about. More generally, we can prove variants of this technique for convex functions, which in particular means we can apply symmetrization when using moment generating function bounds. The proof is exactly the same as above, combined with an application of Jensen's inequality.

\begin{theorem}
    Let $F: [0,\infty) \to \RR$ be an increasing, convex function. Then
    %
    \[ \EE F \left( 0.5 \cdot \left\| \sum \varepsilon_i X_i \right\| \right) \leq \EE F \left( \left\| \sum X_i \right\| \right) \leq \EE F \left( 2 \cdot \left\| \sum \varepsilon_i X_i \right\| \right). \]
\end{theorem}

Later on, we discuss suprema of random processes. We can use symmetrization in this setting as well.

\begin{lemma}
    Let $X_1(t), \dots, X_n(t)$ be independant, mean zero random processes indexed by points $t \in T$. Let $\varepsilon_1, \dots, \varepsilon_n$ be independant, mean zero, symmetric Bernoulli random processes. Then
    %
    \[ 0.5 \cdot \EE \left( \sup_{t \in T} \sum \varepsilon_i X_i(t) \right) \leq \EE \left( \sup_{t \in T} \sum X_i(t) \right) \leq 2 \EE \left( \sup_{t \in T} \sum \varepsilon_i X_i(t) \right). \]
\end{lemma}
\begin{proof}
	The maximum of a set of numbers is a convex function, so if $T$ is finite, this claim is already proved. And if we interpret the expected supremum as the supremum of the expected supremum of finite subsets of indices, this inequality remains true under taking a limit.
\end{proof}

We can also remove the mean zero assumption in some circumstances.

\begin{theorem}
	Let $X_1, \dots, X_n$ be independant, mean zero random vectors in a normed space. Then
	%
	\[ \EE \left\| \sum X_i - \EE X_i \right\| \leq 4 \EE \left\| \sum \varepsilon_i X_i \right\|, \]
	%
	where $\{ \varepsilon_i \}$ are i.i.d and symmetric Bernoulli.
\end{theorem}
\begin{proof}
	Since $\sum X_i' - \EE X_i'$ has mean zero, where $\{ X_i \}$ and $\{ X_i' \}$ are independant and identically distributed,
	%
	\begin{align*}
		\EE \left\| \sum X_i - \EE X_i \right\| &\leq \EE \left\| (\sum X_i - \EE X_i) - (\sum X_i' - \EE X_i') \right\|\\
		&= \EE \left\| \sum X_i - X_i' \right\|
	\end{align*}
	%
	We can now apply the standard symmetrization lemma to $X_i - X_i'$ to yield
	%
	\[ \EE \left\| \sum X_i - X_i' \right\| \leq 2 \EE \left\| \sum \varepsilon_i X_i - \varepsilon_i X_i' \right\| \leq 4 \EE \left\| \sum \varepsilon_i X_i \right\|. \]
	%
	TODO: Can we improve the 4 to a 2?
\end{proof}


\section{Matrix Completion}






\section{Contraction}

There is one more useful inequality we discuss in this chapter, known as \emph{ contraction}. It works kind of like an $l^1$, $l^\infty$ bound.

\begin{theorem}
    Let $x_1, \dots, x_n$ be vectors in some normed space, and let $a_1, \dots, a_n$ be real numbers. Then
    %
    \[ \EE \left\| \sum a_i \varepsilon_i x_i \right\| \leq \| a \|_\infty \left\| \sum \varepsilon_i x_i \right\|. \]
\end{theorem}
\begin{proof}
	Without loss of generality, assume $\| a \|_\infty = 1$. The map $f(a) = \EE \left\| \sum a_i \varepsilon_i x_i \right\|$ is a convex function for $\RR^n$ to $\RR$. The maximum principle for convex functions tells us the maximum of $f(a)$ for all $a$ with $\| a \|_\infty \leq 1$ is attained at an extreme point of the set, i.e. at a value of $a$ with $a_i = \pm 1$ for all $i$. For this point, $a_i \varepsilon_i$ has the same distribution as $\varepsilon_i$, and so
	%
	\[ f(a) = \EE \left\| \sum \varepsilon_i x_i \right\| \]
	%
	which completes the proof.
\end{proof}

\begin{remark}
    We can also prove this identity for convex functions of the norm.
\end{remark}

We can also contract on general distributions.

\begin{theorem}
	Let $X_1,\dots, X_n$ be mean zero random vectors in a normed space, and let $a \in \RR^n$. Then
	%
	\[ \EE \left\| \sum a_i X_i \right\| \leq 4 \| a \|_\infty \EE \left\| \sum X_i \right\| \]
\end{theorem}
\begin{proof}
	We apply symmetrization together with the last contraction principle to conclude
	%
	\begin{align*}
		\EE \left\| \sum a_i X_i \right\| &\leq 2 \EE \left\| \sum a_i \varepsilon_i X_i \right\| \leq 2 \| a \|_\infty \EE \left\| \sum \varepsilon_i X_i \right\| \leq 4 \| a \|_\infty \EE \left\| X_i \right\|. \qedhere
	\end{align*}
\end{proof}

As an application, we can prove a version of symmetrization which introduces Gaussians into a random sum, rather than symmetric Bernoulli distributions.

\begin{theorem}
    Let $X_1, \dots, X_n$ be independant, mean zero random vectors in a norm space. Let $g_1, \dots, g_n \sim N(0,1)$ be independant vectors. Then
    %
    \[ \frac{1}{(\log n)^{1/2}} \EE \left\| \sum g_i X_i \right\| \lesssim \EE \left\| \sum X_i \right\| \leq 3 \EE \left\| \sum g_i X_i \right\| \]
\end{theorem}
\begin{proof}
	We first perform normal symmetrization, so that
    %
    \[  \EE \left\| \sum X_i \right\| \leq 2 \EE \left\| \sum \varepsilon_i X_i \right\|. \]
    %
    But now, since $\EE |g_i| = (2/\pi)^{1/2}$, we can apply Jensen's inequality to conclude
    %
    \begin{align*}
    	\EE \left\| \sum \varepsilon_i X_i \right\| &\leq (\pi/2)^{1/2} \EE_X \left\|  \EE_g \sum \varepsilon_i |g_i| X_i \right\| \\
    	&\leq (\pi/2)^{1/2} \EE \left\| \sum \varepsilon_i |g_i| X_i \right\|\\
    	&= (\pi/2)^{1/2} \EE \left\| \sum g_i X_i \right\|.
    \end{align*}
    %
    where the last equality followed because $\varepsilon_i |g_i|$ is distributed identically to $g_i$. Putting this together with the first inequality gives the upper bound.

    To prove the lower bound, we apply contraction and symmetrization to conclude
    %
    \begin{align*}
    	\EE \left\| \sum g_i X_i \right\| &= \EE \left\| \sum \varepsilon_i g_i X_i \right\|\\
    	&= \EE \| g \|_\infty \cdot \EE_\varepsilon \left\| \sum \varepsilon_i X_i \right\|\\
    	&\leq 2 \EE \| g \|_\infty \cdot \EE_X \left\| \sum X_i \right\|\\
    	&\lesssim 2 (\log n)^{1/2} \cdot \EE \left\| \sum X_i \right\|. \qedhere
    \end{align*}
\end{proof}

We can also prove a version of contraction which works for more general functions, which are {\it contraction} maps.

\begin{theorem}[Talagrand's Contraction Principle]
    Let $T$ be a bounded subset of $\RR^n$, and let $\varepsilon_1, \dots, \varepsilon_n$ be independant symmetric Bernoulli random variables. Let $\phi: \RR \to \RR$ be contraction maps, i.e. Lipschitz functions with $|\phi(x - y)| \leq |x - y|$. Then
    %
    \[ \EE \left( \sup_t \left( \sum \varepsilon_i \phi_i(t_i) \right) \right) \leq \EE \left( \sup_t \sum \varepsilon_i t_i \right) \]
\end{theorem}
\begin{proof}
	TODO.
\end{proof}











\chapter{Chaining}

In this chapter we try and simultaneously control the supremum of a family of random variables $\{ X_t : t \in T \}$, where $T$ is an arbitrary infinite index set. Of course, if the family of random variables are independant and identically distributed with common distribution $X$, this is easy, since $\sup X_t = \| X \|_{L^\infty}$ almost surely. To get interesting results, we need to assume some kind of dependence among the random variables. We will find that if the random variables $\{ X_t \}$ are `sufficiently continuous', then controlling $\sup_t X_t$ relies solely on studying the geometry of the index set $T$.

\begin{example}
	Given a random matrix $A$. It is an important problem to bound the operator norm
	%
	\[ \EE \| A \| = \EE \sup_{|x| = 1} |Ax|. \]
	%
	Here the index set $T = \{ x : |x| = 1 \}$ is the unit sphere. Note that if the rows of $A$ are independant and uniformly subgaussian, with subgaussian norm $K$, then $\| Ax \|_{\psi_2} \leq K |x|$, so by linearity, given $x,y \in T$,
	%
	\[ \| |Ax| - |Ay| \|_{\psi_2} \leq \| |A(x-y)| \|_{\psi_2} \leq K |x - y|. \]
	%
	Thus if $x$ and $y$ are close, then $Ax$ and $Ay$ are very close with high probability. This is what we mean by the process being `sufficiently continuous'.
\end{example}

We make a technical note that though we do assume that our index sets are infinite, we really concentrate on obtaining results for the suprema of finite, or countable index stochastic processes, which are \emph{independant} of the cardinality of the index set. For instance, $\sup X_t$, need not even be measurable if we take the supremum over uncountably many indices $t$. For instance, when we refer to $\EE(\sup X_t)$, what we really study is
%
\[ \sup \left\{ \EE \left( \sup_{t \in T_0} X_t \right): T_0 \subset T, T_0\ \text{finite} \right\}. \]
%
We find this agrees with the standard expectation if the index set is countable, by the monotone convergence theorem. For uncountable index sets that are of interest, such as variants of Brownian motion, or the random matrix process described above, one can use certain continuity properties of the process to ensure that the supremum is a measurable function, and that the standard expectation agrees with the finitary expectation defined above.

\section{Covering Numbers}

We recall some notation from metric space theory. If $T$ is a metric space, a $\varepsilon$ \textbf{net} $S$ is a subset of $T$ such that for any $t \in T$, there is $s \in S$ such that $d(t,s) < \varepsilon$. A $\varepsilon$ \textbf{packing} of $T$ is a $\varepsilon$ separated family of points in $T$. The $\varepsilon$ \textbf{covering number} of $T$ is the smallest cardinality of a $\varepsilon$ net, denoted $N(T,\varepsilon)$. On the other hand, the maximum cardinality of a $\varepsilon$ packing is denoted by $P(T,\varepsilon)$.

\begin{lemma}
	For any metric space $T$ and $\varepsilon > 0$,
	%
	\[ P(T,2\varepsilon) \leq N(T,\varepsilon) \leq P(T,\varepsilon). \]
\end{lemma}
\begin{proof}
	If $S$ is a $2\varepsilon$ packing, and $U$ a $\varepsilon$ net, then each set in $U$ can cover at most one element of $S$, so $|S| \leq |U|$. This gives that $P(T,2\varepsilon) \leq N(T,\varepsilon)$. On the other hand, if $S$ is a maximal $\varepsilon$ packing of $T$, then it is automatically a $\varepsilon$ net, and so $|S| \geq N(T,\varepsilon)$. Since $S$ was arbitrary, $P(T,\varepsilon) \geq N(T,\varepsilon)$.
\end{proof}

It is often useful to apply a `volumetric argument' to calculate covering numbers. We suppose $T$ is a subset of some ambient metric space $S$, which is equipped with a Radon measure $\mu$. We consider the quantities
%
\[ \mu(B(\varepsilon)) = \sup_{t \in S} \mu(B(t,\varepsilon)). \]
%
Given a set $U \subset S$, we let
%
\[ U(\varepsilon) = \{ s \in S: \text{There is}\ u \in U\ \text{such that}\ d(s,u) < \varepsilon \} \]
%
denote the $\varepsilon$ \emph{thickening} of $U$.

\begin{lemma}
	For any $\varepsilon > 0$,
    %
    \[ \frac{\mu(T)}{\mu(B(\varepsilon))} \leq N(T,\varepsilon) \leq P(T,\varepsilon) \leq \frac{\mu(T(\varepsilon/2))}{\mu(B(\varepsilon/2))}, \]
    %
    where $B$ is the unit ball in $\RR^n$.
\end{lemma}
\begin{proof}
	To prove the lower bound, let $U$ denote a $\varepsilon$ net of $T$. Then the $\varepsilon$ balls around each point in $U$ cover $T$, so a union bound gives
    %
    \[ \mu(T) \leq |U| \cdot \mu(B(\varepsilon)). \]
    %
    Since $U$ was arbitrary, taking $U$ of minimum cardinality gives that $\mu(T) \leq N(T,\varepsilon) \mu(B(\varepsilon))$.

    To prove the upper bound, let $U$ denote a packing of $T$. Then the balls of radius $\varepsilon/2$ around the points of $U$ are disjoint from one another, and are contained in $T(\varepsilon/2)$. Thus
    %
    \[ |U| \cdot \mu(B(\varepsilon/2)) \leq \mu(T(\varepsilon/2)). \]
    %
    Since $U$ was arbitrary, taking $U$ of maximum cardinality gives that $P(T,\varepsilon) \cdot \mu(B(\varepsilon/2)) \leq \mu(T(\varepsilon/2))$.
\end{proof}

\begin{example}
	Consider the unit ball $B$ in $\RR^n$, equipped with the Lebesgue measure $| \cdot |$. Because the Lebesgue measure is homogenous, we know that $|B(\varepsilon)| = \varepsilon^n \cdot |B|$ for all $\varepsilon > 0$. Furthermore, $B(\varepsilon/2)$ is the ball of radius $1 + \varepsilon/2$. Thus the volumetric argument implies that
	%
	\[ \frac{1}{\varepsilon^n} \leq N(B,\varepsilon) \leq P(B,\varepsilon) \leq \frac{(2 + \varepsilon)^n}{\varepsilon^n}. \]
	%
	If $S \subset B$ is the unit sphere, then $S(\varepsilon/2)$ is also contained in the ball of radius $1 + \varepsilon/2$, which gives that
	%
	\[ N(S,\varepsilon) \leq P(S,\varepsilon) \leq \frac{(2 + \varepsilon)^n}{\varepsilon^n}. \]
\end{example}

\begin{example}
	Consider the Hamming cube $H = \{ 0, 1 \}^n$, with the Hamming distance metric, defined for two strings $x,y \in H$ by setting
	%
	\[ d(x,y) = \# \{ i: x_i \neq y_i \}. \]
	%
	For any $x \in H$, and any integer $k \leq n$, the ball of radius $k$ around $x$ contains $\sum_{i < k} {n \choose i}$ points. Thus applying the volumetric argument, we find that
	%
	\[ \frac{2^n}{\sum_{i < k} {n \choose i}} \leq N(H,k) \leq P(H,k) \leq \frac{2^n}{\sum_{i < k/2} {n \choose i}}\; . \]
\end{example}

If $T_0 \subset T_1$, it is not necessarily true that $N(T_0,\varepsilon) \leq N(T_1,\varepsilon)$. But there is a remedy to this situation. We define an \textbf{exterior} $\varepsilon$ \textbf{cover} of $T$ as a subset $U$ of $S$ such that for any $t \in T$, there is $u \in U$ such that $d(t,u) < \varepsilon$. The \textbf{exterior} $\varepsilon$ covering number $N(T,S,\varepsilon)$ of $T$ with respect to $S$ is the minimum cardinality of an exterior $\varepsilon$ net.

\begin{lemma}
	For any set $S$, and $\varepsilon > 0$, $N(T,\varepsilon) \leq N(T,S,\varepsilon/2)$.
\end{lemma}
\begin{proof}
	If $U$ is any exterior $\varepsilon/2$ net of $T$, define a set $U_0$, by picking, for each $u \in U$, a point $u_0 \in T$ such that $d(u,u_0) < \varepsilon/2$ (for any minimal $\varepsilon/2$ net, such a point $u_0$ must exist). Then $U_0$ is a $\varepsilon$ net of $T$ by the triangle inequality.
\end{proof}

The exterior covering number is obviously monotone, and the previous lemma implies that if $T_0 \subset T_1$, then
%
\[ N(T_0,\varepsilon) \leq N(T_0,T_1,\varepsilon/2) \leq N(T_1,T_1,\varepsilon/2) = N(T_1,\varepsilon/2). \]
%
Thus $N(T_0,\varepsilon) \leq N(T_1,\varepsilon/2)$, which normally implies $N(T_1,\varepsilon)$ upper bounds $N(T_0,\varepsilon)$, up to a constant.

\section{Chaining}

We begin by studying a central method for bounding the suprema of random processes. Given a random process $\{ X_t : t \in T \}$ indexed by a metric space $T$, we consider a $\varepsilon$ net $S$ of $T$ with cardinality $N(T,\varepsilon)$. Given any $t \in T$, there is $\pi(t) \in S$ such that $d(t,\pi(t)) < \varepsilon$. Thus 
%
\[ \sup_{t \in T} X_t = \sup_{t \in T} X_{\pi(t)} + (X_t - X_{\pi(t)}) \leq \sup_{s \in S} X_s + \sup_{t \in T} (X_t - X_{\pi(t)}). \]
%
We have a quantative bound on the number of random variables in the sub-process $\{ X_s : s \in S \}$ so it is often easy to bound this supremum. And the fact that $d(t,\pi(t)) < \varepsilon$ for each $t$ often enables one to obtain bounds on $X_t - X_{\pi(t)}$, given that $X_t - X_{\pi(t)}$ is small if $d(t,\pi(t))$ is small. This is already enough to obtain certain results.

\begin{example}
	Let $T = \{ x \in \RR^n : |x| = 1 \}$. Then if $S$ is a $\varepsilon$, by linearity, the above equation gives
	%
	\[ \| A \| \leq \sup_{x \in S} |Ax| + \varepsilon \| A \|. \]
	%
	This inequality can be rearranged to read
	%
	\[ \| A \| \leq \frac{1}{1 - \varepsilon} \sup_{x \in S} |Ax| \]
	%
	This removes the problem of bounding the differences completely.
\end{example}

\begin{example}
	Let $A$ be a $m \times n$ matrix. If $x \in \RR^n$, and $y \in \RR^m$ are given, with $|x - x_0| = \varepsilon$ and $|y - y_0| = \varepsilon$, then
	%
	\[ |y^TAx - y_0^TAx_0| \leq |y^TA(x - x_0)| + |(y - y_0)^TAx_0| \leq 2 \| A \| \varepsilon. \]
	%
	Thus if $T_n = \{ x \in \RR^n: |x| = 1 \}$ and $T_m = \{ x \in \RR^m : |x| = 1 \}$, then for any pair of $\varepsilon$ net $S_n \subset T_n$ and $S_m \subset T_m$,
	%
	\begin{align*}
		\| A \| &= \sup_{x \in T_n, y \in T_m} |y^T Ax|\\
		&\leq \sup_{x \in S_n, y \in S_m} |y^T A x| + 2 \| A \| \varepsilon.
	\end{align*}
	%
	This can be rearranged to read
	%
	\[ \| A \| \leq \frac{1}{1 + 2\varepsilon} \sup_{x \in S_n, y \in S_m} |y^T A x|. \]
\end{example}

\section{Matrix Concentration}

Let $A$ be an $m \times n$ matrix. Recall the operator norm $\| A \|$, which is the smallest value such that $|Ax| \leq \| A \| |x|$ for all $x \in \mathbf{R}^n$. We can use a \emph{covering argument} to establish concentration results for the operator norm of random matrices. One difficulty in bounding the operator norm is that we must bound the random quantities $|Ax|$ for \emph{infinitely many} values $x$.

\begin{lemma}
    Let $A$ be an $m \times n$ matrix, and $N$ an $\varepsilon$ net on $S^{n-1}$. Then
    %
    \[ \sup \{ |Ax| : x \in N \} \leq \| A \| \leq (1 - \varepsilon)^{-1} \sup \{ |Ax| : x \in N \}. \]
    %
    If, additionally, $M$ is a $\varepsilon$ net on $S^{n-1}$, then
    %
    \[ \sup \{ (Ax) \cdot y : x \in N, y \in M \} \leq \| A \| \leq \frac{1}{1 - 2\varepsilon} \cdot \sup \{ (Ax) \cdot y : x \in N, y \in M \}. \]
\end{lemma}
\begin{proof}
    We begin with the first equation. The lower bound is obvious. Given $x_0 \in S^{n-1}$, consider $x \in N$ with $|x - x_0| \leq \varepsilon$. Then
    %
    \[ |Ax_0| \leq |Ax| + |A \cdot (x_0 - x)| \leq |Ax| + \varepsilon \| A \| \leq \sup \{ |Ax| : x \in N \} + \varepsilon \| A \|. \]
    %
    Taking suprema on both sides for all $x_0 \in S^{n-1}$ gives
    %
    \[ \| A \| \leq \sup \{ |Ax| : x \in N \} + \varepsilon \| A \|.  \]
    %
    And rearranging gives the upper bound.

    To prove the second equation, we note that for any $y \in \RR^m$,
    %
    \[ |y| = \sup \{ z \cdot y : |z| = 1 \} \]
    %
    This gives the lower bound. To prove the upper bound, for each $x_0 \in S^{n-1}$ and $y_0 \in S^{m-1}$, consider $x \in N$, $y \in M$ with $|x - x_0| \leq \varepsilon$ and $|y - y_0| \leq \varepsilon$. Thus
    %
    \begin{align*}
        (Ax_0) \cdot y_0 &= Ax \cdot y + A(x_0 - x) \cdot y + Ax_0 \cdot (y_0 - y)\\
        &\leq Ax \cdot y + 2\varepsilon \| A \|\\
        &\leq \sup \{ (Ax \cdot y) : x \in N, y \in M \} + 2\varepsilon \| A \|
    \end{align*}
    %
    We then just take suprema over $x_0$ and $y_0$, and rearrange.
\end{proof}

Applying this trick, together with a covering argument and a union bound, easily enables us to bound the norm of a matrix with high probability.

\begin{theorem}
    Let $A$ be an $m \times n$ matrix with independant, mean zero subgaussian entries. There exists a universal constant $C$ such that if $K = \max \| A_{ij} \|_{\psi_2}$, and $t > 0$,
    %
    \[ \| A \| \leq C K (\sqrt{m} + \sqrt{n} + t), \]
    %
    with probability at least $1 - 2\exp(-t^2)$.
\end{theorem}
\begin{proof}
    Fix $\varepsilon = 1/4$. Then consider a $\varepsilon$ net $N$ of $S^{n-1}$ with $|N| \leq 9^n$, and a $\varepsilon$ net $M$ of $S^{n-1}$ with $|M| \leq 9^n$. The last lemma implies
    %
    \[ \| A \| \leq 2 \sup \left\{ (Ax) \cdot y : x \in N, y \in M \right\} \]
    %
    For each $x \in N$ and $y \in M$, we calculate
    %
    \[ (Ax) \cdot y = A_{ij} x_i y_j \]
    %
    This is the sum of $nm$ subgaussian independant random variables. Thus
    %
    \begin{align*}
        \| (Ax) \cdot y \|_{\psi_2}^2 &\lesssim \sum \| A_{ij} x_i y_j \|_{\psi_2}^2 \leq K^2 \sum x_i^2 y_j^2 = K^2.
    \end{align*}
    %
    Thus
    %
    \[ \PP(|(Ax) \cdot y| \geq u) \leq 2 \exp(-cu^2/K^2) \]
    %
    Applying a union bound, we find that
    %
    \begin{align*}
        \PP( \| A \| \leq 2u ) &\geq \PP(\forall x \in N, y \in M: |(Ax) \cdot y| \leq u)\\
        &\geq 1 - 2 \cdot 9^{n+m} \exp(-cu^2/K^2),
    \end{align*}
    %
    setting $u = (C/c)^{-1/2} K(\sqrt{m} + \sqrt{n} + t)$, where $C$ is a large constant, we find that since $(\sqrt{m} + \sqrt{n} + t)^2 \geq (m + n + t^2)$,
    %
    \begin{align*}
        \PP( \| A \| \leq CK(\sqrt{m} + \sqrt{n} + t)) &\geq 1 - 2 \cdot 9^{n+m} \exp(-C (\sqrt{m} + \sqrt{n} + t)^2)\\
        &\geq 1 - 2 \cdot 9^{n+m} \exp(-C(m + n) - Ct^2)).
    \end{align*}
    %
    For $C \geq \log 9$, this is greater than $1 - 2 \exp(-Ct^2) \geq 1 - 2 \exp(-t^2)$.
\end{proof}

\begin{corollary}
    $\EE \| A \| \lesssim K (\sqrt{m} + \sqrt{n})$.
\end{corollary}

\begin{remark}
    The expectation bound is essentially tight. If the entries $\{ A_{ij} \}$ have unit variances, then
    %
    \begin{align*}
        \EE \| A \| &\geq \frac{1}{\min(m,n)^{1/2}} \EE \| A \|_F \geq \frac{1}{\min(m,n)^{1/2}} \left(\sum \EE A_{ij}^2 \right)^{1/2}\\
        &= \max(n,m)^{1/2} \gtrsim n^{1/2} + m^{1/2}.
    \end{align*}
\end{remark}

%Recall the singular value decomposition, which if $A$ has rank $r$, enables us to find constants $s_i > 0$ and two orthonormal families of unit vectors $v_i \in \mathbf{R}^n$, $u_i \in \mathbf{R}^m$ such that $A = \sum_{i = 1}^r s_i u_i v_i^t$. The left singular vectors $u_i$ are the orthonormal eigenvectors of $AA^T$, and the $v_i$ orthonormal eigenvectors of $A^T A$, and the $s_i$ are the square roots of the eigenvalues of $AA^T$. Intuitively, this means that there exists two orthogonal bases such that $A$ is expressed as a diagonal matrix with non-negative entries; after a `twist', $A$ is just given by scaling in certain directions.

%Courant-Fisher's min-max theorem says that if $A$ is a symmetric matrix with eigenvalues $\lambda_1 \geq \dots \geq \lambda_n$, then
%
%\[ \lambda_i = \max_{\dim V = i} \min \left\{ Ax \cdot x : |x| = 1, x \in V \right\} \]
%
%In particular, this means that if $A$ is any matrix with singular values $s_1 \geq \dots \geq s_n$, then
%
%\[ s_i = \max_{\dim V = i} \min \left\{ |Ax| : |x| = 1, x \in V \right\} \]
%
%Thus there is a close correspondence between an operator and it's inverse.

%We also rely on various operator norms to quantify the magnitude of matrices. The {\it operator norm} of a matrix $A$ is
%
%\[ \| A \| = \max_{|x| = 1} |Ax| = \max \frac{|Ax|}{|x|} = \max_{|x| = |y| = 1} |Ax \cdot y| \]
%
%The min-max theorem therefore says that $s_1 = \| A \|$. If $s_n = 0$, then $A$ is not invertible, so we can view the last singular value of a matrix as a measure of non-degeneracy. In fact, $s_n = \| A^+ \|$, where $A^+$ is the {\it Moore-Penrose} inverse of $A$. The {\it Frobenius}, or {\it Hilbert-Schmidt} norm of a matrix $A$ is the $l^2$ norm of it's entries, i.e.
%
%\[ \| A \|_F = \left( \sum |A_{ij}|^2 \right)^{1/2} = \left( s_1^2 + \dots + s_n^2 \right)^{1/2} \]
%
%The Frobenius norm is induced by the inner product $A \cdot B = \text{tr}(A^T B)$. Note that $\| A \|_F = \| s \|_2$, whereas $\| A \| = \| s \|_\infty$. In particular, $\| A \| \leq \| A \|_F \leq (nm)^{1/2}$

%There are two natural norms on the space of matrices. The first is the operator norm, defined by the minimal value $|A|$ such that $|Ax| \leq |A||x|$, or equivalently, the smallest value satisfying the bound $|Ax \cdot y| \leq |A| |x| |y|$ for $x \in \mathbf{R}^n$ and $y \in \mathbf{R}^m$. The operator norm equals the largest singular value of $A$. The Frobenius norm $|A|_F$ is the $L^2$ norm of the entries of $A$, and is given as the norm induced by the inner product $(A,B) = \text{Tr}(A^T B)$.

Working slightly harder, using Bernstein's inequality instead of Hoeffding's ienquality, we can both upper bound and lower bound the behaviour of the operator with high probability; our proof should be directly compared to the fact that a random vector is with high probability close to the sphere with radius $n^{1/2}$.
%Recall that for any $m \times n$ matrix $A$, there exists values $s_1 \geq \dots \geq s_n \geq 0$ and an orthonormal basis of eigenvectors $v_1, \dots, v_n$ such that $A = \sum s_i v_i v_i^T$. The values $\{ s_i \}$ are called {\it singular values}. We have $\| A \| = s_1$, and $s_i$ is the square root of the $i$'th largest eigenvalue of the positive semi-definite matrix $A^TA$. Our result says that with high probability, a matrix $A$ is {\it almost} an isometry multiplied by $\sqrt{m}$, in the sense that for any $x \in \RR^n$,
%
%\[ ||Ax| - m^{1/2} |x|| \leq O(\sqrt{n}). \]

\begin{theorem}
    Let $A$ be an $m \times n$ matrix whose rows are independant, mean zero, subgaussian isotropic random vectors. Then there exists a universal constant $C$ such that if $K$ bounds the maximum subgaussian norm of the rows of $A$, then with probability $1 - 2 \exp(-t^2)$, for any $x \in \RR^n$,
    %
    \[ ||Ax| - m^{1/2}|x|| \leq C K^2 m^{1/2} (n^{1/2} + t) |x|. \]
    %
\end{theorem}
\begin{proof}
	We know that $K \gtrsim 1$, and without loss of generality, we can assume that $K \geq 1$. We prove the stronger conclusion that with probability $1 - 2\exp(-t^2)$,
    %
    \[ \| (A^TA)/m - 1 \| \leq K^2 \max(\delta,\delta^2)\quad\text{where}\quad \delta = C \left( (n/m)^{1/2} + t / m^{1/2} \right). \]
    %
    If this inequality is true, then for any $x$,
    %
    \begin{align*}
    	||Ax|^2 - m |x|^2| &= |(Ax \cdot Ax) - m (x \cdot x)| = |(A^TA - m)x \cdot x|\\
    	&\leq \| A^TA - m \| \cdot |x|^2 \leq \max(m K^2 \delta, (m K^2 \delta)^2) \cdot |x|^2.
    \end{align*}
    %
    Recalling the calculation we made for subgaussian vectors, this implies
 	%
 	\begin{align*}
    	||Ax| - m^{1/2} |x|| &\leq K^2 \delta m |x| \leq C K^2 m^{1/2} (n^{1/2} + t) |x|.
    \end{align*}
    %
    To prove the identity, it suffices to prove that for a $1/4$-net $N$, with $|N| \leq 9^n$, with the required probability we have
    %
    \[ \max_{x \in N} \left| |Ax|^2/m - 1 \right| \leq \varepsilon/2. \]
    %
    Fix $x \in N$. If we set $X_i = A_i \cdot x$, then $|Ax|^2 = \sum X_i^2$. By assumption, the $A_i$ are independant, isotropic, subgaussian random vectors with $\| A_i \|_{\psi_2} \leq K$. Thus the $X_i$ are independant subgaussian random vectors with $\EE X_i^2 = 1$ and $\| X_i \|_{\psi_2} \leq K$. Therefore $X_i^2 - 1$ are independant, mean zero, subexponential random variables with $\| X_i^2 - 1 \| \lesssim K^2$, and we can apply Bernstein's inequality to conclude that there exists a universal constant $c$ such that
    %
    \begin{align*}
        \PP(|Ax|^2/m - 1| \geq \varepsilon/2) &= \PP(|(1/m) \sum X_i^2 - 1| \geq \varepsilon/2)\\
        &\leq 2 \exp(-c m \min(\varepsilon^2/K^4, \varepsilon/K^2))\\
        &= 2 \exp(-c \delta^2 m)\\
        &\leq 2 \exp(-c \cdot C^2 (n + t^2)).
    \end{align*}
    %
    We can then apply a union bound to conclude
    %
    \begin{align*}
    	\PP \left( \max_{x \in N} |Ax|^2/m - 1 \geq \varepsilon/2 \right) &\leq 2 |N| \exp(-c \cdot C^2 (n + t^2))\\
    	&\leq 2 \cdot 9^n \exp(-c \cdot C^2 (n + t^2))\\
    	&\leq 2 \exp((\log 9 - c \cdot C^2) n - c \cdot C^2 t^2).
    \end{align*}
    %
    Choosing $C^2 \geq \log 9 / c$ gives the required inequality.
\end{proof}

In other applications of covering, the value $\sup X_t - X_{\pi(t)}$ is not able to be completely discarded, and we must come up with a more careful method to bound the quantity. We can do this by applying a `multi-scale' covering argument, rather than just a single scale covering. This technique is known as \emph{chaining}. Given a random process $\{ X_t : t \in T \}$, where $T$ is a metric space, we define the Subgaussian norm
%
\[ \| X \|_{\psi_2} = \sup_{t,s \in T} \frac{\| X_t - X_s \|_{\psi_2}}{d(t,s)}. \]
%
If a random process is subgaussian, then chaining enables us to obtain a bound on the expectation of it's supremum. If $\| X \|_{\psi_2} < \infty$, we say $X$ has \textbf{subgaussian increments}.

\begin{theorem}
	Let $\{ X_t : t \in T \}$ be a random process. Then
	%
	\[ \EE \left( \sup_{t \in T} X_t \right) \lesssim \| X \|_{\psi_2} \int_0^\infty \sqrt{\log N(T,\varepsilon)}\; d\varepsilon. \]
\end{theorem}
\begin{proof}
	Without loss of generality, we may assume that $T$ is finite. For each $k$, we consider a $1/2^k$ net $S_k$ with $|S_k| = N(T,2^k)$. If $1/2^M < \min(d(t,t'): t,t' \in T)$, then $S_M = T$, and if $1/2^N > \diam(T)$, $|S_N| = 1$. If for each $t \in T$, we let $\pi_k(t)$ be a point in $S_k$ with $d(t,\pi_k(t)) < 1/2^k$, then $\pi_N$ is constant, and $\pi_M$ is the identity function. Thus for any $t$,
	%
	\[ X_t = \sum_{k = -\infty}^\infty X_{\pi_{k+1}(t)} - X_{\pi_k(t)}. \]
	%
	In particular,
	%
	\[ \sup_{t \in T} X_t \leq \sum_{k = -\infty}^\infty \sup_{t \in T} X_{\pi_{k+1}(t)} - X_{\pi_k(t)}. \]
	%
	Notice that the latter supremum is really over pairs of indices in $S_k$ and $S_{k+1}$. Thus it is a supremum over at most $N(T,1/2^{k+1})^2$ random variables, and each has subgaussian norm at most $K/2^{k-1}$. Thus we conclude that
	%
	\begin{align*}
		\EE \left( \sup_{t \in T} X_{\pi_{k+1}(t)} - X_{\pi_k(t)} \right) &\lesssim (K/2^{k-1}) \sqrt{\log N(T,1/2^{k+1})^2}\\
		&\lesssim (K/2^k) \sqrt{\log N(T,1/2^{k+1})}.
	\end{align*}
	%
	Thus
	%
	\[ \EE \left( \sup_{t \in T} X_t \right) \lesssim K \sum_{k = -\infty}^\infty 2^k \sqrt{\log N(T,1/2^k)} \approx K \int_0^\infty \sqrt{\log N(T,\varepsilon)} d\varepsilon. \qedhere  \]
\end{proof}

If we work slightly harder, then we can obtain a tail bound version of Dudley's inequality, which is often useful.

\begin{theorem}
	With probability $1 - 2 \exp(-u^2)$,
	%
	\[ \sup_{t \in T} X_t \lesssim \| X \|_{\psi_2} \left( \int_0^\infty \sqrt{\log N(T,\varepsilon)}\; d\varepsilon + u \cdot \diam(T) \right). \]
\end{theorem}
\begin{proof}
	A simple union bound argument shows that there exists a constant $C$ such that
	%
	\[ \sup_{t \in T} (X_{\pi_{k+1}(t)} - X_{\pi_k(t)}) \leq (C \cdot K/2^k) \left(\sqrt{\log |S_{k+1}|} + u \right) \]
	%
	with probability $1 - 2\exp(-u^2)$. Next, we note that if $1/2^N \leq \diam(T)$, then $S_N = T$. If $M$ is the greatest integer with $1/2^M \geq \diam(T)$, then a simple union bound gives, for any sequence $\{ u_k \}$,
	%
	\begin{align*}
		\sup_{t \in T} X_t &\leq C K \sum_{k = M}^{\infty} (1/2^k) \cdot \left( \sqrt{\log |S_k|} + u_k \right)\\
		&\leq K \int_0^\infty \sqrt{\log N(T,\varepsilon)}\; d\varepsilon + C K \sum_{k = M}^\infty (u_k/2^k).
	\end{align*}
	%
	with probability $1 - 2 \sum_{k = M}^\infty \exp(-u_k^2)$. If $u_k = u + k + 10$, then
	%
	\[ \sup_{t \in T} X_t \lesssim K \int_0^\infty \sqrt{\log N(T,\varepsilon)}\; d\varepsilon + u \diam(T) + \diam(T) \]
	%
	with probability greater than
	%
	\[ 1 - 2 \exp(-u^2) \exp(-100) \sum_{k = 1}^\infty \exp(-k^2) \geq 1 - 2\exp(-u^2) \]
	%
	Note that if $\varepsilon \leq \diam(T)/2$, then $N(T,\varepsilon) \geq 2$, so
	%
	\[ \int_0^\infty \sqrt{\log N(T,\varepsilon)} \gtrsim \diam(T). \]
	%
	In particular, this means that with probability greater than $1 - 2 \exp(-u^2)$,
	%
	\[ \sup_{t \in T} X_t \lesssim K \int_0^\infty \sqrt{\log N(T,\varepsilon)}\; d\varepsilon + u \diam(T). \qedhere \]
\end{proof}

Here is a fundamental example of the kinds of random processes we study. Given $T \subset \RR^n$, and a Gaussian vector $g \sim N(0,I_n)$, we consider the supremum
%
\[ \sup_{t \in T} g \cdot t. \]
%
The expectation of this supremum is known as the \textbf{Gaussian width} of $T$, denoted $w(T)$. Note that if $X_t = g \cdot t$, then
%
\[ \| X_t - X_s \|_{\psi_2} = \| g \cdot (t - s) \|_{\psi_2} \lesssim |t-s|. \]
%
Thus $\| X \|_{\psi_2} \lesssim 1$. Thus Dudley's inequality shows that
%
\[ w(T) \lesssim \int_0^\infty \sqrt{\log N(T,\varepsilon)}, \]
%
where we use the standard metric on $\RR^n$ to define the covering numbers.

\begin{example}
	Let $T$ be the unit ball in $\RR^n$. Then the volumetric argument we considered early on in this section let us show $N(T,\varepsilon) \leq (3/\varepsilon)^n$, for $\varepsilon \leq 1$, and $N(T,\varepsilon) = 1$ for $\varepsilon \geq 1$. Thus
	%
	\[ w(T) \lesssim \int_0^1 \sqrt{n \log(3/\varepsilon)}\; d\varepsilon \lesssim \sqrt{n}. \]
\end{example}

There are situations where Dudley's inequality is not sharp. But we will later provide lower bounds on the expectation of certain random processes which show in most cases, Dudley's inequality is sharp up to a $\log(n)$ factor.

\begin{example}
	Consider the Gaussian width of
	%
	\[ T = \left\{ \frac{e_k}{\sqrt{1 + \log k}} : k = 1, \dots, n \right\} \]
	%
	We claim that $w(T)$ is bounded independantly of $n$. Obtaining this result is equivalent to bounding
	%
	\[ \EE \left(\max_{1 \leq k \leq n} \frac{g_k}{\sqrt{1 + \log k}} \right), \]
	%
	independantly of $n$, where $g_1, \dots, g_n$ are independant $N(0,1)$ random variables. Applying a union bound, we find there exists a small constant $c$ such that
	%
	\begin{align*}
		\PP \left(\max_{1 \leq k \leq n} \frac{g_k}{\sqrt{1 + \log k}} \geq t \right) &\leq \sum_{k = 1}^n \PP \left(g_k \geq t \sqrt{1 + \log k} \right)\\
		&\leq \sum_{k = 1}^n \exp(-c \cdot (1 + \log k)t^2)\\
	\end{align*}
	%
	If $t^2 \geq 4/c$, we find
	%
	\begin{align*}
		\sum_{k = 1}^n \exp(-c \cdot (1 + \log k) t^2) &= \exp(-(c/2) \cdot t^2) \cdot \sum_{k = 1}^n \exp((c/2 - c (1 + \log k)) \cdot t^2)\\
		&\leq \exp(- (c/2) \cdot t^2) \sum_{k = 1}^\infty k^{-2} \lesssim \exp(-(c/2) \cdot t^2).
	\end{align*}
	%
	Thus
	%
	\begin{align*}
		\EE \left( \max_{1 \leq k \leq n} \frac{g_k}{\sqrt{1 + \log k}} \right) &\lesssim \sqrt{4/c} + \int_{(4/c)^{1/2}}^\infty \exp(-(c/2) \cdot t^2) \lesssim 1.
	\end{align*}
	%
	This gives a bound independant on $n$, so $w(T) \lesssim 1$. On the other hand, we calculate
	%
	\[ \left| \frac{g_i}{\sqrt{1 + \log i}} - \frac{g_j}{\sqrt{1 + \log j}} \right|^2 \leq \frac{2}{1 + \min(\log i, \log j)} \]
	%
	Thus for each $m$, the values
	%
	\[ \left\{ g_{n-m}/\sqrt{1 + \log(n-m)}, \dots, g_n/\sqrt{1 + \log m} \right \} \]
	%
	give a $2/\sqrt{1 + \log(n-m)}$ packing. Thus for each $k$, if
	%
	\[ \frac{2}{\sqrt{1 + \log(k+1)}} \leq \varepsilon \leq \frac{2}{1 + \log(k)}, \]
	%
	then $N(T,\varepsilon) \geq k$. In particular, this means
	%
	\begin{align*}
		\int_0^\infty N(T,\varepsilon) &\geq \sum_{k = 1}^n k \left( \frac{2}{\sqrt{1 + \log(k)}} - \frac{2}{\sqrt{1 + \log(k+1)}} \right)\\
		&\geq \sum_{k = 2}^n k \cdot \frac{\log(1 + 1/k)}{\log(k)^{3/2}}
	\end{align*}
	%
	and this value becomes unbounded as $n \to \infty$.
\end{example}

Gaussian width is very important to the application of the concepts of high dimensional probability to statistics.

\section{Empirical Processes}

An {\it empirical process} is a type of random process whose index set consists of {\it functions}. The motivating example, given i.i.d random variables $X_1, \dots, X_n$ with distribution given by some probability measure $\mu$, to consider the functional
%
\[ f \mapsto \frac{1}{n} \sum_{k = 1}^n f(X_k). \]
%
A natural question is how close this functional is to the functional
%
\[ f \mapsto \int f\; d\mu. \]
%
Approximating this integral by random samples is known as the {\it Monte Carlo method}. The law of large numbers implies that this functional does converge {\it pointwise} for each function $f$. But quantitative estimates on the error are much more useful. We now show that these functional do converge uniformly, over the class $\mathcal{F} = \{ f: [0,1] \to \RR : \| f \|_{\text{Lip}} \leq L \}$, for some $L < \infty$.

For a fixed function $f$, we can calculate that
%
\[ \EE \left| \frac{1}{n} \sum_{k = 1}^n f(X_k) - \EE f(X) \right| \leq  \Var \left( \frac{1}{n} \sum_{k = 1}^n f(X_k) \right)^{1/2} = O(1/\sqrt{n}). \]
%
The next theorem shows we can guarantee this expectation bound {\it uniformly} across an infinite family of functions. Notice that in the theorem, the supremum occurs inside the expectation, which makes the consequence that much more powerful.

\begin{theorem}
	Let $X_1, \dots, X_n$ be i.i.d random variables taking values in $[0,1]$. Then
	%
	\[ \EE \sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum_{k = 1}^n f(X_k) - \EE f(X) \right| \lesssim L \cdot n^{-1/2}. \]
\end{theorem}
\begin{proof}
	Let $X$ denote the common distribution of the $\{ X_k \}$. For each $f \in \mathcal{F}$, set
	%
	\[ X_f = \frac{1}{n} \sum_{k = 1}^n f(X_k) - \EE f(X). \]
	%
	Without loss of generality, we assume $L = 1$. Furthermore, because $X_f$ is translation invariant, we may assume we are working with the class of all 1 Lipschitz functions $f$ with $f(0) = 0$. In particular, this implies that $\| f \|_\infty \leq 1$.

	We are interested in bounding $\EE \sup_{f \in \mathcal{F}} |X_f|$. We may then apply independence and centering to conclude
	%
	\begin{align*}
		\| X_f - X_g \|_{\psi_2}^2 &= n^{-2} \sum \left\| [f(X_k) - g(X_k)] - \EE (f - g)(X) \right\|_{\psi_2}^2\\
		&\leq n^{-2} \sum \| f(X_k) - g(X_k) \|_{\psi_2}\\
		&\lesssim n^{-1} \| f - g \|_{L^\infty(\RR^n)}^2.
	\end{align*}
	%
	Thus $\| X_f - X_g \|_{\psi_2} \lesssim n^{-1/2} \| f - g \|_{L^\infty(\RR^n)}$, so the process is subgaussian with respect to the $L^\infty$ norm on $\mathcal{F}$.

	We wish to apply Dudley's inequality, so we must calculate the covering number of $\mathcal{F}$. For each $m$, form a grid of $1/m \times 1/m$ squares partitioning $[0,1] \times [-1,1]$. Then any function $f: [0,1] \to [0,1]$, we can approximate the graph of $f$ by travelling along squares, up to a $\varepsilon$ or error. Thus we obtain a $1/m$ net by taking the class of all 1 Lipschitz functions obtained by travelling along the net. The number of such functions is bounded by $3^m$. We now apply Dudley's inequality. Since $\| f - g \|_{L^\infty[0,1]} \leq 2$, for each $f,g \in \mathcal{F}$ with $f(0) = g(0) = 0$,
	%
	\begin{align*}
		\EE \sup_{f \in \mathcal{F}} |X_f| &\lesssim n^{-1/2} \sum_{k = -10} 2^{-k} (\log(3^{2^k}))^{1/2}\\
		&\lesssim n^{-1/2} \sum_{k = 0}^\infty 2^{-k/2} \lesssim n^{-1/2}. \qedhere
	\end{align*}
\end{proof}

\begin{remark}
	For any two measures $\mu$ and $\eta$, the {\it Wasserstein distance} is defined to be
	%
	\[ \sup_{f \in \mathcal{F}} \left| \int f d\mu - \int f d\eta \right|. \]
	%
	The theorem we just proved shows that if we draw i.i.d random variables $X_1, \dots, X_n$ with some distribution $\mu$, and define $\mu_n = \sum X_i \delta_{X_i}$ to be the sum of points masses at the same point, then the Wasserstein distance between $\mu$ and $\mu_n$ is, in expectation, $O(n^{-1/2})$.
\end{remark}

\section{Vapnik-Chervonenkis Dimension}

We now discuss Vapnik-Chervonenkis dimension, which is a notion of complexity for families of Boolean functions $\{ 0, 1 \}^\Omega$ where $\Omega$ is some arbitrary set. This notion of complexity is related to covering numbers, and thus we can relate certain learning bounds with Dudley's inequality.

Given a family $\mathcal{F}$ of Boolean functions on some domain $\Omega$, we say a set $\Lambda \subset \Omega$ is {\it shattered} by $\mathcal{F}$ if the restrict map $F: \{ 0, 1 \}^\Omega \to \{ 0, 1 \}^\Lambda$ is surjective. The \textbf{VC Dimension} of $\mathcal{F}$, denoted by $\text{vc}(\mathcal{F})$ is the largest cardinality of a set shattered by $\mathcal{F}$.

\begin{example}
	Let $\mathcal{F} = \{ \chi_{[a,b]} : a < b \}$ be the class of all indicator functions of intervals on $\RR$. The family $\mathcal{F}$ shatters any two point set, but no three point set is shattered; for instance, if $t < s < u$, then there is no $f \in \mathcal{F}$ with $f(t) = 1$, $f(s) = 0$, and $f(u) = 1$. Thus $\text{vc}(\mathcal{F}) = 2$.
\end{example}

\begin{example}
	Let $\mathcal{F} = \{ \chi_{\mathbf{H}} : \mathbf{H}\ \text{is a half plane in $\RR^2$} \}$. Then $\mathcal{F}$ is a family of Boolean functions in $\RR^2$. The family $\mathcal{F}$ shatters any three points in general position. But $\mathcal{F}$ doesn't shatter any four point set, so $\text{vc}(\mathcal{F}) = 3$.
\end{example}

\begin{example}
	If $\Omega = \{ 1, 2 ,3 \}$, then elements of $\{ 0, 1 \}^\Omega$ can be identified with length three binary strings. Consider
	%
	\[ \mathcal{F} = \{ 001, 010, 100, 111 \}. \]
	%
	Then $\mathcal{F}$ shatters any two point set. But it doesn't shatter $\Omega$, since $\mathcal{F} \neq \{ 0, 1 \}^\Omega$, so $\text{vc}(\mathcal{F}) = 2$.
\end{example}

\begin{example}
	Let
	%
	\[ \mathcal{F} = \{ \chi_{[a,b] \cup [c,d]}: a,b,c,d \in \RR \} \]
	%
	be the indicator functions of unions of two intervals. This class can even shatter four point sets. But it cannot shatter five point sets, because if $s < t < u < v < w$, there is no $f \in \mathcal{F}$ with $f(s) = f(u) = f(w) = 1$, and $f(t) = f(v) = 0$. Thus $\text{vc}(\mathcal{F}) = 4$.
\end{example}

\begin{example}
	Let $\mathcal{F}$ be the class of indicator functions of circles in $\RR^2$. Then $\mathcal{F}$ can shatter three point sets, but not four point sets. To prove the latter point, we may assume a four point set is in general position, because otherwise we can reduce to the case of indicator functions of intervals. If one of the points is in the convex hull of the other three, then $\mathcal{F}$ cannot shatter these points, because a circle cannot contain the three points, but not the fourth point. TODO.
\end{example}

\begin{example}
	The class of indicator functions of axis aligned rectangles in $\RR^2$ has VC dimension 4. Conversely, the indicators of axis aligned squares has VC dimension 3.
\end{example}

\begin{example}
	Let $\mathcal{F}$ be the class of indicator functions of all convex polygons in $\RR^2$. Then one can show $\mathcal{F}$ shatters sets of arbitrarily large cardinality. Thus $\text{vc}(\mathcal{F}) = \infty$.
\end{example}

\begin{lemma}[Pajor's Lemma]
	Given any class of Boolean functions $\mathcal{F}$ on $\Omega$,
	%
	\[ |\mathcal{F}| \leq | \{ \Lambda \subset \Omega : \mathcal{F}\ \text{shatters}\ \Lambda \} |. \]
\end{lemma}
\begin{proof}
	Let $S(\mathcal{F}) = \{ \Lambda \subset \Omega : \mathcal{F}\ \text{shatters}\ \Lambda \}$. We prove by induction on $\Omega$. If $\Omega = \emptyset$, then $\{ 0, 1 \}^\Omega = \emptyset$, so $\mathcal{F} = \emptyset$, and $S(\emptyset) = \{ \emptyset \}$, so the inequality in the lemma reads $0 \leq 1$, which is true.

	Next, we suppose $\Omega = \Omega_0 \cup \{ \omega \}$. Then $\mathcal{F} = \mathcal{F}_0 \cup \mathcal{F}_1$, where $\mathcal{F}_i = \{ f \in \mathcal{F} : f(\omega) = i \}$. By induction, $|S(\mathcal{F}_i)| \geq |\mathcal{F}_i|$. To finish the proof, we need only check $|S(\mathcal{F}_0)| + |S(\mathcal{F}_1)| \leq |S(\mathcal{F})|$. We note that for each $\Lambda \in S(\mathcal{F}_0) \cap S(\mathcal{F}_1)$, $\mathcal{F}$ shatters $\Lambda$ and $\Lambda \cup \{ \omega \}$. Thus
	%
	\[ S(\mathcal{F}) \supset S(\mathcal{F}_0) \cup S(\mathcal{F}_0) \cup (S(\mathcal{F}_0) \cap S(\mathcal{F}_1)) \times \{ \omega \} \]
	%
	And this is sufficient to obtain the inequality.
\end{proof}

\begin{theorem}[Sauer-Shelah Lemma]
	Let $\mathcal{F}$ be a class of Boolean functions on an $n$ point set $\Omega$. Then
	%
	\[ |\mathcal{F}| \leq \sum_{k = 0}^{\text{vc}(\mathcal{F})} {n \choose k} \leq \left( \frac{en}{\text{vc}(\mathcal{F})} \right)^{\text{vc}(\mathcal{F})}. \]
\end{theorem}
\begin{proof}
	The combinatorial sum counts the number of subsets of $\Omega$ containing at most $\text{vc}(\mathcal{F})$ points. A simple application of Pajor's lemma completes the proof.
\end{proof}

These lemmas are useful for analyzing finite collections of Boolean functions, but do not give any useful information if the class of functions is infinite. To analyze these functions, it turns out that the covering number of $\mathcal{F}$ becomes useful here.

Given any probability measure $\mu$ on $\Omega$, we can discuss the $L^2$ norm, which for any two Boolean functions $f,g \in \{ 0, 1 \}^\Omega$,
%
\[ \| f - g \|_{L^2(\mu)} = (\EE |f(X) - g(X)|^2)^{1/2} = \PP(f(X) \neq g(X))^{1/2}. \]
%
We can then discuss the covering numbers $N(\mathcal{F},\mu,\varepsilon)$ with respect to this metric. First, we need to discuss a way to reduce the size of the set $\mathcal{F}$.

\begin{lemma}
	Let $\mathcal{F}$ consist of $N$ functions on a probability space $\Omega$ with probability measure $\mu$. If $\| f - g \|_{L^2(\mu)} > \varepsilon$ for any distinct $f,g \in \mathcal{F}$, then there exists $n \lesssim \log N / \varepsilon^4$ and an $n$ point subset $\Lambda$ of $\Omega$ such that if $\eta$ is the uniform probability measure on $\Lambda$, then $\| f - g \|_{L^2(\eta)} > (\varepsilon/2)$ for distinct $f,g \in \mathcal{F}$.
\end{lemma}
\begin{proof}
	We rely on the probabilistic method. Let $X_1, \dots, X_n$ be $n$ independent independent points drawn from the measure $\mu$, and let $\eta$ be the average of point masses on each $X_i$. Set $h = (f - g)^2$. We want to bound the deviation
	%
	\[ \| f - g \|^2_{L^2(\eta)} - \| f - g \|_{L^2(\mu)}^2 = \frac{1}{n} \sum h(X_k) - \EE h(X). \]
	%
	We calculate that
	%
	\[ \| h(X_k) - \EE h(X) \|_{\psi_2} \lesssim \| h(X) \|_{\psi_2} \lesssim \| h(X) \|_\infty \lesssim 1. \]
	%
	Then Hoeffding's inequality implies that there is a constant $c$ with
	%
	\[ \PP \left( \left| \| f - g \|_{L^2(\eta)}^2 - \| f - g \|_{L^2(\mu)}^2 \right| > \varepsilon^2/4 \right) \leq 2 \exp(- c n \varepsilon^4). \]
	%
	And if this is true, then $\| f - g \|_{L^2(\eta)}^2 > 3\varepsilon^2/4$, so
	%
	\[ \| f - g \|_{L^2(\eta)} > (3/4)^{1/2} \varepsilon \geq \varepsilon/2. \]
	%
	Taking a union bound over all $f,g \in \mathcal{F}$, we conclude that with probability at least $1 - 2 |\mathcal{F}|^2 \exp(-cn \varepsilon^4) = 1 - 2N^2 \exp(-cn \varepsilon^4)$, the inequality above holds for all $f,g$. If
	%
	\[ n \gtrsim \log(N) / \varepsilon^4, \]
	%
	this quantity is nonzero, and therefore such a choice of $X_1, \dots, X_n$ exists. Now some of the $X_i$ may be repeated, but removing some of them only decreases the mass of $\eta$, so this isn't a problem.
\end{proof}

\begin{theorem}
	Let $\mathcal{F} \subset \{ 0, 1 \}^\Omega$. Then there is a universal constant $C$ such that for every $\varepsilon \in (0,1)$,
	%
	\[ N(\mathcal{F},\varepsilon) \leq (2/\varepsilon)^{C \cdot \text{vc}(\mathcal{F})}. \]
\end{theorem}
\begin{proof}
	If $\Omega$ is finite, we could employ the Sauer-Shelah lemma to conclude
	%
	\[ N(\mathcal{F},\varepsilon) \leq \left( \frac{en}{\text{vc}(\mathcal{F})} \right)^{\text{vc}(\mathcal{F})}. \]
	%
	This is almost what we want. We apply the dimension-reduction lemma to apply something along the lines of this.

	For a fixed $\varepsilon$, let $\mathcal{G} \subset \mathcal{F}$ be a $\varepsilon$ packing consisting of at least $N(\mathcal{F},\mu,\varepsilon)$ elements. If we apply the reduction lemma, we obtain a set $\Lambda$ with cardinality $O(\log N(\mathcal{F},\mu,\varepsilon) / \varepsilon^4)$ such that $\mathcal{G}$ is still a $\varepsilon/2$ packing with respect to the uniform measure on $\Lambda$. Applying Sauer-Shelah to $\mathcal{G}$, we conclude that if $d$ is the VC dimension of $\mathcal{G}$ on $\Lambda$,
	%
	\[ N(\mathcal{F},\mu,\varepsilon) \leq O \left( \frac{\log N(\mathcal{F},\mu,\varepsilon)}{d \varepsilon^4} \right)^d \]
	%
	But $\log N(\mathcal{F},\mu,\varepsilon)/2d = \log(N^{1/2d}) \leq N^{1/2d}$, so we can simplify to conclude
	%
	\[ N(\mathcal{F},\mu,\varepsilon) \leq O(1/\varepsilon)^{8d}. \]
	%
	The proof is completed when we notice $d \leq \text{vc}(\mathcal{F})$.
\end{proof}

Now we have connected covering numbers to VC dimension, we can now apply Dudley's inequality to upper bound the supremum of deviations of a class of Boolean functions.

\begin{theorem}
	Let $\mathcal{F}$ be a class of Boolean functions on a probability space $\Omega$ with VC dimension $\text{vc}(\mathcal{F})$. If $X_1, \dots, X_n$ are i.i.d samples taken from $\Omega$, with common distribution $X$, then
	%
	\[ \EE \sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum f(X_k) - \EE f(X)  \right| \lesssim \sqrt{\frac{\text{vc}(\mathcal{F})}{n}}. \]
\end{theorem}
\begin{proof}
	A simple symmetrization argument shows that
	%
	\[ \EE \sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum f(X_k) - \EE f(X)  \right| \leq 2 \EE \sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum \varepsilon_k f(X_k) \right|, \]
	%
	where $\{ \varepsilon_k \}$ are i.i.d symmetric Bernoulli variables. Next, we condition on the $\{ X_k \}$, viewing $\{ \varepsilon_k \}$ as the only random quantities. Set $Z_f = \sum \varepsilon_i f(X_i)$, so that we are now interested in bounding $\EE \sup |Z_f|$. Notice that
	%
	\begin{align*}
		\| Z_f - Z_g \|_{\psi_2} &= \left\| \sum \varepsilon_k(f - g)(X_k) \right\|_{\psi_2}\\
		&\lesssim \sqrt{n} \cdot \left( \frac{1}{n} \sum (f - g)(X_k)^2 \right)^{1/2} = \sqrt{n} \cdot \| f - g \|_{L^2(\eta)}.
	\end{align*}
	%
	Thus $\| Z_f - Z_g \|_{\psi_2} \lesssim \sqrt{n} \cdot \| f - g \|_{L^2(\eta)}$, where $\eta$ is the uniform distribution on the conditioned points $\{ X_1, \dots, X_n \}$. If we assume $0 \in \mathcal{F}$, then we can apply Dudley's inequality, combined with the last theorem, to conclude
	%
	\begin{align*}
		\EE \sup |Z_f| = \EE \sup |Z_f - Z_0| &\lesssim \frac{1}{\sqrt{n}} \EE_X \int_0^1 (\log N(\mathcal{F}, \eta, \varepsilon))^{1/2}\; d\varepsilon\\
		&\lesssim \frac{1}{\sqrt{n}} \EE_X \int_0^1 (\text{vc}(\mathcal{F}) \log(2/\varepsilon))^{1/2}\; d\varepsilon\\
		&\lesssim \left( \frac{\text{vc}(\mathcal{F})}{n} \right)^{1/2}.
	\end{align*}
	%
	In general, if $0 \not \in \mathcal{F}$, then we can still apply the previous bound to obtain
	%
	\[ \EE \sup |Z_f| \lesssim \left( \frac{\text{vc}(\mathcal{F} \cup \{ 0 \})}{n} \right)^{1/2}. \]
	%
	But this really proves the result. If $\Lambda$ is shattered by $\mathcal{F} \cup \{ 0 \}$, then $\mathcal{F}$ shatters every proper subset of $\Lambda$, so $\text{vc}(\mathcal{F} \cup \{ 0 \}) \leq \text{vc}(\mathcal{F}) + 1$.
\end{proof}

An important consequence of this result is the Glivenko-Cantelli theorem. How many samples does it take to estimate an arbitrary distribution given by a cumulative distribution function $F$? Given $X_1, \dots, X_n$, we might want to define an estimate
%
\[ \widehat{F}(x) = \frac{\# \{ i : X_i \leq x \}}{n} \]
%
The weak law of large numbers shows $\EE| \widehat{F}(x) - F(x) | \lesssim 1/\sqrt{n}$ for every $x \in \RR$. The Glivenko-Cantelli theorem shows that this expectation bound is achieved uniformly.

\begin{theorem}[Glivenko-Cantelli]
	Given a CDF $F$, samples $X_1, \dots, X_n$, and an induced estimate $\widehat{F}$,
	%
	\[ \EE \| F - \widehat{F} \|_{L^\infty(\mathbf{R})} \lesssim 1/\sqrt{n}. \]
\end{theorem}
\begin{proof}
	Let $\Omega = \RR$, and $\mathcal{F}$ the indicators of $(-\infty,x]$. Then $\text{vc}(\mathcal{F}) \leq 2$, which immediately implies the result.
\end{proof}

\begin{remark}
	A class of functions $\mathcal{F}$ is called {\it uniformly Glivenko-Cantelli} if for any $\varepsilon > 0$,
	%
	\[ \lim_{n \to \infty} \sup_\mu \PP \left( \sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum f(X_k) - \EE f(X) \right| > \varepsilon \right) = 0 \]
	%
	where $\{ X_k \}$ are i.i.d samples with respect to $\mu$, and the supremum is taken over all probability measures $\mu$. Markov's inequality combined with the main result of this section shows any class of Boolean functions with finite VC dimension is uniform Glivenko-Cantelli.
\end{remark}

\begin{theorem}
	Any class of Boolean functions with infinite VC dimension is not uniform Glivenko-Cantelli.
\end{theorem}
\begin{proof}
	Let $\mathcal{F}$ be a collection of Boolean functions, and let $\Omega$ be an $N$ point set shattered by $\mathcal{F}$. If $\mu$ is the uniform distribution on $\Omega$, then for any $X_1, \dots, X_n$ drawn from $\mu$, we can find $f$ with $f(X_k) = 0$ for all $k$, but with $\EE f(X) = 1 - n/N$. Thus
	%
	\[ \sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum f(X_k) - \EE f(X) \right| \geq 1 - n/N. \]
	%
	Thus for all $n$,
	%
	\[ \sup_\mu \PP \left( \sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum f(X_k) - \EE f(X) \right| \right) = 1. \]
\end{proof}

An important application of the theory of complexity of Boolean functions is to classification in mathematical statistics. Given a hidden rule $T: \Omega \to \{ 0, 1 \}$, we consider samples $(X_1,Y_1), \dots, (X_n, Y_n)$ with $X_k \in \Omega$, and $T(X_k) = Y_k$, our goal is to learn $T$ from the samples. An answer this problem would be a function $f: \Omega \to \{ 0, 1 \}$. The {\it risk} corresponding to this function is $R(f) = \PP(f(X) \neq T(X))$, and our goal is to choose $f$ such that $R(f)$ is as small as possible.

If $T$ is arbitrary, it can be very difficult to learn $T$ from samples, if not impossible. To restrict the complexity of $T$, we force our candidate functions $f$ to lie in some hypothesis space $\mathcal{F}$. We must balance {\it fit} and {\it complexity}. Hopefully, some function in $\mathcal{F}$ approximates $T$ accurately, but $\mathcal{F}$ itself has low complexity.

Our best answer to the question would be to compute $f^* = \text{argmin}_{f \in \mathcal{F}} R(f)$. But we are not able to compute $R(f)$, because we do not know the distribution our data came from. But we can always {\it estimate} using our training data. We define the {\it empirical risk} to be
%
\[ \widehat{R}(f) = \frac{1}{n} \sum (f(X_k) - T(X_k))^2. \]
%
Let $\widehat{f^*}$ denote the minimizer of empirical risk. The main question is the difference between $R(\widehat{f^*})$ and $R(f^*)$. We can use the VC dimension to answer this question.

\begin{theorem}
	Let $T$ be a Boolean function, and a hypothesis space $\mathcal{F}$. Then
	%
	\[ \EE \left( R(\widehat{f^*}) \right) \leq R(f^*) + O \left( \left( \frac{\text{vc}(\mathcal{F})}{n} \right)^{1/2} \right). \]
\end{theorem}
\begin{proof}
	Let $\varepsilon = \sup_{f \in \mathcal{F}} |\widehat{R}(f) - R(f)|$. Then
	%
	\begin{align*}
		R \left( \widehat{f^*} \right) \leq \widehat{R} \left( \widehat{f}^* \right) + \varepsilon \leq \widehat{R}(f^*) + \varepsilon \leq R(f^*) + \varepsilon.
	\end{align*}
	%
	Thus $R(\widehat{f^*}) - R(f^*) \leq 2 \varepsilon$. It thus suffices to show that
	%
	\[ \EE(\varepsilon) = \EE \left( \sup_{f \in \mathcal{F}} |\widehat{R}(f) - R(f)| \right) \lesssim \left( \frac{\text{vc}(\mathcal{F})}{n} \right)^{1/2}. \]
	%
	If we set $\mathcal{L} = \{ (f - T)^2 : f \in \mathcal{F} \}$, then
	%
	\[ \varepsilon = \sup_{l \in \mathcal{L}} \left| \frac{1}{n} \sum l(X_k) - \EE l(X) \right| \]
	%
	There is no easy way to relate $\text{vc}(\mathcal{L})$ to $\text{vc}(\mathcal{F})$, so we cannot just apply the normal uniform bound. But if $\eta$ is the uniform distribution on $\{ X_1, \dots, X_n \}$, then for $\varepsilon \in (0,1)$, we do have
	%
	\[ N(\mathcal{L}, \eta, \varepsilon) \leq N(\mathcal{F},\eta,\varepsilon/4). \]
	%
	This is because if $\mathcal{G}$ is a $\varepsilon/4$ net in $\mathcal{F}$, then $\{ (g - T)^2 : g \in \mathcal{G} \}$ is a $\varepsilon$ net in $\mathcal{L}$. And then we apply Dudley's inequality to yield the theorem.
\end{proof}

If we learn from data $X_1, \dots, X_n$, but we can take $n \to \infty$, then by Glivenko Cantelli we can approximate the CDF of the underlying distribution arbitrarily small, and therefore calculate $R$ up to an arbitrarily small error as well. This means that we can find the risk minimizer $f^*$, and $R(f^*)$ is the `risk' associated with choosing the hypothesis class $\mathcal{F}$. But if we cannot let $n$ be arbitrarily large, we occur an additional risk quantity
%
\[ R \left( \widehat{f^*} \right) - R(f^*), \]
%
known as {\it excess risk}. The last theorem says that the expected excess risk is on the order of $(\text{vc}(\mathcal{F})/n)^{1/2}$. Thus if we want the expected excess risk to be smaller than $\varepsilon$, we need $n \sim \text{vc}(\mathcal{F})/\varepsilon^2$ samples. To learn, we need more samples than the complexity of the hypothesis class.

\begin{remark}
	VC dimension is not the end all description of the excess risk incurred by learning. If $\mathcal{F}$ is the class of functions $f: [0,1] \to [0,1]$ with $\| f \|_{\text{Lip}} \leq L$, then similar techniques to our Lipschitz bound on Monte Carlo integration show
	%
	\[ \EE R(\widehat{f^*}) - R(f^*) \lesssim L / \sqrt{n}. \]
	%
	On the other hand, $\mathcal{F}$ has infinite VC dimension.
\end{remark}

\section{Generic Chaining}

Using a slightly more technical chaining argument, we can obtain much tight bounds on the expectation of the supremum of a random process. To do this, we avoid using covering numbers at all. The idea is that, rather than fixing a number $\varepsilon$, and considering the minimum cardinality of a $\varepsilon$ net, we instead consider a fixed cardinality $N$, and try and find the smallest $\varepsilon$ such that there exists a $\varepsilon$ net of cardinality $N$.

We say a sequence of sets $\{ T_k \}$ is \textbf{admissible} if $|T_0| = 1$, and $|T_k| \leq 2^{2^k}$. This sequence of sets induces a sequence of values $\varepsilon_k = \sup_{t \in T} d(t,T_k)$. The chaining technique of the last section then shows
%
\[ \EE \sup_{t \in T} X_t \lesssim \sum_{k = 0}^\infty 2^{k/2} \sup_{t \in T} d(t,T_k) \]
%
The important step is to take the supremum outside the sum, to obtain a single, universal quantity. The $\gamma_2$ \textbf{functional} is defined as
%
\[ \gamma_2(T) = \inf_{\{ T_k \}} \sup_{t \in T} \sum_{k = 0}^\infty 2^{k/2} d(t,T_k). \]
%
Taking the supremum outside of the sum means that $\gamma_2$ is smaller than the bound obtained by Dudley's inequality. It can be minor, but in some cases it is a big difference.

\begin{example}
	Consider the example
	%
	\[ T = \left\{ \frac{e_k}{\sqrt{1 + \log k}} : 1 \leq k \leq n \right\}. \]
	%
	If we set $T_k$ equal to the first $2^{2^k}$ vectors in $T$, then $T_k = T$ for $k \geq \lg \lg n$, and so
	%
	\begin{align*}
		\gamma_2(T) &\leq \sup_{t \in T} \sum_{k = 0}^{\lg \lg n} 2^{k/2} d(t,T_k)\\
		&\lesssim \sup_{1 \leq m \leq n} \sum_{k = 2}^{\lg \lg m} \frac{2^{k/2}}{\sqrt{\log k}} \lesssim 1.
	\end{align*}
	%
	On the other hand, we have already seen that Dudley's inequality gives a value which tends to $\infty$ as $n \to \infty$.
\end{example}

We now improve Dudley's inequality to give a result involving $\gamma_2(T)$.

\begin{theorem}[Generic Chaining]
	Let $\{ X_t \}$ be a mean zero random process. Then
	%
	\[ \EE \left( \sup_{t \in T} X_t \right) \lesssim \| X \|_{\psi_2} \gamma_2(T). \]
\end{theorem}
\begin{proof}
	TODO
\end{proof}

We say a process $\{ X_t \}$ is {\it Gaussian} if all finite dimension distributions are Gaussian. Talagrand's majorizing measure theorem says that $\gamma_2(T)$ gives a tight bound for the expected supremum of the process in this case.

\begin{theorem}[Talagrand's Majorizing Measures Theorem]
	Let $\{ X_t : t \in T \}$ be a mean zero Gaussian process, inducing a canonical metric $d(t,s) = \| X_t - X_s \|_2$ on $T$. Then $\EE \left( \sup X_t \right)$ is comparable to $\gamma_2(T)$.
\end{theorem}

Since $\gamma_2(T)$ {\it upper bounds} any subgaussian process, the majorizing measures theorem often enables us to replace an arbitrary subgaussian process with a Gaussian process.

\begin{corollary}[Talagrand's Comparison Inequality]
	Let $\{ X_t : t \in T \}$ be a mean zero process and let $\{ Y_t : t \in T \}$ be a Gaussian process. If $\| X_t - X_s \|_{\psi_2} \leq K \| Y_t - Y_s \|_2$, then
	%
	\[ \EE \left( \sup X_t \right) \lesssim \EE \left( \sup Y_t \right). \]
\end{corollary}

As a special case of this result, if $\{ X_x : x \in T \}$ is a mean zero process with $T \subset \RR^n$, then
%
\[ \EE \left( \sup X_x \right) \lesssim \| X \|_{\psi_2} \cdot w(T). \]

\section{Chevet's Inequality}

Talagrand's comparison inequality is often useful for studying subgaussian random variables by switching to studying Gaussian processes. Here, we give an application which gives a bound on
%
\[ \sup \{ Ax \cdot y : x \in T, y \in S \}, \]
%
where $A$ is random, and $T$ and $S$ are arbitrary sets. We now give a bound with respect to the Gaussian width of $T$ and $S$, and in terms of the {\it radius} $\text{rad}(T) = \sup_{x \in T} |x|$ and $\text{rad}(S) = \sup_{y \in S} |y|$.

\begin{theorem}
	Let $A$ be an $m \times n$ random matrix whose entries are independant, mean-zero, subgaussian random variables with $\| A_{ij} \|_{\psi_2} \leq K$. Let $T \subset \RR^n$ and $S \subset \RR^m$ be bounded sets. Then
	%
	\[ \EE \sup \{ Ax \cdot y : x \in T, y \in S \} \lesssim K(w(T) \text{rad}(S) + w(S) \text{rad}(T)). \]
\end{theorem}
\begin{proof}
	We employ Talagrand's comparison inequality. Without loss of generality, let $K = 1$. We would like to bound $X_{uv} = Au \cdot v$, for $u \in T$ and $v \in S$. A simple calculation shows
	%
	\[ \| X_{uv} - X_{wz} \|_{\psi_2} \leq |u-w| \text{rad}(S) + |v-z| \text{rad}(T). \]
	%
	Define $Y_{uv} = (g \cdot u) \text{rad}(S) + (h \cdot v) \text{rad}(T)$, where $g \sim N(0,I_n)$ and $h \sim N(0,I_m)$ are independant, Gaussian vectors. We calculate that
	%
	\[ \| Y_{uv} - Y_{wz} \|_2^2 = |u-w|^2 \text{rad}(T)^2 + |v-z|^2 \text{rad}(S)^2. \]
	%
	Comparing these increments, we apply Talagrand's comparison inequality to conclude that
	%
	\begin{align*}
		\EE \sup X_{uv} \lesssim \EE \sup Y_{uv} &= \EE \sup (g \cdot u) \text{rad}(S) + \EE \sup (h \cdot v) \text{rad}(T)\\
		&= w(T) \text{rad}(S) + w(S) \text{rad}(T). \qedhere
	\end{align*}
\end{proof}

\chapter{Gaussian Processes}

We now try and derive lower bounds for \textbf{Gausssian processes}, i.e. processes $\{ X_t \}$ such that all finite dimensional subdistributions are normal, or equivalently, if $\sum a_i X_{t_i}$ is normally distributed for any finite sum of $t_i \in T$ and $a_i \in \RR$. In our analysis, we make the simplifying assumption that the random process we study is centered, so $\EE(X_t) = 0$ for all $t \in T$. Then we can define the covariance function $\Sigma(t,s) = \mathbf{E}(X_t X_s)$. The \textbf{increments} of the random process are defined as $d(t,s) = \| X_t - X_s \|_2$.  These increments naturally give $T$ the structure of a metric space, with $d(t,s) = 0$ if and only if $X_t = X_s$.

\begin{example}
    Let $\{ X_t : t \geq 0 \}$ be a Brownian motion. The metric induced on $[0,\infty)$ is given by $d(t,s) = |t-s|^{1/2}$. Similarily, if we consider independant normal random variables $Z_1, Z_2, \dots$ and set $S_n = Z_1 + \dots + Z_n$, then $\{ S_n \}$ is a process defined on $\mathbf{N}$, and $d(n,m) = \sqrt{n-m}$.
\end{example}

The increments of a process and it's covariance function are tightly related. Indeed,
%
\[ d(t,s)^2 = \EE((X_t - X_s)^2) = \Sigma(t,t) + \Sigma(s,s) - 2 \Sigma(t,s) \]
%
Conversely, if $X_0 = 0$ belongs to the process, then
%
\begin{align*}
    \Sigma(t,s) = \frac{d(t,0)^2 + d(s,0)^2 - d(t,s)^2}{2}
\end{align*}
%
Thus the two functions determine one another. In particular, this means that the finite dimensional distributions of the process are uniquely determined by the metric, if the variances of the random variables are known, and thus the expectation $\EE \sup X_t$. This means we should be able to obtain bounds on the expectation purely from the geometry of the underlying metric space.

\section{Slepian Inequality}

A natural goal is to obtain a bound on $\EE(\sup X_t)$. In all but the most basic process, this is a non-trivial task. The first bound we discuss enables us to replace the problem of bounding a process with bounding another process, whose supremum may be more easily calculated. Given two processes $\{ X_t \}$ and $\{ Y_t \}$, we say $\{ Y_t \}$ \textbf{stochastically dominates} $\{ X_t \}$ if for any $s \in \RR$,
%
\[ \PP \left( \sup X_t \geq s \right) \leq \PP \left( \sup Y_t \geq s \right) \]
%
The method we discuss is called Slepian's inequality, and gives conditions for a random process to be bound by another random process.

Our proof of the method will involve the method of \emph{Gaussian interpolation}. Given two independant Gaussian vectors $X \sim N(0,\Sigma(X))$ and $Y \sim N(0,\Sigma(Y))$, we consider
%
\[ Z_u = \sqrt{u} \cdot X + \sqrt{1-u} \cdot Y, \]
%
defined so that $\Sigma(Z_u) = u \Sigma(X) + (1 - u) \Sigma(Y)$.

\begin{lemma}[Gaussian Integration by Parts]
    Let $X \sim N(0,\Sigma)$. Then for any function $f: \RR^n \to \RR$, $\EE(Xf(X)) = \Sigma \cdot \EE(\nabla f(X))$.
\end{lemma}
\begin{proof}
    Assume first that $f$ has bounded support. If $p(x)$ is the density function of $X$, then
    %
    \begin{align*}
        \Sigma \cdot \EE(\nabla f(X)) &= \EE(\Sigma \cdot \nabla f(X))\\
        &= \int \Sigma \cdot (\nabla f) \cdot p\; dx\\
        &= - \int \Sigma \cdot \nabla p \cdot f(x)
    \end{align*}
    %
    Note that $(\nabla p)(x) = -p(x) \cdot \Sigma^{-1} \cdot x$. Thus
    %
    \begin{align*}
        - \int \Sigma \cdot \nabla p \cdot f(x) &= \int p(x) f(x) x = \EE(Xf(X)). \qedhere
    \end{align*}
\end{proof}

\begin{lemma}
    Let $X \sim N(0,\Sigma(X))$ and $Y \sim N(0,\Sigma(Y))$ be two independant Gaussian vectors. Let $Z_u = \sqrt{u} \cdot X + \sqrt{1-u} \cdot Y$. Then for any twice differentiable function $f: \RR^n \to \RR$,
    %
    \[ \frac{d \EE[f(Z_u)]}{du} = 0.5 \sum_{i,j} \left( \Sigma(X)_{ij} - \Sigma(Y)_{ij} \right) \EE \left( \frac{\pp f}{\pp x_i \pp x_j}(Z_u) \right) \]
\end{lemma}
\begin{proof}
    Using the chain rule, we find
    %
    \begin{align*} 
        \frac{d \EE[f(Z_u)]}{du} &= \sum \EE \left[ \frac{\pp f}{\pp z_i}(Z_u) \frac{d Z_{u,i}}{du} \right]\\
        &= \sum \EE \left[ \frac{\pp f}{\pp z_i}(Z_u) \left( \frac{X_i}{2\sqrt{u}} - \frac{Y_i}{2 \sqrt{1 - u}} \right) \right]
    \end{align*}
    %
    If
    %
    \[ g_i(X,Y) = \left( \frac{\pp f}{\pp z_i} \right)(\sqrt{u} \cdot X + \sqrt{1 - u} \cdot Y) = \left( \frac{\pp f}{\pp z_i} \right)(Z_u), \]
    %
    then we can apply a Gaussian integration by parts to conclude
    %
    \begin{align*}
        \EE \left[X_i \left( \frac{\pp f}{\pp z_i} \right)(Z_u) \bigg| Y \right] &= \EE[X_i \cdot g_i(X,Y) | Y]\\
        &= \sum_j \Sigma(X)_{ij} \EE \left( \frac{\pp g_i}{\partial x_j}(X,Y) \bigg| Y \right)\\
        &= \sqrt{u} \sum_j \Sigma(X)_{ij} \EE \left( \left( \frac{\pp^2 f}{\pp z_j \pp z_i} \right)(Z_u) \bigg| Y \right).
    \end{align*}
    %
    Then we can take expectations with respect to $Y$ on both sides to remove the conditional expectation. Similarily, we calculate
    %
    \begin{align*}
        \EE \left[X_i \left( \frac{\pp f}{\pp z_i} \right)(Z_u) \right] = \sqrt{1 - u} \sum_j \Sigma(Y)_{ij} \EE \left( \left( \frac{\pp^2 f}{\pp z_j \pp z_i} \right)(Z_u) \right)
    \end{align*}
    %
    Putting these two terms together completes the calculation.
\end{proof}

\begin{lemma}
    If $f: \RR^n \to \RR$ is twice-differentiable and for all $i \neq j$,
    %
    \[ \frac{\pp^2 f}{\pp x_i \pp x_j} \geq 0. \]
    %
    Let $X$ and $Y$ be Gaussian vectors such that for all $i$, $\EE X_i^2 = \EE Y_i^2$, and for all indices $i,j$, $\EE [(X_i - X_j)^2] \leq \EE[(Y_i - Y_j)^2]$. Then $\EE f(X) \geq \EE f(Y)$.
\end{lemma}
\begin{proof}
    Note that if $\Sigma(X)$ and $\Sigma(Y)$ are the covariance matrices of $X$ and $Y$, then $\Sigma(X)_{ii} = \Sigma(Y)_{ii}$, and $\Sigma(X)_{ij} \geq \Sigma(Y)_{ij}$. If $\Pi(X,Y)_{ij} = \EE(X_i Y_j)$, then the vector $Z = (X,Y)$ is Gaussian, and
    %
    \[ \Sigma(Z) = \begin{pmatrix} \Sigma(X) & \Pi(X,Y) \\ \Pi(X,Y) & \Sigma(Z) \end{pmatrix} \]
    %
    We can assume that $X$ and $Y$ are independant, because the inequalities we need to prove only rely on the individual distributions of each random variable. Then the last lemma implies that
    %
    \[ \frac{d \EE[f(Z_u)]}{du} \geq 0, \]
    %
    so $\EE[f(Z_u)]$ is increasing in $u$. But this means that $\EE f(X) \geq \EE f(Y)$.
\end{proof}

\begin{theorem}[Slepian's Inequality]
    Let $\{ X_t \}$ and $\{ Y_t \}$ be two mean zero processes. Assume that for all $t,s$, $\EE X_t^2 = \EE X_s^2$ and $d_X(t,s) \leq d_Y(t,s)$ for all $t$ and $s$. Then for any $u$,
    %
    \[ \PP \left( \sup X_t \geq u \right) \leq \PP \left( \sup Y_t \geq u \right) \]
    %
    and consequently, $\EE(\sup X_t) \leq \EE(\sup Y_t)$.
\end{theorem}
\begin{proof}
    We use the techniques of \emph{Gaussian interpolation}. Let $h: \RR \to [0,1]$ be a twice-differentiable, non-increasing approximation of the indicator function $\mathbf{I}(x < s)$. Then the function $f: \RR^n \to [0,1]$ defined by $f(x) = h(x_1) \dots h(x_n)$ is an approximation of $\mathbf{I}(\max(x_1, \dots, x_n) < s)$. Then for $i \neq j$
    %
    \[ \frac{\pp^2 f}{\pp x_i \pp x_j} = h'(x_i) h'(x_j) \prod \{ h(x_k) : k \not \in \{ i, j \} \} \geq 0. \]
    %
    Thus the last lemma implies $\EE h(X) \geq \EE h(Y)$. But since $h$ was essentially arbitrary, we conclude
    %
    \begin{align*}
        \PP(\max(X_1, \dots, X_n) < s) &= \EE \mathbf{I}(\max(X_1, \dots, X_n) < s)\\
        &\geq \EE \mathbf{I}(\max(Y_1, \dots, Y_n) < s)\\
        &= \PP(\max(Y_1, \dots, Y_n) < s).
    \end{align*}
    %
    Taking complements, we find
    %
    \[ \PP(\max(X_1, \dots, X_n) \geq s) \leq \PP(\max(Y_1, \dots, Y_n) \geq s). \]
    %
    Both sides converge monotonely, so we conclude
    %
    \[ \PP(\sup X_t \geq s) \leq \PP(\sup Y_t \geq s). \]
    %
    This gives the first part of Slepian's inequality. And now we find
    %
    \[ \EE(\sup X_t) = \int_0^\infty \PP(\sup X_t \geq s)\; ds \leq \int_0^\infty \PP(\sup Y_t \geq s)\; ds = \EE(\sup Y_t). \qedhere \]
\end{proof}








\section{Sudakov-Fernique Inequality}

Sudakov-Fernique's theorem gives the expectation bound of Slepian's inequality, but works without the assumption of equality of variances.

\begin{theorem}[Sudakov-Fernique]
    Let $\{ X_t \}$ and $\{ Y_t \}$ be mean zero Gaussian processes. If $d^X \leq d^Y$, then $\EE \sup(X_t) \leq \EE \sup(Y_t)$.
\end{theorem}
\begin{proof}
    It suffices to prove this theorem for Gaussian vectors $X$ in $\RR^n$, because the same limiting process as in Slepian's inequality proves the result in general. We also apply Gaussian interpolation. Given $\alpha$
    %
    \[ f_\alpha(x) = \frac{\log \left( \sum e^{\alpha x_i} \right)}{\alpha} \]
    %
    Then as $\alpha \to \infty$, $f_\alpha(x) \to \max(x_1, \dots, x_n)$. Note that
    %
    \[ \frac{d \EE(f(Z_u))}{du} = \frac{1}{2} \sum_{i,j} (\Sigma(X)_{ij} - \Sigma(Y)_{ij}) \EE \left( \frac{\pp^2 f_\alpha}{\pp x_i \pp x_j} \right) \]
    %
    Let $p_i = \pp f / \pp x_i = e^{\alpha x_i} / \sum_k e^{\alpha x_k}$, so $p_1 + \dots + p_n = 1$. We calculate
    %
    \[ \frac{\pp^2 \hspace{-0.2em} f_\alpha}{\pp x_i \pp x_j} = \alpha(\delta_{ij} p_i - p_i p_j) \]
    %
    where $\delta_{ij} = 1$ if $i = j$, and is zero otherwise. Now for any $a_{ij}$,
    %
    \[ \sum a_{ij}(\delta_{ij} p_i - p_i p_j) = \sum_{i \neq j} (a_{ii} + a_{jj} - 2 a_{ij}) p_i p_j. \]
    %
    Setting $a_{ij} = d^X(i,j)^2 - d^Y(i,j)^2$, we conclude
    %
    \[ \frac{d \EE(f_\alpha(Z_u))}{du} = - \sum_{i \neq j} a_{ij} p_i p_j \leq 0 \]
    %
    Thus we find $\EE(f_\alpha(X)) \leq \EE(f_\alpha(Y))$. We note that
    %
    \[ \max(x_1, \dots, x_n) \leq f_\alpha(x) \leq \max(x_1, \dots, x_n) + \frac{\log(n)}{\alpha}, \]
    %
    so
    %
    \[ \EE |f_\alpha(X) - \max(X_1, \dots, X_n)|  = \EE f_\alpha(X) - \max(X_1, \dots, X_n) \leq \frac{\log n}{\alpha}. \]
    %
    Thus $f_\alpha(X) \to \max(X_1, \dots, X_n)$ in $L^1$ norm. Similarily, $f_\alpha(Y) \to \max(Y_1, \dots, Y_n)$ in the $L^1$ norm. So we find $\EE(\max(X_1, \dots, X_n)) \leq \EE(\max(Y_1, \dots, Y_n))$. And now we take limits on both sides to obtain the full theorem.
\end{proof}

\begin{example}
	If $A$ is an $m \times n$ matrix with independant, standard normal entries, then the process $X_{xy} = y \cdot Ax$ is a Gaussian process on $S^{n-1} \times S^{m-1}$. If we define $Y_{xy} = g \cdot x + g' \cdot y$, where $g \sim N(0,I_n)$ and $g' \sim N(0,I_m)$ are independant from one another, then we find that
	%
	\[ \EE((X_{xy} - X_{zw})^2) \leq |x-z|^2 + |y-w|^2 = \EE((g \cdot x + g' \cdot y))^2. \]
	%
	Thus the Sudakov-Fernique inequality implies that
	%
	\begin{align*}
		\EE \sup X_{xy} \leq \EE \sup Y_{xy} &= (\EE \sup g \cdot x) + (\EE \sup g' \cdot y)\\
		&= \EE |g| + \EE |g'| = \sqrt{n} + \sqrt{m}.
	\end{align*}
	%
	Thus $\EE \| A \| \leq \sqrt{n} + \sqrt{m}$. We note that the map $A \mapsto \| A \|$ is $1$-Lipschitz, so we can apply Gaussian concentration to conclude that there is an absolute constant $c$ such that
	%
	\[ \PP \left( |\| A \| - \sqrt{n} - \sqrt{m}| \geq t \right) \leq \exp(-c \cdot t^2). \]
	%
	Thus the expectation calculation gives us a concentration bound automatically.
\end{example}

\section{Gaussian Width}

Before we move on, it is important to discuss the basic properties of Gaussian width, which will become very useful later on.

\begin{theorem}
	Let $T \subset \RR^n$. Then
	%
	\begin{itemize}
		\item $w(T)$ is finite if and only if $T$ is bounded.
		\item Gaussian width is invariant under isometries of $\RR^n$, i.e. $w(T) = w(U \cdot T + s)$ for any $U \in O(n)$ and $s \in \RR^n$.
		\item The Gaussian width of a set is equal to the Gaussian width of it's convex hull, i.e. $w(T) = w(\text{Conv}(T))$.

		\item For any $T,S \subset \RR^n$,
		%
		\[ w(T + S) = w(T) + w(S)\quad\text{and}\quad w(aT) = |a| w(T). \]

		\item For any set $T \subset \RR^n$,
		%
		\[ w(T) = \frac{w(T-T)}{2} = \frac{\EE \sup_{x,y \in T} g \cdot (x-y)}{2}. \]

		\item Forany set $T \subset \RR^n$,
		%
		\[ \frac{\diam(T)}{\sqrt{2\pi}} \leq w(T) \leq \frac{\sqrt{n}}{2} \cdot \diam(T). \]

		\item If $A$ is an $m \times n$ matrix, then $w(AT) \leq \| A \| w(T)$.
	\end{itemize}
\end{theorem}
\begin{proof}
	There exists a constant $c$ such that for any particular $x \in \RR^n$,
	%
	\[ \PP(g \cdot x \leq t |x|) \lesssim 1 - \exp(-c \cdot t^2) \]
	%
	consider a sequence $a_n$ such that
	%
	\[ \sum_{k = 1}^\infty 1 - \exp(c \cdot a_n^2) < \infty. \]
	%
	If we consider a sequence $\{ x_k \} \subset T$ with $|x_k| \geq n/a_n$, then a union bound combined with the Borel-Cantelli lemma implies that $g \cdot x_k \geq a_n |x_k| \geq n$ almost surely for sufficiently large $k$, which means
	%
	\[ \sup g \cdot x \geq \sup g \cdot x_k = \infty\quad\text{a.s.} \]
	%
	So obviously $\EE \sup g \cdot x = \infty$.

	If $U \in O(n)$, and $y \in \RR^n$, then
	%
	\[ w(T) \EE \sup g \cdot (Ux + y) = \EE \sup (Ug) \cdot x + \EE g \cdot y = \EE \sup g \cdot x = w(T). \]
	%
	This proves the second point. Any element $x$ of $\text{Conv}(X)$ can be written as $\sum \alpha_i x_i$ for $x_i \in X$, and $\sum \alpha_i = 1$. But then
	%
	\[ g \cdot x = \sum \alpha_i (g \cdot x_i) \leq \max_i g \cdot x_i. \]
	%
	Thus $\sup_{x \in T} g \cdot x = \sup_{x \in \text{Conv}(T)} g \cdot x$, which proves the third point. Next, we calculate that
	%
	\[ w(T+S) =\sup_{(x + y) \in T + S} g \cdot (x+y) = \sup_{x \in T} g \cdot x + \sup_{y \in T} g \cdot y = w(T) + w(S). \]
	%
	The fact that $w(\alpha T) = |\alpha| w(T)$ is obvious. Thus the fourth property is established. And
	%
	\[ w(T-T) = w(T) + w(-T) = 2w(T), \]
	%
	so the fifth point is established.

	To establish the lower bound $w(T) \geq \diam(T) / \sqrt{2\pi}$, we may assume without loss of generality that $T$ is compact. We then just take two points $x,y \in T$ with $|x-y| = \diam(T)$, and calculate
	%
	\begin{align*}
		w(T) &\geq 0.5 \cdot \EE \max(g \cdot (x-y), g \cdot (y-x))\\
		&\geq 0.5 \cdot \EE |g \cdot (x-y)| = 0.5 \cdot (2/\pi)^{1/2} \cdot |x-y| = (2\pi)^{-1/2} \diam(T).
	\end{align*}
	%
	Conversely, we have
	%
	\begin{align*}
		w(T) &= 0.5 \cdot \EE \left( \sup g \cdot (x-y) \right) \leq 0.5 \cdot \sup |g| |x-y|\\
		&\leq 0.5 \cdot \diam(T) \sup |g| = 0.5 \cdot \sqrt{n} \cdot \diam(T).
	\end{align*}
	%
	Thus the fifth point has been established.

	To establish the final point, we must show
	%
	\[ \EE \left( \sup_{x \in T} g \cdot Ax \right) \leq \| A \| \sup_{x \in T} (g \cdot x). \]
	%
	If we let $X_x = g \cdot x$, and $Y_x = g \cdot Ax$, then
	%
	\[ \| Y_x - Y_y \|_2 = \| A^Tg \cdot (x-y) \|_2 \leq \sqrt{m} \cdot \| A \| |x - y| = \| A \| \| X_x - X_y \|_2. \]
	%
	Applying the Sudakov-Fernique inequality, we conclude
	%
	\[ \EE \sup Y_x \leq \| A \| \EE \sup X_x = \| A \| w(T). \qedhere \]
\end{proof}

The Gaussian width of a set should be compared to the spherical width of a set, i.e.
%
\[ w_s(T) = \EE \left( \sup_{x \in T} \theta \cdot x \right), \]
%
where $\theta$ is a uniformly randomly chosen vector on the unit sphere. Because the distribution is translation invariant, the spherical width of a set also satisfies the translating invariance and homgeneity bounds that the Gaussian width also satisfies. The spherical width and Gaussian width are essentially comparable, once the spherical width is scaled by $\sqrt{n}$.

\begin{theorem}
	There exists an absolute constant $C$ such that for any $T \subset \RR^n$,
	%
	\[ w(T) = w_s(T) \left(\sqrt{n} + O(1) \right). \]
\end{theorem}
\begin{proof}
	We note that $g$ is uniformly distributed on the sphere once normalized, so $\sup g \cdot x$ is identically distributed to $|g| \sup \theta \cdot x$, where $\theta$ is uniformly distributed on the unit sphere and independant of $g$. Thus
	%
	\[ w(T) = \EE \sup g \cdot x = \EE |g| \EE \sup \theta \cdot x = \left( \sqrt{n} + O(1) \right) w_s(T). \qedhere \]
\end{proof}

\begin{example}
	If $B$ is the unit ball, then
	%
	\[ \sup_{x \in B} g \cdot x = |g| \]
	%
	Thus
	%
	\[ w(B) = \EE \sup_{x \in B} g \cdot x = \EE |g| = \sqrt{n} + O(1). \]
	%
	The unit ball has the same width as the unit sphere $S^{n-1}$.
\end{example}

\begin{example}
	Consider the unit cube in the $l^\infty$ norm, i.e. $B^\infty = [-1,1]^n$. Then
	%
	\[ \sup_{x \in B^\infty} g \cdot x = \sup_{x \in \{ -1, 1 \}^n} g \cdot x = |g_1| + \dots + |g_n|. \]
	%
	Thus
	%
	\[ w(B^\infty) = \EE |g_1| + \dots + \EE |g_n| = \sqrt{2/\pi} \cdot n. \]
	%
	Thus $B^\infty$ has almost the same width as the ball of radius $\sqrt{n}$ around the origin, which contains $B^\infty$.
\end{example}

\begin{example}
	Consider the unit ball in the $l^1$ norm, i.e.
	%
	\[ B^1 = \{ x \in \RR^n: |x_1| + \dots + |x_n| \leq 1 \}. \]
	%
	Then
	%
	\[ \sup_{x \in B^1} g \cdot x = \max(g_1, \dots, g_n), \]
	%
	and so
	%
	\[ w(B^1) = \EE \max(g_1, \dots, g_n) = \Theta \left( (\log n)^{1/2} \right). \]
	%
	Thus $B^1$ has a proportionally \emph{tiny} width as compared to the unit balls in the $l^2$ and $l^\infty$ norm. It has essentially the same width as the radius $1/\sqrt{n}$ ball in the $l^2$ norm that it contains. Thus we can conclude that the majority of the mass of the $l^1$ ball is concentrated near the origin.
\end{example}

\begin{example}
	If $X$ is a collection of $n$ vertices with $\diam(X) \leq 1$, then $w(X) \lesssim (\log n)^{1/2}$. We might as well assume $|x| = 1$ for all $x \in X$, because this would only increase $\sup g \cdot x$, and then we are just considering the supremum of $n$ unit variance Gaussians, which gives the result.
\end{example}

It is often useful to work with a squared version of Gaussian width, i.e. $h(T) = (\EE \sup (g \cdot x)^2)^{1/2}$. This is equivalent to the Gaussian width, up to a constant factor.

\begin{theorem}
	For any $T \subset \RR^n$,
	%
	\[ w(T-T) \leq h(T-T) \leq w(T-T) + O(\diam(T)) \lesssim w(T-T). \]
	%
	In particular, $h(T-T)$ is comparable with $w(T)$ for any set $T$.
\end{theorem}
\begin{proof}
	Because $T - T$ is symmetric, since the $L^2$ norm of a random variable is always greater than the $L^1$ norm,
	%
	\begin{align*}
		h(T-T) &= \left(\EE \sup_{x,y \in T} (g \cdot (x-y))^2 \right)^{1/2}\\
		&= \left( \EE \left( \sup_{x,y \in T} |g \cdot (x-y)| \right)^2 \right)^{1/2}\\
		&\geq \EE \sup_{x,y \in T} |g \cdot (x-y)|\\
		&= \EE \sup_{x,y \in T} g \cdot (x-y) = w(T-T).
	\end{align*}
	%
	TODO: UPPER BOUND.
\end{proof}

It's also often useful to discuss the {\it Gaussian complexity} of a set $T$, which is defined as $\gamma(T) = \EE \sup |g \cdot x|$. We obviously have $w(T) \leq \gamma(T)$, but we need $T$ to be symmetric about the origin to conclude that $w(T) = \gamma(T)$. But they are closely related in general.

\begin{lemma}
	For any set $T$, and $y \in T$,
	%
	\[ (1/3) \cdot (w(T) + |y|) \leq \gamma(T) \leq 2(w(T) + |y|). \]
\end{lemma}
\begin{proof}
	TODO
\end{proof}

\section{Stable Dimension}

If $T \subset \RR^n$, it's algebraic dimension $\dim(T)$ is the dimensional of the smallest vector space containing $T$. The algebraic dimension is very unstable, since small changes in $T$ can vastly change the dimension. The Gaussian width helps us come up with a geometric quantity which acts like the algebraic dimension, but is more stable under small pertubations. We define the {\it stable dimension} of a bounded set $T$ as
%
\[ d(T) = \frac{h(T-T)^2}{\diam(T)^2} = \frac{\EE \sup_{x,y \in T} (g \cdot (x - y))^2}{\diam(T)^2} \sim \frac{w(T)^2}{\diam(T)^2}. \]
%
The stable dimension is always bounded by the algebraic dimension, but can be significantly smaller.

\begin{lemma}
	For any $T$, $d(T) \leq \dim(T)$.
\end{lemma}
\begin{proof}
	Suppose $T$ lies in a $k$ dimensional subspace. Without loss of generality, we can assume it has diameter one. If $Q$ denotes the orthogonal projection onto this $k$ dimensional subspace, then
	%
	\[ \EE \sup_{x,y \in T} (g \cdot (x-y))^2 = \EE \sup_{x,y \in T} (Qg \cdot (x-y))^2 \leq \EE |Qg|^2 = k. \]
\end{proof}

The intersection of a unit ball with a $k$ dimensional plane has $d(T) = \dim(T)$. On the other hand, $d(T) \lesssim \log |T|$.

\begin{example}
	Let $B$ be the unit ball in $n$ dimensions, and let $A$ be an $m \times n$ matrix. If $A$ has singular value decomposition $\sum s_i u_i v_i^T$, then
	%
	\[ \sup_{x \in B} (g \cdot Ax)^2 = \sup_{x \in B} \sum s_i^2 (g \cdot u_i)^2 (x \cdot v_i)^2 \]
	%
	This random variable is identically distributed to
	%
	\[ \sup_{x \in B} \sum s_i^2 g_i^2 x_i^2 = \max(s_1^2g_1^2, \dots, s_n^2g_n^2) \]
	%
	TODO: FINISH. and so $h(AB) = \| A \|_F^2$.
\end{example}

The {\it stable rank} of an $m \times n$ matrix $A$ if $r(A) = \| A \|_F^2 / \| A \|^2$. The normal rank of $A$ is the algebraic dimension of $A(B)$, whereas the stable rank is the stable dimension of $A(B)$. Thus $r(A) \leq \text{rank}(A)$ for any set $A$.

\section{Sudakov's Minorization Inequality}

We now establish a lower bound on the expectation of the supremum of a Gaussian process using covering numbers. Since Dudley's inequality upper bounds the supremum using covering numbers, we can fairly easily see how sharp the two inequalities are with respect to one another.

\begin{theorem}
	If $\{ X_t \}$ is a centered, Gaussian process, then for any $\varepsilon > 0$,
	%
	\[ \EE \sup X_t \gtrsim \varepsilon \sqrt{\log N(T,\varepsilon)}. \]
\end{theorem}
\begin{proof}
	We deduce the result from the Sudakov-Fernique inequality. Assume that $N(T,\varepsilon) = N$, and consider a $\varepsilon$ net $S$ of cardinality $N$. We certainly have $\sup_{t \in T} X_t \geq \sup_{s \in S} X_s$. Define $Y_s = (\varepsilon 2^{-1/2}) g_s$, where $\{ g_s \}$ is a family of independant standard normal distributions. Then
	%
	\[ \EE (Y_s - Y_{s'})^2 = \varepsilon^2 \leq \EE (X_s - X_{s'})^2 \]
	%
	Thus the Sudakov Fernique inequality implies that
	%
	\[ c \varepsilon \sqrt{\log N} \leq \EE \sup Y_s \leq \EE \sup X_s \leq \EE \sup X_t. \]
	%
	This completes the proof.
\end{proof}

\begin{example}
	If $P$ is a polytope with $n$ vertices, then $w(P) \lesssim (\log n)^{1/2}$. But Sudakov's minorization inequality implies that $w(P) \gtrsim \varepsilon (\log(P,\varepsilon))^{1/2}$, so we conclude that $N(P,\varepsilon) \lesssim n^{1/\varepsilon^2}$.
\end{example}

\section{Two Sided Sudakov Inequality}

There is a gap between Sudakov's minorization and Dudley's inequality. But when talking about coverings on $\RR^n$, the gap is logarithmically large. The two-sided Sudakov inequality makes this precise.

\begin{theorem}
	Let $T \subset \RR^n$ and define
	%
	\[ s(T) = \sup_{\varepsilon \geq 0} \varepsilon \cdot (\log N(T,\varepsilon))^{1/2}. \]
	%
	Then $s(T) \lesssim w(T) \lesssim \log(n) \cdot s(T)$.
\end{theorem}
\begin{proof}
	TODO.
\end{proof}

\section{Diameters of Random Projections}

Our goal in this section is, given a random projection $Q: \RR^n \to \RR^m$, and some set $T$, does $P$ shrink the diameter of $T$ with high probability. For instance, if $T$ is a finite set, then the Johnson-Lindenstrauss lemma shows that with high probability, provided $m \gtrsim \log |T|$,
%
\[ \diam(Q(T)) \approx (m/n)^{1/2} \cdot \diam(T). \]
%
If $T$ is infinite, then this need not be the case. For instance, if $T$ is the unit ball in $\RR^n$, then $Q(T)$ is the unit ball in $\RR^m$, so $\diam(Q(T)) = \diam(T)$ for any projection $Q$. We now show that random projections {\it do} shrink the diameter of a set, but only up to the spherical width of the set.

\begin{theorem}
	Let $T$ be a bounded set in $\RR^n$, and $Q: \RR^n \to \RR^m$ a random projection onto an $m$ dimensional subplace of $\RR^n$. Then with probability at least $1 - 2 e^{-m}$,
	%
	\[ \diam(Q(T)) \lesssim w_s(T) + (m/n)^{1/2} \diam(T). \]
\end{theorem}
\begin{proof}
	Without loss of generality, assume $\diam(T) \leq 1$. We note that if $z \in S^{m-1}$, then $Q^T z$ is uniformly distributed on $S^{n-1}$. Now
	%
	\[ \diam(Q(T)) = \sup_{x \in T-T} |Qx| = \sup_{x \in T-T} \max_{z \in S^{m-1}} (Qx \cdot z). \]
	%
	We now use a covering argument to bound this quantity. Let $N$ be a $1/2$ net of $S^{m-1}$ with $|N| \leq 5^m$. Then
	%
	\[ \diam(Q(T)) \leq 2 \max_{z \in N} \sup_{x \in T-T} (Q^Tz \cdot x). \]
	%
	If we fix $z \in N$, then
	%
	\[ \EE \sup_{x \in T-T} (Q^T z \cdot x) = w_s(T-T) = 2w_s(T). \]
	%
	Using the concentration inequality for Lipschitz functions on th esphere, the map $f(\theta) =  \sup_{x \in T-T} x \cdot \theta$ has $\| f \|_{\text{Lip}} \leq 1$, because
	%
	\[ \sup_{x \in T-T} x \cdot \eta = \sup_{x \in T-T} x \cdot \theta - x \cdot (\eta - \theta) \geq \sup_{x \in T-T} x \cdot \theta - |\eta - \theta|, \]
	%
	and so
	%
	\[ |f(\theta) - f(\eta)| = \sup_{x \in T-T} x \cdot \theta - \sup_{x \in T-T} x \cdot \eta \leq |\eta - \theta|. \]
	%
	Therefore, we conclude
	%
	\[ \PP \left( \sup_{x \in T-T} (Q^T z \cdot x) \geq 2 w_s(T) + t \right) \leq 2 \exp(-cnt^2). \]
	%
	If we now take a union bound, we find
	%
	\begin{align*}
		\PP \left( \max_{z \in N} \sup_{x \in T-T} (Q^Tz \cdot x) \geq 2w_s(T) + t \right) &\leq 2 |N| \exp(-cnt^2)\\
		&\leq 2 \cdot 5^m \exp(-cnt^2).
	\end{align*}
	%
	Choosing $t = C (m/n)^{1/2}$, where $C$ is large enough, we can bound the probability above by $2 e^{-m}$. And so
	%
	\[ \PP \left( \diam(Q(T)) \geq 4w_s(T) + 2C (m/n)^{1/2} \right) \leq e^{-m}. \qedhere \]
\end{proof}

We can equivalent write the result as saying
%
\[ \diam(Q(T)) \lesssim \max(w_s(T), (m/n)^{1/2} \cdot \diam(T) ). \]
%
The threshold bound at which the spherical mean becomes important occurs when $w_s(T) = (m/n)^{1/2} \cdot \diam(T)$, so
%
\[ m = \frac{n w_s(T)^2}{\diam(T)^2} \sim \frac{w(T)^2}{\diam(T)^2} \sim d(T), \]
%
where $d(T)$ is the {\it stable dimension} of $T$. Thus
%
\[ \diam(Q(T)) \lesssim \begin{cases} (m/n)^{1/2} \cdot \diam(T) & : m \geq d(T) \\ w_s(T) &: m \leq d(T) \end{cases}. \]
%
So the diameter shrinks under the projection up to when we project onto a set which has the same dimension as the stable dimension of the set.






\chapter{Deviations of Random Matrices}

We now use all of the tools we have developed to show that for any $m \times n$ random matrix $A$, $|Ax| \approx \EE |Ax|$ with high probability, for infinitely many values $x$. But what is the error rate if we take points $x$ lying in some set $T$? The answer is the Gaussian complexity $\gamma(T)$. We reduce to the Gaussian case by applying Talagrand's comparison inequality.

\begin{theorem}
	Let $A$ be an $m \times n$ matrix whose rows $A_i$ are independant, isotropic, and subgaussian. Then the process $X_x = |Ax| - \sqrt{m} \cdot |x|$ has sub-gaussian increments, i.e.
	%
	\[ \| X_x - X_y \|_{\psi_2} \lesssim K^2 |x - y|, \]
	%
	where $K = \max \| A_i \|_{\psi_2}$.
\end{theorem}
\begin{proof}
	Assume first that $|x| = 1$, and $y = 0$. Then we need only show
	%
	\[ \left\| |Ax| - m^{1/2} \right\|_{\psi_2} \lesssim K^2. \]
	%
	But this follows because the coordinates $A_i \cdot x$ of $Ax$ are independant with $\EE (A_i \cdot x)^2 = 1$, and $\| A_i \cdot x \|_{\psi_2} \leq K$, so we can apply the concentration of norm theorem to yield the result.

	Now we assume $|x| = |y| = 1$. We then have to prove that
	%
	\[ \left\| |Ax| - |Ay| \right\|_{\psi_2} \lesssim K^2 |x - y|. \]
	%
	We first prove a version of this for the squared process. TODO: Fill in details here.
\end{proof}

We then obtain our main result immediately.

\begin{theorem}
	Let $A$ be an $m \times n$ matrix whose rows $A_i$ are independent, isotropic, and subgaussian random vectors. Then for any $T \subset \RR^n$,
	%
	\[ \EE \sup_{x \in T} \left| |Ax| - m^{1/2} \cdot |x| \right| \lesssim K^2 \gamma(T), \]
	%
	where $\gamma(T)$ is the Gaussian complexity, and $K = \max \| A_i \|_{\psi_2}$.
\end{theorem}
\begin{proof}
	s
\end{proof}










\chapter{Applications of Random Processes}

By applying some matrix calculus, we can obtain variants of Hoeffding and Bernstein's inequality for matrices. We recall that if $A$ is an $n \times n$ symmetric matrix, then it has a diagonalization $A = \sum \lambda_i u_i u_i^T$, where $\lambda_i \in \RR$ and the collection of vectors $\{ u_i \}$ is an orthogonal basis for $\RR^n$. Given a real-valued function $f$ defined on a neighbourhood of the eigenvalues of $A$, we set $f(A) = \sum f(\lambda_i) u_i u_i^T$. If $f$ is a polynomial, i.e. $f(x) = a_0 + a_1x + \dots + a_m x^m$, then $f(A) = a_0 + a_1A + \dots + a_m A^m$. Given two symmetric matrices $A,B$, we say $A \preceq B$ if $B - A$ is positive definite. Our proof of the matrix Hoeffding and Bernstein's inequality will be based on the moment generating proofs in the scalar case. But if $A$ and $B$ are independant, it is no longer necessarily true that $\EE(e^{A+B}) = \EE(e^A) \EE(e^B)$. We circumvent this by applying a trace estimate, konwn as Lieb's inequality, which we don't prove.

\begin{theorem}[Lieb's Inequality]
    Let $H$ be a symmetric $n \times n$ matrix. Then the function $f(A) = \tr[\exp(H + \log A)]$ is convex on the space of positive-definite $n \times n$ symmetric matrices.
\end{theorem}

Applying Lieb's inequality to $e^Z$ for some symmetric random matrix $A$, and applying Jensen's inequality yields the following corollary.

\begin{corollary}
    If $H$ is a symmetric $n \times n$ matrix, and $A$ is a random symmetric matrix, then $\EE(\tr(e^{H + A})) \leq \tr(e^{H + \log \EE e^Z})$.
\end{corollary}

Now we can prove Bernsteins' inequality.

\begin{theorem}
    Let $A_1, \dots, A_n$ be independant, mean zero, $m \times m$ symmetric random matrices with $\| A_i \| \leq K$. Then
    %
    \[ \PP \left( \left\| \sum A_i \right\| \geq t \right) \leq 2m \exp \left( \frac{-t^2/2}{\|\sum \EE A_i^2 \| + Kt/3} \right). \]
\end{theorem}
\begin{proof}
    It suffices to control the largest eigenvalue of $S = \sum A_i$. To do this, we apply a Chernoff bound. Thus for any $\lambda > 0$,
    %
    \[ \PP \left( \| S \| \geq t \right) \leq e^{- \lambda t} \EE \left( e^{\lambda \| S \|} \right) = e^{-\lambda t} \EE \left( \| e^{\lambda S} \| \right). \]
    %
    where the last equality follows because the largest eigenvalue of $e^{\lambda S}$ is equal to the exponential of the largest eigenvalue of $S$. Since all eigenvalues of $e^{\lambda S}$ are positive, $\| e^{\lambda S} \| \leq \tr(e^{\lambda S})$. Applying Lieb's inequality iteratively, we conclude
    %
    \[ \EE(\tr(e^{\lambda S})) \leq \tr \left( e^{\sum \log \EE(e^{\lambda A_i})} \right) \]
    %
    All that remains is to bound $\EE(e^{\lambda A_i})$. Note that if $|z| < 3$, then
    %
    \[ e^z \leq 1 + z + \frac{1}{1 - |z|/3} \frac{z^2}{2} \]
    %
    If $z = \lambda x$, then if $|x| \leq K$ and $|\lambda| < 3/K$ this ineuality implies that
    %
    \[ e^{\lambda x} \leq 1 + \lambda x + g(\lambda) x^2 \]
    %
    where $g(\lambda) = (\lambda^2/2)/(1 - |\lambda|K/3)$. Applying this inequality to symmetric matrices yields
    %
    \[ e^{\lambda A_i} \preceq 1 + \lambda A_i + g(\lambda) A_i^2 \]
    %
    Taking expectations on both sides gives that
    %
    \[ \EE(e^{\lambda A_i}) \preceq 1 + g(\lambda) A_i^2 \preceq e^{g(\lambda) A_i^2}. \]
    %
    where we used the fact that $1 + g(\lambda) x^2 \leq e^{g(\lambda) x^2}$. Thus
    %
    \[ \tr \left( \exp \left( \sum \log \EE e^{\lambda X_i} \right) \right) \leq \tr \left( \exp(g(\lambda) Z) \right) \]
    %
    where $Z = \sum \EE(X_i^2)$. But now
    %
    \[ \tr \left( \exp(g(\lambda) Z) \right) \leq n \exp(g(\lambda) \| Z \|) \leq n \exp(g(\lambda) \sigma^2) \]
    %
    Putting this inequality back to the original, and setting $\lambda = t/(\sigma^2 + Kt/3)$ completes the proof.
\end{proof}

\begin{remark}
    To make this inequality look closer to the classical Bernstein's inequality, we note that it implies there is a universal constant $c$ such that
    %
    \[ \PP \left( \left\| \sum A_i \right\| \geq t \right) \leq 2m \exp \left( -c \min(t^2/\sigma^2, t/K) \right) \]
\end{remark}

\begin{corollary}
    Given the $A_i$ as in the last proof,
    %
    \[ \EE \left\| \sum A_i \right\| \lesssim \left\| \sum \EE(A_i^2) \right\|^{1/2} (\log m)^{1/2} + K \log m \]
\end{corollary}
\begin{proof}
    TODO
%    We just apply the integral identity for expectation to conclude
    %
 %   \begin{align*}
  %      \EE \left\| \sum A_i \right\| &= \int_0^\infty \PP \left( \left\| \sum A_i \right\| \geq t \right)\; dt\\
   %     &\leq \int_0^{\sigma^2/K} 2m \exp(-c t^2/\sigma^2)\; dt + \int_{\sigma^2/K}^\infty 2m \exp(-ct/K)\; dt\\
    %    &\leq 2m c^{1/2} \sigma \int_0^{\sigma/K c^{1/2}} \exp(-s^2)\; ds + (2mK/c) \exp(-c\sigma^2)\\
    %    &\lesssim 2m c^{1/2} \sigma (1 - \exp(-\sigma^2/cK^2))
    %\end{align*}
\end{proof}

Similar techniques yield further concentration inequalities for matrices.

\begin{theorem}[Hoeffding]
    Let $\varepsilon_1, \dots, \varepsilon_n$ be independant symmetric Bernoulli random variables and let $A_1, \dots, A_n$ by deterministic symmetric $m \times m$ matrices. Then
    %
    \[ \PP \left( \left\| \sum \varepsilon_i A_i \right\| \geq t \right) \leq 2m \exp(-t^2/2\sigma^2), \]
    %
    where $\sigma^2 = \left\| \sum A_i^2 \right\|$.
\end{theorem}
\begin{proof}
    TODO
\end{proof}

TODO: LIST OTHER MATRIX CONCENTRATION TECHNIQUES IN VERSHYNIN'S BOOK SECTION 5.5.







\chapter{Applications of High Dimensional Concentration}

\section{Community Detection}

For each positive even integer $n$, and $p,q \in [0,1]$, we construct a random graph $G(n,p,q)$ by dividing $n$ vertices into two sets of $n/2$ vertices, which we call communities. We connect two vertices in a common community independantly with probability $p$, and two vertices in separate communities with probability $q$. We assume $p > q$ here so vertices in a common community are more likely to be connected. A natural problem, given such a graph, is to be able to partition the vertices into two communities given no prior knowledge about the graph.

To obtain such an algorithm for this process, we apply our results about matrix concentration. Let $A$ denote the \emph{random} adjacency matrix for $G(n,p,q)$. We can write $A = D + R$, where $D = \EE(A)$ is the deterministic part of the adjacency matrix, and $R$ is the random part. It is easy to so the matrix $D$ has rank two. For illustration, if $n = 4$, then after reordering the vertices, we find
%
\[ D = \begin{pmatrix} p & p & q & q \\ p & p & q & q \\ q & q & p & p \\ q & q & p & p \end{pmatrix}. \]
%
It therefore has two non-zero eigenvalues
%
\[ \lambda_1 = \left( \frac{p + q}{2} \right) \cdot n\quad\text{and}\quad \lambda_2 = \left( \frac{p - q}{2} \right) \cdot n. \]
%
Corresponding to the two eigenvectors $u_1$ and $u_2$. Note that $u_{1i} = 1$ for all $i$, and gives no useful information. But $u_{2i} = 1$ if $i$ is in the first community, and $u_{2i} = -1$ if $i$ is in the second community. If we could identify $u_2$, we could identify the communities precisely.

We do not have access to $D$, but we have access to $D + R$, and we can certainly diagonalize this matrix. Matrix concentration tells us that with probability $1 - 4e^{-n}$, we have $\| R \| \leq C \cdot n^{1/2}$ for some universal constant $C$. Thus if $p - q > 0$, for large $n$ $\| R \|$ is much smaller than $\| D \|$, which is proportional to $n$. Now this means that all the eigenvalues of $A$ differ from $D$ by at most $C \cdot n^{1/2}$. Furthermore, the Davis-Kahan theorem says the eigenvectors corresponding to these eigenvalues do not differ much from each other either.

\begin{theorem}[Davis-Kahan]
	Let $S$ and $T$ by symmetric $n \times n$ matrices, and let $\lambda_i(S), \lambda_i(T)$, $v_i(S)$ and $v_i(T)$ denote the $i$'th largest eigenvalues and unit eigenvectors of the matrices. If $|\lambda_i(S) - \lambda_j(S)| \geq \delta$ for all $j \neq i$, then
	%
	\[ v_i(S) \cdot v_i(T) \geq \left( 1 - \frac{4 \| S - T \|^2}{\delta^2} \right)^{1/2}. \]
	%
	This means there exists a sign $\theta \in \{ -1, 1 \}$ such that
	%
	\[ |v_i(S) - \theta v_i(T)| \leq \frac{2^{3/2} \| S - T \|}{\delta}. \]
\end{theorem}

In particular, since $\| A - D \| \leq C \cdot n^{1/2}$, if we set
%
\[ \delta = \min \left( \frac{p - q}{2}, \left( \frac{p + q}{2} - \frac{p - q}{2} \right) \right) \cdot n = \min(0.5 \cdot (p-q), q) \cdot n. \]
%
Then we find there is $\theta \in \{ -1 , 1 \}$ such that
%
\[ |v_i(A) - \theta v_i(D)| \lesssim \frac{n^{-1/2}}{\min(q,p-q)} \]
%
Thus the signs of most of the coefficients of $A$ and $D$ must agree. The number of disagreeing signs between $v_2(A)$ and $v_2(D)$ is bounded up to a constant by $\min(q,p-q)^{-1}$. Thus by finding the second largest eigenvector to $A$, and clustering the two communities by the sign of the vector, we will be correct with high probability, with few errors. This is known as a \emph{spectral clustering} method. This is efficiently computable even when $n$ is large.

\section{Covariance Estimation}

Suppose we are analyzing data in high dimensions, represented as points $X_1, \dots, X_m$ sampled from a distribution in $\RR^n$. One of the standard tools for studying such data is principal component analysis. Given a distribution $X$, the distribution can be understood by computing the spectral decomposition of $\Sigma(X)$. The direction corresponding to the largest eigenvalue is known as the \emph{first principal direction}. This explains most of the variability in the data. In some cases, only of the few of the eigenvalues of $\Sigma(X)$ are large, and projection onto the eigenspaces corresponding to these eigenvalues represents the information of the data in a low dimensional space. If only three eigenvalues are significant, this even makes the data visualizable.

In practice, $\Sigma(X)$ cannot be calculated exactly. But we can calculate the sample covariance matrix
%
\[ \Sigma = \frac{1}{m} \sum_{i = 1}^m X_i X_i^T \]
%
We certainly have $\EE(\Sigma) = \Sigma(X)$, and the law of large numbers implies $\Sigma \to \Sigma(X)$ almost surely as $m \to \infty$. But we want a non asymptotic result. Of course, dimensional considerations mean we need at least $m = \Omega(n)$ in order for $\Sigma$ to be close to $\Sigma(X)$. In fact, $O(n)$ results suffice.

\begin{theorem}
	Suppose $X$ is a random vector such that for any $x \in \RR^n$,
	%
	\[ \| X \cdot x \|_{\psi_2} \leq K \| X \cdot x \|_{L^2(\Omega)} \]
	%
	Then
	%
	\[ \EE \| \Sigma - \Sigma(X) \| \lesssim K^2 \left( (n/m)^{1/2} + (n/m) \right) \| \Sigma(X) \| \]
	%
	In particular, if $m \geq n$, then $\EE \| \Sigma - \Sigma(X) \| \lesssim K^2 (n/m)^{1/2}$.
\end{theorem}
\begin{proof}
	Consider the isotropic random vectors $Z$ and $Z_1, \dots, Z_m$ such that $X = \Sigma(X)^{1/2} Z$ and $X_i = \Sigma(X)^{1/2} Z_i$. Then the subgaussian assumption implies $\| Z \|_{\psi_2} \leq K$ and $\| Z_i \|_{\psi_2} \leq K$. If we set $\Pi = m^{-1} \sum Z_i Z_i^T - I_n$, then
	%
	\[ \| \Sigma - \Sigma(X) \| = \| \Sigma^{1/2}(X) \Pi \Sigma^{1/2}(X) \| \leq \| \Pi \| \| \Sigma(X) \| \]
	%
	But we know from our matrix concentration results tha
	%
	\[ \EE \| \Pi \| \lesssim K^2 \left( (n/m)^{1/2} + (n/m) \right). \]
	%
	which gives the result for free.
\end{proof}

Consider the following application of this theorem. We consider two normal distributions $N(\mu,I_n)$ and $N(-\mu,I_n)$, with means $\mu$ and $-\mu$. We then pick $m$ points $X_1, \dots, X_m$, which each have a 50/50 chance of being picked by one of the distributions. A natural goal is to cluster these vectors into whether they were picked from one distribution or the other. Just as in community detection, we can use a spectral clustering algorithm.

Note that if $X = \theta \mu + g$ is identically distributed to $X_1, \dots, X_m$, where $\theta$ is a symmetric Bernoulli random variable and $g$ is Gaussian. Note that $X$ is not isotropic. Instead, $\Sigma(X) = I_n + \mu \mu^T$. Note that $\mu$ is the only eigenvector, corresponding to the eigenvalue $1 + |\mu|^2$. This makes sense, since the only interesting non-noise related features of the data correspond to whether the data comes from $N(\mu,I_n)$ or $N(-\mu,I_n)$. If $m \sim \varepsilon^{-2} n$, then our results about covariance estimation imply that if we define $\Sigma = m^{-1} \sum X_i X_i^T$, then $\EE \| \Sigma - \Sigma(X) \| \leq \varepsilon (1 + |\mu|^2)$. In the case that $\| \Sigma - \Sigma(X) \| \lesssim \varepsilon (1 + |\mu|^2)$, we can apply the Davis-Kahan theorem to show that there is $\theta' \in \{ -1 , 1 \}$ such that if $v$ is the principal eigenvector of $\Sigma$, then
%
\[ |v - \theta' \mu| \lesssim \varepsilon \]
%
Note that if $X_i$ belongs to $N(\mu,I_n)$, then $X_i \cdot \mu > 0$ with high probability. Thus if we partition $X_1, \dots, X_m$ depending on whether $X_i \cdot v > 0$ or $X_i \cdot v < 0$, then we cluster the data effectively. TODO: Fill in details here.






\end{document}