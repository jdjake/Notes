\input{../../style.tex}

\title{Metamathematics}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents

\part{Mathematical Logic}

\chapter{What's Logic All About?}

\pagenumbering{arabic}

The mathematical method uses rigorous arguments to discover new facts about assumptions. Metamathematics turns the subject on its head, using the mathematical method to analyze mathematical methods themselves! Traditionally, metamathematics, or logic, was used by philosophers to find valid forms of reasoning in arguments. But in the early 20th century, a desire for rigour and an invention of an incredibly powerful system of mathematics unvealed a plague of foundational mathematical paradoxes in previously believed valid forms of reasoning. It was in the resultant firestorm where true metamathematics was forged...

In the late 1800s, a German mathematician named Georg Cantor invented an entirely new way of thinking about mathematics. Instead of discussing particular numbers or shapes or functions, Cantor decided instead to talk about collections of mathematical objects, known as sets, and the ways these sets can be manipulated and described. In a flourish, he solved a problem which had troubled mathematicians for centuries. A number is called {\it algebraic} if it is the root of a polynomial
%
\[ a_0 + a_1 X + \dots + a_n X^n \]
%
with rational coefficients $a_0, \dots, a_n \in \QQ$.  Since the time of Archimedes, mathematicians had wondered whether all numbers were algebraic, or whether there exists non-algebraic numbers, known as transcendental. Cantor answered this question negatively. Rather than finding an example of a trancendental number, he counted the {\it set} of algebraic numbers, counted the {\it set} of all numbers, and proved that there were far too many numbers for them all to be algebraic. But by creating a powerful linguistic framework for collect mathematical concepts together, Cantor changed the way mathematicians think about mathematics, opening the floodgates for mathematicians to encapsulate the entirety of mathematics, and here the paradoxes emerged.

\begin{example}[Cantor's Paradox]
    For any set $X$, we can consider the powerset $2^X$, which consists of all sets $Y$ which are subsets of $X$. Cantor proved that the cardinality of $2^X$ is always strictly greater than the cardinality of $X$. But if $X$ is the set of all mathematical objects, including sets, and sets whose objects are sets, and so on and so forth, then $2^X \subset X$, which implies the cardinality of $2^X$ is less than or equal to the cardinality of $X$, contradicting Cantor's theorem.
\end{example}

\begin{example}[Burali-Forti Paradox]
    Consider the set $\Omega$ of all ordinals. Then $\Omega$ is a well-ordered set, and hence is order isomorphic to some ordinal, and thus to a proper segment of itself. But no well-ordered set is order isomorphic to a proper subset of itself!
\end{example}

\begin{example}[Russell's Paradox]
    Consider the set of all sets which do not contain themselves, expressed in set theoretic notation as
    %
    \[ X = \{ x: x \not \in x \} \]
    %
    Russell's paradox rests on an innocent question: Is $X$ an element of $X$? If $X \in X$, then by definition of the set $X$, we conclude that $X \not \in X$. So we are lead to believe that $X \not \in X$. But then, by construction of $X$, we conclude $X$ is in $X$ after all! A more colloquial explanation of the paradox considers a town with a single barber, shaving everyone who does not shave themselves. Russell's paradox emerges when we ask who shaves the barber?
\end{example}

\begin{remark}
    These paradoxes are not disjoint from each other. Russell's paradox is the simplest set-theoretic paradox, for it relies on no advanced set theory, such as the theory of cardinals and ordinals. However, Cantor's paradox was discovered first, and is essentially equivalent. The proof of Cantor's theorem takes an arbitrary surjective map $f: X \to 2^X$, considers $Y = \{ x \in X: x \not \in f(x) \}$, and then considers $y$ with $f(y) = Y$. We conclude that $y \in Y$ if and only if $y \not \in Y$, which is a contradiction. If we replace an arbitrary set $X$ with the set of all sets, then $2^X = X$, so if we take the identify function as $f$, then $Y = \{ x \in X: x \not \in x \}$, which is exactly the set involved in Russell's paradox. In fact, it was exactly when going through Cantor's paradox that Russell discovered his simpler paradox.
\end{remark}

All of these paradoxes rely on the fact that set theory can discuss mathematical objects that are `too big' to be understood by logic used on simple, finitary objects that occur in classical mathematics. Russell's paradox is the most mathematically elegant of the set-theoretic paradoxes, for it relies on no advanced knowledge of cardinals nor ordinals. However, the paradox is essentially equivalent to these paradoxes. For instance, the proof of Cantor's theorem takes an arbitrary surjective map $f: X \to 2^X$, considers $Y = \{ x \in X: x \not \in f(x) \}$, and considers $y$ with $f(y) = Y$. We conclude that $y \in Y$ if and only if $y \not \in Y$. If we replace an arbitrary set $X$ with the set of all sets, then $2^X = X$, so if we take the identity function as $f$, then $Y = \{ x \in X : x \not \in x \}$, which is exactly the set involved in Russell's paradox.

Henri Poincar\'{e} saw the paradoxes as proof that set theory was a ``plague on mathematics'', which could only be cured when the theory was eradicated as an acceptable mathematical tool. But paradoxes were not limited to set-theoretic concepts. In their work on the Continuum hypothesis, Julius K\"{o}nig and Jules Richard found paradoxes attacking the ability for mathematicians to specify mathematical objects by phrases of the english language.

\begin{example}[Richard \& K\"{o}nig's Paradoxes]
    The set of all english expressions is countable, because there are only finitely many expressions of a certain length, and the union of countably many disjoint finite sets is countable. Of particular interest is the set of english expressions describing positive real numbers, which we denote by $X$. The way we interpret these english expressions can be thought of as a function $f: X \to \RR$, which takes some expressions $x \in X$ and tells us which number this expression corresponds to. Since $\RR$ is uncountable, there are some real numbers which cannot be described in english. We call the numbers in $f(X)$ the \emph{definable real numbers}. As the image of a countable set, the set of all definable real numbers can be ordered by some enumeration $x_1, x_2, x_3, \dots$, and so Cantor's diagonal method allows us to take any enumeration of real numbers, and use the enumeration to construct a canonical real number which isn't in the enumeration. Let this undefinable real number be denoted $x$. We can then construct the expression $S$, which is
    %
    \begin{center}
        ``The number constructed from Cantor's diagonal argument on the enumeration considered.''
    \end{center}
    %
    then we would expect $S$ to define a real number, and in fact, define the number $x$, yet we have already seen that $x$ is not definable. Similarily, if we let $\omega$ be some well-ordering of the real numbers, then we could consider the expression $T$, which is
    %
    \begin{center}
        ``The least number not definable in english with respect to $\omega$.''
    \end{center}
    %
    and then $T$ should describe some definable number $f(T)$, which by our understanding of the semantics of english phrases, we would expect $f(T)$ not to be definable.
\end{example}

It seems obvious that a definition is simply a description of the qualities of an object under consideration, but if this were true, there would be no problem with the arguments above, so we are at an impasse. A precise definition of definability is a key discovery in metamathematics, from which we will obtain the beautiful results of G\"{o}del and Tarski. A related class of paradoxes results from self-reference.

\begin{example}[L\"{o}b]
    Consider the Preposition $B$, which is true when $B \Rightarrow A$ is true. If $B$ is true, then $B \Rightarrow A$ is true, so $A$ is true. But this means we have proven that $B \Rightarrow A$ is true, so $B$ is true, and we therefore conclude by inference that $A$ is true. Since $A$ was arbitrary, we can conclude that every logical statement is true from this form of argument!
\end{example}

It can be argued that L\"{o}b's paradox fails because self referential statements are naturally circular, but Curry showed that self reference can be seen much more subtly in naive set theory.

\begin{example}[Curry]
    For any property $P$, consider
    %
    \[ C = \{ x : (x \in x) \Rightarrow P(x) \} \]
    %
    Then $C \in C$ holds if and only if $C \in C \Rightarrow P(C)$ holds. We must have $C \in C$, for if $C \not \in C$, then $C \in C \Rightarrow P(C)$ holds vacantly, and so we conclude that $C \in C$. But this implies $C \in C \Rightarrow P(C)$, so $P(C)$ is true, irrespective of the content of the statement $P(C)$.
\end{example}

It became clear to the mathematicians of the early 1900s that the current state of logic was not sharp enough to fix and explain away the paradoxes of set theory. Even if set theory was to be discarded, it was still important to understand why it's methods give us paradoxes, so we don't fall into the same trap when inventing new mathematical methods. Thus the modern field of metamathematics was formed. As mathematicians, our only hope to attacking these paradoxes is to face them head on, and to apply the precise weapon of mathematical rigor. In the face of adversity, we do what mathematicians do best: define and conquer.

\section{Formal Systems}

In order to analyze mathematics mathematically, a careful method must be employed to avoid making our reasoning circular. The main framework for constructing abstract models simulating the procedure of mathematics, while being clearly separated from mathematical technique, is the idea of a formal system. The main idea of this process is that the {\it form} of an argument contains all the necessary information required to validify and understand an argument. Formal systems theory takes solely the forms of a mathematical argument, and abstracts them; once an argument has been encoded in a formal system, it is just a sequence of symbols on a page. In this way, mathematical logic is effectively `mathematical linguistics', studying the languages that a mathematicians reasons with.

To formalize the language of mathematicians, we must first estimate the procedure of of a mathematician at work. First, she accepts some fundamental statements as `obviously true', known as axioms. Using previously agreed upon logical derivations, she obtains additional statements from the axioms. This describes completely the form of a mathematical argument. It is the basic principle of formal systems that the thought of process of a mathematician, while an important part of the mathematical process, is extraneous to the actual content of the mathematics the mathematician produces, and so statements and derivations contain all effective information to understand the mathematical process. Thus we need only model what a statement is, and what a logical derivation is, and we then have a toolkit for analyzing all of mathematics by mathematical means!

% We emphasize that the choice of axioms is completely arbitrary. They can be determined individually by the person who is performing a logical deduction. We would hope our logical analysis holds irrespective of the axioms, and since virtually no statements are accepted as universally true, our logical analysis can assume very little about what is assumed. We assume the choice of axioms is determined by the person at hand. To a scientist, what is obvious is that determined by experiment. To a priest, what is obvious is that which is written in a holy book. In mathematics, we do not care how one determines that statements are obvious, only that some statements are accepted as obvious, and of the statements we are interested in, we are able to clearly separate which are accepted and which are not accepted, so we can proceed apriori. The methods of logic should carry through unperturbed.

Both statements and derivations will be built from a deceptively simple system of mathematical objects. We take a set $\Lambda$, known as an \emph{alphabet}, and consider \emph{strings} over that alphabet, which are finite (possibly empty) sequences of elements in $\Lambda$. Strings have turned out to be the right formalization for the majority of mathematical logic, and we shall find they suffice to express all the formal systems we shall consider in these notes.

\begin{example}
    The whole reason $\Lambda$ is called an alphabet is because we could take the alphabet to be the set of all upper case and lower case characters, and some `space character'
    %
    \[ \Lambda = \{ A,B,C,\dots,Z, a,b,c,\dots,z,\textvisiblespace \} \]
    %
    example strings over this alphabet consist of complete garble, like $(A,F,c,B,V,v)$, and actual strings which occur in english literature, like
    %
    \[ (T,o,\textvisiblespace,b,e,\textvisiblespace,o,r,\textvisiblespace,n,o,t,\textvisiblespace,t,o,\textvisiblespace,b,e) \]
    %
    it is customary to denote a string $(v_1, \dots, v_n)$ by $``v_1 \dots v_n''$, or even $v_1 \dots v_n$ if the statement isn't ambiguous. To distinguish strings  Thus the garbled string would be denoted AFcBVv, and the second by ``Have\textvisiblespace I\textvisiblespace gone\textvisiblespace mad'', or even ``Have I gone mad'', if it is clear that the spaces correspond to some particular separator character.
\end{example}

The concatenation of two strings $s = s_1 \dots s_n$ and $w = w_1 \dots w_m$, denoted $sw$, is the string $s_1 \dots s_n w_1 \dots w_m$. If we view concatenation as an associative algebraic operation, then the set of all strings $\Lambda^*$ is the smallest {\it monoid} containing $\Lambda$: it is essentially the standard construction of the free monoid with generators $\Lambda$. This is an interesting viewpoint which is useful in certain specialized areas of mathematical logic, like automata theory, though we will not touch on this viewpoint much here.

Since most of our languages will be constructed from putting basic strings together, we may wish to take complicated strings, such as
%
\begin{center}
    ``It was the best of times, it was the worst of times''
\end{center}
%
and identify more basic components in them, such as ``worst'', or ``best''. A \emph{substring} of a string $s$ is a string $w$ obtained from taking a continguous sequence of characters in $s$. This is equivalent to the existence of two strings $w$ and $v$ such that $s = wuv$. A \emph{prefix} is a substring occuring at the beginning of a string ($s = uv$), and a \emph{suffix} is a substring occuring at the end ($s = wu$). A \emph{language} is a subset of strings over an alphabet. It allows us to separate meaningful strings over the alphabet, like the string
%
\begin{center}
    ``The quick brown fox jumps over the lazy dog''
\end{center}
%
from complete nonsense, like the string
%
\begin{center}
    ``iovuiaesfpauaauupewbpvapbuib''
\end{center}
%
As should be expected, the english language is a mathematical language, obtained from taking the subset of strings over the english alphabet which represent coherent english sentences.

In mathematical logic, the languages of study consist of the sets of meaningful formulae in some logical calculus, and various interesting families of sublanguages, such as the provable formulas in some deductive system, the semantically true formulas, or the satisfiable formulas. The construction of the sublanguage of `provable formulas' is encapsulated in what is called a \emph{formal system}. The most general definition consists of a language $L$ over an alphabet $\Lambda$, a set of axioms, which form a subset of $L$, and a set of inference rules, pairs $(\Gamma, s)$ where $\Gamma \subset L$ is the premises of the inference, and $s$ is the conclusion. The \emph{theorems} of a formal system are members of the smallest set such that
%
\begin{enumerate}
    \item Every axiom is a theorem
    \item If $(\Gamma, s)$ is an inference rule, and all elements of $\Gamma$ are theorems, then $s$ is a theorem.
\end{enumerate}
%
If $X$ is a formal system, we shall let $\vdash_X s$ state that $s$ is a theorem of $X$. Normally though, we will just write $\vdash s$, for the formal system is almost always clear from context. Just as algebra is the study of groups, rings, and fields, metamathematics is the exploration of the different formal systems we can use to model mathematics.

\section{Why Trust Mathematical Logic?}

Before we get to the real work though, we need to settle an important question: How can we ensure the models we apply to analyze mathematics are robust enough to enable us to prove facts about real mathematics? David Hilbert's plan, along with the rest of the formalist school, was to construct a formal system powerful enough to describe all of the present mathematical systems, and one that could discuss itself, and prove itself consistant (without paradox). If such a system could be constructed, we could model all past, present, and future proofs of mathematics in this universal system, and we would be shielded from future paradoxes. Thus the model is proved robust by the fact that all proofs could be modelled in the ssytem. Hilbert would have essentially reduced mathematics to abstract symbol pushing inside the system, but Hilbert did not see this as an issue; this symbol pushing is no different from the symbol pushing inside our minds when we solve a problem, albeit more explicit. However, regardless of whether you believe this approach would be valid, we shall find Hilbert's approach is doomed from the beginning, for no sufficiently advanced consistant formal system can prove itself consistant.

So how can we ensure that our formal systems give correct results about everyday mathematics? If you desire absolute facts, you will be dissapointed. It is unlikely that any of our physical models of the universe are completely accurate. A physicists's models are ideals, carved from reality in all senses but experimental parameters. No model describes a systems evolution exactly, and it is myopic to suggest a model's perfection. In spite of this, physics still does a bloody good job! In metamathematics, we attempt to form a mathematical model of mathematical principles. Some principles are pinned down for examination, others lost. We hope this model has enough vitality to provide key insights into real-life mathematics. Whether the method is successful can only be determined by the correspondence between the results of metamathematics, and evidence in actual mathematics. So far, the results of metamathematics in propositional and first order logic have not been proven inaccurate by mathematical innovation.

But even if you don't accept formal systems as a correct model of ordinary mathematics, mathematical logic is still of interest, because we can obtain rigorous theorems connecting the study of strings to the study of mathematical objects. This allows us to relate theorems about interesting axiom systems, like the theory of fields or the theory of vector spaces, to our theory of strings. This occurs regardless of whether you think that strings model `all' there is in mathematics, and these rigorous theorems tell us that the strings at least model `part' of mathematics.

A source of confusion in physics is the stylistic treatment of assumptions as absolute facts. A physicist describes ``a planet moving according to the equation $\ddot{x} = -m/x^2$'' even if he is actually talking about the dynamical system whose evolution is described by the differential equation $\ddot{x} = -m/x^2$, which {\it models} the motion of a planet. Such expressions are unavoidable, since they make the study of mechanics much more viceral and appealing to intuition, whereas eschewing the natural language makes the formal equivalent dry to the bone. Keep this principle in mind as we begin to build models of logic. Every theorem we proved is only true in the model of mathematics, and must be judged for authenticity outside of the mathematical model we have created, execept for rigorous mathematical theorems connecting the study of strings to the study of other mathematical objects.






\chapter{Prepositional Logic}

We shall begin our study of formal systems with propositional logic, the simplest formal system to analyze truth. To understand propositional logic, we construct a mathematical model, known as a formal language, which represents the language in which mathematics is performed. The formal language is then analyzed by common mathematical deduction rules. The standard formal language for logic is an analysis of strings, sequences of abstract symbols from a given alphabet. Strings represent mathematical statements; manipulating these strings models how a mathematician infers some mathematical statement from another. It is best to see the tool in action to understand its utility, so we proceed swiftly into the technicalities involved in the construction of the logic.

\section{Syntax}

Normally, a formal system makes colloquial speech, so each symbol in the alphabet provides a precise representation of some forms of colloquial speech. We begin with prepositional logic, which models statements with sentences of mathematics which are composed of basic statements which are true or false, and independent of one another. Some statements are \emph{atomic} in the propositional logic, because they cannot be divided into more base statements. ``Socrates is a man'' is an atomic statement, as is ``every woman is human''. ``Socrates is a man and every woman is a human'' is not atomic, for the statement consists of two separate statements, composed by the connective ``and''. In English, ``every woman is a human'' can be broken into statements such as ``Julie is a human'' and ``Laura is a human'', yet propositional logic still considers this statement as atomic; the model does not have the capability to precisely model understanding of these complex statements, which are the realm of predicate logic, discussed in the next chapter.

Let $\Lambda$ be a set disjoint from $\{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \}$. These symbols will represent the atomic statements in our representation of propositional logic. The \emph{propositional language with atoms in $\Lambda$}, denoted $\text{SL}(\Lambda)$, is the smallest subset of strings over the alphabet $\Lambda \cup \{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \}$ such that
%
\begin{enumerate}
    \item $\Lambda \subset \text{SL}(\Lambda)$.
    \item If $\phi, \psi \in \text{SL}(\Lambda)$, then $(\neg \phi), (\phi \wedge \psi), (\phi \vee \psi), (\phi \Rightarrow \psi), (\phi \Leftrightarrow \psi) \in \text{SL}(\Lambda)$.
\end{enumerate}
%
An element of $\text{SL}(\Lambda)$ is called a \emph{formula} or \emph{statement}.

Each \emph{connective} of prepositional logic represents a certain linguistical form. Later on, the connection of symbols to meaning will become clear. For now, they are abstract symbols without intrinsic meaning. Viewing a formal system as meaningless symbol shifting is known as the {\it syntactical view} of a formal system. For now, the table below gives the interpretations we will later give for the logical connectives.
%
\begin{center}
\begin{tabular}{| c | c | c |}
    \hline Connective & Name of Connective & Meaning of statement \\
    \hline $\neg \phi$ & Negation & $\phi$ is {\it not} true\\
    $\phi \wedge \psi$ & Conjuction & $\phi$ {\it and} $\psi$ both are true\\
    $\phi \vee \psi$ & Disjunction & either $\phi$ {\it or} $\psi$ is true\\
    $\phi \Rightarrow \psi$ & Implication & {\it If} $\phi$ is true, {\it then} $\psi$ is true\\
    $\phi \Leftrightarrow \psi$ & Bicondition & $\phi$ is true, {\it if, and only if,} $\psi$ is true\\
    \hline
\end{tabular}
\end{center}

Take care to notice that $\text{SL}(\Lambda)$ is the {\it smallest} set constructed with the required axioms, in the same way that most `smallest objects' exist in mathematics, because the intersection of sets satisfying the set of statements defining $\text{SL}(\Lambda)$ also satisfy the statements. This property leads to the most useful proof method in logic.

\begin{theorem}[Structural Induction]
    Consider a proposition that can be applied to elements of $\text{SL}(\Lambda)$. Suppose the proposition is true of all elements of $\Lambda$, and that if the proposition is true of $\phi$ and $\psi$, then the proposition is also true of $\neg \phi, \phi \wedge \psi, \phi \vee \psi, \phi \Rightarrow \psi$, and $\phi \Leftrightarrow \psi$. Then the proposition is true for all of $\text{SL}(\Lambda)$.
\end{theorem}
\begin{proof}
    Let $P$ be some property to consider. Note the set
    %
    \[ K = \{ \phi \in (\Lambda \cup \{ (, ), \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \})^* : P(\phi)\ \text{is true} \} \]
    %
    is a set of strings satisfying the axioms (1) and (2) which define $\text{SL}(\Lambda)$, so that we may conclude $\text{SL}(\Lambda) \subset K$.
\end{proof}

The formulas of prepositional logic are just abstract sequences of symbols. They are not defined to have an intrinsic grammatical structure. To start working with this language however, we must proved that the language necessarily does have a grammatical structure. Since we are working over formulas of arbitrary complexity, structural induction will be the most useful method of proof.

\begin{theorem}
    Any sentence in $\text{SL}(\Lambda)$ contains as many left as right brackets.
\end{theorem}
\begin{proof}
    Any atom in $\Lambda$ contains no left brackets, and no right brackets, and thus the same number of each. If $\phi$ and $\psi$ have as many left brackets as right brackets, then so too does $(\neg s)$, and $(s \circ w)$, where $\circ \in \{ \wedge, \vee, \neg, \Rightarrow, \Leftrightarrow \}$. By structural induction, we have proved our claim.
\end{proof}

We need a more in depth theorem to correctly parse statements of propositional logic. If statements can be parsed in two different ways, they become ambiguous. For instance, what is the value of $2 + 7 - 5 - 4$? Is it
%
\[ (((2 + 7) - 5) - 4) = 0 \]
%
or
%
\[ (2 + 7) - (5 - 4) = 8 \]
%
In our language, we would hope that parenthesis allow us to remove ambiguity, so we know which order to apply logical operations. The study of syntax allows us to show that statements can be parsed uniquely, and once we have this understanding, we can work with the language much more fluidly, making the understanding of the semantics of the language much more clear. Equations must be understood before we calculate with them.

\begin{theorem}
    If $w$ is a nonempty prefix of $\phi \in \text{SL}(\Lambda)$, then $w$ has at least as many left brackets as right brackets, and $w = \phi$ if and only if $w$ has the same number of left and right brackets.
\end{theorem}
\begin{proof}
    If $\phi \in \Lambda$, then $w$ is only one letter long, so $\phi = w$, and $\phi$ has no brackets. Now let $\phi$ and $\psi$ satisfy the theorem. We split our proof into two cases.
    %
    \begin{itemize}
        \item $w$ is a prefix of $(\neg \phi)$: March through all cases. Suppose $w$ has the same number of left brackets than right. $w$ cannot equal $($ or $(\neg$, nor $(\neg v$, where $v$ is a prefix of $s$; by induction, $v$ has at least as many left brackets as right brackets, and then $w$ has more left brackets than right brackets. Thus $w$ must equal $(\neg s)$.
        \item $w$ is a prefix of $(\phi \circ \psi)$, for some $\circ \in \{ \wedge, \vee, \Rightarrow, \Leftrightarrow \}$: Continue the string march. $w$ cannot equal $($, nor $(v$ by induction, where $v$ is a prefix of $\phi$. Similarily, $w$ cannot equal $(\phi \circ$, nor $(\phi \circ v$, where $v$ is a substring of $\psi$, so $w$ must equal $(\phi \circ \psi)$.
    \end{itemize}
    %
    Careful analysis of each case also shows that $w$ must have at least as many left brackets as right brackets.
\end{proof}

\begin{corollary}
    Every string in $\text{SL}(\Lambda)$ can be written uniquely as an atom $\Lambda$, or $(\neg \phi)$ and $(\phi \circ \psi)$, where $\phi$ and $\psi$ are elements of $\text{SL}(\Lambda)$. The unique connective in the representative is known as the \emph{principal connective} of the statement.
\end{corollary}
\begin{proof}
    Such representations trivially exist by the construction of $\text{SL}(\Lambda)$. Suppose we have two representations. If one of the representations is an element of $\Lambda$, the other representation must have length one, and is therefore equal to the other representation. If we have two representations $(\neg \phi) = (\neg \psi)$, then by chopping off symbols, we conclude that $\phi = \psi$. It is impossible to have two distinct representations $(\phi \circ \psi) = (\neg \phi)$, for no element of $\text{SL}(\Lambda)$ begins with $\neg$. Finally, suppose we have two representations $(\phi \circ \psi) = (\eta \circ \nu)$. Then either $\phi$ is a prefix of $\eta$, or $\eta$ is a prefix of $\phi$, and both have balanced brackets, which implies $\phi = \eta$, and by chopping letters away, we conclude $\psi = \nu$.
\end{proof}

Given an arbitrary language constructed recursively, it is in general very difficult to verify if a language is ambiguous. In the theory of computability, we discover that there is no general algorithm with which we can prove any given recursively constructed language is ambiguous. However, the particular languages we use to represent formal systems tend to have a fairly simple syntax, and therefore we can prove that these languages are ambiguous with relative ease.

Because we have unique parsing, we can define functions on terms of propositional logic recursively, focusing only on the principal connecting in the definition. For instance, given two sets $\Lambda = \{ A, B, C \}$, and $\Gamma = \{ X, Y, Z \}$, we can obtain a natural bijection from $\text{SL}(\Lambda)$ to $\text{SL}(\Gamma)$ by extending the map
%
\[ X \mapsto A\ \ \ \ \ Y \mapsto B\ \ \ \ \ Z \mapsto C \]
%
by the recursive definition
%
\[ f(\phi \circ \psi) = f(\phi) \circ f(\psi)\ \ \ \ \ f(\neg \phi) = \neg f(\phi) \]
%
Such a map is well defined on all of $\text{SL}(\Lambda)$ by the corollary, and can be shown by a certain structural induction to be bijective. It is easy to see that this map preserves the semantic properties of propositional logic we will soon define, so that all systems of propositional logic are essentially equivalent.

It is useful to have a visual specification of the unique way to parse a formulae. Given a formula $\phi$, define the \emph{parse tree} of $\phi$ inductively by the following process:
%
\begin{itemize}
    \item If $\phi$ is atomic, then the parse tree of $\phi$ consists of a single node, $\phi$ itself.
    \item If $\phi = \eta \circ \nu$, where $\circ$ is a binary connective, then the parse tree has a parent node $\phi$, descending into two subtrees, the first of which being the parse tree of $\eta$, and the second of which the parse tree for $\nu$.
    \item If $\phi = \neg \psi$, then the parse tree has a parent node $\phi$, with a single edge descending into the parse tree of $\psi$.
\end{itemize}
%
The \emph{complexity} of a given formula $\phi$ is the height of its parse tree. A \emph{subformula} of a formula $\phi$ is a formula associated to one of the nodes in the parse tree of $\phi$. It is easy to see that they are the only substrings of $\phi$ which are valid formulas in the language. An occurence of a formula $\psi$ in a formula $\phi$ is a node in the parse tree of $s$ whose associated string is $\psi$.

\begin{example}
    The parse tree for $(A \vee B) \wedge \neg (B \wedge C)$ is
    %
    \[
    \Tree [.{$(A \vee B) \wedge \neg (B \wedge C)$} [.{$A \vee B$} {$A$} !\qsetw{1in} {$B$} ] !\qsetw{1in} [.{$\neg (B \wedge C)$} [.{$B \wedge C$} {$B$} !\qsetw{1in} {$C$} ] ] ]
    \]
    %
    Thus the formula has complexity $3$, for the longest branch in the parse tree has length three. The subformulas of the string are simply the nodes in the tree. The tree also tells us that there are two occurences of $B$ in the formula, whereas only one occurence of $(B \wedge C)$.
\end{example}

Before we finish with the study of the syntax of propositional logic, it is interesting to discuss a less natural, but syntactically simpler method of forming sentences, called \emph{polish notation}, after its inventor, the polish logician Jan \L ukasiewicz. Rather than writing connectives in \emph{infix notation}, like $(u \wedge v)$ and $(u \Rightarrow v)$, we use \emph{prefix notation}, writing these sentences as $\wedge u v$ and $\Rightarrow u v$. Surprisingly, we do not need brackets to parse statements anymore. As a temporary notation, say two strings $s$ and $w$ are \emph{comparable} if one is the prefix of the other.

\begin{lemma}
    If $\phi_1, \dots, \phi_n, \psi_1, \dots, \psi_m$ are formulas in polish notation, and $\phi_1 \dots \phi_n$ is comparable to $\psi_1 \dots \psi_m$, then $n = m$, and $\phi_i = \psi_i$ for each $i$.
\end{lemma}
\begin{proof}
    We prove by induction on the number of characters in $\phi_1 \dots \phi_n$. If $\phi_1 = \neg \eta$, then the first character of $\psi_1$ must be nonempty, and equal to $\neg \nu$ for some formula $\nu$. It then follows that $\eta \phi_2 \dots \phi_n$ is comparable to $\nu \psi_1 \dots \psi_m$, so $n = m$, and $\eta = \nu$, $\phi_i = \psi_i$, and it follows that $\phi_1 = \psi_1$ as well. Similarily, if $\phi_1 = \circ \eta_1 \eta_2$, then $\psi_2 = \circ \nu_1 \nu_2$, and by removing $\circ$, we find $\eta_1 = \nu_1$, $\eta_2 = \nu_2$, and $\phi_i = \psi_i$, and then $\phi_1 = \psi_1$ is easy to see as well.
\end{proof}

\begin{theorem}
    Every character in a formula in polish notation begins a unique subformula in polish notation.
\end{theorem}
\begin{proof}
    We prove by structural induction. If $\phi = X$ is a single sentential variable, the theorem is trivial. If $\phi = \neg \psi$, then $\neg$ begins a unique subformula, because if $\neg \eta$ and $\neg \nu$ are subformulas of $\phi$, then $\neg \eta$ and $\neg \nu$ are comparable, hence $\eta = \nu$. By induction, every character in $\psi$ begins a unique subformula. If $\phi = \circ \eta_1 \eta_2$, then we know that $\circ$ begins a unique subformula (using the last lemma), and there is no way to find a subformula $\psi$ which begins in $\eta_1$ and ends in $\eta_2$, hence we may apply induction to show every other character begins a unique subformula.
\end{proof}

This theorem says that polish notation is an especially dense language with which to discuss compositions of terms in sentential logic -- it is the most efficient formulation provided the subterms of sentential logic are properly represented in the calculus. It is easy to check that these properties continue to hold when we add a more complicated syntax, with functions $f(t_1, \dots, t_n)$ being denoted as $ft_1 \dots t_n$, predicates $P(t_1, \dots, t_n)$ as $Pt_1 \dots t_n$, and quantifiers denoted $\exists x \phi$ and $\forall x \phi$.

\section{Semantics}

We can understand the discussion in the last section without any understanding of what symbols mean. Now we want to interpret the symbols, giving the symbols meaning. A basic semantic method is to define whether a statement is `true'. Define a \emph{truth assignment} on a set $\Lambda$ to be a map $f: \Lambda \to \{ \top, \bot \}$, where $\top$ and $\bot$ are two arbitrarily chosen representatives of truth and falsity. The notation suggests that we will be taking truth assignments on the set of propositional variables in the propositional language $\text{SL}(\Lambda)$, and this will define the semantics of propositional logic.

\begin{example}
    A \emph{Boolean function} is an assignment over the domain $\Lambda = \{ \top, \bot \}^n$. It is common to define such functions by truth tables. For a given Boolean function, we form a table with $n + 1$ columns, and $2^{n}$ rows. In each row, we fill out a particular element of $\{ \top, \bot \}^n$, and in the last column, the value of image of the truth assignment under $f$. One may combine multiple $n$-ary truth functions into the same table for brevity. As an example, we define the Boolean functions $H_\wedge, H_\vee, H_\Rightarrow$, $H_{\Leftrightarrow}$, and $H_\neg$.
    %
    \begin{center}
    \begin{tabular}{| c | c | c | c | c | c | c |}
        \hline $x$ & $y$ & $H_\wedge(x,y)$ & $H_\vee(x,y)$ & $H_\Rightarrow(x,y)$ & $H_\Leftrightarrow(x,y)$ & $H_\neg(x)$ \\
        \hline $\bot$ & $\bot$ & $\bot$ & $\bot$ & $\top$ & $\top$ & $\top$ \\
        $\bot$ & $\top$ & $\bot$ & $\top$ & $\top$ & $\bot$ & $\top$ \\
        $\top$ & $\bot$ & $\bot$ & $\top$ & $\bot$ & $\bot$ & $\bot$ \\
        $\top$ & $\top$ & $\top$ & $\top$ & $\top$ & $\top$ & $\bot$ \\
        \hline
    \end{tabular}
    \end{center}
    %
    These functions express the semantic interpretation of the corresponding operators in propositional logic.
\end{example}

We may extend truth assignments $f: \Lambda \to \{ \top, \bot \}$ naturally to an assignment $f_*: \text{SL}(\Lambda) \to \{ \top, \bot \}$. This is analogous to how homomorphisms between two rings $R$ and $S$ naturally extend to homomorphisms between the polynomial rings $R[X]$ and $S[X]$. Here we construct the assignment recursively. Let $f: \Lambda \to \{ \top, \bot \}$ be an arbitrary truth assignment. Define
%
\[ f_*(\neg \phi) = H_\neg(f_*(\phi))\ \ \ \ \ \ \ \ \ \ f_*(\phi \circ \psi) = H_\circ(f_*(\phi), f_*(\psi)) \]
%
Because of our study of syntax, it is easy to see that $f_*$ is a well defined function on all terms of $\text{SL}(\Lambda)$. We say an arbitrary term $\phi \in \text{SL}(\Lambda)$ is a \emph{tautology}, if, for any truth assignment $f$ on $\Lambda$, $f_*(\phi) = \top$. $\phi$ is a \emph{contradiction} if $f_*(\phi) = \bot$ for any truth assignment $f$. A statement which is neither a tautology nor a contradiction is known as a \emph{contingent term}. We summarize the statement ``$\phi$ is a tautology'' by $\vDash \phi$. We say a family of term $\phi_1, \dots, \phi_n$ \emph{semantically implies} $\psi$, written $\phi_1, \dots, \phi_n \vDash \psi$, if $f_*(\psi) = \top$ whenever $f_*(\phi_1) = \dots = f_*(\phi_n) = \top$.

Suppose that we wish to verify whether $\phi \in \text{SL}(\Lambda)$ is a tautology. Let $x_1, \dots, x_n \in \Lambda$ be all the variables which occur in $s$. Define a boolean function $g: \{ 0, 1 \}^n \to \{ 0, 1 \}$, with
%
\[ g(y_1 \dots, y_n) = f^{(y_1, \dots, y_n)}_*(\phi) \]
%
where $f^{(y_1, \dots, y_n)}$ is a truth assignment formed by mapping $x_i$ to $y_i$. This is well defined, because if $h$ and $k$ are two truth assignments which agree on the $x_i$, then they agree at $\phi$. If $f$ is an arbitrary truth assignment, then
%
\[ g(f_*(x_1), \dots, f_*(x_n)) = f_*(\phi) \]
%
which can be seen from an easy structural induction. Thus $\phi$ is a tautology if and only if $g(y_1, \dots, y_n) = \top$ for all choices of $y_i$. Therefore one need only construct the truth table of $g$ to confirm whether $s$ is a tautology or not. To prevent errors, it is best to construct a truth table containing all subformulas of $\phi$, so that one can verify that calculations are consistant with other calculations. This may be done side by side, in the same table.

\begin{example}
    For any variable $X \in \Lambda$, $X \vee \neg X$ is a tautology, $X \wedge \neg X$ is a contradiction, and $\neg X$ is contingent, which is verified by the truth table
    %
    \begin{center}
    \begin{tabular}{| c | c | c | c | c | c | c |}
        \hline $X$ & $X \vee \neg X$ & $X \wedge \neg X$ & $\neg X$ \\
        \hline $\top$ & $\top$ & $\bot$ & $\bot$ \\
               $\bot$ & $\top$ & $\bot$ & $\top$ \\
        \hline
    \end{tabular}
    \end{center}
    %
    The first formula is an instance of the \emph{law of excluded middle}.
\end{example}

\begin{example}
    Let $\phi \vDash \psi$ be a tautology, and suppose that $\phi$ and $\psi$ have no variables in common. Then we can conclude that $\phi$ is a contradiction, or $\psi$ is a tautology, for if there is a truth assignment $f$ with $f_*(\phi) = \top$, and a truth assignment $g$ on $\psi$ with $g_*(\psi) = \bot$, then we may combine the truth assignments to create a truth assignment $h$ in which $h_*(\phi) = \top$ and $h_*(\psi) = \bot$, and then $h_*(\phi \Rightarrow \psi) = \bot$.
\end{example}

Because of the truth table construction, there is an algorithm for determining if any given term $\phi \in \text{SL}(\Lambda)$ is a tautology. However, there is a catch: if a term $\phi$ contains $n$ variables, the algorithm we have constructed will take $\Omega(2^n)$ steps to compute whether $\phi$ is a tautology. Determining whether a term with 150 variables is a tautology will take more steps than atoms in the observable universe! The $\mathbf{P} \neq \mathbf{NP}$ conjecture states that there is no algorithm which takes $O(n^K)$ determining if a statement of $\text{SL}(\Lambda)$ is a tautology for any integer $K$, so that there is a fundamental limit to the efficiency of determining whether a term is a tautology. There are more efficient methods for determining whether certain subfamilies of terms of propositional logic are tautologies, but it is doubtful whether we can find an efficient algorithm to determine if an arbitrary formula is satisfiable.

\begin{theorem}[Semantic Modus Ponens]
    If $\vDash \phi$ and $\phi \vDash \psi$, then $\vDash \psi$.
\end{theorem}
\begin{proof}
    Let $f$ be a truth assignment. Then $f_*(\phi) = \top$ and
    %
    \[ f_*(\phi \Rightarrow \psi) = H_\Rightarrow(f_*(\phi),f_*(\psi)) = H_\Rightarrow(\top, f_*(\psi)) = \top \]
    %
    This holds only when $f_*(\psi) = \top$.
\end{proof}

The next theorem relies on a useful string manipulation technique which shall later prove a useful formalism. If $\phi \in \Lambda^*$, $X = (X_1, \dots, X_n)$ are distinct letters of $\Lambda$, and $\psi = (\psi_1, \dots, \psi_n) \in \Lambda^*$, then we shall let
%
\[ \phi[\psi_1/X_1, \dots, \psi_n/X_n] = \phi[\psi/X] \]
%
be the \emph{substitution} of $\phi$, denoting the string in $\Lambda^*$ obtained from swapping all occurences of $X_i$ with $\psi_i$.

\begin{theorem}
    If $\vDash \phi$, then $\vDash \phi[\psi/X]$
\end{theorem}
\begin{proof}
    Consider a truth assignment $f$. We shall define another truth assignment $\tilde{f}$ such that $f(\phi[\psi/X]) = \tilde{f}(\phi)$ for all $v$. Define $\tilde{f}(X_i) = f_*(\psi_i)$, and if $y \not \in x$, define $\tilde{f}(y) = f_*(y)$. Our base case, where $\phi$ is a variable, satisfies the claim by construction. Then, by induction, if $\psi = (\eta \circ \nu)$, then
    %
    \[ \tilde{f}_*(\psi) = H_\circ(\tilde{f}_*(\eta), \tilde{f}_*(\nu)) = H_\circ(f_*(\eta[\psi/X]), f_*(\eta[\psi/X])) = f_*(\phi[\psi/X]) \]
    %
    A similar proof answers the case where $\phi = (\neg \eta)$. Now since $\phi$ is a tautology, we conclude that $f_*(\phi[\psi/X]) = \tilde{f}_*(\phi) = \top$, so $\phi[\psi/X]$ is a tautology.
\end{proof}

\begin{corollary}
    If $\eta_1, \dots, \eta_n \vDash \nu$, then $\eta_1[\psi/X], \dots, \eta_n[\psi/X] \vDash \nu[\psi/X]$.
\end{corollary}

\section{Truth Functional Completeness}

We hope that propositional logic can model all notions of truth, such that all truth functions can be formed from our original set. Here we argue why our logic can model all such notions, provided we have infinitely many propositional variables. Let $X$ be a set of boolean functions. The \emph{clone} of $X$ is the smallest set containing $X$ and all projections $\pi_k : \{ 0, 1 \}^n \to \{ 0, 1 \}$ onto the kth coordinate, and in addition, if $g: \{ 0, 1 \}^n \to \{ 0, 1 \}$ and $f_1, \dots, f_n : \{ 0, 1 \}^m \to \{ 0, 1 \}$ are in the clone, then so is $g(f_1, \dots, f_n): \{ 0, 1 \}^m \to \{ 0, 1 \}$. $\Lambda$ is \emph{truth functionally complete} if its clone is the set of all boolean functions.

\begin{example}
    $\{ H_\neg, H_\wedge, H_\vee \}$ is a truth functionally complete, since every formula can be put in \emph{conjunctive normal form}. Given an arbitrary $f: \{ \top, \bot \}^n \to \{ \top, \bot \}$, we write
    %
    \[ f(x_1, \dots, x_n) = \bigvee_{\substack{(y_1, \dots, y_n) \in \{ 0, 1 \}^n\\f(y_1, \dots, y_n) = \top}}\ \  \bigwedge_{i = 1}^n H_\Leftrightarrow(x_i, y_i) \]
    %
    where we define $\bigcirc_{i = 1}^n f_i = H_\circ(f_n, \bigcirc_{i = 1}^{n-1} f_i)$ for any connective $\circ$. Since
    %
    \[ H_\Leftrightarrow(x,y) = H_\wedge(H_\Rightarrow(x,y), H_\Rightarrow(y,x)) = H_\wedge(H_\vee(H_\neg(x), y), H_\vee(H_\neg(y), x)) \]
    %
    we find that $\{ H_\neg, H_\wedge, H_\vee \}$ is truth functionally complete. These connectives can be further reduced, by noticing that
    %
    \[ H_\wedge(x,y) = H_\neg(H_\vee(H_\neg(x), H_\neg(y))) \]
    %
    a truth functional form of Boole's inequality, implying $\{ H_\neg, H_\vee \}$ is truth functionally complete. We can also consider a \emph{disjunctive normal form} of any Boolean function, which we leave to the reader to formulate.
\end{example}

Say a formula $\phi$ is \emph{satisfiable}, if there is some truth assignment $f$ such that $f_*(\phi) = \top$. It is a standard computational problem to verify whether such a formula is satisfiable, as a great many problems can be reduced to satisfiability. For instance, say one wishes to verify whether a graph $(V,E)$ is $m$ colorable (that is, there is a function $f: V \to \{ 1, \dots, m \}$ such that if $(v,w) \in E$, $f(v) \neq f(w)$). For each vertex $v \in V$ and color $i \in \{ 1, \dots, m \}$, let $v_i$ be a variable, which we interpret to be true if $v$ is coloured with color $i$. A graph is colorable if and only if the statement
%
\[ \bigwedge_{v \in V} \left( \bigvee_{i = 1}^m v_i \right) \wedge \bigwedge_{(v,w) \in E} \left( \bigwedge_{i = 1}^m (\neg v_i \vee \neg w_i) \right) \wedge \bigwedge_{v \in V} \left( \bigwedge_{i,j = 1}^m (v_i \vee \neg v_j) \right) \]
%
is satisfiable. The first big clause says that some color is assigned to each vertex, the last that the color is unique. The middle clause is the coloring constraint. If a formula is in disjunctive normal form, it is algorithmically easy to verify whether the formula is satisfiable. We just need to check whether one of the disjunctive clauses is consistant, which can be done in a time proportional to the size of the formula. Checking whether a formula is in conjunctive normal form is much more difficult -- in fact, in computability theory one discovers that almost all interesting problems can be reduced to a satisfiability problem without decreasing the efficiency of the task, so if it was easy to determine if a CNF is solvable, then we could solve a great many seemingly difficult problems in an easy manner. The $\mathbf{P} = \mathbf{NP}$ conjecture implies that there is no polynomial time computable way of turning conjunctive normal forms to equivalent disjunctive normal forms. The standard conversions which exist increase the size of the formula exponentially.

\begin{example}
    The mathematician Henry M. Sheffer found a single truth function which is truth functionally complete. Consider the \emph{Sheffer stroke} $x|y$, also known as \emph{NAND}, defined by the truth table
    %
    \begin{center}
    \begin{tabular}{| c | c | c |}
        \hline $x$ & $y$ & $H_|(x,y)$\\
        \hline $\bot$ & $\bot$ & $\top$\\
        $\bot$ & $\top$ & $\top$\\
        $\top$ & $\bot$ & $\top$\\
        $\top$ & $\top$ & $\bot$\\
        \hline
    \end{tabular}
    \end{center}
    %
    Then $H_\neg(x) = H_|(x,x)$, and $H_\vee(x,y) = H_|(H_\neg(x), H_\neg(y))$, which implies, since this set is truth functionally complete, that the sheffer stroke is truth functionally complete.
\end{example}

The previous example is incredibly important to circuit design. Logical statements can be represented by boolean functions. Since all truth functions can be built from the sheffer stroke, we need only make an atomic circuit for the sheffer stroke, and then all other circuits are constructed by combining sheffer strokes together.




\section{Deduction}

When mathematicians want to derive whether a statement is true, they do not construct truth functions and take a truth table for the function. This would be computationally infeasible, and would not aid in understanding {\it why} the statement is true. Instead, they provide a proof of the result. Here we shall provide the mechanics for modelling a mathematical argument. We will show that the method of truth tables and arguments are equivalent -- a statement is a tautology if and only if it can be proved. This is known as a {\it completeness result}, for it says that our semantic understanding of a theory is the same as our deductive understanding.

First, we thin out the connectives in our theory. Since $\Rightarrow$ and $\neg$ are truth functionally complete, we can consider a system consisting only of these connectives, and reinterpret other formulas as semantically equivalent formulas in the reduced theory. Next, we define the \emph{theorems} of $\text{SL}(\Lambda)$, which are elements of the smallest set of terms such that
    %
    \begin{enumerate}
        \item Any axiom is a theorem, which are statements of the form
        %
        \begin{align*}
            &(A1) & \phi \Rightarrow (\psi \Rightarrow \phi)\\
            &(A2) & (\phi \Rightarrow (\psi \Rightarrow \eta)) \Rightarrow ((\phi \Rightarrow \psi) \Rightarrow (\phi \Rightarrow \eta))\\
            &(A3) & (\neg \phi \Rightarrow \neg \psi) \Rightarrow ((\neg \phi \Rightarrow \psi) \Rightarrow \phi)
        \end{align*}
        \item Modus Ponens holds in our system. If $\phi \Rightarrow \psi$ and $\phi$ are theorems, then $\psi$ is a theorem. When we apply modus ponens, we may write that the theorem was obtained by (MP).
    \end{enumerate}
    %
We shall write $\vdash \phi$ to state that $\phi$ is a theorem.

For the terms of $\text{SL}(\Lambda)$, the `smallness' characterization of the terms of $\text{SL}(\Lambda)$ gives us the method of structural induction. For theorems, the smallness criterion gives us a abstract notion of a `proof'. A statement $\phi$ is a theorem of $\text{SL}(\Lambda)$ if and only if there is a sequence of formulae $(\psi_1, \dots, \psi_n)$ such that $\psi_n = \phi$, and each $\psi_i$ is either an axiom, or is obtained from some $\phi_j$ and $\phi_k$ by modus ponens, where $j,k < i$. This sequence is known as a \emph{proof}. Often, we list a proof from top to bottom, where we reference how we obtained each element of the sequence alongside the proof. A proof is a certificate guaranteeing that a statement is a theorem of $\text{SL}(\Lambda)$, and finding a proof is the only constructive way to guarantee that a statement is a theorem.

\begin{example}
    Let us construct a proof of $\vdash \phi \Rightarrow \phi$, for any $\phi \in \text{SL}(\Lambda)$.
    %
    \[ \fitchprf{\pline{\phi \Rightarrow \phi}}{
        \pline[1.]{(\phi \Rightarrow ((\phi \Rightarrow \phi) \Rightarrow \phi))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A1)]\\
        \pline[2.]{(\phi \Rightarrow ((\phi \Rightarrow \phi) \Rightarrow \phi)) \Rightarrow ((\phi \Rightarrow (\phi \Rightarrow \phi)) \Rightarrow (\phi \Rightarrow \phi))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A2)]\\
        \pline[3.]{((\phi \Rightarrow (\phi \Rightarrow \phi)) \Rightarrow (\phi \Rightarrow \phi))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1),(2),(MP)]\\
        \pline[4.]{(\phi \Rightarrow (\phi \Rightarrow \phi))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A1)]\\
        \pline[5.]{\phi \Rightarrow \phi}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3),(4),(MP)]
    } \]
    %
    In future proofs, we shall be able to use $\vdash \phi \Rightarrow \phi$ implicitly, since we now know the statement can be proved in any of its forms. We will denote its application by $(I)$.
\end{example}

In mathematics, we often work in logical systems where additional axioms are introduced to the system. In group theory, we assume that operations are associative. In geometry, we assume there is a line between any two points. To perform mathematics, we add additional axioms to logic, and prove results from these axioms. If $\Gamma$ is a subset of $\text{SL}(\Lambda)$, then we may consider each member of $\Gamma$ to be an axiom. We write $\Gamma \vdash \phi$ if one may prove $\phi$ assuming all formulae in $\Gamma$ have already been proved. That is, we may write a sequence $(\psi_1, \dots, \psi_n)$, where $\psi_n = \phi$, and each $\psi_i$ is either an axiom, an element of $\Gamma$, or is obtained by modus ponens from previous elements of the sequence.

\begin{theorem}[Deduction Theorem]
    If $\Gamma \cup \{ \phi \} \vdash \psi$, then $\Gamma \vdash \phi \Rightarrow \psi$.
\end{theorem}
\begin{proof}
    We prove the theorem by induction of the size of the proof of $\psi$. Consider a particular proof $(\eta_1, \dots, \eta_n)$ of $\psi$ from $\Gamma \cup \{ \phi \}$. Suppose that $n = 1$. Then $\eta_1 = \psi$, and $\eta$ must either be an axiom, an element of $\Gamma$, or equal to $\psi$. In the first and second case, the proof is equally valid in $\Gamma$, and so $\Gamma \vdash \phi \Rightarrow \psi$ follows from the axiom $(\psi \Rightarrow (\phi \Rightarrow \psi))$. If $\phi = \psi$, Then we have shown that $\vdash \phi \Rightarrow \psi$ (this is the identity rule we just proved), so obviously $\Gamma \vdash \phi \Rightarrow \psi$. Now we consider the problem proved for $m < n$. $\eta_n = \psi$ is either an axiom, an element of $\Gamma$, equal to $\phi$, or proved by modus ponens from $\eta_i = (\eta_j \Rightarrow \psi)$, where $j < i$. We have already justified the constructions of all cases but the last. By induction, $\Gamma \vdash \phi \Rightarrow (\eta_j \Rightarrow \psi)$ and $\Gamma \vdash \phi \Rightarrow \eta_j$. But $(\phi \Rightarrow (\eta_j \Rightarrow \psi)) \Rightarrow ((\phi \Rightarrow \eta_j) \Rightarrow (\phi \Rightarrow \psi))$ is an axiom, so $\Gamma \vdash \phi \Rightarrow \psi$.
\end{proof}

The deduction theorem is constructive, in the sense that it gives an algorithm to compute any proof of $\Gamma \cup \{ \phi \} \vdash \psi$ to $\Gamma \vdash \phi \Rightarrow \psi$. What's more, if the proof has length $n$, then the new proof has length $O(n)$, so the deduction theorem is a polynomial time computable reduction.

\begin{example}
    For any statements $\phi$ and $\psi$, $\{ \phi \Rightarrow \psi, \psi \Rightarrow \eta \} \vdash \phi \Rightarrow \eta$. This follows from a basic application of (A2). But this implies the two cut rules, that
    %
    \begin{gather*}
        \vdash (\phi \Rightarrow \psi) \Rightarrow ((\psi \Rightarrow \eta) \Rightarrow (\phi \Rightarrow \eta))\\
        \vdash (\psi \Rightarrow \eta) \Rightarrow ((\phi \Rightarrow \psi) \Rightarrow (\phi \Rightarrow \eta))
    \end{gather*}
    %
    these statements are a little bit more tricky to prove without the deduction theorem, though technically the proof of the deduction theorem gives a constructive way to obtain a proof with no assumptions. We denote an application of these rules as (CUT).
\end{example}

\begin{example}
    Let us prove the double negation elimination axiom, $\vdash \neg \neg \phi \Rightarrow \phi$ by the deduction theorem.
    %
    \[
    \fitchprf{\pline{\neg \neg \phi \Rightarrow \phi}}{
        \subproof{\pline[1.]{\neg \neg \phi}} {
            \pline[2.]{(\neg \phi \Rightarrow \neg \neg \phi) \Rightarrow ((\neg \phi \Rightarrow \neg \phi) \Rightarrow \phi)}[(A3)]\\
            \pline[3.]{(\neg \neg \phi) \Rightarrow (\neg \phi \Rightarrow \neg \neg \phi)}[(A1)]\\
            \pline[4.]{(\neg \phi \Rightarrow \neg \neg \phi)}[(1),(3),(MP)]\\
            \pline[5.]{(\neg \phi \Rightarrow \neg \phi) \Rightarrow \phi}[(2),(4),(MP)]\\
            \pline[6.]{\neg \phi \Rightarrow \neg \phi}[(I)]\\
            \pline[7.]{\phi}[(5),(6),(MP)]
        }
        \pline[8.]{\neg \neg \phi \Rightarrow \phi}[(1-7),(DT)]
    }
    \]
    %
    In future proofs, application of the statement will be denoted $(\neg \neg E)$. Now lets prove negation introduction, $\vdash \phi \Rightarrow \neg \neg \phi$.
    %
    \[
    \fitchprf{\pline{\phi \Rightarrow \neg \neg \phi}}{
        \pline[1.]{\neg \neg \neg \phi \Rightarrow \neg \phi}[\ \ \ \ \ \ \ $(\neg \neg E)$]\\
        \subproof{\pline[2.]{\phi}} {
            \pline[3.]{(\neg \neg \neg \phi \Rightarrow \neg \phi) \Rightarrow ((\neg \neg \neg \phi \Rightarrow \phi) \Rightarrow \neg \neg \phi)}[\ \ \ \ \ \ \ (A3)]\\
            \pline[4.]{(\neg \neg \neg \phi \Rightarrow \phi) \Rightarrow \neg \neg \phi}[\ \ \ \ \ \ \ (1), (3), (MP)]\\
            \pline[5.]{\phi \Rightarrow (\neg \neg \neg \phi \Rightarrow \phi)}[\ \ \ \ \ \ \ (A1)]\\
            \pline[6.]{\neg \neg \neg \phi \Rightarrow \phi}[\ \ \ \ \ \ \ (2),(5),(MP)]\\
            \pline[7.]{\neg \neg \phi}[\ \ \ \ \ \ \ (4),(6),(MP)]
        }
        \pline[8.]{\phi \Rightarrow \neg \neg \phi}[\ \ \ \ \ \ \ (2-7), (DT)]
    }
    \]
    %
    We shall denote this rule $(\neg \neg I)$.
\end{example}

\begin{example}
    Lets prove $\neg \phi \vdash \phi \Rightarrow \psi$, by proving $\neg \phi, \phi \vdash \psi$.
    %
    \[
    \fitchprf{\pline{\neg \phi \Rightarrow (\phi \Rightarrow \psi)}}{
        \subproof{\pline[1.]{\neg \phi}} {
            \subproof{\pline[2.]{\phi}} {
                \pline[3.]{(\neg \psi \Rightarrow \neg \phi) \Rightarrow ((\neg \psi \Rightarrow \phi) \Rightarrow \psi)}[(A3)]\\
                \pline[4.]{\neg \phi \Rightarrow (\neg \psi \Rightarrow \neg \phi)}[(A1)]\\
                \pline[5.]{\neg \psi \Rightarrow \neg \phi}[(1),(4), (MP)]\\
                \pline[6.]{(\neg \psi \Rightarrow \phi) \Rightarrow \psi}[(3),(5),(MP)]\\
                \pline[7.]{\phi \Rightarrow (\neg \psi \Rightarrow \phi)}[(A1)]\\
                \pline[8.]{\neg \psi \Rightarrow \phi}[(2),(7),(MP)]\\
                \pline[9.]{\psi}[(6),(8),(MP)]
            }
            \pline[10.]{\phi \Rightarrow \psi}[(2-9),(DT)]
        }
        \pline[11.]{\neg \phi \Rightarrow (\phi \Rightarrow \psi)}[(1-10), (DT)]
    }
    \]
    %
    This is a proof of \emph{the law of contradiction}, denoted $(LC)$.
\end{example}

\begin{example}
    Consider the following proof.
    %
    \[
    \fitchprf{\pline{(\phi \Rightarrow \psi) \Rightarrow (\neg \psi \Rightarrow \neg \phi)}}{
        \subproof{\pline[1.]{\phi \Rightarrow \psi}} {
            \subproof{\pline[2.]{\neg \psi}}{
                \pline[3.]{\neg \psi \Rightarrow (\neg \neg \phi \Rightarrow \neg \psi)}[(A1)]\\
                \pline[4.]{\neg \neg \phi \Rightarrow \neg \psi}[(2),(3),(MP)]\\
                \pline[5.]{(\neg \neg \phi \Rightarrow \neg \psi) \Rightarrow ((\neg \neg \phi \Rightarrow \psi) \Rightarrow \neg \phi)}[(A3)]\\
                \pline[6.]{(\neg \neg \phi \Rightarrow \psi) \Rightarrow \neg \phi}[(4),(5),(MP)]\\
                \subproof{\pline[7.]{\neg \neg \phi}}{
                    \pline[8.]{\neg \neg \phi \Rightarrow \phi}[$(\neg \neg E)$]\\
                    \pline[9.]{\phi}[(7),(8),(MP)]\\
                    \pline[10.]{\psi}[(1),(9),(MP)]
                }
                \pline[11.]{\neg \neg \phi \Rightarrow \psi}[(7-10), (DT)]\\
                \pline[12.]{\neg \phi}[(6),(11), (MP)]
            }
            \pline[13.]{\neg \psi \Rightarrow \neg \phi}[(2),(12),(MP)]
        }   
        \pline[11.]{(\phi \Rightarrow \psi) \Rightarrow (\neg \psi \Rightarrow \neg \phi)}[(1-10), (DT)]
    }
    \]
    %
    This is the \emph{law of contraposition} $(LCP)$.
\end{example}

\begin{example}
    Lets prove $\vdash (\phi \Rightarrow \psi) \Rightarrow ((\neg \phi \Rightarrow \psi) \Rightarrow \psi)$.
    %
    \[
    \fitchprf{\pline{(\phi \Rightarrow \psi) \Rightarrow ((\neg \phi \Rightarrow \psi) \Rightarrow \psi)}}{
        \subproof{\pline[1.]{\phi \Rightarrow \psi}} {
            \pline[2.]{(\phi \Rightarrow \psi) \Rightarrow (\neg \psi \Rightarrow \neg \phi)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (LCP)]\\
            \pline[3.]{\neg \psi \Rightarrow \neg \phi}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1),(2),(MP)]\\
            \subproof{\pline[4.]{\neg \phi \Rightarrow \psi}} {
                \pline[5.]{(\neg \psi \Rightarrow \neg \neg \phi)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4),(LCP)]\\
                \pline[6.]{\neg \neg \phi \Rightarrow \phi}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(\neg \neg E)$]\\
                \pline[7.]{(\neg \psi \Rightarrow \neg \neg \phi) \Rightarrow ((\neg \neg \phi \Rightarrow \phi) \Rightarrow (\neg \psi \Rightarrow \phi))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (\text{CUT})]\\
                \pline[8.]{(\neg \neg \phi \Rightarrow \phi) \Rightarrow (\neg \psi \Rightarrow \phi)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (5),(7),(MP)]\\
                \pline[9.]{\neg \psi \Rightarrow \phi}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (6),(8),(MP)]\\
                \pline[10.]{(\neg \psi \Rightarrow \neg \phi) \Rightarrow ((\neg \psi \Rightarrow \phi) \Rightarrow \psi)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A3)]\\
                \pline[11.]{(\neg \psi \Rightarrow \phi) \Rightarrow \psi}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(3),(10),(MP)$]\\
                \pline[12.]{\psi}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (9), (11), (MP)]
            }
            \pline[13.]{(\neg \phi \Rightarrow \psi) \Rightarrow \psi}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(2-9),(DT)$]
        }
        \pline[14.]{(\phi \Rightarrow \psi) \Rightarrow ((\neg \phi \Rightarrow \psi) \Rightarrow \psi)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $(1-10), (DT)$]
    }
    \]
    %
    We have essentially argued that $(\phi \vee \neg \phi) \Rightarrow \psi$ implies $\psi$.
\end{example}

Proofs give us a constructive way to verify whether a theorem is true in propositional logic, but it seems much more tricky to show that a theorem {\it cannot} be proved. Fortunately, semantic truth provides a simple invariant to decide if a theorem isn't provable.

\begin{theorem}
    If $\vdash \phi$, then $\vDash \phi$.
\end{theorem}
\begin{proof}
    A trivial structural induction.
\end{proof}

This theorem shows that there are some statements which are not provable in our system, because there are some statements which are not tautologies. In fact, we know that if $\phi$ is provable, then $\neg \phi$ is not provable, for otherwise we could conclude $\phi \wedge \neg \phi$, which is certainly not consistant. We call an axiom system like this \emph{absolutely consistant}. We shall show that all tautologies are provable, which shows the system is \emph{complete}. A complete system is effectively one in which all theorems which were meant to be able to be proved, are able to be proved.

\begin{lemma}
    Let $X_1, \dots, X_n \in \Lambda$ be variables in some term $\phi \in \text{SL}(\Lambda)$. Let $f$ be a truth assignment, and define $\psi' = \psi$ if $f_*(\psi) = \top$, or $\psi' = \neg \psi$ if $f_*(\psi) = \bot$. Then $X_1', \dots, X_n' \vdash \phi'$.
\end{lemma}
\begin{proof}
    We prove by structural induction. If $\phi = X_1$, then $X_1' = \phi'$, and $\phi' \vdash \phi'$ is a trivial theorem of sentential logic. If $\phi = \neg \psi$, we consider two cases. If $\psi' = \psi$, then $\phi' = \neg \neg \psi$, and by induction, $X_1', \dots, X_n' \vdash \psi$, and by using the theorem $\psi \vdash \neg \neg \psi$, we conclude $X_1', \dots, X_n' \vdash \phi'$. If $\psi' = \neg \psi$, then $\phi' = \phi$, and the theorem is trivial. If $\phi = \eta \Rightarrow \nu$, then either $f_*(\eta) = \top$ and $f_*(\nu) = \top$, or $f_*(\eta) = \bot$. In the first case, we have $X_1', \dots, X_n' \vdash \nu$, from which $X_1', \dots, X_n' \vdash \eta \Rightarrow \nu$ follows by the theorem $\vdash \nu \vdash (\eta \Rightarrow \nu)$. In the second case, $X_1', \dots, X_n' \vdash \neg \eta$, and we can then use $\vdash \neg \eta \vdash (\eta \Rightarrow \nu)$.
\end{proof}

\begin{corollary}[Completeness Theorem]
    If $\vDash s$, $\vdash s$.
\end{corollary}
\begin{proof}
    Let $X_1, \dots, X_n$ be the variables in some tautology $\phi$. By the last lemma, we find that $X_1, \dots, X_n \vdash \phi$, and also that $X_1, \dots, \neg X_n \vdash \phi$. By the deduction theory, we conclude that $X_1, \dots, X_{n-1} \vdash X_n \Rightarrow \phi$, and $X_1, \dots, X_{n-1} \vdash (\neg X_n) \Rightarrow \phi$. But then, since $\vdash (X_n \Rightarrow \phi) \Rightarrow ((\neg X_n \Rightarrow \phi) \Rightarrow \phi)$, we conclude that $x_1, \dots, x_{n-1} \vdash s$ and by a similar construction, that $X_1, \dots, \neg X_{n-1} \vdash \phi$. By induction, we can eliminate all variables on the left hand side to conclude that $\phi$ is provable.
\end{proof}

Notice that every proof we have given leading to the completeness theorem is constructive -- that is, one could effectively write an algorithm which constructs the items in the proof. This means that propositional logic is {\it decidable}, there is an effective algorithm that takes a statement of propositional logic, and returns a proof of that statement, if such a proof exists. The completeness theorem also shows that the {\it decision} problem of propositional logic is also solvable, determining whether some statement is provable. Given a statement of propositional logic, to determine whether we can prove the statement, we need only test whether the statement is true under all truth interpretations.

Before we finish our discussion of semantics, we note that there are many other axioms systems which can be used to define a propositional calculus (in the sense that they prove all tautologies). Most interesting is the axiom system whose only connective is the sheffer stroke, and whose only axiom schema is
%
\[ (B|(C|D))|((E|(E|E))|((F|C)|((B|F)|(B|F)))) \]
%
and whose rule of inference is to infer $D$ from $B|(C|D)$, and $B$. Of course, it is outlandish to attempt proofs in such a system, hence why we did not attempt this chapter using the system, but it is certainly interesting that such a system exists.

\section{Verifying Propositional Formulas}

Because of the completeness theorem, verifying that a particular formula of propositional logic is provable is reduced to an algorithm. If a formula $\phi$ contains variables $X_1, \dots, X_n$, we simply have to consider all truth assignments $f: \{ X_1, \dots, X_n \} \to \{ \top, \bot \}$, and then check that $f_*(\phi) = \top$. The completeness theorem tells us a formula is provable if and only if it is true under all truth assignments, so this suffices to decide whether the formula is provable. For any formula $\phi$ of length $n$, there can be at most $n$ variables in $\phi$, so the algorithm runs in time bounded by $O(n2^n)$. This is a tight bound, because there are formulas $\phi$ of arbitrarily large length $n$ containing $O(n)$ variables. Clearly, such a runtime is undesirable, but without assuming the famous $\mathbf{P} = \mathbf{NP}$ conjecture, it is doubtful whether we can find an algorithm that is any more efficient. Here we introduce certain techniques, which simplify testing whether a formula is a tautology in certain circumstances.

\begin{theorem}[Craig Interpolation]
    Let $\phi \vDash \psi$, and let the set of variables shared by $\phi$ and $\psi$ be $X_1, \dots, X_n$. Then there is a statement $\eta$, known as the \emph{interpolant}, containing only the variables $X_i$, such that $\phi \vDash \eta$ and $\eta \vDash \psi$.
\end{theorem}
\begin{proof}
    We proceed by induction on the number of variables in $\phi$ which do not occur in $\psi$. If every variable in $\phi$ occurs in $\psi$, let $\eta = \phi$. In the general case, fix some variable $X$ in $\phi$ but not in $\eta$, take a variable $Y$ which occurs in both $\phi$ and $\psi$, and define
    %
    \[ \eta = \phi[(Y \wedge \neg Y)/X] \vee \phi[(Y \vee \neg Y)/X] \]
    %
    If $f_*(\phi) = \top$ and $f(X) = \bot$, then $f_*(\phi[(Y \wedge \neg Y)/X]) = \top$, and if $f(X) = \top$, then $f_*(\phi[(Y \vee \neg Y)/X]) = \top$, so $\phi \vDash \eta$. In addition, $\eta \vDash \psi$. Let $f_*(\eta) = \top$. Then $f_*(\eta[(Y \wedge \neg Y)/X]) = \top$, or $f_*(\eta[(Y \vee \neg Y)/X]) = \top$. In the first case, we modify the truth assignment $f$ so that $f(X) = \bot$ (without changing the values of $\phi$ or $\psi$). Then $f_*(\phi) = \top$, so $f_*(\eta) = \top$. The other case follows by letting $f(X) = \top$. By induction, we can find an interpolant for any statement.
\end{proof}

Now suppose we wish to verify a formula of the form $\phi \Rightarrow \psi$ is satisfiable, where the number of variables of $\phi$ and $\psi$ is few in number. The above theorem provides a constructive way to find a formula $\eta$ containing only the variables that occur in both $\phi$ and $\psi$, such that $\eta \Rightarrow \psi$ holds if and only if $\phi \Rightarrow \psi$ holds, and this vastly simplifies the truth table calculation. However, if $\phi$ is length $n$ with $a$ variables, $\psi$ is length $m$ with $b$ variables, and $\phi$ and $\psi$ share $c$ variables in common, then the $\eta$ constructed above will have length $\Theta(n2^{a-c})$, and the calculation of the truth table of $\eta \Rightarrow \psi$ will take $\Theta((n2^{a-c} + m)2^b)$ time, which is still exponential, but in certain cases can be feasible to calculate. This is especially true if we can simplify $\eta$ to a simpler form by eliminating redundant parts of the interpolant.

\begin{example}
    Consider $\phi = (Y_1 \Rightarrow X) \wedge (X \Rightarrow Y_2)$ and $\psi = (Y_1 \wedge Z) \Rightarrow (Y_2 \wedge Z)$. Then $\phi \vDash \psi$. We find an interpolant by the construction above of the form
    %
    \begin{align*}
        &\eta = [(Y_1 \Rightarrow (Y_1 \vee \neg Y_1)) \wedge ((Y_1 \vee \neg Y_1) \Rightarrow Y_2)]\\
        &\ \ \ \ \ \ \vee [(Y_1 \Rightarrow (Y_1 \wedge \neg Y_1)) \wedge ((Y_1 \wedge \neg Y_1) \Rightarrow Y_2)]
    \end{align*}
    %
    This equation can be simplified to $\neg Y_1 \vee Y_2$.
\end{example}

The second technique to reduce the complexity of verifying an equation is known as \emph{resolution}. We assume the formula we are given is in conjunctive normal form. Let $\phi = \psi_1 \wedge \dots \wedge \psi_n$ be be such a formula of propositional logic. We will view a conjunction of clauses as a set $\phi = \{ \psi_1, \dots, \psi_n \}$, where $\phi$ is true in an interpretation only when each $\psi_i$ is true. Without loss of generality, we may assume that no $\psi_i$ contains $X$ and $\neg X$ in disjunctions, for then the conjunct is vacously satisfied. Suppose $\psi_i$ contains an instance of a statement $X$, and $\psi_j$ contains an instance of a statement $\neg X$. The disjunction obtained from $\psi_i$ and $\psi_j$ by concatenating all disjuncts together, dropping all occurrences of $X$ and $\neg X$, and then removing duplicates, is called the resolution of $\psi_i$ and $\psi_j$ with respect to the variable $X$, and we will denote the resulting clause by $\text{res}_X(s_i,s_j)$. Then $\psi_i, \psi_j \vDash \text{res}_X(\psi_i,\psi_j)$, because semantically, either $X$ is true or $\neg X$ is true, and in the first case some clause of $\psi_j$ other than $\neg X$ must be true, and in the second some clause of $\psi_i$ other than $X$ must be true. Given $\phi$, let the \emph{resolution} $\text{res}(\phi)$ be the smallest set of clauses containing all clauses in $\phi$, and closed under resolution. It is clear that $\phi$ is true if and only if $\text{res}(\phi)$ is true, because we have only added clauses which are logically implied by the other clauses of $\phi$.

\begin{example}
    If
    %
    \[ \phi = \{ A \vee \neg B \vee \neg C, \neg A \vee \neg B \vee D, A \vee C \vee D \} = \{ \alpha, \beta, \kappa \} \]
    %
    then $\text{res}(\phi)$ contains the clauses of $\phi$, and in addition the clauses
    %
    \begin{align*}
        \text{res}_A(\alpha, \beta) &= \neg B \vee \neg C \vee D\\
        \text{res}_A(\beta, \kappa) &= \neg B \vee C \vee D\\
        \text{res}_C(\alpha, \kappa) &= A \vee \neg B \vee D\\
        \text{res}_C(\text{res}_A(\alpha, \beta&), \text{res}_A(\beta, \kappa)) = \neg B \vee D
    \end{align*}
    %
    so the resolution has seven clauses.
\end{example}

If $\text{res}_X(\phi,\psi)$ ever equals the empty clause, for some formulas $\phi$ and $\psi$, then $\phi = X$, $\psi = \neg X$, and $\phi$ and $\psi$ can never be simultaneously satisfied. Thus a formula $\phi$ is unsatisfiable if $\text{res}(\phi)$ contains the empty clause. What is more interesting is that the converse is true, though it's a little tricky to prove.

\begin{lemma}
    Let $\text{res}_X(\phi)$ denote the term obtained by concatenating all possible terms by resolution on the variable $X$, and then removing all clauses which contain an instance of $\phi$ or $\neg \phi$. If $\text{res}_X(\phi)$ is satisfiable, and contains no empty clauses, then $\phi$ is satisfiable.
\end{lemma}
\begin{proof}
    Let $f$ be a truth assignment with $f_*(\text{res}_X(\phi)) = \top$. We then claim that by modifying $f$ to set $f(X) = \top$ or $f(X) = \bot$, we can make $f_*(\phi) = \top$. If $\psi$ is a clause of $\phi$ not containing $X$ or $\neg X$, then $f_*(\psi) = \top$. Conversely, if no $\psi \in \phi$ contains $\neg X$, then we may set $f(X) = \top$, and then all clauses are satisfied. Similarily, if no $\psi$ contains $X$, we set $f(X) = \bot$. Thus we are reduced to the case where some clause of $\phi$ contains $X$, and some other clause contains $\neg X$. Let
    %
    \[ g(A) = \begin{cases} \top & A = C\\f(A) & A \neq C \end{cases}\ \ \ \ \ h(A) = \begin{cases} \bot & A = C\\ f(A) & A \neq C \end{cases} \]
    %
    Suppose that $h_*(\eta) = \bot$, and $g_*(\psi) = \bot$. Then $\eta$ must contain an instance of $X$, and $\psi$ must contain an instance of $\neg X$. We note that $\nu = \text{res}_X(\psi,\eta)$ contains no instances of $X$ or $\neg X$ and is nonempty, hence $f_*(\nu) = g_*(\nu) = h_*(\nu) = \top$, hence there is some term in $\eta$ or $\psi$ already satisfied by the interpretation of $\phi$. This implies that the condition $g_*(\eta) = \bot$ and $h_*(\psi) = \bot$ is impossible, so either $g_*(\eta) = \top$ for all clauses, or $h_*(\psi) = \top$ for all clauses, and this completes the proof.
\end{proof}

\begin{theorem}
    $\phi$ is a contradiction iff $\text{res}(\phi)$ contains an empty clause.
\end{theorem}
\begin{proof}
    We prove by induction on the number of variables in the clauses of $\phi$. If $\phi$ contains only a single variable, then either $\phi = \{ X \}$, $\phi = \{ \neg X \}$, or $\phi = \{ X, \neg X \}$. The first two are satisfiable, and their resolution does not contain the empty clause, and the second is a contradiction, and its resolution contains the empty clause. Now if $\phi$ is a contradiction, then either $\text{res}_X(\phi)$ contains an empty clause, or $\text{res}_X(\phi)$ must be a contradiction. In the second case, we apply induction on the number of atoms to conclude that $\text{res}(\text{res}_X(\phi))$ contains an empty clause, and $\text{res}(\text{res}_X(\phi)) \subset \text{res}(\phi)$.
\end{proof}

Resolution gives us an algorithm to calculate whether any formula of prepositional logic is true. Given such a statement $\phi$, take $\neg \phi$, and convert it to a conjunctive normal form $\psi$. Then $\phi$ is a tautology if and only if $\psi$ is a contradiction, so we just determine if $\text{res}(\psi)$ contains an empty clause, and this tells us if $\phi$ is a tautology.

\section{Sequent Calculi}

The complete formal system we have studied is styled in the sense of a great many formal systems, known as \emph{Hilbert systems}. A Hilbert system just takes axioms, and deductive rules, and then forms proofs as sequences $(\phi_1, \dots, \phi_n)$. But there are a great many styles of formal systems, and this section I shall detail my personal favourite, natural deduction. Most actual proofs in mathematics do not follow a linear style. We instead form a proof by combining prior deductions in a non-linear way to reach the conclusion, the end of the proof. Thus natural deduction does not model a proof as a sequence $(\phi_1, \dots, \phi_n)$, but instead as a tree, whose root node is the conclusion we are attempting to form. The nodes of the tree will not consist of formulas, but instead of sequents, which we have almost already seen, which are pairs of sequences of formulas of the form $\phi_1, \dots, \phi_n \vdash \psi_1, \dots, \psi_m$, which we interpret as proving $(\phi_1 \wedge \dots \wedge \phi_n) \Rightarrow (\psi_1 \vee \dots \vee \psi_m)$. Why the assymmetry? It turns out that this will give us symmetry in proofs, which we shall require later. A manifestation of this symmetry is that if $\phi \Rightarrow \psi$ is true, then both $(\phi \wedge \eta) \Rightarrow \psi$ and $\phi \Rightarrow (\psi \vee \eta)$ is true, so the sequents $\phi, \eta \vdash \psi$ and $\phi \vdash \psi, \eta$ may be derived from the sequent $\phi \vdash \psi$. We have already treated the semantics of propositional logic, so we may just state the axioms and deduction rules. Unlike a hilbert system, our system has far more deduction rules than axioms, which is why this system is more {\it natural} -- we more naturally deal with deduction rules. The only axioms are of the form $\phi \vdash \phi$, and the deduction rules are the edges from which we form our tree,
%
\begin{center}
\begin{prooftree}
\Hypo{ \Gamma, \phi \vdash \Delta }
\Infer1[($\wedge L$)]{ \Gamma, \phi \wedge \psi \vdash \Delta }
\end{prooftree}
\ \ \ \ \ \ \ \ \ \
\begin{prooftree}
\Hypo{ \Gamma \vdash \phi, \Delta }
\Infer1[($\vee R$)]{ \Gamma \vdash \psi \vee \phi, \Delta }
\end{prooftree}
\end{center}

\begin{center}
\begin{prooftree}
\Hypo{ \Gamma, \phi \vdash \Delta }
\Hypo{ \Pi, \psi \vdash \Delta }
\Infer2[($\vee L$)]{ \Gamma, \Pi, \phi \vee \psi \vdash \Delta }
\end{prooftree}
\ \ \ \ \ \ \ \ \ \ 
\begin{prooftree}
\Hypo{ \Gamma \vdash \phi, \Delta }
\Hypo{ \Gamma \vdash \psi, \Pi }
\Infer2[($\wedge R$)]{ \Gamma \vdash \phi \wedge \psi, \Delta, \Pi }
\end{prooftree}
\end{center}

\begin{center}
\begin{prooftree}
\Hypo{ \Gamma \vdash \phi, \Delta }
\Hypo{ \Sigma, \psi \vdash \Pi }
\Infer2[($\Rightarrow L$)]{ \Gamma, \Sigma, \phi \Rightarrow \psi \vdash \Delta, \Pi }
\end{prooftree}
\ \ \ \ \ \ \ \ \ \ 
\begin{prooftree}
\Hypo{ \Gamma, \phi \vdash \psi, \Delta }
\Infer1[($\Rightarrow R$)]{ \Gamma \vdash \phi \Rightarrow \psi, \Delta }
\end{prooftree}
\end{center}

\begin{center}
\begin{prooftree}
\Hypo{ \Gamma \vdash \phi, \Delta }
\Infer1[($\neg L$)]{ \Gamma, \neg \phi \vdash \Delta }
\end{prooftree}
\ \ \ \ \ \ \ \ \ \ 
\begin{prooftree}
\Hypo{ \Gamma, \phi \vdash \Delta }
\Infer1[($\neg R$)]{ \Gamma \vdash \neg \phi, \Delta }
\end{prooftree}
\end{center}
%
We have additional deduction rules, known as structural rules, which help show that sequents are the same, without any real logical content.
%
\begin{center}
\begin{prooftree}
\Hypo{ \Gamma \vdash \Delta }
\Infer1[(P)]{ \Gamma' \vdash \Delta' }
\end{prooftree}
\end{center}
%
in the last rule, we mean that $\Gamma'$ is obtained from $\Gamma$ by permuting the sequence, the same for $\Delta'$, as well as removing or adding duplicates. Thus a proof of a sequent $\Gamma \vdash \Delta$ is a tree, whose root is $\Gamma \vdash \Delta$, whose leaves are axioms of the form $s \vdash s$, and such that each edge is annotated by the appropriate deduction rule, for which the deduction is accurate. It turns out it is fairly simple to form deductions, since we may work backwards in most cases to determine which edges to apply.

\begin{example}
    Consider a proof of Pierce's law, $(\phi \Rightarrow \psi) \Rightarrow \phi) \vdash \phi$. To prove this, we likely need to apply $(\Rightarrow L)$, so we must prove $\vdash (\phi \Rightarrow \psi), \phi$ and $\phi \vdash \phi$. Since sequent calculus is meant to model propositional logic, and we will soon prove this system complete, we {\it know} we can prove $\vdash (\phi \Rightarrow \psi), \phi$, and this is obtained from $(\Rightarrow R)$ from the sequent $\phi \vdash \psi, \phi$, and this is obtained from $(P)$ from the axiom $\phi \vdash \phi$. We obtain the following proof tree.
    %
    \begin{center}
    \begin{prooftree}
        \Hypo{\phi \vdash \phi}
        \Infer1[(P)]{\phi \vdash \psi, \phi}
        \Infer1[($\Rightarrow$\ I)]{\vdash (\phi \Rightarrow \psi), \phi}

        \Hypo{\phi \Rightarrow \phi}
        \Infer2[($\Rightarrow$\ L)]{(\phi \Rightarrow \psi) \Rightarrow \phi) \vdash \phi}
    \end{prooftree}
    \end{center}
    %
    Thus this is a theorem of propositional logic.
\end{example}

There is an important additional rule which we could have added to the sequent calculus, known as the cut rule
%
\begin{center}
\begin{prooftree}
\Hypo{ \Gamma \vdash \Delta, \phi }
\Hypo{ \phi, \Sigma \vdash \Pi }
\Infer2[(CUT)]{ \Gamma, \Sigma \vdash \Delta, \Pi }
\end{prooftree}
\end{center}
%
This is distinctly different from the other rules, for it removes complexity from formulas rather than adds complexity. Nonetheless, we shall prove that we do not need the cut rule -- it can always be removed from proofs. We cannot prove the rule in the Sequent calculi, but from the basic axioms, any proof using the cut rule can be converted into a proof without the cut rule. Define the \emph{degree} of a formula $\phi$ to be the height of the parse tree of $\phi$, define the degree of a sequent to be the sum of the degrees of the nodes in the sequent which are not the root node (so the degree of a variable is 0). The degree of a cut is the sum of the degrees of the two sequents which form the premise of the deduction rule.

%
which applies when $\Sigma$ and $\Delta$ contain some common formula $s$, where $\Sigma^*$ and $\Delta^*$ both have this formula removed. The mix rule is a generalization of the cut rule, so obvious the cut rule can be proved in the mix rule. Conversely, the cut rule implies the mix rule, by a simple induction. The proof is nasty, and can be skipped --

\begin{theorem}
    If a sequent $\Gamma \vdash \Delta$ is provable using the cut rule, then it is provable without the use of the cut rule.
\end{theorem}
\begin{proof}
     The contraction measure of the cut is the number of variables removed by a (P) rule before the cut, and the rank of the cut is the...

    We perform a triple induction, first on the rank, then on the contraction measure, and then on the degree. It is clear the rank of a cut is at least 2. The base case occurs when the degree and contraction measure is zero. This implies the use of the cut rule occurs right after introduction of axioms, and the cut rule is of the form
    %
    \begin{center}
    \begin{prooftree}
    \Hypo{ s \vdash s }
    \Hypo{ s \vdash s }
    \Infer2[(CUT)]{ s \vdash s }
    \end{prooftree}
    \end{center}
    %
    clearly, such a derivation is redundant.


    If the degree of the cut is zero, then we apply a cut rule consisting only of propositional variables
    %
    \begin{center}
\begin{prooftree}
\Hypo{ w \vdash u, s }
\Hypo{ s, t \vdash v }
\Infer2[(CUT)]{ w, t \vdash u, v }
\end{prooftree}
\end{center}
\end{proof}

We first note that the cut rule is {\it not provable} using the axioms of the calculus, but just that it may always be replaced by a more complicated proof.

\begin{example}
    The sequent $s, s \Rightarrow w \vdash w$ is provable
    %
    \begin{center}
    \begin{prooftree}
        \Hypo{s \vdash s}
        \Hypo{w \vdash w}
        \Infer2[($\Rightarrow L$)]{s, s \Rightarrow w \vdash w}
    \end{prooftree}
    \end{center}
    %
    The sequent $\Rightarrow L$ is essentially modus ponens.
\end{example}

We desire to show this system is complete. To do this, we could cheat. Since we already have completeness of a Hilbert system, we just need to form an encoding of the Hilbert system in this system, and an encoding of the sequent calculus in the Hilbert system, such that all axioms are provable. Consider the translation $f$ of the sequent calculus into the Hilbert system, by the map
%
\[ f(``{s_1, \dots, s_n \vdash w_1, \dots, w_m}'') = (s_1 \wedge \dots \wedge s_n) \Rightarrow (w_1 \vee \dots \vee w_m) \]
%
and the translation $g$ of the Hilbert system into the sequent calculus, defined by
%
\[ g(s) = ``{\vdash s}'' \]
%
We define a sequent $s_1, \dots, s_n \vdash w_1, \dots w_m $ to be semantically true, if, under every truth assignment that makes each $s_i$ true, one of the $w_j$ is true. Proving the system is sound is fairly trivial. First, we prove that $f$ and $g$ preserve provability. If $S$ is provable in the sequent calculus, then $f(x)$ is provable in the Hilbert system, and if $s$ is provable in the Hilbert system, then $g(y)$ is provable in the Sequent calculus (we need only prove the axioms and deductions). Second, we prove that $f$ and $g$ preserve semantic completeness, that if $S$ is a semantically true sequent, then so if $f(S)$, and if $\vDash s$, then $g(s)$ is semantically true. This allows us to prove completeness, suppose that $s_1, \dots, s_n \vdash w_1, \dots, w_m$ is semantically true. Then
%
\[ \vDash (s_1 \wedge \dots \wedge s_n) \Rightarrow (w_1 \vee \dots \vee w_m) \]
%
By completeness of the Hilbert system,
%
\[ \vdash (s_1 \wedge \dots \wedge s_n) \Rightarrow (w_1 \vee \dots \vee w_m) \]
%
Hence the sequent
%
\[ \vdash (s_1 \wedge \dots \wedge s_n) \Rightarrow (w_1 \vee \dots \vee w_m) \]
%
is provable in the sequent calculus (note the last formula is completely different from this formula, it is unfortunate the equations coincide). We require that we have already shown that
%
\[ (s \Rightarrow w), s \vdash w \]
%
are provable sequents. This may be combined with the previous sequent, letting $s = (s_1 \wedge \dots \wedge s_n)$, $w = (w_1 \vee \dots \vee w_m)$, and applying the cut rule, we find
%
\[ (s_1 \wedge \dots \wedge s_n) \vdash (w_1 \vee \dots \vee w_m) \]
%
TODO: Finish this section.



\chapter{First Order Logic}

It is logical to conclude that ``Julie is a human'' and ``Laura is a human'' from the general statement that ``All women are human''? In Propositional logic, we are unable to model this deduction. Predicate logic is a formal system modelling these derivations.

\section{Language}

The syntax of predicate logic is less homogenous, for our language must contain nouns, like ``Julie'' and ``Laura'', which are the things we talk about, and separate words we apply to nouns, obtaining truth values. These are known as \emph{terms} and \emph{quantifiers} respectively. Terms should model both definite nouns, such as ``Julie'' and ``Laura'', as well as variables, such as $X$ and $Y$, which can stand for many definite nouns at once, together with relational nouns, such as ``The school $X$ went to'', a statement describing a noun which varies in interpretation based on the value of $X$. Definite nouns are known as \emph{constants}, and relational nouns are known as \emph{functions}. Functions will be separated based on their \emph{arity}, the number of arguments they take. ``$X$'s favourite $Y$''is a `2-ary' function, ``$X$'s birthday'' is a `1-ary' function. Formally, we take a set $\Lambda$ of variables, a set $\Delta$ of constants, and for each $n$, a set $\Psi_n$ of $n$-ary functions. The set of \emph{terms} of first order logic is the smallest set $T(\Lambda, \Delta, \Psi)$ such that $\Lambda, \Delta \subset T(\Lambda, \Delta, \Psi)$, and if $t_1, \dots, t_n \in T(\Lambda, \Delta, \Psi)$, and $f$ is a function in $\Psi_n$, then $f(t_1, \dots, t_n) \in T(\Lambda, \Delta, \Psi)$.

In addition to the usual connectives of sentential logic, we also require \emph{predicates}, which are functions of nouns representing a statement about those nouns. For instance, ``X is a Human'' is a predicate. Predicates, like functions, are separated based on arity. For each $n$, let $\Pi_n$ be a set of $n$-ary predicates. Given $\Lambda$, $\Delta$, and $\Psi$ like before, we shall define an \emph{atomic formula} to be a string of the form $P(t_1, \dots, t_n)$, where $P \in \Pi_n$, and $t_1, \dots, t_n \in T(\Lambda, \Delta, \Psi)$. Then the first order language $\text{FO}(\Lambda, \Delta, \Psi, \Pi)$ is defined to be the smallest language containing all atomic formulae, and also closed under the logical operations $\wedge, \vee, \Rightarrow, \neg, \Leftrightarrow$, as in sentential logic, and such that if $X \in \Lambda$ is a variable, and $\phi$ is a statement, then $(\forall x: \phi)$ and $(\exists x: \phi)$ are formulae in the language. We shall abbreviate the string $(\forall x_1 : (\forall x_2: \dots (\forall x_n: \phi)\dots))$ as $(\forall x_1, x_2, \dots, x_n : \phi)$, and larger chains of quantifiers like $(\forall x: (\exists y: \phi))$ as $(\forall x, \exists y: \phi)$.

Using much the same methods as in the syntax of propositional logic, we can verify that all formulas have a principal connective, and therefore can be parsed uniquely. However, this is a bit of work, and doesn't give any interesting results. Given the readers experience, we leave them to fill in the syntactical details when needed when analyzing the interesting portion of first order logic.

In sentential logic, one may substitute arbitrary formulas into variables, and the meaning of the statement will not change. In first order logic, things are more complicated. Consider the statement
%
\begin{quotation}
    ``there exists $X$, such that if $X$ is a man, then $X$ is mortal''
\end{quotation}
%
First off, we cannot replace the initial $X$, for when we replace it with a definite noun the statement becomes nonsense. We may replace the other $X$'s, but this changes the meaning of the statement
%
\[ \text{``there exists $X$, such that if Laura is a man, then Laura is mortal''} \]
%
Another problem occurs when we substitute $X$ for $Y$ in the formula
%
\[ \text{``there is $Y$ such that if $X$ is a man, then $Y$ is a dog''} \]
%
The resulting substitution is
%
\[ \text{``there is $Y$ such that if $Y$ is a man, then $Y$ is dog''} \]
%
We wish to perform these types of substitutions formally, but in a way which avoids changing the meaning of a statement. Let $\phi$ be an arbitrary string in a first order language. An occurrence of a variable $x$ is \emph{bound} in $\phi$ if it occurs in a subformula $(\forall x: \psi)$ or $(\exists x: \psi)$. An occurrence is \emph{free} if it is not bound, and a variable $y$ is \emph{free for $x$ in} $\phi$ if $x$ does not occur in any subformula of the form $(\forall y: \psi)$ or $(\exists y: \psi)$, where $y$ is a free variable in $\phi$. A substitution $\phi[\psi_1/x_1, \dots, \psi_n/x_n]$ is only valid when each variable in $\psi_i$ is free for substitution for $x_i$ in $\phi$. This avoids the interpretation problems above. In the first example, $X$ is bound, so cannot be substituted. In the second $X$ is free, but is not free for substitution for $Y$. Given a formula $\phi$ with free variables $x_1, \dots, x_n$, it will often be convenient to denote it by $\phi(x_1, \dots, x_n)$, and to let $\phi(t_1, \dots, t_n)$ be $\phi[t/x]$.

\section{Interpretation}

Languages are defined in terms of the subject manner we wish to study, but may be interpreted in many different ways. For instance, the axioms which define the logic of group theory may be interpreted relative to whichever group we interpret the axioms as agreeing with. We would hope that a statement is true if and only if it is true in every interpretation of the axioms. To begin discussing this, we must precisely define what we mean by interpretation, as formulated by Alfred Tarski.

An \emph{interpretation} $M$ of a first order language $\text{FO}(\Lambda, \Delta, \Phi, \Pi)$ is a set $U_M$, known as the \emph{universe of discourse}, and an interpretation of the relations; for each constant $c \in \Delta$, we have an associated element $c_M \in U_M$, for each function $f \in \Phi_n$, we have a function $f_M: U_M^n \to U_M$, and for each proposition $P \in \Pi_n$, we have an $n$-ary relation $P_M$ on elements of $U_M$.

\begin{example}
    We can consider a first order language with the 2-ary function $+$, the constant $0$, and the 2-ary predicate $=$. Then $\NN$, $\ZZ$, $\RR$, and $\CC$, with the standard intepretation of $0$ and $+$, are interpretations of this first order language.
\end{example}

In sentential logic, when we assign a truth value to a set of variables, we may extend the definition of truth to all formulas. When we assign a meaning to each variable in a first order language, we may define a meaning on all terms, and from these meanings, assign truth to statements in the corresponding language. Consider a particular interpretation of a first order language with variables $\Lambda$, and consider an assignment $f: \Lambda \to U_M$. Define $f_*: T(\Lambda, \Delta, \{ \Psi_n \}) \to U_M$ by
%
\[ f_*(x) = f(x)\ \ \ \ \ f_*(c) = c_M\ \ \ \ \ f_*(g(t_1, \dots, t_n)) = g_M(f_*(t_1), \dots, f_*(t_n)) \]
%
Using this definition, we may define whether a formula is satisfied in a model of a first order theory. We shall now define what it means for an assignment to \emph{satisfy} a formula in an interpretation. For simplicity, write $f[a/x]$ for the assignment
%
\[ f[a/x](y) = \begin{cases} f(y) & y \neq x \\ a & y = x \end{cases} \]
%
\begin{enumerate}
    \item $f$ satisfies $P(t_1, \dots, t_n)$ if $P_M(f_*(t_1), \dots, f_*(t_n))$ holds.
    \item $f$ satisfies $(\forall x: \phi)$ if $\phi$ is satisfied by $f[a/x]$, for all $a \in U_M$. $f$ satisfies $(\exists x: \phi)$ if there is some $a \in U_M$ such that $f[a/x]$ satisfies $\phi$.
    \item An assignment $f$ satisfies $\phi \circ \eta$ or $\neg \phi$, where $\circ$ and $\neg$ are logical connectives, if the truth evaluation of $\phi$ and $\eta$ is consistant with the connectives as in sentential logic.
\end{enumerate}
%
If $M$ is an interpretation, then a formula $\phi$ is \emph{valid} for an interpretation, denoted $\vDash_M \phi$, if $\phi$ is true under every assignment under $M$. A statement is false if it is true under no interpretation, or alternatively, if the negation of the statement is true under the interpretation. An interpretation is a \emph{model} for a set of formulas $\Gamma$ if every formula in $\Gamma$ is true for the interpretation.

\begin{example}
    The formula $(\forall x, \exists y: x + y = 0)$ in the language of arithmetic we have seen before, is satisfied by the interpretations $\ZZ$, $\RR$, and $\CC$, but not $\NN$. Thus $\ZZ$, $\RR$, and $\CC$ are models of this formula.
\end{example}

\begin{lemma} If $\vDash_M \phi$ and $\vDash_M \phi \Rightarrow \psi$, then $\vDash_M \psi$. \end{lemma}

\begin{lemma} If a formula $\phi$ contains free variables $x_1, \dots, x_n$, and two assignments $f$ and $g$ agree on the free variables, then $f$ satisfies $\phi$ if and only if $g$ satisfies $\phi$. \end{lemma}
\begin{proof}
    We shall first verify that if $t$ is a term containing variables $x_1, \dots, x_n$, on which $f$ and $g$ agree, then $f_*(t) = g_*(t)$. If $t$ is a variable, or a constant, the proof is easy. But then by induction, for an $n$-ary function $u$,we have
    %
    \begin{align*}
        f_*(u(t_1, \dots, t_n)) &= u_M(f_*(t_1), \dots, f_*(t_n))\\
        &= u_M(g_*(t_1), \dots, g_*(t_n))\\
        &= g_*(u(t_1, \dots, t_n))
    \end{align*}
    %
    Thus the theorem is verified by structural induction. If $\phi$ is an atomic formula $P(t_1, \dots, t_n)$, then $f$ satisfies $P(t_1, \dots, t_n)$ if and only if $g$ does, because $f_*(t_i) = g_*(t_i)$. If $\phi$ is $(\forall x: \psi)$, and $f$ satisfies $\phi$, then $f[a/x]$ satisfies $\psi$ for all $a \in U_M$. But then $g[a/x]$ agrees on all free variables of $f[a/x]$, so $g[a/x]$ satisfies $\psi$ by induction. It follows that $g$ satisfies $\phi$ as well. The remaining cases are easily shown, and are left as an exercise.
\end{proof}

\begin{theorem}
    If $\phi$ contains no free variables, then either $\vDash_M \phi$ or $\vDash_M \neg \phi$.
\end{theorem}
\begin{proof}
    This follows from the fact that any two assignments that agree on the free variables of a formula agree on the satisfiability. Thus if there are no free variables, all assignments agree, and in particular all satisfy the formula or all do not satisfy the formula.
\end{proof}

\begin{lemma} $\vDash_M (\exists x: \phi)$ if and only if $\vDash_M \neg (\forall x: \neg \phi)$. \end{lemma}
\begin{proof}
    If an assignment $f$ satisfies $(\exists x: \phi)$, then $\phi$ is satisfied by some $f[a/x]$. But then $f$ does not satisfy $(\forall x: \neg \phi)$ for $f[a/x]$ does not satisfy $\neg \phi$. Conversely, if an assignment $f$ does not satisfy $(\exists x: \phi)$, then every $f[a/x]$ satisfies $\neg \phi$.
\end{proof}

\begin{lemma}
    $\vDash_M \phi$ if and only if $\vDash_M (\forall x: \phi)$.
\end{lemma}
\begin{proof}
    If $\vDash_M \phi$, then every assignment $f$ satisfies $\phi$, so certainly every $f[a/x]$ satisfies $\phi$, and thus $f$ satisfies $(\forall x: \phi)$, hence $\vDash_M (\forall x: \phi)$. Conversely, suppose $\vDash_M (\forall x: \phi)$. Then, every assignment satisfies $(\forall x: \phi)$, and thus in particular satisfies $\phi$.
\end{proof}

Let $\phi$ contain free variables $x_1, \dots, x_n$. The \emph{closure} of $\phi$ is the string $(\forall x_1, x_2, \dots, x_n: \phi)$. The above proof shows a formula is satisfied if and only if its closure is, so we may assume in the study of semantics that every formula is closed -- that is, it has no free variables.

\begin{theorem}
    Consider a form of sentential logic, whose variables are all atomic formulas, and we interpret formulas of the form $(\forall x: \phi)$, and $(\exists x: \phi)$ as `variables' as well. Then if a statement is a tautology, then it is satisfied under all interpretations.
\end{theorem}
\begin{proof}
    The connectives of predicate logic are exactly the connectives of sentential logic once we hide away the existential and universal quantifiers. If the statement is a tautology, then regardless of how we interpret the formula, the statement will be satisfied.
\end{proof}

\begin{lemma}
    If $t$ and $u$ are terms, $x$ is a variable, and $f$ is an assignment, then
    %
    \[ f[f_*(u)/x]_*(t) = f_*(t[u/x]) \]
\end{lemma}
\begin{proof}
    If $t$ is a variable unequal to $x$, or $t$ is a constant, then
    %
    \[ f[f_*(u)/x]_*(t) = f(t) = f_*(t[u/x]) \]
    %
    If $t = x$, then
    %
    \[ f[f_*(u)/x]_*(t) = f_*(u) = f_*(t[u/x]) \]
    %
    For a structural induction, let $t = g(t_1, \dots, t_n)$. Then
    %
    \begin{align*}
        f[f_*(u)/x]_*(t) &= g_M(f[u/x]_*(t_1), \dots, f[u/x]_*(t_n))\\
        &= g_M(f_*(t_1[u/x]), \dots, f_*(t_n[u/x])) = f_*(t[u/x])
    \end{align*}
    %
    Thus the theorem holds in general.
\end{proof}

\begin{lemma}
    $f$ satisfies $\phi(u)$ if and only if $f[f_*(u)/x]$ satisfies $\phi(x)$.
\end{lemma}
\begin{proof}
    If $\phi(x)$ is $P(t_1(x), \dots, t_n(x))$, then $\phi(u) = P(t_1(u), \dots, t_n(u))$, and
    %
    \[ P_M(f_*(t_1(u)), \dots, f_*(t_n(u))) = P_M(f[f_*(u)/x]_*(t_1), \dots, f[f_*(u)/x]_*(t_n)) \]
    %
    Which shows that $f$ satisfies $\phi(u)$ if and only if $f[f_*(u)/x]$ satisfies $\phi$. If $\phi$ is formed by standard sentential connectives, the theorem is trivial. If $\phi = (\forall y: \psi)$, where $y \neq x$, then $\phi(u) = (\forall y: \psi[u/x])$, and by definition, $f$ satisfies $\phi(u)$ if and only if $f[a/y]$ satisfies $\psi(u)$ for all $a$, which by induction implies that $f[f_*(u)/x][a/y]$ satisfies $\psi$ for all $a$, so $f[f_*(u)/x]$ satisfies $\phi$. Similar results hold if $\phi$'s primitive connective is the existential quantifier.
\end{proof}

\begin{theorem}
    For any formula $\phi$ and term $t$ free for $x$, $\vDash_M (\forall x : \phi) \Rightarrow \phi(t)$
\end{theorem}
\begin{proof}
    Let $f$ be an assignment satisfying $(\forall x: \phi)$. Then $f[f_*(t)/x]$ satisfies $\phi$, so $f$ satisfies $\phi(t)$.
\end{proof}

\begin{theorem}
    If $\phi$ does not contain $x$ as a free variable, then
    %
    \[ \vDash_M (\forall x: \phi \Rightarrow \psi) \Rightarrow (\phi \Rightarrow (\forall x: \psi)) \]
\end{theorem}
\begin{proof}
    If $f$ satisfies $(\forall x: \phi \Rightarrow \psi)$ and $\phi$, then $f[a/x]$ satisfies $\phi \Rightarrow \psi$, and since $\phi$ does not contain $x$ as a free variable, $f[a/x]$ also satisfies $\phi$, so $f$ satisfies $(\forall x : \psi)$.
\end{proof}

Given a first order language $F$, together with a specific interpretation with universe of discourse $U_M$, we enhance the first order language we are discussing to include $U$ as constants. Such an extension is denoted as $F(U_M)$. $U_M$ can be extended canonically to interpret $F(U_M)$, in the obvious manner. This allows us to discuss formulas of the form $\vDash_M \phi(u_1, \dots, u_n)$, where $u_1, \dots, u_n \in U$ are formulas containing elements of $U$. If the free variables of a formula $\phi$ are $x_1, \dots, x_n$, then the set of $u_1, \dots, u_n$ such that $M$ models $\phi(u_1, \dots, u_n)$ will be called the relation relative to $\phi$.

A statement is \emph{logically valid} if it is true under all possible interpretations. A statement is \emph{satisfiable} if it is true under at least one interpretation, and \emph{contradictory} if it is false under every interpretation. A set of statements is satisfiable if they are all true under a single interpretation. A statement $s$ is a \emph{logical consequence} of a set of statements $\Gamma$ if every interpretation which satisfies every statement of $\Gamma$ also satisfies $s$.

What we have argued in this chapter is that certain rules preserve logical validity. Thus they are valid for a formal argument in predicate logic. In the next section, we will formally introduce these rules, and in the system, prove they are all the formulae needed to prove anything valid about predicate logic. We will therefore obtain a completeness property for the logic.






\section{First Order Formal Systems}

We have the syntax and semantics to begin discussing formal systems in the first-order languages. We would like to find axioms which only find theorems satisfied by every model of the system, a \emph{sound} axiom system. Even better, if we can find \emph{complete} axioms, which can proved anything satisfied by every model of the system. For simplicity, we consider only formulae in the connectives $\forall$, $\neg$, and $\Rightarrow$, since all other formulae are equivalent to formulas formed by these connectives. We shall use an axiom consisting of five schemata, the original (A1), (A2), and (A3) found in predicate logic, as well as two new first order schema (A4) and (A5). Let $s$ and $w$ be formula, and $t$ a term substitutable for a variable $x$. Then (A4) is
%
\[ (\forall x: \phi(x)) \Rightarrow \phi(t) \]
%
provided that $t$ is free for substitution for $x$ in $\phi$, and (A5) is
%
\[ (\forall x: \phi \Rightarrow \psi) \Rightarrow (\phi \Rightarrow (\forall x: \psi)) \]
%
where $\phi$ contains no free occurences of $x$. Our rules of inference are modus ponens (MP), to infer $\psi$ from $\phi$ and $\phi \Rightarrow \psi$, as well as universal generalization (UG), inferring $(\forall x: \phi)$ from $\phi$. As in predicate logic, we shall let $\vdash \phi$ mean that $\phi$ is a theorem of the system. If $\Gamma$ is a set of formulae, we shall let $\Gamma \vdash \phi$ state that $\phi$ is provable assuming $\Gamma$ are axioms. The rule of universal generalization is very subtle. One cannot put it into an axiomatic schema of the form $\phi \Rightarrow (\forall x: \phi)$, for this statement is not sound in all models of predicate logic.

\begin{example}
    Consider the formula $P(x) \Rightarrow (\forall x: P(x))$. Take a model $M$ whose universe consists of two elements $a$ and $b$, and let $P_M$ be the relation only satisfied by $a$. Then, under the assignment $f(x) = a$, $P(x)$ is satisfied, but $(\forall x: P(x))$ is not. Thus $P(x) \Rightarrow (\forall x: P(x))$ is not semantically valid, and hence not provable in first order logic.
\end{example}

Nonetheless, if we can {\it prove} $\phi$, universal generalization allows us to conclude that $(\forall x: \phi)$ is also valid; if $\phi$ contains $x$ as a free variable, it is only an inbetween statement which we use to eventually conclude that $(\forall x: \phi)$ is true. It is this little annoyance which lead Moses Sch\"{o}nfinkel and Haskell Curry to invent combinatory logic, which is a logical system designed to avoid quantified variables. Common arguments are most easily adapted to our model of first order logic, so for the time being we will stick to this system.

From the theorems we proved about first order semantics, we already know that this axiom system is sound, for each axiom is valid under all interpretations, and our deductions preserve truth. We shall, after some work, conclude this system is complete -- one may prove a theorem if and only if it is valid under all interpretations of the theory. To begin with, we notice our system includes (A1), (A2), (A3), and (MP), all the rules of propositional calculus, which justifies a very useful property of our new formal system.

\begin{theorem}
    Let $\phi$ be a formula in first order logic, and replace it with a corresponding formula in sentential logic by replacing all the main occurences of the quantifiers $(\forall x: \phi)$ with variables separate from the rest of the variables occuring in $\phi$. Then if the formula obtained is a tautology, $\phi$ is provable in first order logic.
\end{theorem}

We shall denote an application of the deduction theorem by $(\text{TAUT})$. There are many useful schemas obtained from this, that are trivial to prove.
%
\begin{itemize}
    \item Negation Elimination (NE): $\vdash \neg \neg \phi \Rightarrow \phi$
    \item Negation Introduction (NI): $\vdash \phi \Rightarrow \neg \neg \phi$
    \item Conjunction Elimination (CE): $\vdash \phi \wedge \psi \Rightarrow \phi$, $\vdash \phi \wedge \psi \Rightarrow \psi$.
    \item Conjunction Introduction (CI): $\vdash \phi \Rightarrow (\psi \Rightarrow (\phi \wedge \psi))$.
\end{itemize}
%
One can just apply the completeness theorem of sentential logic to obtain these statements as theorems.

\begin{example}
    If $t$ is free for $x$ in $\phi(x)$. Then $\phi(t) \Rightarrow (\exists x: \phi)$ is a theorem of first order logic, a formalization of existential introduction.
    %
        \[
    \fitchprf{\pline{\phi(t) \Rightarrow \neg (\forall x: \neg \phi(x))}}{
        \pline[1.]{(\forall x: \neg \phi(x)) \Rightarrow \neg \phi(t)}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (A4)]\\
        \pline[2.]{((\forall x: \neg \phi) \Rightarrow \neg \phi(t)) \Rightarrow (\phi(t) \Rightarrow \neg (\forall x: \neg \phi(x)))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (TAUT)]\\
        \pline[3.]{\phi(t) \Rightarrow \neg (\forall x: \neg \phi(x))}[\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1),(2),(MP)]
    }
    \]
    %
    Without the tautology theorem, this deduction would be much longer. It will be useful in further proofs to note that we never applied $(UG)$ to any formulae. We denote an application of this proof by $(EI)$.
\end{example}

The deduction theorem cannot be carried directly over to first order logic, for assumptions in proofs and premises in logical statements are interpreted differently in certain circumstances. $\phi \vdash (\forall x: \phi)$ is always true for any statement $\phi$, yet $\vdash \phi \Rightarrow (\forall x: \phi)$ is not always a theorem. The problem is, of course, the free variables again causing trouble. In a proof $(\psi_1, \dots, \psi_n)$ of $\Gamma \vdash \phi$, we say $\psi_i$ \emph{depends on} $\eta \in \Gamma$ if $\psi_i$ is $\eta$, or $\psi_i$ is inferred from $\psi_j$ and $\psi_j$ depends on $\eta$.

\begin{theorem}
    Suppose that $\Gamma, \phi \vdash \psi$, and there is a proof of $\psi$ which involves no application of universal generalization on a formula which depends on $\phi$, by a variable $x$ which is free in $\phi$. Then $\Gamma \vdash \phi \Rightarrow \psi$.
\end{theorem}
\begin{proof}
    We perform induction on the length of proofs. If $\psi$ can be proved in one statement, then $\psi$ is either an instance of an axiom, or an element of $\Gamma \cup \{ \phi \}$. If $\phi \neq \psi$, then $\Gamma \vdash \psi$,  since the proof is equally valid here, and hence $\Gamma \vdash \phi \Rightarrow \psi$. If $\phi = \psi$, then $\phi \Rightarrow \psi$ is a tautology, so $\Gamma \vdash \phi \Rightarrow \psi$ is valid. Now suppose we have a proof $(\eta_1, \dots, \eta_{n+1})$. By induction, for each $\eta_i$, we have $\Gamma \vdash (\phi \Rightarrow \eta_i)$. If $\eta_{n+1}$ is an axiom, an element of $\Gamma$, or equal to $\phi$, we have already justified the implication. Suppose $\eta_{n+1}$ was inferred by modus ponens from $\eta_i$ and $\eta_j$, where $\eta_j$ has the form $\eta_i \Rightarrow \eta_{n+1}$. Then $\Gamma \vdash (\phi \Rightarrow \eta_i)$, and $\Gamma \vdash (\phi \Rightarrow (\eta_i \Rightarrow \eta_{n+1}))$ by induction. The statement
    %
    \[ \nu = ((\phi \Rightarrow (\eta_i \Rightarrow \eta_{n+1})) \Rightarrow (\phi \Rightarrow \eta_i)) \Rightarrow (\phi \Rightarrow \eta_{n+1}) \]
    %
    is a tautology, so $\Gamma \vdash \nu$, and by modus ponens, we find $\Gamma \vdash (\phi \Rightarrow \eta_{n+1})$. Otherwise $\eta_{n+1}$ is of the form $(\forall x: \eta_i)$, obtained from some $\eta_i$ by universal generalization. By induction, $\Gamma \vdash \phi \Rightarrow \eta_i$, so $\Gamma \vdash (\forall x: \phi \Rightarrow \eta_i)$. By assumption, $x$ does not occur as a free variable of $\phi$, so we have the axiom $(\forall x: \phi \Rightarrow \eta_i) \Rightarrow (\phi \Rightarrow (\forall x: \eta_i))$, and we conclude $\Gamma \vdash \phi \Rightarrow (\forall x: \eta_i)$.
\end{proof}

\begin{example}
    We have the theorem $\vdash (\forall x, \forall y: \phi)) \Rightarrow (\forall y, \forall x: \phi)$, for any statement $\phi$. To see this, we apply our newly established deduction theorem.
    %
    \[
    \fitchprf{\pline{(\forall x, \forall y: \phi) \Rightarrow (\forall y, \forall x: \phi)}}{
        \subproof{\pline[1.]{(\forall x, \forall y: \phi)}} {
            \pline[2.]{(\forall x, \forall y: \phi) \Rightarrow (\forall y: \phi)}[(A4)]\\
            \pline[3.]{(\forall y: \phi)}[(1),(2),(MP)]\\
            \pline[4.]{(\forall y: \phi) \Rightarrow \phi}[(A4)]\\
            \pline[5.]{\phi}[(3),(4),(MP)]\\
            \pline[6.]{(\forall x: \phi)}[(5),(UG)]\\
            \pline[7.]{(\forall y, \forall x: \phi)}[(6),(UG)]
        }
        \pline[8.]{((\forall x, \forall y: \phi) \Rightarrow (\forall y, \forall x: \phi)}[$(1-7), (DT)$]
    }
    \]
    %
    Care needs to be taken in order to ensure these steps are accurate, and do not apply universal generalization on a free variable.
\end{example}

\begin{example}
    For any $\phi$ and $\psi$, $\vdash (\forall x: \phi \Leftrightarrow \psi) \Rightarrow ((\forall x: \phi) \Leftrightarrow (\forall x: \psi))$.
    %
    \[
    \fitchprf{\pline{(\forall x: \phi \Leftrightarrow \psi) \Rightarrow ((\forall x: \phi) \Leftrightarrow (\forall x: \psi))}}{
        \subproof{\pline[1.]{(\forall x: \phi \Leftrightarrow \psi)}} {
            \pline[2.]{(\phi \Leftrightarrow \psi)}[(A4)]\\
            \subproof{\pline[3.]{(\forall x: \phi)}}{
                \pline[4.]{\phi}[(A4), (3), (MP)]\\
                \pline[5.]{\psi}[(2),(4),(MP)]\\
                \pline[6.]{(\forall x: \psi)}[(UG)]
            }
            \pline[7.]{(\forall x: \phi) \Rightarrow (\forall x: \psi)}[(3-6),(DT)]\\
            \dots\\
            \pline[8.]{(\forall x: \phi) \Leftarrow (\forall x: \psi)}\\
            \pline[9.]{(\forall x: \phi) \Leftrightarrow (\forall x: \psi)}
        }
        \pline[10.]{(\forall x: \phi \Leftrightarrow \psi) \Rightarrow ((\forall x: \phi) \Leftrightarrow (\forall x: \psi))}
    }
    \]
    %
    The proof involved in the elipses is exactly the same as (3) through (6).
\end{example}

A useful theorem is an immediate consequence of the deduction theorem, left to the reader.

\begin{theorem}
    If $\phi$ has no free occurrences of $y$, then
    %
    \[ \vdash ((\forall x: \phi(x)) \Leftrightarrow (\forall y: \phi(y))) \]
\end{theorem}

The next theorem is more involved, but very useful. We define substitution on formulas in the following way. Given formulae $\phi,\psi,\eta$, we define the formula $\phi[\psi/\eta]$, obtained from swapping $\eta$ with $\psi$, by the base case $\eta[\psi/\eta] = \psi$, and the recursive case by delving into subformulas, {\it provided the formula isn't just $\eta$}.

\begin{theorem}
    Let $x_1, \dots, x_n$ be all free variables of $\phi$ that occurs as a bound variable in $\psi$ or $\eta$. Then
    %
    \[ (\forall x_1, \dots, x_n: \eta \Leftrightarrow \psi) \Rightarrow (\phi \Leftrightarrow \phi[\psi/\eta]) \]
    %
    The theorem also holds if $\phi[\psi/\eta]$ is replaced with a formula obtained only be swapping some (but perhaps not all) occurences of $\eta$.
\end{theorem}
\begin{proof}
    We prove, like always, by structural induction. If no occurrences are swapped, we are left with the formula
    %
    \[ (\forall x_1, \dots, x_n: \psi \Leftrightarrow \eta) \Rightarrow (\phi \Leftrightarrow \phi) \]
    %
    which is a tautology, hence trivial. Thus we may assume that $\eta$ does occur in $\phi$. If $\phi$ is an atomic formula, then we are left with the case that $\phi = \eta$, and then
    %
    \[ (\forall x_1, \dots, x_n: \psi \Leftrightarrow \eta) \Rightarrow (\psi \Leftrightarrow \eta) \]
    %
    is an instance of (A4). The remaining cases are relatively simple, and left to the reader.
\end{proof}

It is also useful in mathematics to make arguments of the following form. Suppose we have a theorem of the form $(\exists x: \phi(x))$. We introduce a new constant $c$, which is not used in any axiom of the formal system, and consider the formula $\phi(c)$. If we end up with a formula $\eta(c)$, we conclude that $(\exists x: \phi(x))$ implies $(\exists x: \eta(x))$. Though our system is not capable of expressing these arguments, it is satisfying to know that such arguments do not increase the amount of theorems one may prove in first order logic. Temporarily, we shall write $\vdash_C \phi$ for a proof of this form. Formally, we write $\vdash_C \phi$ if there is a sequence of formulas $(\eta_1, \dots, \eta_n)$ such that each $\eta_i$ is an axiom, is inferred by (MP) or (UG) from a previous formula, or there is a preceding formula $\eta_j = (\exists x: \psi(x))$, where $\eta_i = \psi(c)$, and $c$ is a new constant which does not occur in any prior formulae or explicitly in axioms of the formal system. We require that no application of $(UG)$ is made on a variable which is free in some $(\exists x: \phi)$, in a formula which depends on this formula. Finally, we require that $\eta_n$ does not contain any of the constants introduced by the final rule. To prove that this method can be applied to our formal system, we require a certain formula, proved now, and integral to the proof that $\vdash_C$ is redundant.

\begin{example}
    If $x$ is not free in $\psi$, $((\exists x: \phi(x)) \Rightarrow \psi) \Leftrightarrow (\forall x: \phi(x) \Rightarrow \psi)$.
    %
    \[
    \fitchprf{\pline{((\exists x: \phi(x)) \Rightarrow \psi) \Leftrightarrow (\forall x: \phi(x) \Rightarrow \psi)}}{
        \subproof{\pline[1.]{(\exists x: \phi(x)) \Rightarrow \psi}} {
            \subproof{\pline[2.]{\phi(x)}}{
                \pline[3.]{(\exists x: \phi(x))}[(EI)]\\
                \pline[4.]{\psi}[(1),(3),(MP)]
            }
            \pline[5.]{\phi(x) \Rightarrow \psi}[(2-4),(DT)]\\
            \pline[6.]{(\forall x: \phi(x) \Rightarrow \psi)}[(UG)]
        }
        \pline[6.]{((\exists x: \phi(x)) \Rightarrow \psi) \Rightarrow (\forall x: \phi(x) \Rightarrow \psi)}[(1-6),(DT)]\\

        \subproof{\pline[7.]{(\forall x: \phi(x) \Rightarrow \psi)}}{
            \pline[8.]{\phi(x) \Rightarrow \psi}[(A4)]\\
            \pline[9.]{\neg \psi \Rightarrow \neg \phi(x)}[(8),(\text{TAUT}),(MP)]\\
            \pline[10.]{(\forall x: \neg \psi \Rightarrow \neg \phi(x))}[(UG)]\\
            \pline[11.]{\neg \psi \Rightarrow (\forall x: \neg \phi(x))}[(10),(A5),(MP)]\\
            \pline[12.]{\neg (\forall x: \neg \phi(x)) \Rightarrow \psi}[(11),(TAUT),(MP)]
        }
        \pline[13.]{(\forall x: \phi(x) \Rightarrow \psi) \Rightarrow ((\exists x: \phi(x)) \Rightarrow \psi)}[(7-12),(DT)]
    }
    \]
\end{example}

\begin{theorem}
    If $\vdash_C \phi$, then $\vdash \phi$.
\end{theorem}
\begin{proof}
    Let $(\eta_1, \dots, \eta_n)$ be a proof of $\phi$, and suppose that
    %
    \[ \eta_{i_1} = (\exists x_1: \nu_1(x_1))\ \ \ \ \ \eta_{i_2} = (\exists x_2: \nu_2(x_2))\ \ \ \ \ \dots\ \ \ \ \ \eta_{i_m} = (\exists x_m: \nu_m(x_m)) \]
    %
    are all existence formulae used in the proof, from which new constants $c_1, \dots, c_m$ are introduced. Certainly $\nu_1(c_1), \dots, \nu_m(c_m) \vdash \phi$. By the universal generalization condition on $\vdash_C$, we can apply the deduction theorem to conclude that $\nu_1(c_1), \dots, \nu_{m-1}(c_{m-1}) \vdash \nu_m(c_m) \Rightarrow \phi$. In the proof of this statement, replace all instances of $c_m$ with a new variable $y_m$. This is then still a valid proof (for variables can be operated on in at least the same capacity as constants), hence $\nu_1(c_1), \dots, \nu_{m-1}(c_{m-1}) \vdash \nu_m(y_m) \Rightarrow \phi$. This implies $\nu_1(c_1), \dots, \nu_{m-1}(c_{m-1}) \vdash (\forall y_m: \nu_m(y_m) \Rightarrow \phi)$, and by applying the recently proved example, since we know $y_m$ does not occur in $\phi$ at all, we conclude $\nu_1(c_1), \dots, \nu_{m-1}(c_{m-1}) \vdash (\exists y_m: \nu_m(y_m)) \Rightarrow \phi$. By induction, we may assume that $\nu_1(c_1), \dots, \nu_{m-1}(c_{m-1}) \vdash (\exists y_m: \nu_m(y_m))$, which implies, by a particular use of (MP), that $\nu_1(c_1), \dots, \nu_{m-1}(c_{m-1}) \vdash \phi$, and we may recursively prove that $\vdash \phi$.
\end{proof}

\section{Completeness Theorem}

The completeness theorem is best understood in the context of \emph{first order theories}, which are subsets of formulas in a first order system, which we think of as axioms. An \emph{extension} of a theory is just a theory which is a superset of the original theory. If $\Gamma$ is a theory, we let $\Gamma^{\mathfrak{d}}$ denote the \emph{deductive closure} of $\Gamma$, which is the extension consisting of all formulas $\phi$ such that $\Gamma \vdash \phi$. Recall that a theory $\Gamma$ is \emph{consistant} if for any $\phi$, we do not have both $\Gamma \vdash \phi$ and $\Gamma \vdash \neg \phi$. A \emph{complete} theory is a theory in which $\Gamma \vdash \phi$ or $\Gamma \vdash \neg \phi$ always holds. If some theory is consistance or complete, so is its logical closure.

At times, we shall consider extensions which lie in extensions of first order languages, in the sense of adding additional prepositions, constants, and variable symbols to the language. It is simple to see that whenever $\text{FOL}(\Lambda',\Delta',\Phi',\Pi')$ is a first order language extending $\text{FOL}(\Lambda,\Delta,\Phi,\Pi)$, if $\Gamma$ is a theory in $\text{FOL}(\Lambda,\Delta,\Phi,\Pi)$, and if $\Gamma_0$ is the deductive closure of $\Gamma$ in $\text{FOL}(\Lambda,\Delta,\Phi,\Pi)$, $\Gamma_1$ the deductive closure of $\Gamma$ in $\text{FOL}(\Lambda',\Delta',\Phi',\Pi')$, then $\Gamma_0 = \Gamma_1 \cap \text{FOL}(\Lambda,\Delta,\Phi,\Pi)$. Surely the left is included in the second. Alternatively, suppose there is a proof $(\eta_1, \dots, \eta_n)$ in $\text{FOL}(\Lambda',\Delta',\Phi',\Pi')$ of $\Gamma \vdash \phi_n$, with $\phi_n \in \text{FOL}(\Lambda,\Delta,\Phi,\Pi)$. For each variable and constant in the proof which is not in $\text{FOL}(\Lambda,\Delta,\Phi,\Pi)$, swap it out with a unique unused variable in $\text{FOL}(\Lambda,\Delta,\Phi,\Pi)$. Since variables can do everything a constant can do, the proof is still valid. Similarily, swap each proposition in the proof which is not in $\text{FOL}(\Lambda,\Delta,\Phi,\Pi)$ with some proposition in $\text{FOL}(\Lambda,\Delta,\Phi,\Pi)$. This might be a little harder to see, but one verifies by induction on proof length that this works out, and shows that we may prove $\eta_n$ solely in $\text{FOL}(\Lambda,\Delta,\Phi,\Pi)$. Thus it is consistant to talk of extensions of theories that add new variables without introducing additional information.

\begin{lemma}
    If a theory has a model, then it is consistant.
\end{lemma}
\begin{proof}
    If $\Gamma$ has a model, and $\Gamma \vdash \phi$ and $\Gamma \vdash \neg \phi$, then $\phi$ and $\neg \phi$ must both be valid in the model, which is clearly impossible.
\end{proof}

\begin{example}
    The theory of groups is based on the language consisting of some variables, the constant $e$, a binary function $*$, and the binary preposition $=$ (applied in infix notation), with the axiom system
    %
    \[ x(yz) = (xy)z \]
    \[ ex = x\ \ \ \ \ xe = x \]
    \[ (\forall x, \exists y: xy = e \wedge yx = e) \]
    %
    together with the equality axioms
    %
    \[ (x = y) \Rightarrow (y = x)\ \ \ \ \ ((x = y) \wedge (y = z)) \Rightarrow (x = z)\ \ \ \ \ (x = y) \Rightarrow (xz = xy) \]
    %
    This set of statements is a first order theory. It is also interesting to note that the theory is \emph{finitely axiomatizable}, in the sense that the theory $\Gamma$ is a finite set. Any model of this theory is just a plain old group, with a couple nuances -- we need not interpret the $=$ relation in our theory as true equality in our model. We need only interpret the relation as an equivalence relation. In the case where the $=$ relation is interpreted as true equality, we can conclude that the model is actually a group. The completeness theorem will show that every theorem proved in this formal system is valid in any group. This theory is consistant (if you believe a group exists), but certainly not complete, for we may add the abelian axiom $xy = yx$, and we certainly have groups which do not satisfy this axiom.
\end{example}

\begin{example}
    The theory of equality has a single 2-arity predicate $=$, with the axioms
    %
    \[ x = y \wedge y = z \Rightarrow x = z\ \ \ \ x = x\ \ \ \ x = y \Rightarrow y = x \]
    %
    A model of the theory of equality is exactly a set with an equivalence relation. We can extend this theory to the theory of partial orders, by adding another 2-arity predicate $<$, and considering the axioms
    %
    \[ x = y \Rightarrow \neg (x < y)\ \ \ \ x < y \wedge y < z \Rightarrow x < z \]
    %
    the theory of partial orders can be extended to the theory of linear orders by adding the axiom
    %
    \[ x < y \vee x = y \vee y < x \]
    %
    and we can ensure there is no last element to the theory of linear orders by adding the axiom
    %
    \[ (\forall x, \exists y: y < x)\ \ \ (\forall x, \exists y: x < y) \]
    %
    The theory of dense partial orders has the axiom
    %
    \[ (x < y) \Rightarrow (\exists z: x < z \wedge z < y) \]
\end{example}

Models of algebraic systems are interesting as a toy example of mathematical logic, but it also is used for interesting applications of logic in algebra. For instance, a simple classification of the algebraically closed fields is obtained through model theoretic methods.

\begin{theorem}
    If $\Gamma$ is consistant, and we cannot prove $\phi$ or $\neg \phi$ in $\Gamma$, then $\Gamma \cup \{ \phi \}$ is also consistant.
\end{theorem}
\begin{proof}
    Suppose that $\Gamma \cup \{ \phi \} \vdash \psi$ and $\Gamma \cup \{ \phi \} \vdash \neg \psi$. Without loss of generality, we may assume that $\phi$ contains no free variables, for the logical closure of $\Gamma \cup \{ \phi \}$ is the same as the logical closure of $\Gamma \cup \{ (\forall x_1, \dots, x_n: \phi) \}$, because of axiom (A4) and (UG). But then, by the deduction theorem,
    %
    \[ \Gamma \vdash \phi \Rightarrow \psi\ \ \ \ \ \ \ \ \ \ \Gamma \vdash \phi \Rightarrow \neg \psi \]
    %
    But then $\Gamma \vdash \neg \phi$, because
    %
    \[ ((\phi \Rightarrow \psi) \wedge (\phi \Rightarrow \neg \psi)) \Rightarrow \neg \phi \]
    %
    is a tautology, a contradiction.
\end{proof}

\begin{lemma}[Lindenbaum]
    If $\Gamma$ is a consistant theory, then there is a complete consistant extension of $\Gamma$.
\end{lemma}
\begin{proof}
    Let $\mathcal{K}$ be the set of all consistant extensions of $\Gamma$, ordered by inclusion. If $\mathcal{A}$ is a chain of consistant extensions of $\Gamma$, then we claim $\bigcup \mathcal{A}$ is consistant. Suppose that
    %
    \[ \bigcup \mathcal{A} \vdash \phi\ \ \ \ \ \ \ \ \bigcup \mathcal{A} \vdash \neg \phi \]
    %
    Then there is a proof $(\eta_1, \eta_2, \dots, \eta_n)$ of $\phi$ from $\bigcup \mathcal{A}$, and a proof $(\nu_1, \nu_2, \dots, \nu_n)$ of $\neg \phi$. Since each side is finite, each uses only finitely many axioms, which implies that there is $\Gamma \in \mathcal{A}$ containing all axioms. But then $\Gamma \vdash \phi$ and $\Gamma \vdash \neg \phi$, which implies $\Gamma$ is not consistant, a contradiction. By Zorn's lemma, there is a maximal consistant extension $\Gamma$. $\Gamma$ is complete, by the last lemma. If $\Gamma \not \vdash \neg \phi$, then $\Gamma \cup \{ \neg \phi \}$ is consistant, implying $\Gamma \cup \{ \neg \phi \} = \Gamma$, so $\phi \in \Gamma$, implying $\Gamma \vdash \phi$.
\end{proof}

It is important to note this theorem as the first non constructive theorem in these notes. There is effectively no way to make a general version of this theorem constructive, because we will find that there are certain limitations to constructive arguments in first order logic, which manifest in undecidability results, like G\"{o}del's incompleteness theorem.

A theory $\Gamma$ is a \emph{scapegoat theory} if for any formula $\phi$ which only has one free variable $x$, there is a closed term $t$ for which
%
\[ \Gamma \vdash (\exists x: \neg \phi) \Rightarrow \neg \phi[t/x] \]
%
Scapegoat theorys are useful for proving the completeness theorem, for it is easier to move constants into interpretations than with plain formulas.

\begin{lemma}
    Every consistent theory $\Gamma$ has a consistant scapegoat extension $\Gamma$ which has the same cardinality as $\Gamma$ if $\Gamma$ is infinite, or is denumerable if $\Gamma$ is finite.
\end{lemma}
\begin{proof}
    The deductive closure of $\Gamma$ has the same cardinality as $\Gamma$ in the infinite case, or is denumerable in the finite case. Since the deductive closure extends $\Gamma$, we may, without loss of generality, assume $\Gamma$ is deductively closed. Let $\mathcal{X}$ be the set of all variables in all formulas of $\Gamma$ which have exactly one free variable, and biject them with a set $\mathcal{C}$ disjoint from all preexisting characters in the first order language of $\Gamma$. Let $c_x$ be the element of $\mathcal{C}$ in bijection with $\mathcal{X}$. Let $\Gamma_\infty$ be the theory obtained from $\Gamma$ by adding $\mathcal{C}$ as constants to the theory, together with the formulas $(\exists x: \neg \phi) \Rightarrow \neg \phi(c_x)$. $\Gamma_\infty$ is surely a scapegoat by construction. We claim that $\Gamma_\infty$ is consistant and scapegoat. It suffices, since every proof is finite, to show that every finite subextension is consistant. Consider any particular variables $x_1, x_2, \dots, x_n \in \mathcal{X}$, and let $\Gamma_0 = \Gamma$, and $\Gamma_k$ be the theory obtained from $\Gamma_{k-1}$ be adding the contant $c_{x_k}$ and the axiom $(\exists x_k: \neg \phi_k(x_k)) \Rightarrow \neg \phi_k(c_x)$. We prove consistency by induction on $k$. Suppose $\Gamma_{k-1}$ is consistant, and $\Gamma_k$ is inconsistant. Then we may prove all statements in $\Gamma_{k-1}$ in $\Gamma_k$. In particular, $\Gamma_k \vdash \neg ((\exists x_k: \neg \phi_k(x_k)) \Rightarrow \neg \phi_k(c_{x_k}))$. Since this formula is closed (because $x$ is the only free variable in $\phi_k$), we may apply the deduction theorem to conclude
    %
    \[ \Gamma_{k-1} \vdash ((\exists x_k: \neg \phi_k(x_k)) \Rightarrow \neg \phi_k(c_{x_k})) \Rightarrow \neg ((\exists x_k: \neg \phi_k(x_k)) \Rightarrow \neg \phi_k(c_{x_k})) \]
    %
    which allows us to conclude (by the tautology $(A \Rightarrow \neg A) \Rightarrow \neg A$), that $\Gamma_{k-1} \vdash \neg ((\exists x_k: \neg \phi_k(x_k)) \Rightarrow \neg \phi_k(c_{x_k}))$, hence
    %
    \[ \Gamma_{k-1} \vdash (\exists x_k: \neg \phi_k(x_k))\ \ \ \ \ \ \ \ \Gamma_{k-1} \vdash \phi_k(c_{x_k}) \]
    %
    Since $c_{x_k}$ does not occur in the axioms of $\Gamma_{k-1}$, we conclude that $\Gamma_{k-1} \vdash \phi_k(y_k)$, by replacing all occurences of $c_{x_k}$ in the proof of $\phi_k(c_{x_k})$ by a new variable $y_k$ which does not occur in the proof. But then $\Gamma_{k-1} \vdash (\forall y_k: \phi_k(y_k))$, so $\Gamma_{k-1} \vdash \phi_k(x_k)$, hence $\Gamma_{k-1} \vdash (\forall x_k: \phi_k(x_k))$, contradicting $(\exists x_k: \neg \phi_k)$, and implying that $\Gamma_{k-1}$ is inconsistant. Thus we have shown $\Gamma_\infty$ is consistant.
\end{proof}

\begin{lemma}
    Let $\Gamma$ be a consistant, complete, scapegoat theory. Then $\Gamma$ has a model $M$ whose universe of discourse is the set of closed terms in $\Gamma$.
\end{lemma}
\begin{proof}
    For each constant $c$ in the language, let $c^M = c$. For each function $f$, let $f^M(t_1, \dots, t_n) = f(t_1, \dots, t_n)$ (by assumption, each $t_i$ is closed). For each predicate $P$, let $(t_1, \dots, t_n) \in P^M$ if and only if $\vdash P(t_1, \dots, t_n)$. We shall show that $M \vDash \phi$ if and only if $\Gamma \vdash \phi$, for any closed formula $\phi$. This implies that $M$ is a model of $\Gamma$, for then, if $\phi$ is any axiom of $\Gamma$, then $\Gamma \vdash (\forall x_1, \dots, x_n: \phi)$, hence $M \vDash (\forall x_1, \dots, x_n: \phi)$, which occurs if and only if $M \vDash \phi$. Thus $M$ models all axioms. We will prove our statement by structural induction.
    %
    \begin{enumerate}
        \item If $\phi$ is $P(t_1, \dots, t_n)$, the statement is trivial by construction.

        \item Suppose $\phi$ is $\neg \psi$. If $M \vDash \neg \psi$, then $M \not \vDash \psi$, so $\Gamma \not \vdash \psi$, and so $\Gamma \vdash \neg \psi$ by completeness. Conversely, if $M \not \vDash \neg \psi$, then $M \vDash \psi$, so by induction $\Gamma \vdash \psi$, hence by consistency $\Gamma \not \vdash \neg \psi$.

        \item If $\phi$ is $\psi \Rightarrow \eta$, since $\phi$ is closed, $\psi$ and $\eta$ are closed. If $M \not \vDash \psi$, then $M \vDash \psi \Rightarrow \eta$, and by induction $\Gamma \not \vdash \psi$, so $\Gamma \vdash \neg \psi$, and then $\Gamma \vdash \psi \Rightarrow \eta$ by tautology. The remaining cases are again treated by tautology and completeness.

        \item If $\phi$ is $(\forall x: \psi)$, then either $\psi$ is closed, or $\psi(x)$ has a single free variable $x$. If $\psi$ is closed, the statement follows from tautologies and completeness fairly easily. We shall treat the other case in detail. Suppose that $M \vDash (\forall x: \psi(x))$, yet $\Gamma \not \vdash (\forall x: \psi(x))$. Thus $\Gamma \vdash \neg (\forall x: \psi(x))$. Since $\Gamma$ is scapegoat, there is a constant $c$ such that $\Gamma \vdash \neg \psi(c)$, from which we conclude $M \vDash \neg \psi(c)$ by induction, contradicting that $M \vdash (\forall x: \psi(x))$. Conversely, suppose $M \not \vDash (\forall x: \psi(x))$, yet $\Gamma \vdash (\forall x: \psi(x))$. Then $\Gamma \vdash \psi(t)$ for any term $t$, so by induction, $M \vDash \psi(t)$ for all closed terms $t$. Let $f$ be an assignment on $M$ such that which does not satisfy $\psi$. Let $f(x) = t$. Since $t$ is a closed term in the interpretation, $f_*(t) = t$, and $f = f[f_*(t)/x]$, so a previous lemma implies that $f$ cannot satisfy $\psi(t)$. Thus $M \not \vDash \psi(t)$, a contradiction.
    \end{enumerate}
    %
    We have addressed all cases, so our proof is complete.
\end{proof}

\begin{theorem}
    Every consistant theory has a model whose cardinality is the same as the theory itself, unless the theory is finite, in which the model is denumerable.
\end{theorem}
\begin{proof}
    Let $\Gamma$ be a consistant theory. Extend $\Gamma$ to a consistant, complete, scapegoat theory $\Gamma'$, which is denumerable if $\Gamma$ is finite or denumerable, or else $\Gamma$ has the same cardinality as $\Gamma'$. But then $\Gamma'$ has a model consisting of closed terms in $\Gamma'$, whose cardinality is the same as the $\Gamma$.
\end{proof}

We've built up enough theory to prove the first fundamental result about first order logic: ``G\"{o}del's completeness theorem''. Our version is the proof is constructed by Leon Henkin.

\begin{theorem}[G\"{o}del]
    A formula is provable in a theory if and only if it is true under all interpretations.
\end{theorem}
\begin{proof}
    Let $\Gamma$ be a theory. Without loss of generality, assume $\Gamma$ is consistant, because otherwise $\Gamma$ has no models, in which case the completeness theorem holds vacously. If $\Gamma \vdash \phi$, then we have already shown $\phi$ is true in all models. If $\Gamma \not \vdash \phi$, then $\Gamma \cup \{ \neg \phi \}$ is consistant, so $\Gamma \cup \{ \neg \phi \}$ has a model $M$, but then $M \vDash \neg \phi$, so $M \not \vdash \phi$, and $M$ is a model for $\Gamma$.
\end{proof}

Thus syntax and semantics coincide in the classical case for first order logic. We actually proved something much stronger, that a formula is provable in a theory if and only if it is true under all interpretations whose cardinality is that theories cardinality, or denumarable if the theory is finite. This is important, because it hints at further results that first order logic is not very good at distinguishing between models of different cardinalities. It is also important to note that our proof is non-constructive. We did not construct a formal proof given that the formula was true under all interpretations. This foreshadows the general result that there is no constructive procedure for generating a proof of a the formula, proved by G\"{o}del a decade later.

\section{The Compactness Theorem}

The fact that every consistent theory has a model is very important, because we can now use the finiteness condition of syntactic proofs with the semantic results about models to obtain powerful results about first order theories.

\begin{theorem}[The Compactness Theorem]
    $\Gamma$ is a consistant theory if and only if every finite subset of $\Gamma$ is consistant. Conversely, $\Gamma$ has a model if and only if every finite subset of $\Gamma$ has a model.
\end{theorem}
\begin{proof}
    If $\Gamma$ is consistant, than a finite subset is consistant. Conversely, suppose $\Gamma$ is inconsistant, and consider proofs of two statements $\Gamma \vdash \phi$ and $\Gamma \vdash \neg \phi$. These proofs only use finitely many axioms in $\Gamma$, so there is some finite subset $\Delta$ of $\Gamma$ such that $\Delta \vdash \phi$ and $\Delta \vdash \neg \phi$, so some finite subset is inconsistant.
\end{proof}

There are applications to the compactness theorem in widely varying areas of mathematics.

\begin{example}
    The theory of fields can be expressed as a first order theory, such that any normal model of this first order theory is a field. For any field $K$, enlarge the language of the first order family to contain all elements of $K$ as constants, and contain all algebraic relations in $K$ as axioms. This theory is certainly consistant, because $K$ is a model of this theory. Every normal model of this new theory is a field extension of $K$. For each polynomial $P \in K[X]$, consider the statement $(\exists x: P(x) = 0)$. If $P_1, \dots, P_n$ is a finite family of polynomials over $K$, then there is a field $L$ in which each polynomial has a root (Take the splitting field, for instance). By the compactness theorem, we conclude that there is a field $K_0$ extending $K$ in which every polynomial in $K[X]$ has a root. We may continue this process, obtaining a sequence of fields $K_0, K_1, K_2, \dots$ in which every polynomial in $K_i[X]$ has a root in $K_{i+1}$. Then $L = \bigcup K_i$ is a field, and every polynomial in $L$ has only finitely many coefficients, hence the coefficients lie in some $K_n$, and thus the polynomial has a root in $L$. We conclude that every field has an algebraic closure. This proof is much easier than the standard algebraic proof of this fact.
\end{example}

\begin{example}
    The theory of fields can be extended to the theory of fields of characteristic $p$, for some prime $p > 0$, by adding the axiom $p = 0$, where $p = 1 + \dots + 1$. A normal model of this theory is exactly a field of characteristic $p$. The fields of characteristic 0 are models of the theory obtained by addition the axioms $p \neq 0$ for any prime $p$. Let $s$ be a closed formula of the theory of fields which is true for all fields of characteristic 0. Then $s$ is provable using only finitely many of the axioms $p_1 \neq 0$, $p_2 \neq 0, \dots, p_n \neq 0$, so if the theorem remains true in fields of characteristic $p$, for $p \geq \max(p_1, \dots, p_n)$.
\end{example}

\begin{example}[De Bruijn Erd\"{o}s Theorem]
    A graph $G$ with possibly infinitely many vertices $V$ is $n$-colorable if there is a map $f: V \to \{ 1, \dots, n \}$ with $f(v) \neq f(w)$ if $(v,w)$ is an edge in $G$. We claim that if every finite subgraph of a graph is $n$-colorable, then the entire graph is $n$-colorable. Consider the first order theory with a 2-ary equality predicate $=$, a 2-ary `edge' predicate $E$, and $n$ predicates $P_1, \dots, P_n$. We also add constants $a_v$, corresponding to the nodes of $G$. In addition to the usual axioms of equality (see the next chapter), we consider the additional axioms
    %
    \[ \neg E(x,x)\ \ \ \ \ E(x,y) \Rightarrow E(y,x) \]
    \[ A_1(x) \vee A_2(x) \vee \dots \vee A_n(x) \]
    \[ \left( A_i(x) \wedge A_i(y) \right) \Rightarrow \neg E(x,y) \]
    %
    and for $i \neq j$, and $v \neq w$
    %
    \[ \neg A_i(x) \vee A_j(x)\ \ \ \ \ a_v \neq a_w \]
    %
    any normal model of this theory is an $n$-colorable graph containing $G$ as a subgraph, so it suffices to show the theory is consistent. Any proof of consistency only uses a finite number of the constants $a_{v_1}, \dots, a_{v_n}$, and the theory obtained by only considering these constants is consistence, because it has a model consisting of the finite subgraph of $G$ containing the vertices $v_1, \dots, v_n$, which is $n$-colorable by assumption. Applying the compactness theorem, we conclude the entire theory is consistent. Alternative proofs of this theorem without mathematical logic are very difficult, employing topological results like Tychonoff's theorem.
\end{example}

Consider the space of all consistant theories in a first order logic, and in particular, take the family $U_\phi = \{ \Gamma : \Gamma \vdash \phi \}$. Then $U_\phi$ defines a basis for a topology, for $U_\phi \cap U_\psi = U_{\phi \wedge \psi}$. A net $\{ \Gamma_\alpha \}$ converges to some theory $\Gamma$ in this topology if and only if every statement provable in $\Gamma$ is eventually provable in the net. Every element of the basis is clopen, for $U_\phi^c = U_{\neg \phi}$, so the space is completely disconnected. The compactness theorem is equivalent to the fact that the set $\mathcal{C}$ of complete, consistant theories being topologically compact, for inconsistant theories correspond to open covers of $\mathcal{C}$. If $\Gamma$ is an inconsistant theory, then $\{ U_{\neg \phi} : \phi \in \Gamma \}$ is a cover of $\mathcal{C}$. This follows for any complete theory not in any $U_{\neg \phi}$ must be able to prove every element of $\Gamma$, and therefore be inconsistant. Conversely, suppose $\mathcal{U}$ is an open cover of $\Gamma$ by sets of the form $U_\phi$. Then $\{ \neg \phi : U_\phi \in \mathcal{U} \}$ is inconsistant, for otherwise it is contained in a complete, consistant, deductively closed theory $\Gamma$, and $\Gamma \not \in U_{\neg \phi}$ for any $U_{\neg \phi} \in \mathcal{C}$.

Now suppose $\mathcal{C}$ was compact, and let $\Gamma$ be an inconsistant theory. Then $\{ U_{\neg \phi} : \phi \in \Gamma \}$ forms a cover of $\mathcal{C}$.  Thus $U_{\neg \phi}$ has a finite subcover. This corresponds to a finite inconsistant subtheory of $\Gamma$. We shall prove $\mathcal{C}$ is compact using the compactness theorem. Let $\mathcal{U}$ be an open cover of $\mathcal{C}$. Without loss of generality, assume each element of $\mathcal{U}$ is of the form $U_\phi$. Then $\Gamma = \{ \neg \phi : U_\phi \in \mathcal{U} \}$ is an inconsistant theory, and therefore has a finite inconsistant subtheory, which corresponds to a finite subcover of $\mathcal{C}$.

Similarily, consider the class $\mathcal{M}$ of all models of a first order system, and take a topology on it by considering the family $V_\phi = \{ M \in \mathcal{M} : M \vDash \phi \}$ of open neighbourhoods. Then $M_i \to M$ only when any formula $\phi$ satisfied by $M$ is eventually satisfied in $M_\phi$. For any model $M$, define $\text{Th}(M)$ to be the set of all {\it closed} formulas $\phi$ for which $M \vDash \phi$. Then $\text{Th}(M)$ is a consistant theory, which we claim is also complete. Let $\phi$ be any formula, and let $\phi'$ be its closure. Then either $M \vDash \phi'$ or $M \vDash \neg \phi'$, which implies either $\phi$ or $\phi'$ is in $\text{Th}(M)$, but then either $\text{Th}(M) \vdash \phi$ or $\text{Th}(M) \vdash \neg \phi$, by universal elimination. We claim that the map $M \mapsto \text{Th}(M)$, from $\mathcal{M}$ to $\mathcal{C}$, is continuous. Given a formula $\phi$, consider all $M$ such that $\text{Th}(M) \vdash \phi$. If $\phi'$ is the closure of $\phi$, then $U_\phi = U_{\phi'}$, so we may assume $\phi$ is closed. But then either $M \vDash \phi$ or $M \vDash \neg \phi$, and since $\text{Th}(M)$ is consistant, we must have $M \vDash \phi$. If $M \vDash \phi$, then obviously $\text{Th}(M) \vdash \phi$, so $\text{Th}^{-1}(U_\phi) = V_\phi$. The surjectivity of $\text{Th}$ is equivalent to the fact that every complete consistent theory has a model. However, we shall see in the next section that it is impossible for $\text{Th}$ to be injective, unless we limit our models to only having a certain cardinality. Since $\text{Th}(U_\phi) = V_\phi$ for each closed formula $\phi$, the map is also open, implying $\mathcal{M}$ is compact. A useful corollary is that

\begin{theorem}
    Any sequence of models contains a convergent subsequence.
\end{theorem}

This theorem has interesting consequences. First, note that if $\Gamma$ is a theory, then the set of all models of a theory $\Gamma$ is closed in $\mathcal{M}$, since the set of all models $M$ which model $\Gamma$ are also all models such that $\text{Th}(\Gamma)$ proves all of $\Gamma$, which is
%
\[ \text{Th}^{-1} \left(\bigcap_{\phi \in \Gamma} U_\phi \right) \]
%
and each $U_\phi$ is closed, since its complement is open.

\begin{example}
    Supplement the standard definition of $\NN$ by adding an additional constant $c$ to the theory. Technically, this increases the number of models of the theory, but any model of this theory can be restricted to an interpretation of Peano arithmetic. For each $n \in \NN$, let $\NN_n$ be the model of extended Peano arithmetic consisting of the standard $\NN$, but with $c$ interpreted as $n$. By compactness, there is a subsequence $\NN_{n_i}$ converging to some theory $\NN_{n_*}$ of extended peano arithmetic, where $n_*$ is the interpretation of $c$ in this model. For each $k$, since $M_{n_i} \vDash c > k$ for all $i$ sufficiently large, hence $M_{n_*} \vDash c > k$ for all $k$, so $n_* > k$. Thus $M_{n_*}$ is a peano model which has `infinitely large' elements.
\end{example}

Given a class $\mathcal{C}$ of mathematical objects, we say the class is \emph{axiomatizable} if there is an axiom system such that the only normal models are the class of mathematical objects. A class is \emph{finitely axiomatizable} if there is an axiom system describing the class with only finitely many axioms.

\begin{theorem}
    If $\mathcal{A}$ is finitely axiomatizable, and $\mathcal{A}$ is the class of models of some theory $\Gamma$, then some finite subset of $\Gamma$ axiomatizes $\Gamma$.
\end{theorem}
\begin{proof}
    If the theorems of $\Gamma$ are the same as theorems of some finite subset $\Lambda$, then the elements of $\Lambda$ are provable using only finitely many of the axioms in $\Gamma$, and these axioms are sufficient to axiomatize $\Gamma$.
\end{proof}

\begin{theorem}
    $\mathcal{A}$ is finitely axiomatizable iff $\mathcal{A}$ and $\mathcal{A}^c$ are axiomatizable.
\end{theorem}
\begin{proof}
    If $\Delta$ axiomatizes $\mathcal{A}$, and $\Pi$ axiomatizes $\mathcal{A}^c$, then $\Delta \cup \Pi$ axiomatizes $\mathcal{A} \cap \mathcal{A}^c = \emptyset$, so $\Delta \cup \Pi$ is inconsistant, the compactness theorem implies that there is some inconsistant $\phi_1, \dots, \phi_n \in \Delta$, and $\psi_1, \dots, \psi_m \in \Pi$. which are inconsistant. But then the models of $\{ \phi_1, \dots, \phi_n \}$ contain $\mathcal{A}$, and the models of $\{ \psi_1, \dots, \psi_m \}$ contain $\mathcal{A}^c$, hence the models are equal since there are no models which satisfy both axioms simultaneously.
\end{proof}

\begin{example}
    The theory of fields is finitely axiomatizable, as is the theory of fields of characteristic $p > 0$, for some specific $p$ (we just add the axiom $1 + \dots + 1 = 0$). The fields of characteristic $0$ is axiomatizable, by adding the axioms $p \neq 0$, for all integers $p \neq 0$. However the fields of positive characteristic is not finitely axiomatizable, because if it was then it would be finitely axiomatizable by the field axioms and $p_1 \neq 0, \dots, p_n \neq 0$ for some integers $p_1, \dots, p_n$, and there is a finite field satisfying these axioms.
\end{example}

\section{Skolem-L\"{o}wenheim}

We have seen that Lindenbaum's technique allows us to construct models from any consistent theory. It was Thoralf Skolem and Leopald L\"{o}wenheim, which focused on the cardinality of the models under consideration.

\begin{theorem}[Skolem-L\"{o}wenheim]
    Any consistent theory has a model whose cardinality is the same as the theory, or is denumerable if the theory is finite.
\end{theorem}
\begin{proof}
    The proof essentially follows from Lindenbaum's lemma.
\end{proof}

We are actually able to prove a much strong proposition.

\begin{corollary}
    If $\omega$ is an infinite cardinality greater than or equal to the cardinality of a consistant theory $\Gamma$, then there is a theory of size $\omega$.
\end{corollary}
\begin{proof}
    Let $M$ be a model of $\Gamma$. Fix $x \in U^M$. Extend $U^M$ to a set $U^N$ of cardinality $\omega$. Each new element will behave exactly like $x$. We define $f^N(t_1, \dots, t_n) = f^M(t_1', \dots, t_n')$, where $t_k' = t_k$ if $t_1 \in U^M$, else $t_k' = x$. Similarily, let $(t_1, \dots, t_n) \in P^N$ if and only if $(t_1', \dots, t_n') \in P^M$. Define constants the same as constants are defined in $U^M$. Then $N$ is a model of $\Gamma$ of cardinality $\omega$.
\end{proof}

This theorem appears to contradict various classical results of mathematics, such as the fact that any complete, ordered fields are isomorphic (and thus $\RR$ should be `the only model' of such fields, which contradicts the cardinality argument). Remember that first order theories are only a model of real mathematics, and thus do not sufficiently encapsulate all mathematical proofs. We therefore learn from the L\"{o}wenheim theorem that one cannot prove that $\RR$ is the only complete ordered field, using only the principles of first order logic. In this case, first order logic is not powerful enough to model distinct terms, because even if we have an equality predicate in our first order system, a model is not required to interpret the equality predicate as the standard equality predicate. Thus we can hide various model-theoretic `elements' of a theory inside a single syntactic element of the theory. It might be hoped that if we force models to interpret equality as actual equality, then we can prevent Lowenheim Skolem from occuring, but we shall find that we still have peculiarities.

We now specialize our study, adding additional axioms that occur in almost every useful first order model. We shall say a first order system possesses equality if the theory has a binary predicate $=$, possessing the additional axiom (A6), $x = x$, and (A7), $(x = y) \Rightarrow (\phi(t) \Rightarrow \phi(s))$, for any variables $x$ and $y$, and terms $t$ and $s$, where $s$ is obtained from $t$ by swapping some numbers of occurences of $x$ with occurences of $y$.

\begin{example}
    In any theory with equality, $t = t$ for any term $t$. This follows from $(A6)$ by substitution. Similarily, $t = s \Rightarrow s = t$ holds.
    %
    \[
    \fitchprf{\pline{(t = s \Rightarrow s = t)}}{
        \subproof{\pline[1.]{(x = y)}}{
            \pline[2.]{(x = y) \Rightarrow ((x = x) \Rightarrow (y = x))}[(A7)]\\
            \pline[3.]{(x = x) \Rightarrow (y = x)}[(1),(2),(MP)]\\
            \pline[4.]{(x = x)}[(A6)]\\
            \pline[5.]{(y = x)}[(3),(4),(MP)]
        }
        \pline[6.]{(x = y) \Rightarrow (y = x)}[(1-6),(DT)]\\
        \pline[7.]{(\forall x,y : (x = y) \Rightarrow (y = x))}[(UG)]\\
        \pline[8.]{(t = s \Rightarrow s = t)}[(7),(A4),(MP)]
    }
    \]
    %
    We shall also need the theorem $t = s \Rightarrow (s = r \Rightarrow t = r)$, which is an instance of (A7) after some universal generalization and specification.
\end{example}

We have already seen the theory of groups as a first order theory with equality. Here is another example.

\begin{example}
    The theory of fields is built on a first order theory with constants $0$ and $1$, and functions $+$ and $\cdot$. The new axioms (in addition to the axioms of equality) are
    %
    \[ x + (y + z) = (x + y) + z\ \ \ \ \ \ \ \ x + 0 = x\ \ \ \ \ \ \ \ (\forall x, \exists y: x + y = 0) \]
    \[ x + y = y + x\ \ \ \ \ \ \ \ x \cdot (y \cdot z) = (x \cdot y) \cdot z\ \ \ \ \ \ \ \ x \cdot (y + z) = (x \cdot y) + (x \cdot z) \]
    \[ x \cdot y = y \cdot x\ \ \ \ \ \ \ \ x \cdot 1 = x\ \ \ \ \ \ \ \ x \neq 0 \Rightarrow (\exists y: x \cdot y = 1)\ \ \ \ \ \ \ \ 0 \neq 1 \]
    %
    If we add another binary predicate symbol $<$, and add axioms
    %
    \[ x < y \Rightarrow x + z < y + z\ \ \ \ \ \ \ \ x < y \wedge 0 < z \Rightarrow x \cdot z< y \cdot z \]
    %
    Then we obtain the theory of ordered fields.
\end{example}

\begin{example}
    One of the most important historical axiom systems was a system for geometry. The new predicates of the system are $I$, $P$, and $L$. $P(x)$ means $x$ is a point, $L(x)$ means $x$ is a line, and $I(x,y)$ means $x$ lies on $y$ (is incident to). The new axioms are
    %
    \[ P(x) \Rightarrow \neg L(x)\ \ \ \ \ I(x,y) \Rightarrow P(x) \wedge L(y)\ \ \ \ \ L(x) \Rightarrow (\exists y \neq z: I(y,x) \wedge I(z,x)) \]
    \[ P(x) \wedge P(y) \wedge x \neq y \Rightarrow (\exists z: L(z) \wedge I(x,z))\ \ \ \ \ (\exists x,y,z: P(x) \wedge P(y) \wedge P(z) \wedge \neg C(x,y,z)) \]
    %
    where $C(x,y,z)$ is the collinear relation
    %
    \[ (\exists u: L(u) \wedge L(x,u) \wedge L(y,u) \wedge L(z,u) \]
    %
    The parallel postulate is not true in this system, because the theory has Euclidean, projective, and hyperbolic geometries as models, so that the theory is not complete.
\end{example}

In theories of equality it is possible to define new symbols which are useful for abreviating formulae. We define $(\exists ! x: \phi(x))$ to means that there is only one element $x$ satisfying $\phi$. That is, the symbol is short for
%
\[ (\exists x: \phi(x)) \wedge (\forall x,y: \phi(x) \wedge \phi(y) \Rightarrow x = y) \]
%
Thus there is a unique term with the property $\phi$.

In any model $M$ of a theory with equality, the relation $=^M$ is an equivalence relation. The model $M$ is \emph{normal} if the equivalence relation is trivial, in which case equality really does correspond to equality. Any model $M$ can be contracted into a normal model $M'$. Given a model $M$, we quotient $U^M$ by $=^M$ to obtain $U^{M'}$. For a function $f$ and predicate $P$, define
%
\[ f^{M'}([t_1],[t_2],\dots,[t_n]) = [f^M(t_1, \dots, t_n)] \]
%
\[ ([t_1],[t_2],\dots,[t_n]) \in P^{M'}\ \ \ \text{iff}\ \ \ (t_1, \dots, t_n) \in P^{M'} \]
%
These definitions are well defined, since the model interprets equality correctly. All axioms are correctly interpreted by the model as well. Since every consistent theory with equality has a normal model, the completeness theorem holds even if we restrict our attention to the normal models. We obtain an extension to the L\"{o}wenheim theorems in this scenario.

\begin{prop}
    Any consistant theory of equality has a normal model whose cardinality is less than or equal to the cardinality of the theory in question.
\end{prop}
\begin{proof}
    Take any (not necessarily normal) model of the theory, whose cardinality is equal to the cardinality of the theory, and then contract the model to obtain a normal model.
\end{proof}

Now we can state the true L\"{o}wenheim-Skolem theorem, since the other proof relied on a big cheat, which we cannot rely on in normal models.

\begin{corollary}[L\"{o}wenheim-Skolem]
    Any theory with equality with an infinite normal model of any infinite cardinality greater than the theory itself has normal models of any greater cardinality.
\end{corollary}
\begin{proof}
    Let $\Gamma$ be a consistant theory with an infinite normal model $M$. Given any cardinal $\omega$, add to $\Gamma$ new constants $c_\alpha$ for $\alpha \in \omega$, with axioms $c_\alpha \neq c_\beta$ if $\alpha \neq \beta$, forming a new theory $\Gamma'$. We claim $\Gamma'$ is consistant. Suppose that $\Gamma' \vdash \phi \wedge \neg \phi$, which is without loss of generality also a formula in the first order theory related to $\Gamma$. The proof of $\phi \wedge \neg \phi$ only used a finite number of axioms of the form $c_{\alpha_1} \neq c_{\beta_1}, \dots, c_{\alpha_m} \neq c_{\beta_m}$. Let $M$ be an infinite model of $\Gamma$. Since $M$ is infinite, we may extend the model to a model $M'$, with the same universe of discourse, interpreting $c_{\alpha_i}$ and $c_{\beta_i}$ in such a way that $c_{\alpha_i} \neq c_{\beta_i}$ is satisfied in $M$. But then $M'$ is a model for $\Gamma \cup \{ c_{\alpha_1} \neq c_{\beta_1}, \dots, c_{\alpha_m} \neq c_{\beta_m} \}$, which therefore must be consistant, a contradiction. Thus $\Gamma'$ is consistant, and has an model $M$ with cardinality is equal to $\omega$. The contradiction $M'$ of $M$ is a normal model, and must has cardinality $\omega$ since the interpretation of $c_\alpha$ and $c_\beta$ cannot be identified if $\alpha \neq \beta$.
\end{proof}

Another application of theories of equality is to enable us to introduce new constants and formulas to a logic, and add new axioms to the logic related to these constants and formulas, without increasing the number of theorems that can be proved about the original logic.

\begin{theorem}
    Let $\Gamma$ be a theory with equality, and suppose that we can prove $\Gamma \vdash (\exists ! x: \phi(x,y_1, \dots, y_n))$. Consider a new theory $\Gamma'$ with a new function symbol $f$, and the additional axiom $\phi(f(y_1, \dots, y_n),y_1, \dots, y_n)$, assuming that $f(y_1, \dots, y_n)$ is free for substitution for $x$ in $\phi(x,y_1, \dots, y_n)$. Then every formula $\eta$ in the new first order logic can be converted into a formula $\nu$ in the old first order logic such that $\Gamma' \vdash \eta \Leftrightarrow \nu$ and $\Gamma \vdash \nu$ if and only if $\Gamma' \vdash \phi$.
\end{theorem}
\begin{proof}
    Define a simple $f$ term to be a term of the form $f(y_1, \dots, y_n)$, where $f$ does not occur in any of the terms $y_i$. Consider the procedure which takes a formula $\eta$, finds the leftmost occurence of a simple $f$ term in the formula, and replaces it with a variable $x$ not yet occuring in $\phi$, obtaining a formula $\eta_0$. Consider the formula $\psi = (\exists x: \phi(x,y_1,\dots,y_n) \wedge \eta_0(x))$. Then we can prove $\Gamma' \vdash \eta \Leftrightarrow \psi$, because $\eta = \eta_0(f(y_1,\dots,y_n))$, and we have a proof
    %
    \[
    \fitchprf{\pline{\eta_0(f(y_1,\dots,y_n)) \Leftrightarrow (\exists x: \phi(x,y_1,\dots,y_n) \wedge \eta_0(x))}}{
        \subproof{\pline[1.]{\eta_0(f(y_1,\dots,y_n))}}{
            \pline[2.]{\phi(f(y_1,\dots,y_n),y_1,\dots,y_n) \wedge \eta_0(f(y_1,\dots,y_n))}\\
            \pline[3.]{(\exists x: \phi(x,y_1,\dots,y_n) \wedge \eta_0(x))}
        }
        \pline[4.]{\eta_0(f(y_1,\dots,y_n)) \Rightarrow (\exists x: \phi(x,y_1,\dots,y_n) \wedge \eta_0(x))}\\
        \subproof{\pline[5.]{(\exists x: \phi(x,y_1,\dots,y_n) \wedge \eta_0(x))}}{
            \subproof{\pline[6.]{\phi(c,y_1,\dots,y_n) \wedge \eta_0(c)}}{
                \pline[7.]{(\exists ! x: \phi(x,y_1,\dots,y_n))}\\
                \pline[8.]{c = f(y_1,\dots,y_n)}\\
                \pline[9.]{\eta_0(f(y_1,\dots,y_n))}
            }
            \pline[10.]{\eta_0(f(y_1,\dots,y_n))}
        }
        \pline[11.]{(\exists x: \phi(x,y_1,\dots,y_n) \wedge \eta_0(x)) \Rightarrow \eta_0(f(y_1,\dots,y_n))}
    }
    \]
    %
    Continuing this process $\eta_1, \eta_2, \dots, \eta_n$, each successively containing one less instance of the formula $f$, until we end up with a formula $\psi$ which contains no instances of $f$. By transitivity, we know that $\Gamma' \vdash \phi \Leftrightarrow \psi$. It will suffice to prove the theorem assuming that $\eta$ contains no instances of $f$ to being with, because if $\Gamma' \vdash \eta_n$ implies $\Gamma \vdash \eta_n$, we may work backwards to conclude that if $\Gamma' \vdash \eta$, then $\Gamma' \vdash \eta_n$, and therefore that $\Gamma \vdash \eta_n$. We also assume $\eta$ is a closed formula. Let $M$ be any normal model of $\Gamma$. Then for any $y_1, \dots, y_n \in M$, there is a unique $x$ such that $M \vDash \phi_M(x,y_1,\dots,y_n)$. If we define $f: M^n \to M$ by $f(y_1,\dots,y_n) = x$, then we may extend $M$ to an interpretation $M'$ of $\Gamma'$. If $\Gamma' \vdash \eta$, this implies that $M' \vDash \eta$, and therefore, since $M'$ has the same domain as $M$, that $M \vDash \eta$. Since every normal model of $\Gamma$ thereby satisfies $\eta$, we may apply the completeness theorem to conclude that $\Gamma \vdash \eta$.
\end{proof}

It is important to note that if $\phi$ has length $n$, then the formula obtained by removing occurences of $f$ converts $\phi$ to a formula of length $O(n)$, hence the process is not only computable, but efficiently computable.

\begin{example}
    In the theory of groups, we can prove that $(\exists ! y: x * y = e)$. We can therefore introduce a new function $f$ which satisfies $x * f(x) = e$, but we often denote $f(x)$ by $x^{-1}$, or in the theory of abelian groups, by $-x$. In the theory of abelian groups, we find that $(\exists ! z : x = y + z)$, hence we can introduce a function $g$ with $x = y + g(x,y)$, and we often denote $g$ by $(x - y)$.
\end{example}

\begin{example}
    In the theory of fields, we do not have the theorem $(\exists ! y : x \cdot y = 1)$, though we do have $(\exists ! y : x + y = 0)$, so we can introduce the $(-x)$ function. First note that $0 \cdot x = 0$ is a theorem of the theory of fields, because $0 \cdot x + 0 \cdot x = (0 + 0) \cdot x = 0 \cdot x$, and also $0 \cdot x + 0 = 0 \cdot x$, hence $0 = 0 \cdot x - 0 \cdot x = 0 \cdot x$. If $(\exists ! y : x \cdot y = 1)$ was a theorem of the theory of fields, then we could find introduce a constant $c$ such that $0 \cdot c = 1$, then $x = 1 \cdot x = (0 \cdot c) \cdot x = 0 \cdot (c \cdot x) = 0$, hence $x = 0$, and by substitution we find that $1 = 0$. Since the theory of fields is consistant (it has a model), we find that $\neg (\exists y: 0 \cdot y = 1)$. Nonetheless, we do have the theorem $(\exists ! y: (x = 0 \wedge y = 0) \vee x \cdot y = 1)$, hence we could define an inversion operation $x^{-1}$, such that $x^{-1}$ really is the multiplicative inverse of $x$ if $x$ is nonzero, and $x^{-1} = 0$ if $x = 0$. The problem is that the functions of standard first order logic can be applied to arbitrary terms of the logic. To obtain a more natural view of `partial functions', we would need to take a detour down the theory of types, invented by Bertrand Russell.
\end{example}

A nice result of the last theorem is that in theories with equality, one can replace functions and constants by predicates, so the objects are essentially redundant. Given any $n$-ary function $f$, we can introduce an $n$-ary predicate $P$, remove $f$ from the theory, and add the axiom $(\exists ! y: P(y,x_1,\dots,x_n))$, and replacing axioms involving $f$ with axioms involving the new predicate $P$.

\begin{example}
    In group theory, we can remove $*$ and e from the theory of groups by adding a 3-ary predicate $P$ and a 1-ary predicate $Q$, where we interpret $P(x,y,z)$ as ``z = xy'', and $Q(x)$ as ``x = 0''. We add the new axioms $(\exists ! z : P(x,y,z))$ and $(\exists ! x : Q(x))$, and convert the old axioms such that
    %
    \begin{itemize}
        \item $x(yz) = (xy)z$ becomes
        %
        \[ P(x,y,a) \wedge P(y,z,b) \wedge P(a,z,c) \wedge P(x,b,d) \Rightarrow c = d \]
        \item $ex = x$ becomes
        %
        \[ Q(x) \wedge P(x,y,z) \Rightarrow y = z \]
        %
        Similarily, $xe = x$ becomes
        %
        \[ Q(x) \wedge P(y,x,z) \Rightarrow y = z \]
        \item The associative law $x(yz) = (xy)z$ becomes
        %
        \[ P(x,y,a) \wedge P(a,z,b) \wedge P(y,z,c) \wedge P(x,c,d) \Rightarrow b = c \]
        \item The inversion law $(\exists y: xy = e)$ becomes
        %
        \[ (\exists y: Q(a) \wedge P(x,y,z) \Rightarrow a = z) \]
    \end{itemize}
    %
    Thus the theory involves no functions or constants to specify.
\end{example}

This is very important in the theory of logic programming languages, which attempt to describe the functions in computation as predicates as in the theorem, so that computation becomes expressed as a series of inference rules rather than as a description of the functions in the computation.

A formula $\phi$ is in \emph{prenex normal form} if it can be written as $(Q_1 x_1, \dots, Q_n x_n : \psi)$, where each $Q_i$ is either $\forall$ or $\exists$, and $\psi$ contains no quantifiers. We shall use our theory of equality to construct a prenex normal form equivalent to any given formula $\phi$. Even more strongly, we can also construct a \emph{skolem normal form} for any formula $\phi$, which is a prenex normal form in which all occurences of $\exists$ occur before occurences of $\forall$.

\begin{theorem}
    s
\end{theorem}

\section{Isomorphisms of Models}

It is obvious that theories do not have unique models, because we can obtain a new model $M'$ from any model $M$ by taking a different underlying set, considering a bijection onto this set, and then pushing the constants and formulas on one set to the other set. Nonetheless, we would like these models to consider these models as `the same model'.

Let $\Gamma$ be a first order theory. A \emph{$\Gamma$-morphism} between two models $M$ and $N$ of $\Gamma$ is a map $f: M \to N$ such that for any predicate $P$, and any $x_1, \dots, x_n \in M$ with $M \vDash P(x_1,\dots,x_n)$, $M' \vDash P(f(x_1), \dots, f(x_n))$, and for any first order function $g$, $f \circ g_M = g_{M'} \circ f$ (in particular, for constants $c$, $f(c_M) = c_{M'}$). A bijective $\Gamma$-morphism whose inverse is also a $\Gamma$-morphism is called a $\Gamma$-isomorphism, and we say that the two models are isomorphic.

\begin{theorem}
    If $f: M \to N$ is a $\Gamma$ morphism, then for any interpretation $g$ of the variables of $\Gamma$ in $M$ which satisfies some statement $\phi$, $f \circ g$ satisfies $\phi$ in $M'$, hence if $M \vDash \phi$, $N \vDash \phi$.
\end{theorem}

\begin{example}
    Let $G$ and $H$ be two groups, which are normal models of the first order theory of groups. A morphism between $G$ and $H$ in this theory is a map $f: G \to H$ such that $f(e) = e$, and $f(xy) = f(x) f(y)$. This is exactly a group homomorphism. The fact that $f(e) = e$ follows from $f(xy) = f(x) f(y)$ because $e$ is the unique idempotent element of $G$, and this manifests from the fact that we can replace the constant $e$ with the axiom $(\exists ! x, \forall y: xy = y)$, and the inversion axiom with $(\forall y, \exists x, \forall z: (xy)z = z \wedge z(xy) = z)$, without increasing or decreasing the models of the theory.
\end{example}

A theory is \emph{$\mathfrak{a}$ categorical} if the theory has a model of cardinality $\mathfrak{a}$ every two normal models of cardinality $\mathfrak{a}$ are isomorphic. It is the closest thing we can obtain in the semantic theory of first order logic to having unique theories, because the Lowenheim Skolem theorem says that we can infinite models of arbitrarily large cardinality if the theory has any model of infinite cardinality.

\begin{example}
    Let $\Gamma$ be the pure theory of equality with the additional axiom $(\exists x,y: x \neq y \wedge (\forall z: x = z \vee y = z))$. Then every normal model contains two elements, and every bijection between two normal models is a $\Gamma$-isomorphism, so the theory is 2-categorical. Similarily, we can find extensions of the pure theory of equality which are $n$-categorical for any finite $n$.
\end{example}

\begin{example}
    The theory of dense linear orderings with neither a first nor a last element is $\aleph_0$ categorical, but not $\mathfrak{a}$ categorical for any cardinality $\mathfrak{a} > \alpha_0$. Let $M$ and $N$ be two countable dense ordered sets, and consider two enumerations
    %
    \[ M = \{ x_1, x_2, \dots \}\ \ \ \ \ N = \{ y_1, y_2, \dots \} \]
    %
    We shall construct an order preserving invertible map between $M$ and $N$, which will therefore be an isomorphism between the two models. Define $f(x_1) = f(y_1)$, and assuming that $f$ has been defined for $x_1, x_2, \dots, x_n$, let $f(x_{n+1}) = y_i$, where $i$ is the smallest index such that if $x_j < x_{n+1}$ for $j < n$, then $f(x_j) < y_i$, and if $x_j > x_{n+1}$, then $f(x_j) > y_i$. It is always impossible to find such an index $i$, since the sets are densely ordered. After finding $x_{n+1}$, we pick the smallest index $i$ such that $y_i$ is not in the range of $f$, find the smallest index $j$ such that $f(x_j)$ is not yet defined, and such that $f$ remains order preseriving if $f(x_j) = y_i$. By induction, we find that $f$ can be defined for all $i$, that $f$ is injective, and that $f$ is also surjective.
\end{example}

\begin{example}
    The theory of vector spaces over the rational numbers can be defined as a simple extension of the theory of abelian groups such that if we define $ny = y + y + \dots + y$, then we add the axioms $(\exists ! y: ny = x$ for each positive integer $n \geq 2$. The models of this theory are exactly the vector spaces over the rational numbers, and the morphisms are the vector space morphisms. We know from basic linear algebra that these models are classified by the dimension of the underlying vector space, so that the theory is not $\aleph_0$ categorical, but the dimension of a vector space is always the cardinality of the vector space for uncountable cardinalities, so the theory is $\mathfrak{a}$ categorical for all uncountable cardinalities $\mathfrak{a}$.
\end{example}

\begin{example}
    If we add the axiom $x + x = 0$ to the theory of abelian groups, we instead obtain the theory of vector spaces over $\mathbf{F}_2$, and we find the theory if $\mathfrak{a}$ categorical for all cardinalities upon which an $\mathbf{F}_2$ vector space can be defined (any infinite cardinality, and any power of 2).
\end{example}

\begin{theorem}
    If a theory $\Gamma$ has $\mathfrak{a}$ symbols, and is $\mathfrak{b}$ categorical for some infinite infinite cardinality $\mathfrak{b} \geq \mathfrak{a}$, and has no finite models, then $\Gamma$ is complete.
\end{theorem}
\begin{proof}
    If $\Gamma \not \vdash \phi$ and $\Gamma \not \vdash \neg \phi$, then $\Gamma \cup \{ \phi \}$ and $\Gamma \cup \{ \neg \phi \}$ are consistent, and therefore both have $\mathfrak{b}$ categorical models $M$ and $N$ by the Lowenheim-Skolem theorems. Assuming without loss of generality that $\phi$ is a closed formula, it is clear that $M$ and $N$ cannot be $\Gamma$ isomorphic, because $M \vDash \phi$ and $N \vDash \neg \phi$.
\end{proof}

In particular, a theory of equality with no finite models that is categorical for any cardinality is complete. This rules out the categoricity of the theory of fields of characteristic zero and the theory of geometries with infinitely many points, for instance, since these theories are not complete.

If a theory $\Gamma$ has only countably many symbols, we say it is \emph{axiomatic} if there is an effective procedure for determining if a given formula is an axiom, that is, if it is an element of $\Gamma$.

\begin{theorem}
    For any complete, consistent, axiomatic theory $\Gamma$, there is an effective procedure to determine if any formula is a theorem of the theory.
\end{theorem}
\begin{proof}
    If a theory is axiomatic, there is a way to effectively enumerate all theorems of the theory. We first find an effective enumeration of all axioms, and enumerate all variables of the theory $x_1, x_2, \dots$. We will build the set of all theorems constructively, beginning with an empty set $\emptyset$. At the $k$th step,
    %
    \begin{enumerate}
        \item Add the $k$th axiom to the set.
        \item Attempt Modus Ponens on all elements of the set, and all applications of universal generalization on elements of the set, using only the variables $x_1, \dots, x_k$.
    \end{enumerate}
    %
    eventually we will add each axiom to the set, and we will eventually be able to apply any application of universal generalization, hence we will eventually add all theorems to the set. Given any formula $\phi$, either $\Gamma \vdash \phi$ or $\Gamma \vdash \neg \phi$, but not both, and we can decide whether $\phi$ is provable by waiting for either $\phi$ or $\neg \phi$ to end up in the enumeration.
\end{proof}

A theory with a way to determine if any given formula is a theorem of the theory is known as a \emph{decidable theory}. In a decidable theory, there are normally more elegant algorithms which determine if a given formula is a theorem of the system.

\begin{example}
    Consider the theory of densely ordered sets with no first nor last element. Given any formula $\phi$ in the theory (which we may assume closed), we can effectively convert $\phi$ to a prenex normal form $(Q_1 x_1, \dots, Q_n x_n: \psi)$, where $\psi$ contains no quantifiers. We may assume all the quantifiers are existential quantifiers, by replacing $(\forall x: \eta)$ with $\neg (\exists x: \eta)$. In the theory of densely ordered sets, we can replace $\neg (x = y)$ with $x < y \vee y < x$, and $\neg (x < y)$ with $x = y \vee y < x$, so we can effectively reduce any formula to a prenex normal form with no negations, and we may also assume that $\psi$ is in disjunctive normal form, so $\psi$ is a disjunct of clauses, where each clause is the conjunct of terms of the form $x < y$ and $x = y$. Now $(\exists x: \phi_1 \vee \phi_n)$ is equivalent to $(\exists x: \phi_1) \vee \dots \vee (\exists x: \phi_n)$, so it suffices to consider $(\exists x: \phi_1 \wedge \dots \wedge \phi_n)$, where each $\phi_i$ is an elementary predicate $y = z$ or $y < z$. If $\phi_1$ does not involve $x$, then $(\exists x: \phi_1 \wedge \dots \wedge \phi_n)$ is equivalent to $\phi_1 \wedge (\exists x: \phi_2 \vee \dots \vee \phi_n)$, so we may assume that each $\phi_i$ contains $x$. If $\phi_i$ is $x = x$, we may remove this term from the clause without changing the clauses validity. If $\phi_i$ is $x < x$ then replace the entire conjunct $(\exists x: \phi_1 \wedge \dots \wedge \phi_n)$ with $x < x$. If $\phi_i$ is $x = y$ for $y \neq x$, then replace all occurences of $x$ with $z$, and erase the existential quantifier over the particular clause we are considering. If no conjuncts of the clause are of this form, then we can write $\phi_1 \wedge \dots \wedge \phi_n$ as $y_1 < x \wedge \dots y_k < x \wedge x < y_{k+1} \wedge \dots \wedge x < y_n$, and we replace the entire clause with $y_i < y_j$, for each $i \leq k$ and $j > k$. In each case, we have replaced the clause containing one less quantifier. Repeating this process, we obtain an equation without quantifiers. If any components of the equation contain $x = y$ or $x < y$ for $x \neq y$, the theorem is not provable. Thus we are reduced to equations containing only $x = x$ or $x < x$, and we find that the equation is either a tautology or a contradiction, and hence is provable or unprovable. This method extends to prove the decidability of real-closed fields, and the theory of abelian groups.
\end{example}




\section{Logic Programming}

In its general form, deduction in first order logic is uncomputable. That is, there is no algorithm which returns a proof of any given provable formula in first order logic, or rejects the input if there is no proof. However, it is easy to see that there is an algorithm which, given some provable formula, constructs a proof of this formula -- we can simply enumerate all proofs in a clever manner, and then just check, given each proof, whether the conclusion of the proof is the statement we needed to prove. This means that it is not impossible to find algorithms which can find proofs of statements, and reject a large majority of statements which have no proof. The incompleteness theorem just implies that this algorithm must run into an infinite loop on some inputs. In logic programming, we find efficient ways of deciding whether some formula is provable, while trying to avoid infinite loops in a large majority of cases.

A \emph{definite clause} (also known as a \emph{Horn clause} after logician Alfred Horn) is a formula of the form $\phi_1 \wedge \phi_2 \wedge \dots \wedge \phi_n \Rightarrow \psi$, where $\phi_1, \dots, \phi_n, \psi$ are atomic predicates (so all the quantifiers are global over the entire formula). $\psi$ is known as the \emph{head} of the formula, and $\phi_1 \wedge \dots \wedge \phi_n$ the \emph{body}. Since $\phi_1 \wedge \dots \wedge \phi_n \Rightarrow \psi$ is logically equivalent to $\psi \vee \neg \phi_1 \vee \dots \vee \neg \phi_n$, which tells us that the universal quantification over variables which occur only in the body of the formula become existential quantifiers over the body. That is, if $x$ does not occur in $\psi$, then $(\forall x: \phi_1 \wedge \dots \wedge \phi_n \Rightarrow \psi)$ is equivalent to $(\exists x: \phi_1 \wedge \dots \wedge \phi_n) \Rightarrow \psi$. When we express a definite clause as a disjunction of clauses, it is possible to concisder clauses with an empty body, or an empty head, and we shall treat these as definite clauses. Thus, by $\Rightarrow \psi$ we mean the logical formula $\psi$, and by $\phi_1, \dots, \phi_n \Rightarrow$ we mean $\neg \phi_1 \vee \dots \neg \phi_n$. We shall call a definite clause with an empty body a \emph{fact}, and a clause with an empty head a \emph{definite goal}. We will find that this particular subfamily of first order logic is somewhat effectively computable.

The problem of logic programming is to determine if a formula $\psi$ of the form $\neg (\phi_1 \wedge \dots \wedge \phi_n)$, known as a \emph{definite goal}, is the logical consequence of a finite set of definite clauses, known as a \emph{definite program}. If we universally quantify this formula over the variables $x_1, \dots, x_n$, and then carry through a negation, we find this formula is equivalent to the formula $\neg (\exists x_1, \dots, x_n: \phi_1 \wedge \dots \wedge \phi_n)$, so the problem is equivalent to determining if $\phi_i$ are simultaneously satisfiable. We could even consider this problem in a more complex, constructivist form, which is to find a particular set of closed terms $t_1, \dots, t_m$ such that $\phi_1(t_1, \dots, t_m), \dots, \phi_n(t_1, \dots, t_m)$ all hold simultaneously. Suprisingly, it turns out for the class of theories which consist of finitely many Horn clauses, both problems can be solved constructively.

Given any theory $\Gamma$, assumed to be defined over a language containing one or more constant symbols, the \emph{Herbrand universe} of the theory is the set of all closed terms in the theory (in this context, a closed term is also called a \emph{ground term}). On any Herbrand universe, we have standard interpretations of the functions and constants of the theory, and thus of all the terms of the theory. The set of all atomic predicates formed from the ground terms (also known as a \emph{ground formula}) is known as the \emph{Herbrand base} of the theory. A particular interpretation of the predicates gives what is called a \emph{Herbrand interpretation} of the theory, and a \emph{Herbrand model} of a theory is a Herbrand interpretation is actually a model. Herbrand universes are particularly useful to the semantics of definite programs. First, they give an easy proof that all definite programs are consistant theories, because the interpretation $H$ such that $H \vDash P(t)$ for all ground atoms $P(t)$ is a Herbrand model of the definite program.

\begin{example}
    Consider the definite program consisting of the constant $0$, the successor function $S$, and the predicate $\text{odd}$, and with the definite clauses $\text{odd}(s(0))$ and $\text{odd}(x) \Rightarrow \text{odd}(s(s(x)))$. The Herbrand universe $H$ of this program consists of the terms $\{ 0, s(0), s(s(0)), \dots \}$, which is essentially the same as the set $\NN$ of natural numbers, and a Herbrand interpretation assigns a particular subset of the terms to be odd. The Herbrand models correspond to subsets $X \subset H$ assigned to the predicate $\text{odd}$ which contain all the odd natural numbers.
\end{example}

\begin{theorem}
    If $\phi_1, \dots, \phi_n$ is a definite program, and $\psi$ is a goal, then for any model $M$ of $\{ \phi_1, \dots, \phi_n, \psi \}$, if we consider the interpretation $P_H$ on the Herbrand universe $H$ for each predicate $P$ given by $P_H(t)$ if $M \vDash P(t)$ gives a Herbrand model of $\{ \phi_1, \dots, \phi_n, \psi \}$.
\end{theorem}
\begin{proof}
    If $\phi_i$ is $\eta_1(x_1, \dots, x_k) \wedge \dots \wedge \eta_m(x_1, \dots, x_k) \Rightarrow \nu(x_1, \dots, x_k)$, then $M \vDash \phi_i$, hence in particular if $t_1, \dots, t_k$ are closed terms of the theory, then we find $M \vDash \eta_1(t_1, \dots, t_k) \wedge \dots \wedge \eta_m(t_1, \dots, t_k)$, hence in particular if $H$ is the Herbrand universe with the interpretation given above, then $H \vDash \phi_i(t_1, \dots, t_k)$ for all closed terms $t_1, \dots, t_k$, hence $H \vDash \phi_i$. If $\psi$ is the formula $\neg (\psi_1(x_1, \dots, x_k) \wedge \dots \wedge \psi_n(x_1, \dots, x_k))$, then $M \vDash \psi$, so in particular $M \vDash \neg (\psi_1(t_1, \dots, t_k) \wedge \dots \wedge \psi_n(t_1, \dots, t_k))$ for all closed terms $t_1, \dots, t_n$, hence we find that $H \vDash \neg(\psi_1 \wedge \dots \psi_n)$.
\end{proof}

\begin{corollary}
    If a goal $\psi$ is satisfiable given $\phi_1, \dots, \phi_n$ (which is equivalent to finding a model of the program and the goal), then there is a Herbrand model of $\phi_1, \dots, \phi_n$ in which $\psi$ is satisfiable.
\end{corollary}

This is a fact which {\it does not} hold for general theories. That is, to verify a goal is satisfiable in a general theory, it is not sufficient to check all the Herbrand models.

\begin{example}
    There are no Herbrand models of the pair of the pair formulas $\neg P(c)$ and $(\exists x: P(x))$, because the Herbrand universe is just the singleton set $\{ c \}$, whereas there are certainly models of this pair, so the theory consisting of the pair is consistant.
\end{example}

An important property of Herbrand models of definite programs is that if $\{ M_\alpha \}$ is a family of Herbrand models, then $\bigcap M_\alpha$ is also a Herbrand model. A Herbrand model exists, because we can always take the full relation on any given family of formulas. Thus by taking the intersection of all Herbrand models of a theory, we can consider a \emph{least Herbrand model}, which contains all the necessary semantics of the definite program.

\begin{theorem}
    If $M$ is the least Herbrand model of a given definite program $\Gamma$, then $\Gamma \vdash \phi$ holds if and only if $M \vDash \phi$.
\end{theorem}
\begin{proof}
    First, note that since $M$ is a model of $\Gamma$, then $\Gamma \vdash \phi$ implies $M \vDash \phi$. Conversely, define a Herbrand interpretation $H$ by letting $H \vDash P(t)$ if $\Gamma \vdash P(t)$. Then $H$ is actually a Herbrand model of $\Gamma$, because for any formula $\phi \in \Gamma$, it is trivial to show $\Gamma \vdash \phi$. But this implies that since $M$ it the {\it least} Herbrand model, if $M \vDash P(t)$ then $H \vDash P(t)$, hence $H \vdash P(t)$.
\end{proof}

\begin{example}
    Consider the formula $P(a) \vee Q(b)$. We have two Herbrand models $H_1, H_2$ of this formula, where $H_1 \vDash P(a)$ and $H_1 \vDash \neg Q(b)$, and $H_2 \vDash \neg P(a)$ but $H_2 \vDash Q(b)$. The intersection of these two models is not a Herbrand model of the formula, so the model property does not hold. This is because the formula cannot be specified as part of a definite program.
\end{example}

Though the model intersection property does hold for Herbrand models of any logical theory, we cannot necessarily take the intersection of arbitrary models of a theory because they may be defined over a particular set, and we have seen that we cannot reduce the study of the semantics of that logical theory to the semantics defined over a Herbrand universe. This is what makes the study of definite programs so special. In order to verify that $\phi_1 \vee \dots \vee \phi_n$ is satisfiable in some model, we just need to check if $\neg (\phi_1 \vee \dots \vee \phi_n)$ is {\it not} provable in the theory, which means that the minimal Herbrand model has closed terms $t_1, \dots, t_m$ such that $\phi_1(t_1, \dots, t_m) \vee \dots \vee \phi_n(t_1, \dots, t_m)$. Thus, given $\phi_1 \vee \dots \vee \phi_n$ and a definite program $\Gamma$, the final piece of the puzzle is to give an algorithm to find closed terms in the minimal model of $\Gamma$ which satisfy the formula.

Our algorithm to find these closed terms will be given recursively, which will correspond to a constructive method of finding the minimal Herbrand model. We cannot simply use provability to construct the model, because the whole point of our discussion was to verify a way to prove facts in definite programs, and we have no constructive way to prove particular statements. In turn, the inductive construction will give a recursive way to verify if an arbitrary definite goal is in the minimal model.
%
\begin{itemize}
    \item If the definite program contains a fact $\psi(x_1, \dots, x_k)$, then $\psi(t_1, \dots, t_k)$ is in the minimal model for any closed terms $t_1, \dots, t_k$.
    \item If the definite program contains a rule $\phi_1(x_1, \dots, x_k), \dots, \phi_k(x_1, \dots, x_k) \Rightarrow \psi(x_1, \dots, x_k)$, and $\psi_1(t_1, \dots, t_k), \dots, \psi_k(t_1, \dots, t_k)$ are in the minimal model, then $\psi(t_1, \dots, t_k)$ are in the minimal model.
\end{itemize}
%
These two rules, applied inductively, suffice to construct a Herbrand interpretation which is the minimal Herbrand model. We can actually have an algorithm to find all closed terms which satisfy a given goal $\psi_1, \dots \psi_n$. We just iterate through each rule of the definite program of the form $\phi_1 \wedge \dots \wedge \phi_m \Rightarrow \eta$, where there is some terms $t_1, \dots, t_m$ with $\eta(t_1, \dots, t_m) = \psi_1(t_1, \dots, t_m)$, and then we find all closed terms which satisfy the recursive goal $\phi_1(t_1, \dots, t_m), \dots, \phi_k(t_1, \dots, t_m), \psi_2(t_1, \dots, t_m), \psi_n(t_1, \dots, t_m)$ (this is the process of \emph{unification}, which we will algorithmically specify shortly). This is sufficient to find all the required closed terms. Unfortunately, there can be infinitely many terms which satisfy the formula, and this iteration process is not guaranteed to terminate, but in certain cases it gives a surefire way of constructing closed terms.

\begin{example}
We should not expect the process to terminate in the general case (or even to determine if a particular computable subset of closed terms satisfies the set), because we can reduce the Post correspondence problem into constructing closed terms of a definite program, which we know to be uncomputable. Indeed, the problem is given an alphabet $\Lambda$, and finitely many words $\alpha_1, \dots, \alpha_n$ and $\beta_1, \dots, \beta_n$ over the alphabet, the correspondence problem is to determine whether there are indices $i_1, \dots, i_m$ with $\alpha_{i_1} \dots \alpha_{i_n} = \beta_{i_1} \dots \beta_{i_n}$. If we write $\alpha_i = x_1^i \dots x_{i_N}^i$, and we consider the definite program
%
\begin{align*}
    &\vdash x = x\\
    &\vdash c(x,c(y,z)) = c(c(x,y),z)\\
    &\vdash \alpha_i = c(x_1^i, c(x_2^i, \dots, c(x_{i_N-1}, x_{i_N})))\\
    &\vdash \alpha_i = c(s)\\
    y = x &\vdash x = y\\
    x = y, y = z &\vdash x = z\\
    x_0 = x_1, y_0 = y_1 &\vdash c(x_0,x_1) = c(y_0,y_1)\\
\end{align*}
%
with addition constants given by the alphabet $\Lambda$, and where we think of $c$ as the composition of two words, then determining if there are closed terms $t_0,t_1$ over the universe consisting of the variables $\alpha_i$ and $\beta_j$ and composition such that $t_0 = t_1$ is equivalent to the Post correspondence problem.
\end{example}

\begin{example}
    Consider the definite program
    %
    \begin{align*}
        &\vdash \text{odd}(s(0))\\
        \text{odd}(x) &\vdash \text{odd}(s(s(x)))
    \end{align*}
    %
    To find all terms satisfying $\text{odd}(y)$, we first find take reduce the term to the fact $\text{odd}(s(0))$, hence $s(0)$ satisfies the terms. Next, we find that letting $y = s(s(x))$ makes the goal unify with the head of the second axiom of the program, hence all other closed terms satisfying the term $\text{odd}(y)$ are of the form $\text{odd}(s(s(x))$, where $\text{odd}(x)$ is true. Continuing recursively, we find that $s(s(s(0)))$ is a closed term satisfying the clause, then $s(s(s(s(s(0)))))$, and so on and so forth. The algorithm never terminates, but successfully enumerates all the terms satisfying the clause. That is, the algorithm enumerates all the odd natural numbers.
\end{example}

We now formally define unification, and define an algorithmic way to unify terms. It will be helpful here to introduce some new notation. A substitution will be viewed as a map $\sigma$ taking the variables in the logic and returning some term. Then $\sigma$ can be uniquely extended to be defined on the set of all terms. Often we only need to understand the definition of some subset of some variables $X_1, \dots, X_m$, in which case we write $\sigma = \{ t_1/X_1, \dots, t_n/X_n \}$ (this does not uniquely specify $\sigma$, but it specifies the map `uniquely enough' for most purposes). Given two substitutions $\sigma$ and $\tau$, the composition $\sigma \circ \tau$ is just the composition of the two maps extended to terms. A substitution is idempotent if $\sigma^2 = \sigma$. If $t_1, \dots, t_n$ and $u_1, \dots u_n$ are two families of terms, then we say they unify if there is a substitution $\sigma$ such that $\sigma(t_i) = \sigma(u_i)$ holds for each $i$, and we call $\sigma$ a \emph{unifier}. There is also an inductive definition which will correspond to a (non deterministic) algorithm to unify two lists of terms.
%
\begin{itemize}
    \item For any two constants $c$ and $k$, $c$ unifies with $k$ if and only if $c = k$.
    \item For any variable $X$ and term $t$ which does not contain $X$, $X$ unifies with $t$. If $t$ contains $X$, then $t$ does not unify with $X$.
    \item If $f$ and $g$ are formulas, then $f(t_1, \dots, t_n)$ and $g(u_1, \dots, u_n)$ unify if and only if $t_1, \dots, t_n$ and $u_1, \dots, u_n$ unify, and $f = g$.
    \item $t_1, \dots, t_n$ and $u_1, \dots, u_n$ unify if and only if $t_i$ unifies with $u_i$ for each $i$, and the unification is the same for each $i$.
\end{itemize}
%
A unification $\sigma$ between $t_1, \dots, t_n$ and $u_1, \dots, u_n$ is more general than another unification $\tau$ if the corresponding substitution can unify if $\tau = \eta \circ \sigma$ for some other substitution $\eta$. This gives a partial ordering on the set of all unifications, and a maximum element is called the \emph{most general unifier}. The idea is that if we try to unify $t_1, \dots, t_n$ and $u_1, \dots, u_n$ by constructing a most general unifier between each $t_i$ and $u_i$, then it is still possible to concatenate this unifier with the other unifiers to obtain a universal unifier, unless such a unification process is impossible.
%
\begin{itemize}
    \item For any two formulas $f(t_1, \dots, t_n)$ and $g(u_1, \dots, u_n)$, we cannot unify if $f \neq g$, and if $f = g$ then it suffices to find a most general unifier for $t_1, \dots, t_n$ and $u_1, \dots, u_n$.
    \item A most general unifier for the identity unification $X = X$ is the identity substitution.
    \item A most general unifier for $t = X$ or for $X = t$, where $t \neq X$ is impossible to construct if $t$ contains an occurence of $X$, and otherwise it is obtained by swapping $X$ with $t$.
    \item To unify $t_1, \dots, t_n$ and $u_1, \dots, u_n$, obtain a most general unifier $\sigma$ between $t_1$ and $u_1$, and $\tau$ between $t_2, \dots, t_n$ and $u_2, \dots, u_n$, and then consider the concatenation $\sigma \circ \tau$, which will be a most general unifier.
\end{itemize}
%
If we can construct a most general unifier 

\chapter{Combinatory Logic}

Standard propositional logic formalizes the mathematical process of proof, pincering mathematical statements in a formal system where they can be examined in detail. Rather than looking at statements which can be verified true or false, Combinatory logic instead analyzes higher order functions, and our limitations in defining them. These functions are universally found in mathematics. The statement $(\forall x: \neg P(x) \vee Q(x))$ contains the functions $\neg$, $\vee$, $P$, $Q$, and $\forall$, and it is semantically interesting to interpret the statement not as a schema which can be applied to assert truths about objects of some domain of discourse, but instead as a single truth about the logical function obtained by composing the atomic logical operations. Combinatory logic studies the formal representation of these higher order functions, revealing the limitations in the functions we can define.

The primal nature of the substitution operator was discovered early on in the theory of combinatory logic. It is fundamental tool for constructing functions. For instance, if we represent numbers as tallies, so that $1$ is $\cdot$, $2$ is $\cdot \cdot$, and 7 is $\cdot \cdot \cdot \cdot \cdot \cdot \cdot$, then we can add 3 to any number by substituting it into the expression $\cdot \cdot \cdot x$ for the variable $x$. Similarily, we may multiply a number by two by substituting it into the equation $xx$ for $x$. Addition and multiplication is thus a special case of substitution, and we shall find that a more complicated specification of this process will suffice to represent any function. Substitution is so important to combinatory logic that the field is often seen as the formal analysis of substitution.

Combinatorial logic was initially designed to provide a foundation for mathematics, where we can view inference rules as operators on formulas which can be formally analyzed. The operation of substitution plays a subtle role in much of this process, and this encouraged Combinatorial logic forerunner Moses Sch\"{o}nfinkel to introduce substitution as an explicit connective in a formal system. It turns out that unrestricted substitution is an incredibly volatile operator, especially in the context of first order logic. While it provides expressive power, it also opens the floodgate to paradoxes which can easily lead to an inconsistant system. Even if these careless systems are refined, the systems are likely not expressive enough to handle all mathematical concepts. Nonetheless, the formal systems of combinatory logic are sufficiently expressive to describe all realistic forms of computation, and it is this application which makes the theory useful to modern computing science.

\section{The $\lambda$ Calculus}

The $\lambda$-calculus is the most famous formalization used to study combinatory logic. Every formula in the calculus represents an operator over the other formulas in the calculus, and there are only two fundamental connectives we can use to manipulate these operators. The first is \emph{function application}, which takes a function $f$ and applies it to an argument $a$, denoted $(f a)$. The second is \emph{function abstraction}, which transforms a term $t$ into a function $(\lambda x.t)$, which takes some argument $a$ and substitutes it for $x$ in $t$. We then use substitution rules on terms to express computation, and this is where the real fun of $\lambda$ calculus begins.

Terms of the $\lambda$ calculus can be applied arbitrarily to other terms, include the term itself. This distinguishes the view of functions in set theory, where it is impossible to consider a function application $f(f)$ without contradicting the axiom of regularity. Thus terms of the $\lambda$ calculus model something very different to standard set theoretic functions. To distinguish this form of the calculus from other formal systems where the operators have particular domains, we call this the \emph{untyped} version of the $\lambda$ calculus. Nonetheless, unrestricted application is not a deficiency in the calculus -- it has a reasonable interpretation in mathematics, which we will discuss later, and is required to construct the class of all computable functions.

So lets define the terms of $\lambda$ calculus formally. First, we consider a symbol set consisting of variables, constants, the abstractor symbol $\lambda$, parenthesis, and a period as a separator. We define terms to be the smallest set of strings over these symbols such that
%
\begin{itemize}
    \item All variables and constant are terms.
    \item If $M$ and $N$ are terms, then $(MN)$ are terms.
    \item If $M$ is a term, and $x$ is a variable, then $(\lambda x.M)$ is a term.
\end{itemize}
%
If there are no constants, we call this a \emph{pure} version of the $\lambda$ calculus, because there are no constants, so every term of the calculus is a pure function. There is essentially no distinction between constants and variables in the $\lambda$ calculus, except that we can perform $\lambda$ abstraction on variables, so we won't consider constants in these notes.

We introduce some shorthand to ensure we don't get overwhelmed by parenthesis. First, we let terms associate to the left, so that $N_1 \dots N_n$ is shorthand for the term $( \dots ((N_1 N_2) N_3) \dots ) N_n)$, and we write $(\lambda x_1 \dots x_n.M)$ for the term $(\lambda x_1.(\lambda x_2.(\dots(\lambda x_n.M)\dots)))$. This shorthand is meant to let of think of a series of $\lambda$ abstractions as representing a multidimensional function. This is the technique of Currying: we can think of a function which takes two arguments as a function which takes a single argument, and then returns a function which takes another argument to calculate the overall result!

On the set of terms of the $\lambda$-calculus, we introduce \emph{reduction rules}, which not only simplify formulas, but act as the formal model of computation required for the theory.
%
\begin{itemize}
    \item \emph{$\alpha$ reduction} is a safe way to rename variables. If $M$ is a term, and $y$ is a variable not occuring anywhere in $M$, then we have a one-step $\alpha$ reduction $(\lambda x.M) \rhd_{\alpha,1} (\lambda y.M[y/x])$. More generally, if $M$ contains a subterm of the form $(\lambda x.N)$, and if $M'$ is the operation formed by replacing an occurence of $(\lambda x.N)$, then we will let $M \rhd_{\alpha,1} M'$. The transitive closure of the $\rhd_{\alpha,1}$ relation will be denoted $\rhd_\alpha$.

    \item \emph{$\beta$ reduction} introduces a semantic computational step into the calculus. Given a \emph{$\beta$ redex}, a term which can be written $(\lambda x.M)N$, we write $(\lambda x.M)N \rhd_{\beta,1} M[N/x]$, provided that $N$ is `safe to substitute' for $x$ in $M$. This means that every free occurrence of $x$ in $M$ occurs in a subterm of the form $(\lambda y.M_0)$, where $y$ is a free variable in $N$. Similarily, we will $\rhd_{\beta,1}$ be allowed on subterms of a general term, and the transitive closure will be denoted $\rhd_\beta$.

    \item A relation $\rhd$ satisfies the \emph{$\xi$ rule} if $M \rhd N$ implies $\lambda x.M \rhd \lambda x.N$.

    \item A relation $\rhd$ is closed under composition if $M_0 \rhd N_0$ and $M_1 \rhd N_1$ implies $(M_0 M_1) \rhd (N_0 N_1)$.
\end{itemize}
%
The general reduction relation, which we shall denoted by $\rhd$ or $\rhd_{\lambda}$, is the smallest transitive relation $\rhd$ containing $\alpha$ and $\beta$ reduction. It satisfies the $\xi$ rule, and is closed under composition, because $\alpha$ and $\beta$ reduction are allowed over subterms. Conversely, $\rhd$ is also the smallest transitive relation containing $\alpha$ and $\beta$ reduction {\it not} over subterms, closed under composition, and satisfying the $\xi$ rule. As an example, since
%
\[ (\lambda yx.xy)xy \rhd_\alpha (\lambda y_0x_0.x_0y_0)xy \rhd_\beta yx \]
%
we find that $(\lambda yx.xy)xy \rhd yx$. If we enlarge reduction to be the smallest reflexive, symmetric relation, we obtain the notion of equivalence, which we write as $M =_\lambda N$, or if we want to emphasize the proof theoretic aspects, as $\lambda \vdash M = N$.

As $\rhd$ is the smallest transitive relation containing $\alpha$ and $\beta$ reduction, we obtain an inductive method to prove that a property $R(x,y)$ over the terms of the $\lambda$ calculus satisfies $R(M,N)$ for any $M$ which reduces to $N$. It suffices to verify that
%
\begin{itemize}
    \item $R$ is transitive.
    \item $R((\lambda x.M)N, M[N/x])$, where $N$ is safe for substitution for $x$ in $M$.
    \item If $R(M_0, M_1)$ and $R(N_0, N_1)$ hold, then $R(M_0 N_0, M_1 N_1)$ also holds.
    \item If $R(M,N)$ holds, then $R(\lambda x.M, \lambda x.N)$ holds.
\end{itemize}
%
because then $R$ is a transitive relation containing $\alpha$ and $\beta$ reduction, and satisfying the $\xi$ and composition rule, hence $R$ contains the relation of reduction, and so if $M \rhd N$, then $R(M,N)$ necessarily holds. If $R$ is symmetric and reflexive, then we can also conclude that if $M =_\lambda N$, then $R(M,N)$ holds.

It is important to consider how important it is that we only perform $\beta$ reduction on terms safe for substitution. If not, then we could conclude that
%
\[ (\lambda xy.x)y \rhd_\beta (\lambda y.y) \]
%
\[ (\lambda xy.x)y \rhd_\alpha (\lambda xy_0.x)y \rhd_\beta \lambda y_0.y \]
%
We would like to interpret reduction as contracting a function definition to a shorter definition which defines an equivalent function. However, the two functions above certainly should not be equivalent. Indeed, the first represents the identity function $f(x) = x$, and the second represents the constant function $g(x) = y$, for some $y$. Even without a semantic interpretation, we can further use these reductions to conclude that
%
\[ \lambda \vdash M = (\lambda y.y)M = (\lambda y_0.y)M = y \]
%
so we find that any two terms of the $\lambda$ calculus are equal. This is clearly not desirable in an actual theory for representing interesting classes of operators.

We say a term $M$ is in \emph{$\beta$ normal form} if it contains no subterms which form a $\beta$ redex. This means exactly that the only reductions possible from $M$ are $\alpha$ reductions: If $M \rhd N$, then $M \rhd_\alpha N$. Since $\alpha$ conversion isn't much of an interesting calculation, we view $\beta$ normal forms as forms which have been completely computed. We can then interpret $\beta$ normal forms as terms representing algorithms which eventually halt.

\begin{example}
    The term $(\lambda x.(\lambda y.yx)z)a$ has normal form $za$, because
    %
    \[ (\lambda x.(\lambda y.yx)z)a \rhd_\beta (\lambda x.zx)a \rhd_\beta za \]
    %
    and $za$ does not contain any $\beta$ redexes.
\end{example}

\begin{example}
    The term $\Omega = (\lambda x.xx)(\lambda x.xx)$ has no normal form, because $\alpha$ reduction only changes the term to $(\lambda y.yy)(\lambda z.zz)$, for some variables $y$ and $z$, and $\beta$ reduction on any term of this form results in $(\lambda z.zz)(\lambda z.zz)$. Thus the set of all compositions of terms of this form is closed under $\beta$ and $\alpha$ reduction, and all of them contain a $\beta$ redex, so we conclude that $\Omega$ has no normal form. It is singular because it is the only term up to $\alpha$ conversion which cannot be reduced to some other term.
\end{example}

There is a more formal way to define the class of all normal forms. We take the following inductive definition:
%
\begin{itemize}
    \item All variables are in normal form.
    \item If $M$ is in normal form, and $x$ is a variable, then $xM$ is in normal form.
    \item If $M$ is in normal form, then $\lambda x.M$ is in normal form.
\end{itemize}
%
Surely any element of this grammar is in normal form. Conversely, if $xM$ is in normal form, then so too is $M$ because the subterms of $M$ cannot be $\beta$ redexes, and if $(\lambda x.M)$ is in normal form, then so too is $M$ because $M$ is in normal form. Thus the problem of deciding whether a term of the $\lambda$ calculus is in $\beta$ normal form can be expressed as a context free grammar, a computable operation. However, the problem of determining whether a term of the $\lambda$ calculus can be {\it reduced} to a term in $\beta$ normal form is undecidable. It is essentially the same problem as proving a Turing machine halts.

\section{Consistency and Church-Rosser}

We can express the $\lambda$ calculus as an equational theory of logic. However, the theory is not a first order logic, because the terms of the calculus contain variables which may be bound, and this is not possible in vanilla first order logic. In order to get around this, we form a formal theory with only a single predicate, so that the propositions of the system are exactly of the form $M = N$, for some terms $M$ and $N$, and we consider the reduction rules as inferences from the basic equality axioms. Because the $\lambda$ calculus is not modelled as a first order theory, the model theory will have to be slightly different. The axioms of the system are
%
\begin{align*}
    (\alpha)\ \ \ \ \ \ \ \lambda x.M = \lambda y.M[y/x]\\
    (\beta)\ \ \ \ \ (\lambda x.M)N = M[N/x]\\
    (\text{id}) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ M = M
\end{align*}
%
and the inference rules are
%
\begin{center}
\begin{prooftree}
\Hypo{M = N}
\Infer1[($\mu$)]{LM = LN}
\end{prooftree}
\ \ \ \ \
\begin{prooftree}
\Hypo{ M = N }
\Hypo{ N = L }
\Infer2[($\tau$)]{ M = L }
\end{prooftree}
\ \ \ \ \
\begin{prooftree}
\Hypo{M = N}
\Infer1[($\nu$)]{LM = LN}
\end{prooftree}
\end{center}

\begin{center}
\begin{prooftree}
\Hypo{M = N}
\Infer1[($\sigma$)]{N = M}
\end{prooftree}
\ \ \ \ \ \ \ \ \ \ \ \
\begin{prooftree}
\Hypo{ M = N }
\Infer1[($\xi$)]{ \lambda x.M = \lambda x.N }
\end{prooftree}
\end{center}
%
An equation is provable in this axiom system if and only if the equation is true for the $\lambda$ terms.

Since the $\lambda$ calculus is equational, there is no way to talk about the standard consistency of the inference rules -- there is no such thing as a contradiction, because we don't necessarily have a negation connective. However, the syntax does give rise to some predicates in the metalanguage, those being the equality predicates $\lambda \vdash M = N$. Since formal systems using classical logic are inconsistant precisely when every statement in the language can be proved, we will say that the $\lambda$ calculus is consistent if {\it not every} equation can be proved. That is, there is some $M$ and $N$ such that $M = N$ cannot be proved.

The Church-Rosser theorem is the central result to proving the consistency of the $\lambda$ calculus. It is clear from the result that a $\beta$ normal form is unique up to $\alpha$ reduction. Thus if two terms $M$ and $N$ are in normal form, and cannot be $\alpha$ converted into one another, then we cannot prove that $M = N$. We surely have two terms which are not $\alpha$ equivalent, so we can conclude that the $\lambda$ calculus is a consistent theory, as a direct result.

\begin{theorem}[Church-Rosser for $\rhd$]
    If $M \rhd M_0$ and $M \rhd M_1$, then there is a term $N$ such that $M_0 \rhd N$ and $M_1 \rhd N$.
\end{theorem}
\begin{proof}
    Annoyingly technical, so I'll prove it some other time.
\end{proof}

The property that the Church-Rosser theorem proves for $\rhd$ is called \emph{confluence}. There is an analogous result for equality of terms. Note that for any symmetric relation, confluence is trivial, so the theorem has to be strightly stronger than this to be interesting.

\begin{theorem}
    If $\lambda \vdash M = N$, then there is a term $L$ such that $M \rhd L$ and $N \rhd L$.
\end{theorem}
\begin{proof}
    First, note that if $\vdash M = N$, then there is $L_1, \dots, L_n$ with $L_0 = M$, $L_n = N$, and either $L_k \rhd L_{k+1}$ or $L_{k+1} \rhd L_k$. We shall prove this theorem by induction on $n$. If $n = 1$, the theorem is trivial, because $M$ is syntactically equal to $N$. If $n = 2$, then either $M \rhd N$ or $N \rhd M$, and then the theorem is just the Church-Rosser theorem for $\rhd$. Otherwise, by induction, we may assume that there is $L$ with $L_1 \rhd L$ and $L_{n-1} \rhd L$. If $L_n \rhd L_{n-1}$, then $L_n \rhd L$, hence the theorem is complete. Thus we may assume $L_{n-1} \rhd L_n$. But then by applying Church-Rosser for $\rhd$, we conclude that there is $K$ such that $L \rhd K$ and $L_n \rhd K$, and this completes the proof, because $L_1 \rhd L \rhd K$.
\end{proof}

Essentially, what we have proved is that if $R$ is any transitive confluent relation, and we take the smallest symmetric extension $R'$, then if $R'(x,y)$, then $R(x,z)$ and $R(y,z)$ for some $z$.

\begin{corollary}
    If $\lambda \vdash M = N$, and $N$ is in $\beta$ normal form, then $M \rhd N$.
\end{corollary}
\begin{proof}
    The last theorem shows that $M \rhd L$ and $N \rhd L$ for some term $L$. But then $L$ is alpha congruent to $N$, hence $L \rhd N$, and so $M \rhd N$.
\end{proof}

\begin{corollary}
    If $\lambda M = N$, and $M$ and $N$ are both in $\beta$ normal form, then $M$ and $N$ are $\alpha$ equivalent.
\end{corollary}

Terms without a normal form correspond to algorithms which don't terminate. Such terms are essentially meaningly in the $\lambda$ calculus. But the fact that a term has a normal form does not imply that subterms of the term have a normal form. For instance, $(\lambda x.y)\Omega$ has a normal form $y$, whereas we know $\Omega$ does not have a normal form. This means that a `meaningful' term may have meaningless subterms. This seems undesirable, so it is of interest to reduce the terms of the $\lambda$ calculus so that subterms of terms with normal forms have normal forms. Church found the system $\lambda I$ with this property. It is defined in essentially the same way as the standard $\lambda$ calculus except that when forming the terms of the system, we only let $(\lambda x.M)$ be a term when $x$ is a free variable in $M$.



\section{Combinators}

An alternative formal system in which to discuss combinatory logic is the theory of combinators. One can see this theory as the subtheory of the $\lambda$ calculus generated by the closed terms. Suprisingly, this subtheory turns out to be equivalent in expressive power to the entire $\lambda$ calculus theory. This may seem unreasonable in the context of $\lambda$ calculus, for we need free variable terms in order to form future functions. However, we can represent $\lambda$ abstraction via the composition of closed $\lambda$ terms, so that there is a formal system for combinatory logic with composition as the only connective. We will introduce variables into combinatory logic for convenience, but we note they are not required for most discussions of the calculus, and are much simpler than variables in the $\lambda$ calculus because there is no way to bind variables.

Combinatory logic was invented a decade before the $\lambda$ calculus, and can be developed with no mention of Curry's theory at all. One starts with a set of primitive combinators, and a set of variables, and inductively constructs the set of all combinators.
%
\begin{itemize}
    \item Every primitive combinator and variable is a combinator.
    \item If $A$ and $B$ are combinators, then $(AB)$ are combinators.
\end{itemize}
%
Equivalence of terms is generated based on substitution rules for the primitive combinators. We associate with each base combinator $C$ a substitution rule of the form $Cx_1 \dots x_n \rhd_A A_C$, where $A_C$ is another term in the $\lambda$ calculus. We can then define one step reduction as the relation formed by the rules $CB_1 \dots B_n \rhd_{C,1} A_C[B_1/x_1, \dots, B_n/x_n]$, or more generally, if this reduction occurs in a subterm of a combinator. The union of all $\rhd_{C,1}$ is one step reduction $\rhd_1$, and the transitive closure is general reduction $\rhd$. We can then consider equivalences by taking the reflexive symmetric closure, and we denote this by $M =_{\text{CL}} N$, or $\text{CL} \vdash M = N$.

\begin{example}
    There are some classical combinators which are universally known.
    %
    \begin{center}
    \begin{tabular}{ c c c }
    $Ix \rhd x$ & $Bxyz \rhd x(yz)$ & $Sxyz \rhd xz(yz)$ \\ 
    $Kxy \rhd x$ & $Cxyz \rhd xzy$ & $Wxy \rhd xyy$ \\  
    $Mx \rhd xx$ & $B'xyz \rhd y(xz)$ & $Yx \rhd x(Yx)$
    \end{tabular}
    \end{center}
    %
    Note that all the combinators but the $Y$ combinator have axioms which are formed from the variables in the definition. We call these types of combinators \emph{proper combinators}, rather than \emph{improper combinators}.
\end{example}

Combinators can be classified by five features, based on how the substitution relations work. \emph{Identity combinators} are those which operate by reductions of the form $Cx_1 \dots x_n \rhd_C x_1 \dots x_n$. The classical $I$ combinator above is an identity combinator of arity one. The combinator $BI$ satisfies $BIxy \rhd_B I(xy) \rhd_I xy$, hence $BI$ acts as an identity combinator of arity two. An \emph{associator} is a combinator which groups the input terms. The $B$ combinator is an associator, and is the only non-trivial associator with arity three (the other associator would be $Bxyz \rhd (xy)z$, but this is just an identity combinator of arity three). An example of a more complicated combinator is a combinator $C$ with the axiom $Cxyzvw \rhd_C x((yz)vw)$. \emph{Cancellators} remove variables from an input, like $Kxy \rhd_K x$, and \emph{permutators} permutes arguments. Finally, \emph{duplicators}, as expected, duplicate arguments.

Since combinatory logic is an equational theory, we can also talk about consistency. It is often useful to use a sequent calculus for describing the theory. The only new axiom is primitive reduction
%
\[ (\rho)\ \ \ \ \ CB_1 \dots B_n = A_C[B_1/x_1, \dots, B_n/x_n] \]
%
in addition to the axiom $M = M$. We then use the standard inference rules
%
\begin{center}
\begin{prooftree}
\Hypo{M = N}
\Infer1[($\mu$)]{LM = LN}
\end{prooftree}
\ \ \ \ \
\begin{prooftree}
\Hypo{M = N }
\Hypo{N = L }
\Infer2[($\tau$)]{M = L }
\end{prooftree}
\ \ \ \ \
\begin{prooftree}
\Hypo{M = N}
\Infer1[($\nu$)]{LM = LN}
\end{prooftree}
\end{center}
\begin{center}
\begin{prooftree}
\Hypo{M = N}
\Infer1[($\sigma$)]{N = M}
\end{prooftree}
\end{center}
%
One interesting difference between combinatory logic and the lambda calculus is that, because combinatory logic has no variable binding operations, the system can be formalized as a first order equational theory. This will have interesting consequences for the differences between the model theory of combinatory logic and the model theory of the $\lambda$ calculus.

A \emph{weak redex} is a combinator of the form $AB_1 \dots B_n$, on which we can perform a reduction. If a term contains no weak redexes, we say it is in \emph{weak normal form}, which is essentially the end result of a calculation. There is a version of the Church Rosser theorem for reduction in combinatory logic, so that $\rhd$ is a confluent operation, and the system of combinatory logic is consistant. That is, weak normal forms are unique if they exist. Furthermore, if $M =_{\text{CL}} N$, there is a combinator $L$ such that $\text{CL} \vdash M \rhd L$ and $\text{CL} \vdash N \rhd L$.

Intuitively, any base term in combinatory logic can be modelled in the $\lambda$ calculus, because the axioms for primitive combinatory logic are analogous to closed $\lambda$ abstractions. For instance, the standard combinator axioms
%
\begin{center}
\begin{tabular}{ c c c }
 $Ix \rhd x$ & $Bxyz \rhd x(yz)$ & $Sxyz \rhd xz(yz)$ \\ 
 $Kxy \rhd x$ & $Cxyz \rhd xzy$ & $Wxy \rhd xyy$ \\  
 $Mx \rhd xx$ & $B'xyz \rhd y(xz)$ & $Yx \rhd x(Yx)$
\end{tabular}
\end{center}
%
are analogous to
%
\begin{center}
\begin{tabular}{ c c c }
 $\lambda x.x$ & $\lambda xyz.x(yz)$ & $\lambda xyz.xz(yz)$ \\ 
 $\lambda xy.x$ & $\lambda xyz.xzy$ & $\lambda xy.xyy$ \\  
 $\lambda x.xx$ & $\lambda xyz.y(xz)$ & $\lambda xyzv.xy(xvz)$
\end{tabular}
\end{center}
%
Conversely, any closed $\lambda$ term with all the $\lambda$ terms to one side is analogous to an axiom. The position of the $\lambda$ terms is important to the theory, which hints at why the syntactic theory of combinatory logic and the $\lambda$ calculus is different. However, we should expect that sufficiently powerful combinators should lead to a form of logic expressive enough to represent all substitution rules.

We will say a combinatory logic is \emph{combinatorially complete} if, for any axiom rule of the form $Cx_1 \dots x_n = A_C$, there is a combinator $C'$ (not necessarily primitive) such that $C'B_1 \dots B_n \rhd A_C[B_1/x_1, \dots, B_n/x_n]$ for all combinators $B_i$ (we need only show this for a set of variables). The most well known combinatorially complete logic consists of the primitive combinators $\{ S, K \}$, but other combinatory basis, like $\{ I,B,C,W,K \}$ and $\{ I,J,K \}$, exist. We shall actually prove that $\{ S, K, I \}$ is combinatorially complete, rather than $\{ S, K \}$, but since $SKKx \rhd I$ for all combinators $x$, the combinatorial completeness of $\{ S, K, I \}$ implies the combinatorial completeness of $\{ S, K \}$. Our method to do this is to introduce a form of `lambda abstraction' which forms a part of the metatheoretic language to express all substitution rules.

Given a variable $x$ and a combinator $M$, we shall define a combinator $[x].M$ such that $[[x].M]y \rhd M[y/x]$ for all combinators $y$. This will enable us to write arbitrary substitution rules in the calculus.
%
\begin{itemize}
    \item If none of the $x$ occur in $M$, define $[x].M = KM$. Then $KMy \rhd M$, and $M = M[y/x]$, hence $[x].M$ satisfies the required property.

    \item If none of the $x$ occur in $M$, defined $[x].Mx = M$. Then $My \rhd My$, and $My = [Mx][y/x]$. This is not necessary, because it is covered by other cases, but leads to a simple formula for $\lambda$ abstraction.

    \item Let $[x].x = I$. Then $Iy \rhd y$, and $y = x[y/x]$.

    \item Otherwise, let $[x].(MN) = S[[x].M][[x].N]$. Then by induction,
    %
    \[ S[[x].M][[x].N]y \rhd ([[x].M] y)([[x].N] y) \rhd M[y/x] N[y/x] \]
    %
    and $M[y/x] N[y/x] = (MN)[y/x]$.
\end{itemize}
%
Note that $[x].M$ does not contain the variable $x$. Thus if we extend substitution multidimensionally, so that $[x_1, \dots, x_n].M$ is defined recursively as $[x_1].[[x_2, \dots, x_n].M]$. By induction, we find
%
\[ [[x_1, \dots, x_n].M] x_1 \dots x_n \rhd M \]
%
And because $[[x_1, \dots, x_n].M$ does not contain the variables $x_i$, we find by substitution that $[[x_1, \dots, x_n].M] y_1 \dots y_n \rhd M[y/x]$. This uses the general principle that if $M \rhd N$, then $M[y/x] \rhd N[y/x]$. So now, if we have an axiom of the form $Cx_1 \dots x_n \rhd_C A_C$, we let $B = [x].A_C$. Then we have shown that $By_1 \dots y_n \rhd A_c[y/x]$, hence $\{ K, S \}$ is combinatorially complete. Essentially, this is the main technique to proving a set of combinators is complete, albeit reducing the set to a basis which is already proved complete.

Since we have a form of `lambda abstraction' for combinators, it is an interesting question to ask whether the $\xi$ rule holds in our calculus. That is, if $\vdash_{\text{CL}} M = N$, then does it necessarily hold that $\vdash_{\text{CL}} [x].M = [x].N$. One difference between combinatory logic and the lambda calculus is that the $\xi$ rule need not hold in combinatory logic.

\begin{example}
    If $M = Sxyz$ and $N = xz(yz)$, then $M \rhd_S N$, yet
    %
    \[ [x].M = S(S(S(KS)I)(Ky))(Kz) \]
    %
    \[ [x].N = S(SI(Kz))(S(Ky)(Kz)) \]
    %
    and both elements are in normal form, hence they cannot be $\text{CL}$ equivalent.
\end{example}

The lack of the $\xi$ rule is normally no problem, but sometimes it does cause issue, which means $\lambda$ calculus becomes a more applicable theory. Conversely, combinatory logic has a more elegant theory of substitution, so we must make a tradeoff between the two. The $\xi$ rule will return when we analyze the similarities between the formal theory of the $\lambda$ calculus and the formal theory of combinatory logic.

\section{Extensionality}

An intensional process is one which is defined by its description, whereas an extensional process is one uniquely defined by its inputs and outputs. The functions of set theory are extensional, because two functions are equal if and only if they agree on all inputs. Conversely, the terms of the $\lambda$ calculus and the combinators of combinatory logic are viewed as intensional descriptions of processes, because they describe the process of substitution, and just because $Mx$ is equivalent to $Nx$ for all inputs $x$ does not imply that $M$ is equivalent to $x$. Algorithms in computing science are intensional, because two algorithms which solve the same problem are not necessarily viewed as equivalent, especially if the runtime of the algorithms is under scrutiny.

It turns out that the theory of the $\lambda$ calculus is equivalent to combinatory logic as a formal system, provided we introduce extensionality to the system. Recall that a definition of function is extensional if functions are identified by their inputs and outputs, as in set theory. That is, if $fx = gx$ for all $x$, then $f = g$. The $\lambda$ calculus is an intensional theory because this need not hold. For instance, if $f = y$, and $g = (\lambda x.yx)$, then
%
\[ \lambda \vdash gx = (\lambda x.yx)x = yx = fx \]
%
yet we cannot prove that $g = f$ in the $\lambda$ calculus, since both terms are in normal form. Similar results hold for combinatory logic -- we can define two primitive terms with the same substitution rule, yet the two terms will be unequal in the theory.

We extend the formal theory of the $\lambda$ calculus to be intensional by adding an additional inference scheme to the proof system. If $x$ is a variable, and $M$ and $N$ are terms not containing $x$ as a free variable, we add the rule
%
\begin{center}
\begin{prooftree}
\Hypo{Mx = Nx}
\Infer1[($\zeta$)]{M = N}
\end{prooftree}
\end{center}
%
The extended theory is known as the $\lambda \zeta$ calculus, and we write $\lambda \zeta \vdash M = N$ for entailment of the equation theory. We can also consider adding the axiom
%
\[ (\eta)\ \ \ \ \ (\lambda x.Mx) = M \]
%
where $x$ is not a free variable of $M$, and we write $\lambda \eta \vdash M = N$ for this formal theory.

\begin{theorem}
    The inference rule $\zeta$ is correct in the $\lambda \eta$ theory, in the sense that if $\lambda \eta \vdash Mx = Nx$, and $M$ and $N$ do not contain $x$ as a free variable, then $\lambda \eta \vdash M = N$. Conversely, the axiom $\eta$ is derivable in the $\lambda \zeta$ theory.
\end{theorem}
\begin{proof}
    The first part of the theorem is proved by the following sequent tree in the $\beta \eta$ calculus.
    %
    \begin{center}
    \scalebox{0.8}{
    \begin{prooftree}
    \Hypo{(\lambda x.Mx) = M\ \ \ (\eta)}
    \Infer1[($\sigma$)]{M = (\lambda x.Mx)}

    \Hypo{Mx = Nx}
    \Infer1[($\xi$)]{(\lambda x.Mx) = (\lambda x.Nx)}

    \Hypo{(\lambda x.Nx) = N\ \ \ (\eta)}
    \Infer2[($\tau$)]{(\lambda x.Mx) = N}

    \Infer2[($\tau$)]{M = N}
    \end{prooftree}
    }
    \end{center}
    %
    The second is a very simple derivation of the axiom $\eta$ in the $\beta \zeta$ calculus.
    %
    \begin{center}
    \begin{prooftree}
        \Hypo{(\lambda x.Mx)x = Mx\ \ \ (\beta)}
        \Infer1[($\zeta$)]{(\lambda x.Mx) = M}
    \end{prooftree}
    \end{center}
    %
    Hence the $\beta \eta$ and $\beta \zeta$ systems have the same theorems. $\beta \eta \vdash M = N$ if and only if $\beta \zeta \vdash M = N$.
\end{proof}

The $\xi$ rule is essential to proving that $\beta \zeta$ and $\beta \eta$ are equivalent systems, and this is one reason why $\xi$ is known as the principle of weak extensionality. Another reason is that $\xi$ becomes redundant in an extensional system, in the sense that it is provable in the $\beta \zeta$ if we remove the $\xi$ rule. Conversely, $\zeta$ is not provable if we remove the $\xi$ rule from the $\beta \eta$ formal system.

Since $\eta$ gives us an additional reductions $\rhd_\eta$ of the form $(\lambda x.Mx) \rhd_\eta M$, certain $\beta$ normal forms can be further reduced. The induced reduction operation is still confluent -- if $M \rhd M_0$ and $M \rhd M_1$, there is $N$ such that $M_0 \rhd N$ and $M_1 \rhd N$. A term is in $\beta \eta$ normal form if it is in $\beta$ normal form, and if there are no subterms of the form $(\lambda x.Mx)$, where $M$ does not contain $x$ as a free variable. This is essentially the termination point of computations in the $\beta \eta$ calculus. Since there is more than one $\beta$ $\eta$ normal form, even modulo $\alpha$ congruence, the $\beta \eta$ calculus is consistant.

\begin{theorem}
    A term has a $\beta$ normal form iff it has a $\beta \eta$ normal form.
\end{theorem}
\begin{proof}
    We use the inductive construction of $\beta$ normal forms to prove that every $\beta$ normal form has a $\beta$ $\eta$ normal form.
    %
    \begin{itemize}
        \item If $M$ is a variable, then $M$ is already in $\beta \eta$ normal form.

        \item If $M = xN$, and $N \rhd N_0$, where $N_0$ is in $\beta \eta$ normal form, then $M \rhd xN_0$, and $xN_0$ is in $\beta \eta$ normal form.

        \item If $M = \lambda x.N$, and $N \rhd N_0$, where $N_0$ is in normal form, then $M \rhd \lambda x.N_0$, and either $\lambda x.N_0$ is in $\beta \eta$ normal form, or $N_0 = N_1x$, where $N_1$ is in $\beta \eta$ normal form, and $M \rhd \lambda x.N_0 \rhd_\eta N_1$, and so $M$ is reducable to $\eta$ normal form.
    \end{itemize}
    %
    Essentially, $\eta$ reduction always reduces the complexity of a term, so the induction step works.
\end{proof}

By the same process, we find that if $M \rhd_{\beta \eta} N$, then $M \rhd_\beta M_0$, and $M_0 \rhd_\eta N$, so we can postpone $\eta$ reduction to the end of calculations.

Combinatory logic is also a non-extensional system. For instance, two prime combinators may have equal axioms without being formally equal in the system. Remember that $\text{CL}$ does not satisfy the $\xi$ rule, and thus does not possess the principle of weak extensionality. There is no problem with adding the $\xi$ rule as an axiom, however.
%
\begin{center}
\begin{prooftree}
\Hypo{M = N}
\Infer1[$(\xi)$]{[x].M = [x].N}
\end{prooftree}
\end{center}
%
What's more, we could think of adding the extensionality principles as axioms of $\text{CL}$.
%
\begin{center}
\begin{prooftree}
\Hypo{Mx = Nx}
\Infer1[$(\xi)$]{M = N}
\end{prooftree}
\end{center}
%
\[ [x].Mx = M\ \ \ \ (\eta) \]
%
where $M$ and $N$ do not contain $x$ as a free variable. The formal theory of equality found by adding the $\xi$ rule is denoted $\text{CL} \xi$, and the theory of equality found by adding $\zeta$ is denoted $\text{CL} \zeta$.

\begin{example}
    In $\text{CL}$, we find that
    %
    \[ SKyx \rhd (Kx)(yx) \rhd x \]
    %
    for any combinator $y$. Therefore in $\text{CL} \zeta$, we conclude that $SKy = SKz$ for all combinators $y$ and $z$. Similarily, we find that $KIyx \rhd x$, hence $SKy = KIy$ in $\text{CL} \zeta$, hence $SK = KI$.
\end{example}

In $\lambda$ calculus, the $\xi$ rule was essential to proving that $\lambda \beta \eta$ was equivalent to $\lambda \beta \zeta$. This was not so important, because the $\xi$ rule was imbedded in the basic theory. Conversely, in combinatory logic the $\eta$ rule already holds by definition of the term $[x].Mx$. Using the same techniques as for the extensionality of the $\lambda$ calculus, we can therefore prove that $\text{CL} \zeta$ is an equivalent theory of equality to $\text{CL} \xi$. We remark that we could have left out the special case $[x].Mx$ in the definition of substitution, in which ase the $\eta$ rule would not hold by definition, and then $\text{CL} \xi$ would be a strictly weaker theory than $\text{CL} \zeta$. Various definitions of substitution, and the corresponding $\xi$ rules will result in different systems of equality.

Now to prove the consistency of $\text{CL} \zeta$, we apply the standard technique: find a system of reduction which specifies the equality, prove confluence, and show that more than one normal form exists. We define \emph{strong reduction} $\rhd_s$ on combinators to be the standard theory of reduction, with an additional result that if $M \rhd_s N$, then $[x].M \rhd_s [x].N$ (and conversely, we sometime denote normal reduction as {\it weak reduction} $M \rhd_w N$). I know of no proof of the Church Rosser property for strong reduction which is directly proven from this definition. The main way that strong reduction is proved confluent is by relating the notion directly to the extensional $\lambda$ calculus. The normal forms here are called \emph{strongly irreducible}. We note, however, that strong reduction is very difficult to work with, which is one reason why little attention to it has been considered.

\section{Equivalence of Combinatory Formal Systems}

Both combinatory logic and the $\lambda$ calculus accurately model functions obtained by substituting terms. They seem to have very similar properties, albeit from a few small differences. The $\xi$ rule fails in Combinatory logic, whereas the $\eta$ rule fails in $\lambda$ calculus. It turns out that if we add these rules to the formal systems, thereby considering their extensional forms, the two systems will have very strong equivalence properties.

First, we shall begin by considering a slight modification to the $\lambda$ calculus. The terms will not consist of strings of symbols, but rather an equivalence class of strings defined modulo $\alpha$ conversion. In this form of the $\lambda$ calculus, $\alpha$ reduction does not even need to be considered in the theory, because it is just equality in this modified system. We will let the set of equivalence classes of terms in this calculus be denoted by $\Lambda$.

Now given a particular combinatory logic with combinators, with the same variable set as a corresponding $\lambda$ calculus we shall define a \emph{$\lambda$ transform} $M \mapsto M_\lambda$ which takes combinators in the logic to $\alpha$ identified terms in $\Lambda$ which preserves the operation of reduction. This will be the first form of equivalence. If a primitive combinator $C$ has the axiom $Cx_1 \dots x_n \rhd_C A_C$, then we shall define $C_\lambda = \lambda x_1 \dots x_n. A_C$. We can then define the transform of general combinators by letting $(MN)_\lambda = M_\lambda N_\lambda$. Each $M_\lambda$ is a \emph{closed} term of the $\lambda$ calculus, and we call such closed terms combinators, because of this. As a first result, we note that $[M[N/x]]_\lambda = M_\lambda [N_\lambda/x]$.

\begin{lemma}
    If $M \rhd_w N$ in $\text{CL}$, then $M_\lambda \rhd_\beta N_\lambda$. Thus if $M =_{\text{CL}} N$, then $M_\lambda =_\beta N$. Conversely, if $M =_{\text{CL} \zeta} N$, then $M_\lambda =_{\beta \zeta} N_\lambda$.
\end{lemma}

This is just proved by induction on the length of a proof of $M \rhd_w N$ and $M =_{\text{CL} \zeta} N$, and is left to the reader.

To obtain an if and only if result, we require a combinatory logic with a basis of combinators expressive enough to represent all possible solutions in the $\lambda$ calculus. Thus from now on, we assume our combinatory logic contains only the primitive operators $S$, $K$, and $I$. We note that the $\lambda$ transform of this form of combinatory logic is specified by
%
\[ I_\lambda = (\lambda x.x)\ \ \ \ \ K_\lambda = (\lambda xy.x)\ \ \ \ \ S_\lambda = (\lambda xyz.xz(yz)) \]
%
We can then define $[x].M$ for any combinatory term $M$, and this allows us to form an inverse $\lambda$ transform. We define the $\text{CL}$ transform $M \mapsto M_{\text{CL}}$ of any term in $\Lambda$. We start by letting $x_{\text{CL}} = x$ for variables $x$, let $(MN)_{\text{CL}} = M_{\text{CL}} N_{\text{CL}}$, and let $(\lambda x.M)_{\text{CL}} = [x].[M_{\text{CL}}]$. Because $[x].[M_{\text{CL}}]$ does not contain any instances of the variable $x$, the term is well defined up to $\alpha$ congruence.

\begin{lemma}
    For any combinator $M$, $[M_\lambda]_{\text{CL}} = M$.
\end{lemma}

Thus the $\lambda$ transform has a left inverse. Note, however, that the $\text{CL}$ transform is not injective, because
%
\[ (\lambda x.yx)_{\text{CL}} = [x].yx = S(Ky)I \]
%
and
%
\[ [S(Ky)I]_\lambda = ((\lambda uvw.uw(vw)) ((\lambda uv.u)y))(\lambda u.u) \]
%
Though it is surjective.

\begin{lemma}
    If $M =_{\beta \zeta} N$, then $M_{\text{CL}} =_{\text{CL} \zeta} N_{\text{CL}}$.
\end{lemma}

As a corollary, we see that $M = N$ in $\text{CL} \zeta$ if and only if $M_\lambda = N_\lambda$ in the $\lambda \beta \zeta$ theory, and $M = N$ in the $\lambda \beta \zeta$ theory if $M_{\text{CL}} = N_{\text{CL}}$ in $\text{CL} \zeta$.

We note that the correspondence for the reduction rules of $\text{CL}$ and $\lambda$ are nowhere near as elegant. It is easy to prove that if $M \rhd_{\beta \zeta} N$, then $M_{\text{CL}} \rhd_s N_{\text{CL}}$, but the converse does not hold. Since reduction is really only used to form an equivalence of terms, this isn't too much of an issue.

\section{The Power of $\lambda$ calculus}

The core problem with the $\lambda$ calculus, and a combinatorially complete combinatory logic, is that it is too expressive in its full form. One result is the following paradox, known as the fixed point theorem.

\begin{theorem}
    For any term $M$, there is a term $N$ such that $MN =_\lambda N$.
\end{theorem}
\begin{proof}
    Let $N = \lambda x.f(xx)$, and let $M = NN$. Then
    %
    \[ M = NN = (\lambda x.f(xx)) N \rhd_\beta f(NN) = f(M) \]
    %
    Thus we have found a fixed point.
\end{proof}

If we are to interpret terms of the $\lambda$-calculus as real functions, we must be very careful, because otherwise this theorem would imply every theorem has a fixed point! This is clearly not true for all functions -- a particular example in logic is the negation operator, and this theorem would imply $\neg x = x$ for some $x$. We will address these problems once we have developed a syntax theory for the calculus.

\section{Models of the $\lambda$ Calculus and Combinatory Logic}

The formal theories of combinatorial logic are fun to play around with, but it is an interesting question what the theory actually {\it represents}. Combinatory logic is strange among formal systems in the sense that it has no immediate semantic interpretation, because it is immediately too powerful to be represented by set theoretic functions.

Lambda abstraction is a difficult concept to try and model, so let's begin by focusing only on composition, via models of combinatory logic. We cannot model combinators as functions directly, but there is a common method in mathematics to apply elements of sets to the set themselves. For instance, in Hilbert space theory, we can associate with each point $x$ in the space the functional $\langle x |$, which operates on the space by the inner product. In the theory of groups, we can compose elements together by a multiplication operation. Thus we would expect a suitable environment to model combinatory logic as elements of a set $X$ together with a composition operation $X \times X \to X$. The composition operation will be written $(x,y) \mapsto xy$ except where this becomes ambiguous, and we assume terms are left associative, so we can consider products of the form $x_1 x_2 \dots x_n$ for $x_i \in X$. Another way of getting around the fact is to represent functions on $X$ as points $x \in X$. We shall let the function associated with $x \in X$ be denoted $f_x : X \to X$. We can then define composition of functions as $(f_x f_y) = f_{f_x(y)}$. This is essentially the same method as with the abstract composition operation, since by currying an association $F: X \to X^X$ of points with functions on sets naturally reduces to $F: X \times X \to X$. We shall call a set $X$ with a composition operation an \emph{applicative structure}.

A function $f: X^n \to X$ is \emph{representable} on an applicative structure $X$ if there is an element $a \in X$, such that $f(x_1, \dots, x_n) = ax_1 \dots x_n$ for all $x_i \in X$. Just because we have an applicative structure on a set $X$, does not imply that {\it all} functions on the set are representable as elements of the set. In fact, we can guarantee that this does not occur, because Cantor proved that $X^X$ always has a cardinality strictly greater than $X$, so there cannot exist a surjective map $X \to X^X$. However, we shouldn't expect a model of combinatory logic to model all functions on a set, because one reason for combinatory logic's existence was to model only the {\it computable functions}.

We shall define a model of combinatory logic with primitive combinators $\mathbf{B}$ to be an applicative structure $X$ with more than one element together with an association $\rho: \mathbf{B} \to X$, where we denote $\rho(C)$ as $x_C$. If a general combinator $C$ has free variables $y_1, \dots, y_n$, we will let $x_C[x_1/y_1, \dots, x_n/y_n]$ be the element of $X$ fromed by mapping $y_i$ to $x_i$, and then considering the term closed under composition. For a model of combinatory logic to be a true model, the association $x_Cx_1 \dots x_n = A_C[x_1/y_1, \dots, x_n/y_n]$ for any $x_i \in X$, where $C$ has a substitution axiom $C y_1 \dots y_n \rhd A_B$. Thus the composition rule of the applicative structure naturally represents the substitution rule for each primitive combinator. Now give a model $X$, and two terms $M$ and $N$ in combinatory logic, we write $X \vDash M = N$ if, for all choices of $x_i \in X$, $x_M[x_1/y_1, \dots, x_n/y_n] = x_N[x_1/y_1, \dots, x_n/y_n]$,  where the $y_i$ are free variables in $M$ and $N$. This gives us a semantic theory for combinatory logic.

Note that what we have done is not really any more general than the model theory for first order logic, since we can write the terms of combinatory logic as terms of a first order system with a single equality predicate, where the primitive combinators are constants. A model is then just a set $X$ together with a map $f$ from the primitive combinators to $X$, and we can define an applicative structure on $X$ by taking the interpretation of composed terms in the first order logic.

Two terms in $a,b \in X$ are \emph{extensionally equivalent} if $ax = bx$ for all $x \in X$, or equivalently, if $f_a = f_b$. An applicative structure $X$ is extensional if $f_a = f_b$ holds if and only if $a = b$. A model of extensional combinatory logic should naturally be an extensional applicative structure, and this is certainly true if we interpret a model of this logic as a normal model of the first order theory. By consistency, there exists a model of extensional combinatory logic.















\part{Set Theory}

Almost all mathematicians use the following axioms of set theory, specified by Zemerlo-Fraenkel. The axioms of set theory consider only a universe consisting only of a single type of objects, the \emph{sets}, and a single primitive binary relation of \emph{elementhood}, denoted as $A \in B$:
%
\begin{itemize}
    \item \emph{Axiom of Extensionality}: If two sets $X$ and $Y$ have the same elements, then $X = Y$.

    \item \emph{Axiom Scheme of Separation}: If $P$ is a property of sets, then for any set $X$, there is a set
    %
    \[ Y = \{ x \in X : P(x)\ \text{is true} \}, \]
    %
    i.e. for any $s$, $s \in Y$ if and only if $s \in X$ and $P(s)$ is true.

    \item \emph{Axiom of Union}: For any set $\mathcal{X}$, there exists a set
    %
    \[ \bigcup \mathcal{X} = \Big\{ s: \text{there is $X \in \mathcal{X}$\ \text{such that} $s \in X$} \Big\} \]
    %
    known as the \emph{union} of $X$.

    \item \emph{Axiom of Power Set}: For any $X$, there exists a set
    %
    \[ \mathcal{P}(X) = \{ S : S \subset X \}, \]
    %
    known as the \emph{power set of $X$}, where $S \subset X$ is the property that for any $s$, if $s \in S$, then $s \in X$.

    \item \emph{Axiom of Infinity}: There exists an infinite set.

    \item \emph{Axiom Scheme of Replacement}: If $F$ is a function, then for any $X$ there exists a set $Y = F[X] = \{ F(x): x \in X \}$.

    \item \emph{Axiom of Regularity}: Every nonempty set has a $\in$-minimal element.

    \item \emph{Axiom of Choice}: Every family of nonempty sets has a choice function.
\end{itemize}
%
The theory with all these axioms is denoted ZFC, and ZF without the axiom of choice.












\part{Computability}

In 1931, Kurt G\"{o}del proved all sufficiently complicated axiomatic systems had unprovable theorems, but a fundamental question remained: by what method could we decide whether a theorem could be proved? It took a decade for Alonzo Church and Alan Turing to deduce the impossibility of such a claim. Fifty years later, `theoretical computation' had become a common reality. In this section, we introduce the mathematical models which formed the foundation for the computer, as well as more modern models which analyze the limitations of various computational methods.

Turing and Church's major breakthrough was precisely defining what a `computational procedure' is. It is often the case that precise definitions give rise to easy proofs of the most surprising consequence. We shall spend many chapters contemplating what power a computational procedure should have. Philosophically, one should be able to define such a procedure without reference to a computer, for humans computed long before microchips. On the other hand, models should reflect physical reality, since one needs a physical mechanism in order to compute, whether electronic or mental. If your computational model is too strong or too weak, it will not accurately represent the limitations of real life.

We will begin by analyzing the automaton, a model of computation without stored memory. We will expand the amount of expression of the automaton by considering context-free grammars. Finally, we add memory by considering a Turing machine. It is the Church Turing thesis that this is the ultimate model of computation -- any real world computation can be modelled as an action on a Turing machine. There has been no evidence in the past century to contradict this thesis, and every realistic model of computation is not as powerful as that of the Turing machine, so we accept the claim. From this model, and with the hypothesis of Church and Turing, we can make precise, philisophically interesting statements about the nature of computation in the real world, addressing theorems about the uncomputability and complexity of certain problems.

As in mathematical logic, the objects of study are strings of symbols over a certain alphabet. One studies the notion of computation syntactically. One of the main ideas of computability theory is that a mental decision can be modelled as a \emph{decision problem} -- find a computational model which will `accept' certain strings over an alphabet. Suppose our problem is to verify whether the addition of two numbers is correct. We are given $a$, $b$, and $c$, and we must decide whether $a + b = c$. Our symbol set is $\{ 0, 1, \dots, 9, : \}$, and we wish to model a computation which accepts all strings of the form $``a:b:c''$, where $a$, $b$, and $c$ are decimal strings for which $a + b = c$. Thus we must design machine to accept strings in a specified language, and determining whether a problem is solvable reduces to studying the structure of languages over a finite alphabet. We shall find that, obviously, this problem is possible to compute on a Turing machine, but it is not so simple -- there are some models of computation which are unable to decide whether addition is correct.

As a more dynamic discipline than mathematical logic, we need more operations on strings to obtain languages from other languages. We obviously need concatenation, but also \emph{reversal}, which will be denoted $s^R$. These operations are extended to languages by applying the operations on a component by component basis:
%
\[ S \circ W = \{ s \circ w : s \in S, w \in W \}\ \ \ \ \ S^R = \{ s^R : s \in S \} \]
%
A \emph{palindrome} is a string $s$ for which $s^R = s$. If $\Sigma$ is a set of strings, we shall let $\Sigma_\varepsilon = \Sigma \cup \{ \varepsilon \}$.



\chapter{Finite State Automata}

Our initial model of computability is a computer with severely limited, finite amount of memory. Surprisingly, we shall still be able to compute a great many things. The idea of this model rests on explicitly representing memory as a finite amount of states, whose behaviour is uniquely determined by the state upon which it stands at a point of time. We can represent this process via a state diagram. Suppose we would like to describe an algorithm determining if a number is divisible by three. We shall represent a number by a string of dashes. For instance, $``-----''$ represents the number 5. We describe the algorithm in a flow chart below
%
\begin{center}
\begin{tikzpicture}[scale=0.2]
\tikzstyle{every node}+=[inner sep=0pt]
\draw [black] (39.8,-19.2) circle (3);
\draw [black] (39.8,-19.2) circle (2.4);
\draw [black] (46.4,-30) circle (3);
\draw [black] (34,-30) circle (3);
\draw [black] (39.8,-13.3) -- (39.8,-16.2);
\fill [black] (39.8,-16.2) -- (40.3,-15.4) -- (39.3,-15.4);
\draw [black] (41.36,-21.76) -- (44.84,-27.44);
\fill [black] (44.84,-27.44) -- (44.85,-26.5) -- (43.99,-27.02);
\draw (43.74,-23.32) node [right] {$-$};
\draw [black] (43.4,-30) -- (37,-30);
\fill [black] (37,-30) -- (37.8,-30.5) -- (37.8,-29.5);
\draw (40.2,-30.5) node [below] {$-$};
\draw [black] (35.42,-27.36) -- (38.38,-21.84);
\fill [black] (38.38,-21.84) -- (37.56,-22.31) -- (38.44,-22.78);
\draw (36.22,-23.43) node [left] {$-$};
\end{tikzpicture}
\end{center}
%
The algorithm proceeds as follows. Begin at the top node. we proceed clockwise around the triangle, moving one spot for each dash we see. If, at the end of our algorithm, we end up back at the top node, then the number of dashes we have seen is divisible by three. The basic idea of the finite automata is to describe computation via these flow charts -- we follow a string around a diagram, and if we end up at a `accept state', then we accept the string. A mathematical model for this description is a finite state automaton.

A \emph{deterministic finite state automaton} is a 5-tuple $(Q, \Sigma, \Delta, q_0, F)$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet, $q_0 \in Q$ is a start state, $F \subset Q$ are the accept states, and $\Delta: Q \times \Sigma \to Q$ is the transition function. A finite state machine `works' exactly how our original algorithm worked. Draw a directed graph whose nodes are states, and draw an edge between a state $q$ and each related state $\Delta(q, \sigma)$, for each symbol $\sigma \in \Sigma$. Take a string $s \in \Sigma^*$. We begin at the start state $q_0$. Sequentially, for each symbol in $s$, we follow the directed edge from our current state to the state on the edge related to the current symbol in $s$. If, we end at an accept state in $F$, then $s$ is an `accepted' string. Formally, we define this method by extending $\Delta$ to $Q \times \Sigma^*$. We define, for $s \in \Sigma^*$, $t \in \Sigma$,
%
\[ \Delta(q, \varepsilon) = q\ \ \ \ \ \Delta(q, st) = \Delta(\Delta(q,s), t) \]
%
A state machine $M$ \emph{accepts} a string $s$ in $\Sigma^*$ if $\Delta(s,q_0) \in F$. We call the set of all accepting strings the \emph{language} of $M$, and denote the set as $L(M)$. A subset of $\Sigma^*$ which is a language of a deterministic finite state automata is known as a \emph{regular language}.

\begin{example}
    Consider $\Sigma = \{ - \}$. Then the set of all `dashes divisible by three' is regular, as in the introductory diagram. Formally, take
    %
    \[ Q = \mathbf{Z}_3\ \ \ \ \ \Delta(x,-) = x+1\ \ \ \ \ q_0 = 0\ \ \ \ \ F = \{ 0 \} \]
    %
    then $(Q, \Sigma, \Delta, q_0, F)$ recognizes dashes divisible by three. The `graph' of the automata is exactly the graph we've already drawn.
\end{example}

Arithmetic is closed under certain operations. Given two numbers, we can add them, subtract them and multiplication, and what results is still a number. In the theory of computation, the operations have a different flavour, but are nonetheless just as important. We shall find that all regular languages can be described from very basic languages under certain compositional operators, under which the set of regular languages is closed.

\begin{theorem}
    If $A, B \subset \Sigma^*$ are regular languages, then $A \cup B$ is regular.
\end{theorem}
\begin{proof}
    let $M = (Q, \Sigma, \Delta, q_0, F)$ and $N = (R, \Sigma, \Gamma, r_0, G)$ be automata recognizing $A$ and $B$ respectively. We shall define a finite automata recognizing $A \cup B$. Define a function $(\Delta \times \Gamma) : (Q \times R) \times \Sigma \to (Q \times R)$, by letting
    %
    \[ (\Delta \times \Gamma)(q,r,\sigma) = (\Delta(q,\sigma), \Gamma(r,\sigma)) \]
    %
    Consider
    %
    \[ H = \{ (q,r) \in S : q \in F\ \text{or}\ r \in G \} \]
    %
    We contend that
    %
    \[ ((Q \times R), \Sigma, \Delta \times \Gamma, (q_0, r_0), H) \]
    %
    recognizes $A \cup B$. By induction, one verifies that for any $s \in \Sigma^*$,
    %
    \[ (\Delta \times \Gamma)(q,r,s) = (\Delta(q,s), \Gamma(r,s)) \]
    %
    Thus $(\Delta \times \Gamma)(q_0, r_0, s) \in H$ if and only if $\Delta(q_0, s) \in F$ or $\Gamma(r_0, s) \in G$.
\end{proof}

\begin{theorem}
    If $A$ is a regular language, then $A^c$ is regular.
\end{theorem}
\begin{proof}
    If $M = (Q,\Sigma,\Delta,q_0,F)$ recognizes $A$. Then define a new machine $N = (Q,\Sigma,\Delta,q_0,F^c)$. The transition $\Delta(q_0, s)$ is in $F^c$ if and only if $\Delta(q_0, s)$ is not in $F$.
\end{proof}

\begin{corollary}
    If $A$ and $B$ are regular languages, then $A \cap B$ are regular.
\end{corollary}
\begin{proof} $A \cap B = (A^c \cup B^c)^c$. \end{proof}

\section{Non Deterministic Automata}

An important concept in computability theory is the introduction of non-determinism. Deterministic machines must follows a set protocol when understanding input. Non deterministic machines can execute one of many different specified protocols. If any of the protocols accepts the input, then the entire machine accepts the input. Thus non-deterministic machines are said to multitask, for they can be seen to run every protocol specified at once, checking one of a great many protocols to see a pass. An alternative viewpoint is that the machines make a lucky guess -- they always seem to choose the write protocol which results in an accepted string.

A \emph{non-deterministic finite state automaton} is a 5-tuple $(Q, \Sigma, \Delta, q_0, F)$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet, $q_0$ is the start state, $F \subset Q$ are the accept states, and $\Delta: Q \times \Sigma_\varepsilon \to \mathcal{P}(Q)$ is the non-deterministic transition function.

In a non-deterministic finite state automata, we \emph{accept} a string $s$ if $s = s_1 \dots s_n$, where each $s_i \in \Sigma_\varepsilon$, and there are a sequence of states $t_0, \dots, t_{n+1}$, with $q_0 = t_0$ and $t_n \in F$, such that $t_{k+1} \in \Delta(t_k, s_k)$. The set of accepting strings of a machine $M$ form the language $L(M)$. We draw a graph with nodes $Q$, and with directed edges $v$ to $w$ if $w \in \Delta(v, \Sigma_{\varepsilon})$. We begin at $q_0$. For a string $s$, we attempt to find a path from $q_0$ to an accept state, by following edges whose corresponding symbol is in $s$ (or whose symbol is $\varepsilon$, in which we get for free). The string is accepted if such a path is possible. Some call non-deterministic methods a lucky guess methods, since they always make a lucky guess of which deterministic path to take to accept a string.

There is a nicer criterion of acceptance than described above, which is easier to work with in proofs. First, assume there are no $\varepsilon$-transitions in a machine $M$; that is, $\Delta(q, \varepsilon) = \emptyset$ for all states $q$. We may then extend $\Delta: Q \times \Sigma \to \mathcal{P}(Q)$ to $\Delta: \mathcal{P}(Q) \times \Sigma^* \to \mathcal{P}(Q)$ recursively by
%
\[ \Delta(X, \varepsilon) = X \ \ \ \ \ \Delta(X, st) = \Delta(\Delta(X,s), t) \]
%
where $\Delta(X,s) = \{ \Delta(x,s) : x \in X \}$. A string $s$ is accepted by $M$ if and only if an accept state is an element of $\Delta(q_0, s)$. If $s = s_1 \dots s_n$, and there are $t_0, \dots, t_n$ with $t_0 = q_0$, $t_n$ an accept state, and $t_{k+1} \in \Delta(t_k, s_k)$, then by induction one verifies that $t_n \in \Delta(q_0, s)$. Conversely, an induction on $s$ verifies that if $q \in \Delta(q_0, s)$, and if $s = s_1 \dots s_n$, then there is a state $q'$ with $q' \in \Delta(q_0, s_1 \dots s_{n-1})$, $q \in \Delta(q', s_n)$. But this implies that if there is an accept state in $\Delta(q_0, s)$, then $s$ is accepted by $M$. We can always perform this trick, for we may always remove $\varepsilon$ transitions.

\begin{lemma}
    Every non-deterministic automata is equivalent to a non deterministic automata without $\varepsilon$ transitions, in the sense that they both recognize the same language.
\end{lemma}
\begin{proof}
    Consider a non-deterministic automata.
    %
    \[ M = (Q, \Sigma, \Delta, q_0, F) \]
    %
    Define a state $u$ to be $\varepsilon$ reachable from $t$ if there is a sequence of states $q_0, \dots, q_n$ with $q_0 = t$, $q_n = u$, and $q_{i + 1} \in \Delta(q_i, \varepsilon)$. Let $E(u)$ be the set of all states $\varepsilon$ reachable from $u$. Define
    %
    \[ N = (Q, \Sigma, \Delta', q_0, F) \]
    %
    Where
    %
    \[ \Delta'(q, s) = \begin{cases} \bigcup_{u \in E(q)} \Delta(u, s) & s \neq \varepsilon \\ \emptyset & s = \varepsilon \end{cases} \]
    %
    Then it is easily checked that $L(N) = L(M)$, for we may skip over $\varepsilon$ transitions in $N$.
\end{proof}

It seems to be a much more complicated procedure to find if a string is accepted by a non-deterministic automata, but it turns out that every non-deterministic automata can be converted into a deterministic automata. The proof relies on the fact that we may exploit the operations of non-determinism, described using power sets of a set.

\begin{theorem}
    A non-deterministic finite state automata language is regular.
\end{theorem}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ be a non-deterministic automata. Assume $M$ has no $\varepsilon$ transitions, without loss of generality. Let
    %
    \[ N = (\mathcal{P}(Q), \Sigma, \Gamma, \{ q_0 \}, \{ S \in \mathcal{P}(Q) : S \cap F \neq \emptyset \} \]
    %
    where
    %
    \[ \Gamma(S, t) = \Delta(S, t) \]
    %
    We have already verified this deterministic machine recognizes $L(M)$, for $\Delta(\{ q_0 \}, s)$ contains an accept state if and only if $s$ is accepted.
\end{proof}

It is now fair game to use non-deterministic automata to understand regular languages, for the language of every non-deterministic automata is regular.

\begin{theorem}
    If $A$ and $B$ are regular languages, then $A \circ B$ is regular.
\end{theorem}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ and $N = (R, \Sigma, \Gamma, r_0, G)$ be deterministic automata accepting $A$ and $B$ respectively. Without loss of generality, assume $Q$ is disjoint from $R$. Consider the non-deterministic machine
    %
    \[ O = (Q \cup R, \Sigma, \Pi, q_0, G) \]
    %
    Define
    %
    \[ \Pi(s,t) = \begin{cases} \{ \Delta(s,t) \} & s \in Q\text{, and}\ t \neq \varepsilon\ \text{or}\ s \not \in F \\ \{ \Delta(s,t), r_0 \} & s \in F, t = \varepsilon\\ \{ \Gamma(s,t) \} & s \in R\\
    \emptyset & \text{otherwise} \end{cases} \]
    %
    We quickly verify that if $s \in L(M)$ and $w \in L(N)$, then $sw \in L(O)$. If $v \in L(O)$, there is some substring $k$ such that $r_0 \in \Pi(q_0, k)$ (for this is the only path from $q_0$ to $G$). If we choose $k$ to be the shortest such string, then $k$ must also be an accept string in $L(M)$, since any substring of $k$ cannot map to states in $R$, and thus $k$ must map via the $\varepsilon$ transition from an accept state of $M$. If $v = kr$, then $\Pi(r_0, r)$ is an accept state in $N$, so $r \in L(N)$. Thus $v \in L(M) \circ L(N)$.
\end{proof}

\begin{theorem}
    If $A$ is a regular language, $A^*$ is regular.
\end{theorem}
\begin{proof}
    Let $M = (Q, \Sigma, \Delta, q_0, F)$ be a deterministic language which accepts $A$. Form a non-deterministic automata $N = (Q \cup \{ i \}, \Sigma, \Gamma, i, F)$, where
    %
    \[ \Gamma(q,s) = \begin{cases} \{ \Delta(q,s) \} & s \neq \varepsilon \\ \{ q_0 \} & s = \varepsilon, q = i \\ \{ i \} & s = \varepsilon, q \in F \\ \emptyset & \text{otherwise} \end{cases} \]
    %
    Then $L(N) = A^*$, for if $s = a_1 \dots a_n$, with $a_i \in A$, then we may go from $i$ to $q_0$, then to an accept state via $a_1$, return to $i$ with a $\varepsilon$ transition, and continue, so $s$ is accepted. If we split a string accepted to $L(N)$ into when $\varepsilon$ transitions to $i$ are used, then we obtain strings in $A$.
\end{proof}

In turns out that the operations of concatenation, union, and `kleene starification' are enough to describe all regular languages. Thus we may take an algebraic approach to understanding regular languages, using the symbology of regular expressions.

\section{Regular Expressions}

Automata are equivalent to a much more classical notion of computation -- regular expressions.

\begin{definition}
    A \emph{regular expression} over an alphabet $\Sigma$ is the smallest subset of $(\Sigma \cup \{ \emptyset, \cup, *, (, ) \})^*$ such that
    %
    \begin{enumerate}
        \item $\emptyset$ is a regular expression, as are $s \in \Sigma^*$.
        \item If $\lambda$ and $\gamma$ are regular expressions, then so are $\lambda^*, (\lambda \cup \gamma)$, and $(\lambda \circ \gamma)$.
    \end{enumerate}
\end{definition}
%
Every regular expression describes a language. A string $s \in \Sigma^*$ is recognized by an regular expression $\lambda$ if
%
\begin{enumerate}
    \item $\lambda \in \Sigma^*$, and $s = \lambda$.
    \item $\lambda = (\lambda \cup \gamma)$, and $s$ is recognized by $\lambda$ or by $\gamma$.
    \item $\lambda = \gamma \circ \delta$, and $s = wl$, where $w$ is recognized by $\gamma$, and $\delta$ recognizes $l$.
    \item $\lambda = \gamma^*$, and $s = \varepsilon$ or $s = s_1 \dots s_n$, where $s_i$ is recognized by $\gamma$.
\end{enumerate}
%
The regular language corresponding to a regular expression $\lambda$ is $L(\lambda)$, the set of strings recognized by $t$. The set of all regular expressions on an alphabet $\Sigma$ will be denoted $R(\Sigma)$.

It is a simple consequence of our discourse that every language recognized by a regular expression {\it is actually} a regular language. In fact, we can show that every regular language is described by a regular expression. We shall describe an algorithm for converting a finite state automaton to a regular expression. We shall make a temporary generalization, by allowing non deterministic finite automata to have regular expressions in their transition functions. A \emph{generalized non-deterministic finite state automaton} is a 5-tuple $M = (Q,\Sigma, \Delta, q_0, f_0)$, where $\Delta: Q - \{ f_0 \} \times Q - \{ q_0 \} \to R(\Sigma)$ is the generalized transition function, and $q_0$ is the start state, $f_0$ is the end state. A generalized automaton accepts  $s \in \Sigma^*$ if we may write $s = s_1 \dots s_n$, and there are a sequence of states $q_0, \dots, q_n$ where $q_n = f_0$, and $\Delta(q_i, q_{i+1})$ recognizes $s_i$.

\begin{theorem}
    Any generalized finite-state automaton describes the language of a regular expression.
\end{theorem}
\begin{proof}
    If the generalized automaton has two states, then there is only one transition from start state to begin state, and this transition describes a regular expression for the automaton. We will reduce every automaton to this form by induction. Suppose an automaton has $n$ states. Fix a state $q \in Q$, which is neither the beginning or accepting state. Define a new automaton
    %
    \[ N = (Q - \{ q \}, \Sigma, \Delta', q_0, f_0) \]
    %
    where $\Delta'(a,b) = (\Delta(a,b) \cup \Delta(a,q) \Delta(q,q)^* \Delta(q,b))$
    %
    Then $N$ is equivalent to $M$, and has one fewer state, and is thus equivalent to a regular expression.
\end{proof}

\begin{corollary}
    Every regular language is described by a regular expression.
\end{corollary}
\begin{proof}
    Clearly, every DFA and NFA is equivalent to a generalized NFA, for by adding new states, we may ensure no states map back to the start state, and that there is only one end state.
\end{proof}


\section{Limitations of Finite Automata}

We've discovered a menagerie of different problems we can solve with finite automata, but it has already been foreshadowed that better machines await. Here we discover methods which attack languages, showing that they cannot be recognized by regular expressions, not finite automata.

\begin{theorem}[Pumping Lemma]
    Let $L$ be a regular language. Then there is a number $p$, called the pumping length, such that any $s \in L$ which satisfies $|s| \geq p$, then we may write $s = wuv$, where $|u| > 0$, $|wu| \leq p$, and $wu^iv \in L$ for all $i \geq 0$.
\end{theorem}
\begin{proof}
    Let $L$ be a regular language, and $M$ a deterministic automata recognizing $L$ with $p$ states. Let $s$ be a string with $|s| \geq p$, with $s \in L(M)$. Write $s = s_1 \dots s_n$ and let $q_k = \Delta(q_0, s_1, \dots, s_k)$. Then we obtain $|s| + 1$ states $q_0, q_1, \dots, q_{|s|}$. By the pidgeonhole principle, since $q_i$ equals some $q_j$, for $i < j$. Let $w = s_1 \dots s_i$, $u = s_{i+1} \dots s_j$, $v = s_{j+1} \dots s_{|s|}$. Then $\Delta(\Delta(q_0, w), u) = \Delta(q_0, w)$, so
    %
    \[ \Delta(q_0, wu^iv) = \Delta(q_0, wuv) \]
    %
    So $wu^iv \in L$ for all $i$.
\end{proof}

\begin{example}
    $L = \{ 0^k10^k : k \geq 0 \}$ is not regular. If it was regular, it would have a pumping length $p$. Since $0^p10^p$ is in $L$, so we may write $0^p10^p = wuv$, where $|wu| \leq p$, and $wu^iv \in L$. Then $u = 0^k$, for $k > 0$, and $wv = 0^{p-k}10^p \in L$, which is clearly not in the language, contradicting regularity.
\end{example}

\begin{example}
    $L = \{ 1^{n^2} : n \in \mathbf{N} \}$ is not regular. Suppose we had a pumping length $p$. Then $1^{p^2} \in L$, so there is $0 < k \leq p$ with $1^{p^2 + k} \in L$. But
    %
    \[ (p + 1)^2 = p^2 + 2p + 1 > p^2 + k \]]
    %
    And there is no perfect square between $p^2$ and $(p+1)^2$, a contradiction.
\end{example}

\begin{example}
    The language $L = \{ 0^i1^j : i > j  \}$ is not regular. If we had a pumping lenght $p$, then $0^p1^{p-1} \in L$. But then there is $0 < k \leq p$ such that $0^{p + (i - 1)k}1^{p-1} \in L$ for all natural numbers $i$. In particular, for $i = 0$, we find $0^{p - k}1^{p-1} \in L$ But $p - k \leq p -1$, a contradiction.
\end{example}

There is a much more mathematically elegant and complete way of separating regular languages from non-regular ones, discovered by John Myhill and Anil Nerode. Consider an alphabet $\Sigma$, and a particular language $L \subset \Sigma^*$. Call two strings $a$ and $b$ in $\Sigma^*$ $L$-indistinguishable if $az \in L$ if and only if $bz \in L$ for any $z \in \Sigma^*$. This forms an equivalence relation on $\Sigma^*$. We shall define the index of $L$, denoted $\text{Ind}\ L$, to be the cardinality of the partition.

\begin{theorem}[Myhill-Nerode]
    $L$ is regular if and only if $\text{Ind}\ L < \infty$, and $\text{Ind}\ L$ is the number of states of the smallest finite state machine to recognize $L$.
\end{theorem}
\begin{proof}
    Let $M$ be a deterministic finite state machine recognizing $L$ with $n$ states, transition function $\Delta$, and start state $q_0$. Let $a_1, \dots, a_{n+1}$ be $n + 1$ strings in $\Sigma^*$. We claim that at least one pair is indistinguishable. if we take $q_i = \Delta(q_0, a_i)$, then some $q_i = q_j$. These two strings are then indistinguishable in $L$. Conversely, suppose that $\text{Ind}\ L < \infty$. Let $A_1, \dots, A_n$ be the equivalence classes of $\Sigma^*$. If $s \in A_i$ is contained in $L$, then every other $w \in A_i$ is in $L$, for otherwise $s$ and $w$ can be distinguished. Build a finite state machine whose states are $A_i$, whose start state is $[\varepsilon]$, and whose transition function is
    %
    \[ \Delta([s], t) = [st] \]
    %
    $[s]$ is accepted if and only if $s \in L$. This finite state machine recognizes $s$.
\end{proof}

In some circumstances, the Myhill Nerode theorem is very powerful.

\begin{example}
    For $\Sigma = \{ a, b, \dots, z \}$, consider the set $L$ of words $w$ whose last letter has not appeared before. For example, the words ``apple'', ``google'', ``k'', and $\varepsilon$ are in $L$, but the words ``potato'' and ``nutrition'' are not. Is this language regular? We apply Myhill Nerode. If the letters in one word are different to the letters in another word, these words are distinguishable. If the letters in one word are the same as the letters in another word, and both are not accepted or both are not accepted, these words are indistinguishable. Thus the index of the language in consideration is the same as the number of different subsets of the set of letters in a word, counted twice for repeated and non-repeated characters. Subtracting one from the fact that $\varepsilon$ need only be counted once, we find that $2 \cdot 2^{26} - 1 = 2^{27} - 1$. Since this is finite, the language is regular, and this is the minimal number of sets in a finite state machine recognizing the language.
\end{example}

\section{Function representation}

Decision problems are sufficient to model a large variety of computational models, but for completeness, we should at least mention how we determine whether problems with multiple outputs can be. The counterpart of a finite-state automata is a finite-state transducer. A \emph{finite state transducer} is a 5 tuple $(Q, \Sigma, \Lambda, \Delta, \Gamma, q_0)$, where $Q$ is the set of states, $\Sigma$ is the input alphabet, $\Lambda$ is the output alphabet, $\Delta: Q \times \Sigma \to Q$ is the transition function, $\Gamma: Q \times \Sigma \to \Lambda_\varepsilon$ is the transduction function, and $q_0$ is the start state. Each finite-state transducer $M$ gives rise to a function $f_M: \Sigma^* \to \Lambda^*$, defined by
%
\[ f_M(\varepsilon) = \varepsilon\ \ \ \ \ f_M(st) = f_M(s) \circ \Gamma(\Delta(q_0,s), t) \]
%
A \emph{regular function} is one computable by a finite state transducer.

\begin{example}
    Consider an alphabet $\mathbf{F}_2$ and consider the function $f$ taking and returning strings over $\mathbf{F}_2$, inverting the strings on even positions, and leaving the strings on the odd positions. Then $f$ is a regular function, for it may be computed by the automata below.
\end{example}










\chapter{Context Free Languages}

Finite state machines are useful on a specific set of problems, but have limitations. We would like our notion of computability to decide on a much more complicated set of problems. Thus we must define new classes of computable languages.

\section{Context Free Grammars}

\begin{definition}
    A \emph{Context Free Grammar} is $(V,\Sigma,R,S)$, where $V$ is a set of variables, $\Sigma$ is a character set, disjoint from $V$, $R$ is a relation between $V$ and $(V \cup \Sigma)^*$, and $S \in V$ is the start variable. We call elements of $R$ derivation rules, and write $(a,s) \in R$ as $a \rightarrow s$.
\end{definition}

A string of the form $uvw$ is \emph{directly derivable} from $uAw$ if $A \rightarrow v$ is a derivation rule. The smallest transitive relation containing the `directly derivable' relation is the derivability relation. To reiterate, a string $u$ is \emph{derivable} from $w$ if there is a sequence of direct derivations
%
\[ w \rightarrow s_0 \rightarrow \dots \rightarrow s_n \rightarrow u \]
%
Such a sequence is known as a \emph{derivation}. The language of a grammar is the set of all strings in $\Sigma^*$ derivable from the start state $S$. As in finite automata, the language of a grammar $G$ will be denoted $L(G)$. A language is \emph{context-free} if it is the language of a context free grammar.

\begin{lemma}
    Let $G = (V,\Sigma,R,S)$ and $H = (V,\Sigma,R',S)$ be two different context free languages. If every $(v,s) \in R'$ is a derivation in $G$, then $L(H) \subset L(G)$.
\end{lemma}
\begin{proof}
    If we can show that every direct derivation in $H$ is a derivation in $G$, then all derivations in $H$ are derivations in $G$, since derivations form the smallest transitive relation containing the direct derivations in $H$, and the derivations in $G$ certainly satisfy this. If $w$ is directly derived from $s$ in $H$, then there is a rule $(B,u)$, $s = rBt$, and $w = rut$. There is a sequence $s_0 \dots s_n$ deriving $u$ from $B$ in $G$. But then $(rs_0t) \dots (rs_nt)$ is a derivation of $w$ from $s$, so $w$ is derivable from $s$ in $G$.
\end{proof}

A leftmost derivation is a derivation which always swaps out the leftmost variable. A string is \emph{ambiguous} if it has two different leftmost derivations. A grammar is ambiguous if its language contains an ambiguous string. Ambiguity is unfortunate when parsing a language, since it means we may be able to interpret elements of the language in two different ways.

\begin{example}
    First order logic can be defined as an unambiguous grammar. To develop the language, we took a bottom up approach, but a top up approach can also be taken. We must take a finite alphabet to develop the language, which we take to be
    %
    \[ \{ (, ), \forall, \exists, \neg, \wedge, \vee, \Rightarrow, x, f, P, 0, 1, \dots, 9 \} \]
    %
    we cannot take an `infinite number of variables', in the sense of an infinite number of symbols, for then formal language theory does not apply. We must instead assume our variables, predicates, and functions are themselves words in a finite alphabet. For instance, we will enumerate our variables
    %
    \[ \Lambda = \{ x, x_0, x_{00}, \dots, x_{0000000000}, \dots \} \]
    %
    The trick to forming functions and predicates is to put $n$ ones after an $f$ or a $P$ to denote that is is an $n$-ary function, so
    %
    \[ \mathcal{F}^n = \{ \left.f^{111 \dots 11}\right., \left.f^{111\dots11}\right._0, \dots, \left.f^{111 \dots 11}\right._{000000000}, \dots \} \]
    %
    \[ \mathcal{P}^n = \{ P^{1111\dots11}, \dots \} \]
    %
    Then we may form predicate logic as a context free grammar. The variables are easiest to form
    %
    \[ X \rightarrow {x}\ |\ {X_0} \]
    %
    Terms are tricky to define because we must form functions as well. The trick is to introduce new variables $Y$ and $Z$ which add enough terms to the function.
    %
    \begin{align*}
        T &\rightarrow {X}\ |\ {f^1U)}\\
        U &\rightarrow {V(T}\ |\ {^1U, T}\\
        V &\rightarrow \varepsilon\ |\ V_0
    \end{align*}
    %
    Finally, we form the formulas of the calculus. Again, the only trick part are the atomic formulae
    %
    \begin{align*}
        F &\rightarrow {(F \wedge F)}\ |\ {(F \vee F)}\ |\ {(\neg F)}\ |\ {(F \Rightarrow F)}\ |\ {(\forall x: F)}\ |\ {(\exists x: F)}\ |\ {P^1Z)}\\
        Y &\rightarrow {Z(T}\ |\ {^1Y, T}\\
        Z &\rightarrow \varepsilon\ |\ Z_0
    \end{align*}
    %
    We showed the formulas are derived unambiguously, but this took a lot of hard work. It is impossible to find a general procedure to decide whether a language is ambiguous, which is what makes verifying ambiguity so difficult.
\end{example}

It is useful to put grammars in a simple form for advanced theorems. A grammar $(V,\Sigma,R,S)$ is in \emph{Chomsky Normal Form} if the only relations in $R$ are of the form
%
\[ S \rightarrow \varepsilon\ \ \ \ \ A \rightarrow BC\ \ \ \ \ A \rightarrow a \]
%
where $A,B,C \in V$ and $B,C \neq S$, and $a \in \Sigma$.

\begin{theorem}
    Every context-free language can be recognized by a context-free grammar in Chomsky normal form.
\end{theorem}
\begin{proof}
    We shall reduce any context free grammar $G = (V,\Sigma,R,S)$ to a context free grammar in normal form in a systematic fashion, adding each restriction one at a time.

    \begin{enumerate}
        \item \emph{No derivation rules map to the start variable}:\\
        Create a new start variable mapping onto the old start variable.

        \item \emph{There are no $\varepsilon$-transition rules except from the start variable}:\\
        Define a variable $A \in V$ to be nullable if we may derive $\varepsilon$ from $A$. Let $W$ be the set of all nullable variables. Define a new language $G'(V,\Sigma,R',S)$ such that, if $A \rightarrow A_1 \dots A_n$ is a derivation rule in $G$, and $A_{i_1}, \dots, A_{i_m}$ are nullable, then we add $2^m$ new rules to $G'$ by removing some subset of the $A_{i_k}$. Then $L(G') = L(G) - \{ \varepsilon \}$, so that if $\varepsilon \in L(G)$, we need only add an $\varepsilon$ rule to $S$ to make the two languages equal.

        We will prove that if $A$ is a variable in $G'$, then $A$ can derive $w \in \Sigma^*$ in $G'$ if and only if it can derive it in $G$ and $w \neq \varepsilon$. One way is trivial, the other a proof by induction on the length of the derivation. Suppose we have a derivation in $G$
        %
        \[ A \rightarrow s_0 \rightarrow \dots \rightarrow s_n \rightarrow w \]
        %
        Let $s_0 = A_1 \dots A_n$. Then each $A_i$ derives $w_i$ in $G$, where $w = w_1 \dots w_n$. We can choose such a derivation to be shorter than the derivation of $G$. But this implies that $A_i$ derives $w_i$ in $G'$, provided $w_i \neq \varepsilon$. Let $w_{i_1} \dots w_{i_m} \neq \varepsilon$. Then $m \neq 0$, since $w \neq \varepsilon$. We have a corresponding production rule $A \rightarrow A_{i_1} \dots A_{i_n}$ in $G'$, since the other variables are nullable. Thus, by induction, $A$ can derive $w$.

        \item \emph{There are no derivation rules $A \rightarrow B$, where $B$ is a variable}:\\
        Call $B$ unitarily derivable from $A$ if there are a sequence of derivation rules
        %
        \[ A \rightarrow V_1 \rightarrow \dots \rightarrow V_n \rightarrow B \]
        %
        Define a new grammar $G' = (V,\Sigma,R',S)$. If $B$ is directly derivable from $A$, and $B$ has a derivation rule $B \rightarrow s$, then $G'$ has a production rule $A \rightarrow s$, provided that $s$ is not a variable. Then $G'$ has no rules of the form $A \rightarrow B$, and generates the same language. This is fairly clear, and left to the reader to prove.

        \item \emph{Every rule is of the form $A \rightarrow AB$ or $A \rightarrow a$}:\\
        If we have a rule in $G$ of the form $A \rightarrow s$, where $s = s_1 \dots s_n$, and $s_{i_1}, \dots, s_{i_m} \in \Sigma$, then add new unique variables $A_{s_{i_k}}$ for each $s_i \in \Sigma$, and replace the rule with new rules of the form
        %
        \[ A \rightarrow s_1 \dots A_{s_{i_1}} \dots A_{s_{i_m}} \dots s_n \]
        %
        \[ A_{s_{i_m}} \rightarrow s_{i_m} \]
        %
        Thus we may assume every rule of the form $A \rightarrow A_1 \dots A_n$ (where we may assume $n \geq 2$) only maps to new variables. But then we may add new variables $V_2 \dots V_{n-1}$, and swap this rule with rules of the form
        %
        \[ A \rightarrow V_{n-1} A_n \]
        %
        \[ V_k \rightarrow V_{k-1} A_k \]
        %
        \[ V_2 \rightarrow A_1 A_2 \]
        %
    \end{enumerate}
    %
    Now every derivation rule is in the correct form, and we have reduced every grammar to Chomsky normal form.
\end{proof}

Chomsky normal form allows us to prove a CFG pumping lemma.

\begin{theorem}
    If $L$ is a context free language, then there is $p > 0$, such that if $s \in L$, and $|s| \geq p$, then we may write $s = uvwxy$, where $|vwx| \leq p$, $v,x \neq \varepsilon$, and for all $i \geq 0$, $uv^iwx^iy \in L$.
\end{theorem}
\begin{proof}
    Let $A$ be the language of the grammar $G$, which we assume to be in Chomsky normal form. Let there be $v$ variables in $G$. If the parse tree of $s \in A$ has height $k$, then $|s| \leq 2^{k-1}$, which follows because the tree branches in two except at roots, so there is at most $2^{k-1}$ roots. If $|s| \geq 2^{3v}$, then every parse tree of $s$ has height greater than $v$. Pick a particular parse tree of smallest size. There is a sequence of variables $A_1, A_2, \dots, A_{3v}$, such that $A_i$ is the parent of $A_{i+1}$. Because of how many variables there are, some variable must occur at least 3 times (for otherwise we may remove the variables in pairs, to conclude that $3v - 2v = v$ variables contain no variables, a contradiction). What's more, they much occur within a height of $3v$ of each other. Let $A_i = A_j = A_k$, for $i < j < k$. Let $A_i$ produce $a A_j b$, let $A_j$ produce $x A_k y$, and let $A_k$ produce $r$. Write $s = m a x r y b n$. By virtue of the minimality of the tree, we may assume that $a$ or $b$ is nonempty, and one of $x$ or $y$ is nonempty. First, if both $ax$ and $yb$ are assumed non-empty, then we may pump these strings up, and have proved our lemma. So suppose $ax$ is empty. Then $b$ and $y$ are nonempty, and $s = mrybn$. Since $A_i$ produces $A_j b$, and $A_j$ produces $A_k y$, $A_i$ may be pumped to produce $A_j b^i$, $A_j$ may be pumped to produce $A_k y^i$, and $mry^ib^in$ is in the context free language, so we have non-empty strings to pump. The proof is simlar if $by$ is empty, for then $a$ and $x$ are non-empty. The constraint $|vxy| \leq k$ is satisfies for $yb$ and $ax$, for the $A_i$ and $A_j$ lie within $3v$ of each, so the string produces in this production is at most as long as $2^{3v}$, which is less than or equal to the pumping length.
\end{proof}

\section{Pushdown Automata}

Regular languages have representations as the languages of regular expressions or as finite automata. Context-free languages also have dual representations, as `machines' or as abstraction operations. It is good to represent a language as a machine for it may hint as to what hardware capabilities a computer must have to be able to solve problems related to the languages. The key machine component for a context-free language is a stack. A pushdown automata is a finite state automata with the addition of a stack.

\begin{definition}
    A \emph{(non-deterministic) pushdown automata} is a tuple $(Q, \Sigma, \Lambda, \Delta, q_0, F)$, where $Q$ is a finite set of states, $\Sigma$ is an alphabet, $\Lambda$ is the stack alphabet, $\Delta: Q \times \Sigma_\varepsilon \times \Lambda_\varepsilon \to \mathcal{P}(Q \times \Lambda_\varepsilon)$ is the state transition function, $q_0 \in Q$ is the start state, and $F \subset Q$ are the accept states.
\end{definition}

It turns out that deterministic pushdown automata are less powerful than non-deterministic automata, so we do not discuss deterministic automata. It is interesting to note that the languages of deterministic automata are connected to unambiguous grammars, though we will not have time to discuss this further.

Let us describe a pushdown automata intuitively. The automata has a stack of symbols from $\Lambda$, which it can push and pull from when deciding how to move through the machine. A stack is a string in $\Lambda^*$. Thus, formally, a string $s$ is \emph{accepted} by a push-down automata $M$ if there are a sequence of states $q_0, \dots, q_n$, and stacks $w_0, \dots, w_n \in \Lambda^*$ such that $q_n \in F$, $w_0 = \varepsilon$, and if we write $w_i = w \lambda$, with $\lambda \in \Lambda_\varepsilon$, then $w_{i+1} = w_i \lambda'$, with $(q_{i+1}, \lambda') \in \Delta(q_i,\lambda)$. Thus we pop and pull off the rightmost character in the string when moving between states.

Pushdown automata have enough versatile memory to recognize context free languages. The stack can `remember' variables it has yet to parse, and check when symbols are used. We shall allow a mild generalization of pushdown automata, which can push multiple symbols to the stack at a time. This is fine, without loss of generality, because we could have instead introduced new states that take nothing from the stack, and push the symbols on one at a time. We shall also assume a pushdown automata starts with a $\$$ symbol at the bottom of its stack, which is fine, because we could have added another start state to the automata which pushes the $\$$ on as we begin running the machine.

\begin{theorem}
    Every context free language is accepted by a pushdown automata.
\end{theorem}
\begin{proof}
    Consider a context free language $(V, \Sigma, R, S)$. Consider a pushdown automata $(Q,\Sigma, \Lambda,\Delta,q_0,F)$, with the stack language $\Lambda = \Sigma \cup V$, and $Q$ just two states $q_0$ and $f_0$. For each derivation $A \rightarrow s \in R$ we have
    %
    \[ (q_0, s) \in \Delta(q_0, \varepsilon, A) \]
    %
    And for each $a \in \Sigma$, we have
    %
    \[ (q_0, \varepsilon) \in \Delta(q_0, a, a) \]
    %
    And a finale transition
    %
    \[ (f_0, \varepsilon) \in \Delta(q_0, \varepsilon, \$) \]
    %
    It is clear that this automata parses the context free language.
\end{proof}

A converse also holds, so that pushdown automata are equivalent computers of context free languages. To do this, we assume that the automata pushes everything off its stack before it finishes, and has a single accept state $f_0$. In addition, we shall assume that a state only pops and pulls in one action, and doesn't do both at the same time. Adding additional states means this is no loss of generality.

\begin{theorem}
    Each pushdown automata language is context free.
\end{theorem}
\begin{proof}
    The gist of our approach is as follows. Let $(Q, \Sigma, \Gamma, \delta, q_0, \{ f_0 \})$ be a pushdown automata. We shall define a context grammar with variables $A_{pq}$, with $p,q \in Q$. This variable should be able to generate all possible strings which can start in $p$ with an empty stack, and end up in $q$ with an empty stack. Our start variable will then be $A_{q_0, f_0}$. The first rules are most basic
    %
    \[ A_{pp} \rightarrow \varepsilon \]
    %
    Such a path may end up empty halfway through the path, so we have these rules, for each $p,q,r \in Q$,
    %
    \[ A_{pq} = A_{pr} A_{rq} \]
    %
    We can also pop something on the stack, and save it for a long time later. If $t \in \Sigma$, and $(r, t) \in \Delta(p, a, \varepsilon)$, and $(q, \varepsilon) \in \Delta(s, b, t)$, then we add the derivation rule
    %
    \[ A_{pq} = a A_{r,s} b \]
    %
    We claim these rules describe all possible derivations we could make in the pushdown automata. It is clear that all such derivations in this context language are accepted in the pushdown automata.

    We shall prove that if we can move from $p$ to $q$ using a string $x$, both with an empty stack, then $A_{pq} \rightarrow x$. This is done by induction on the number of steps to accept the string in the automata. If we do this in one step, then the string is empty, and we have a rule $(q, \varepsilon) \in \Delta(p, \varepsilon)$, or the string consists of a single letter, and we have a rule $(q, \varepsilon) \in \Delta(p, t)$. In the first case, we have a derivation $A_{p, q} \rightarrow \varepsilon$, and in the second, we have a derivation
    %
    \[ A_{p,q} \rightarrow t A_{q,q} \varepsilon \rightarrow t \varepsilon \varepsilon = t \]
    %
    Now consider a machine that runs for a length $n$. Suppose the stack empties at some state $k$, after running through $x_1$ of the string, for $x = x_1 x_2$. Then we have, by induction, a derivation
    %
    \[ A_{pq} \rightarrow A_{pk} A_{kq} \rightarrow x_1 x_2 \]
    %
    Thus we may assume that the stack never empties except at beginning and end. Then the first action must be to push a symbol $t$ to the stack, and the last action to remove $t$. If we move from $p$ to $r$ in the first action by reading $a$, and from $s$ to $q$ in the last action by reading $b$, then we may write $x = acb$, and by induction, we have the derivation
    %
    \[ A_{pq} \rightarrow a A_{rs} b \rightarrow acb = x  \]
    %
    Thus we have verified the equivalence of the pushdown automata and context free language.
\end{proof}

Pushdown automata are easy to connect to their finite state cousins.

\begin{corollary}
    Every regular language is context free.
\end{corollary}



\chapter{Turing Machines and Uncomputability}

Finite state machines are good at modelling machines with a small amount of memory, and pushdown automata are good at modelling stack processes. Nonetheless, there are many problems that `should be computable', since we can solve them which these automata cannot solve. In this chapter, we introduce a much more robust model, the \emph{Turing machine}, which satisfies this criteria. Every known algorithm can be implemented in this model.

The basic idea is that a Turing machine runs off of an infinite tape, which the machine can scan through, swerving left and right to read off the tape. The tape begins with the input
%
\[ q_0 x_1 x_2 \dots x_n \]
%
A notation which implies that the machine is in state $q_0$, and the tape head is looking at $x_1$, and the tape consists of the letters $x_1, x_2 \dots, x_n$, and then the rest of the tape consists of blank spaces.

The Turing machine has finitely many states, which describe how the machine reacts when it sees a current character -- it chooses to swap the current character with a different character, and moving either left or right one space. We may also choose to accept the string or reject the string at any time.

Formally, a turing machine can be described as a tuple
%
\[ (Q, \Sigma, \Gamma, \Delta, q_0, q_{\text{accept}}, q_{\text{reject}}) \]
%
where $Q$ is a set of states, $\Sigma$ is an input alphabet, $\Gamma$ is a tape alphabet (which we assume includes the blank space $-$), $q_0 \in Q$ is the start state, $q_{\text{accept}} \in Q$ is the accept state, and $q_{\text{reject}} \in Q$ is the reject state, and $\Delta: Q \times \Gamma \to Q \times \Gamma \times \{ L , R \}$. We interpret this as given a state and a currently read letter, we move to a new state, we replace the letter with a new character, and we move left or right.

To compute the operation of a Turing machine on a string, we recursively define a computation on a string. A specific state of the machine will be represented by a string $sqw$, where $q \in Q$, $s,w \in \Gamma^*$. We say that $sqtw$ \emph{yields} $suq'w$ if $(q', u, R) = \Delta(q,t)$.

\chapter{Complexity Theory}

We have now discovered what it means to solve a problem algorithmically. We formalize the problem into a certain language over an alphabet, then find a Turing machine over that alphabet which terminates on every input, and accepts exactly theres strings that are an element of a language. But just because a problem is solvable, does not mean that it is {\it feasibly solvable}; there may be an algorithm which will eventually solve a problem, but if it takes centuries to find the answer, the solution is not effective. In this section, we restrict our attention to the computable problems, and describe the degree of difficulty of certain problems. First, we must find a measure of a problem's difficulty, so that we can classify problems based on how difficult they are.

\section{Measuring Complexity}

Let $M$ be a turing machine over an alphabet $\Sigma$, which halts on all inputs. Then, for each string $s \in \Sigma^*$, $M$ deterministically executes a certain number of steps, eventually terminating. Define $t_M(s) \in \mathbf{N}$ to be the number of configurations $M$ steps through before it halts on input $s$. The \emph{time complexity} of $M$ is the function
%
\[ f_M(n) = \max \{ t_M(s): s \in \Sigma^*, |s| = n \} \]
%
It could be argued that a better notion of complexity is the average running time over inputs of a certain length, but we rarely know how often certain inputs will occur. Often, in practice, slow inputs occur much more often that fast inputs, so worst time analysis better reflects an algorithm's speed. Furthermore, worst time analysis gives us an elegant and useful theory which gives meaningful results, which without this simplification would be impossible. We should not be dogmatic in this approach, because some algorithms do benefit from an average case analysis (for instance, the ellipsoid algorithm in linear programming), but the worst-case approach has turned out to be the most useful.

We rarely specify a turing machine exactly, and thus have difficulty expressing $f_M$ as an exact number. Even if we described a machine exactly, $f_M$ likely will not have a simple formula specifying an algorithms speed. Thus we apply the theory of asymptotics. Recall that a real-valued function $f$, the `big O' set $O(f)$ consists of all functions $g$ for which $|g| \leq c|f|$ {\it eventually} holds, where $c$ is some positive constant. Similarily, the `little o' set $o(f)$ consist of all functions $g$ satisfying the sharper relation
%
\[ \lim_{x \to \infty} \left|\frac{g(x)}{f(x)}\right| = 0 \]
%
Less important to our needs, but canonically include are the $\Omega(f)$ and $\omega(f)$ sets, which consist of functions $g$ such that $f \in O(g)$ and $f \in o(g)$ respectively. We define $\Theta(f) = \Omega(f) \cap O(f)$, the class of functions which are asymptotically equal to $f$.

\begin{example}
    Let $P \in \RR[X]$ be a polynomial of degree $m$. For $x \geq 1$, and $i \leq j$, the inequality $x^i \leq x^j$ holds, so if $P$ is expressed in coefficients as
    %
    \[ P = \sum c_i X^i \]
    %
    then for $x \geq 1$,
    %
    \[ |P(x)| \leq \sum |c_i| x^i \leq \left( \sum |c_i| \right) x^m \]
    %
    and we have shown that $P \in O(x^m)$.
\end{example}

\begin{example}
    Since $O(f)$ is a subset of a space of functions, they can be manipulated under real-valued operations. As an example, we consider the set $2^{O(\log n)}$, which consists of all functions of the form $2^f$, where $f \in O(\log n)$. We contend this is just the set of all functions $g$ in $O(n^c)$ for some $c$. First note that if
    %
    \[ f(x) \leq c \log x \]
    %
    then because the exponential function is monotone, we obtain that
    %
    \[ 2^{f(x)} \leq 2^{c \log x} = x^c \]
    %
    so if the first inequality eventually holds, the second inequality eventually holds. Conversely, suppose $f \in O(x^c)$. Suppose that eventually
    %
    \[ f(x) \leq K x^c \]
    %
    Then if $x \geq e$,
    %
    \[ \log_2 f(x) \leq \log_2 K + c \log_2 x \leq \frac{c + 2 \log_2 K}{\log 2} \log x \]
    %
    which implies $\log_2 f \in O(\log n)$, and $f = 2^{\log_2 f} \in 2^{O(\log n)}$.
\end{example}

\begin{example}
    A commonly used relation is that $O(f) + O(g) = O(f + g)$. Let $h$ and $k$ be functions with a constant $K$ and $n_0$, such that
    %
    \[ h \leq Kf\ \ \ \ \ k \leq Kg \]
    %
    eventually holds. Then
    %
    \[ h + k \leq K(f + g)\]
    %
    eventually holds as well, so $h + k \in O(f + g)$. Conversely, suppose $h \in O(f + g)$, and let
    %
    \[ h \leq K(f + g) \]
    %
    Define
    %
    \[ h_1 = \min(h,Kf)\ \ \ \ \ h_2 = h - h_1 \]
    %
    Then $h_1 + h_2 = h$. It is easy to see $h_1 \in O(f)$. Now if $n \geq n_0$, then
    %
    \[ h(n) - h_1(n) \leq Kf + Kg - h_1(n) \leq Kg \]
    %
    Thus $h_2 \in O(g)$, and we have verified the equality. Similar arguments show
    %
    \[ O(fg) = O(f)O(g) \]
\end{example}

Certainly, a machine $M$ with $f_M(n) = n^2$ runs much faster than a machine $N$ with $f_N(n) = 2^8 n^2$, and certainly a machine $L$ with $f_L(n) = 2^n$ runs faster than $f_N$ for the first 20 input sizes, but regardless of the constant, the exponential machine will quickly become much slower even if we choose a much larger constant ($2^{100} n^2$ becomes smaller than $2^n$ only after the input size increases to 125). What's more, we have a theorem that, by constructing a new machine, we can always decrease the constant in an algorithm, with at most a linear increase in time.

\begin{theorem}
    For any two-tape Turing machine $M$, and $\varepsilon > 0$, there is an equivalent two-tape machine $N$ such that
    %
    \[ f_N(x) \leq \varepsilon f_M(x) + (1 + \varepsilon) n \]
    %
    eventually holds.
\end{theorem}
\begin{proof}
    Consider the following strategy. Let $M$ have tape alphabet $\Sigma$ and tape alphabet $\Sigma \cup \Sigma^n$. Construct $N$ with tape alphabet $\Sigma \cup \Sigma^m$, and with states $Q \times \Sigma^m \cup Q'$. Our machine takes the first $m$ characters $w_1, \dots, w_m$ in input, and places $(w_1, w_2, \dots, w_m) \in \Sigma^m$ on the second tape. This takes $m$ steps. If the input does not contain $m$ strings, we assume that we write in whitespace instead. Continue this until we reach whitespace. After $m \lceil n/m \rceil$ steps, we have a condensed input. After this preprocessing, we can execute $m$ steps of the original machine in 6 steps of our algorithm. Our state in $N$ will now be of the form $(q,t)$, where $q$ is a state in $M$, and $t$ is the current position in the tuple $(w_1, \dots, w_m)$ we are pointing at in the second string that the simulated machine should be at. We move left once, and then right twice, in order to see the states to the left and right of our current states. Given this information, we can predict whether we will end up on the left or right after $m$ steps of the simulated machines, and what the other states will look like. Only states in the current tuple, and the left or right tuple (but not both) will be changed. Based on our computation, we move left or right, changing the states we are currently in, and, if needed, move again to change the states in our new tuple. Thus in six steps, we have simulated $m$ steps. If $M$ takes $k$ steps to halt on some input, $N$ takes $m \lceil n/m \rceil + \lceil k/m \rceil$. For $n$ large enough,
    %
    \[ m \lceil n/m \rceil \leq (1 + \varepsilon) n \]
    %
    which implies that eventually
    %
    \[ f_N \leq \lceil f_M/m \rceil + m \lceil n/m \rceil \]
    %
    If we choose $m$ large enough, we obtain the inequality desired.
\end{proof}

If we only use one tape, then the increase becomes $\varepsilon f_M + 2 n^2 + 2$. We also note that programs which run in linear time can only improved to $(1 + \varepsilon) n$, for some $\varepsilon$. This makes sense, for if some program runs in time bounded by $cn$ for $c < 1$, then for large inputs the program must eventually not even look at all the input, which is unfeasible in most problems.

Since we do not describe Turing machines formally, we rely on heuristic requirements to determine upper bounds for certain algorithms. Our intuition, guided by our knowledge of formal Turing machines, should carry us through, provided we describe algorithms in enough detail that the underlying Turing machine can be seen in enough detail.

We now have the formality to classify problems by how long it takes to compute them. Given a function $g$, define $\mathbf{TIME}(g)$ to be the class of all languages $L$ which are recognized by a machine $M$, and $f_M \in O(g)$. Thus $\mathbf{TIME}(g)$ consists of all problems which can be computed `roughly in time with $g$'. This is the first example of a \emph{complexity class}, a set of languages characterized by how slow it takes to decide upon the language.

\begin{example}
    Consider the language $A = \{ 0^k1^k : k \geq 0 \}$. Determine a turing machine deciding the language from the algorithm
    %
    \begin{enumerate}
        \item Scan across input, checking whether the input is of the form $0^i 1^j$ for some $i,j$. We ensure the string only contains 0s and 1s, and that the string contains no 0s after it contains 1s. Reject in any other circumstance. Afterwords, return to the beginning of the tape.
        \item While there are still unmarked 0s and 1s on the tape, tick off a new 0 on the tape and a new 1. If there are no 0s but still 1s, or 0s but no 1s, reject the input.
        \item We have ticked off one 0 for each 1, so there are the same number of zeroes as ones. Accept the input.
    \end{enumerate}
    %
    We analyze the three steps individually, putting the asymptotics together once we're done. Step 1 takes roughly a constant number of steps for each element of the input, for we perform the same operation on each character. Thus the time to compute step 1 is in $O(n)$. The number of times step 2 is executed is at most half the characters in the input, and each iteration is in $O(n)$, for the iteration involes moving from the beginning to the end of the tape a finite number of times. Thus step 2 is in
    %
    \[ O(n/2)O(n) = O(n)^2 = O(n^2) \]
    %
    Finally, step 3 is a constant time operation, and is therefore in $O(1)$. Thus the entire algorithm is in
    %
    \[ O(n) + O(n^2) + O(1) = O(n^2 + 2n + 1) = O(n^2) \]
    %
    Thus $A \in \mathbf{TIME}(n^2)$. A divide and conquer variation of this algorithm shows that $A \in \mathbf{TIME}(n \log n)$, left as an exercise.
\end{example}

\section{Models of Complexity}

We have considered various variants of Turing machines. All turn out to decide the same class of languages, hence the adoption of the machine as an applicable model of real world computation. But we run into issues when applying this argument to complexity theory, for using a different model of turing machine may reduce the time complexity of an algorithm.

\begin{example}
    Consider the $0^k1^k$ problem. The best algorithm we found ran in $O(n \log n)$ time. But consider a 2-tape Turing machine, which first shifts all 1s in the input to a 2nd tape, then procedurally checks off 0s and 1s. This runs in $O(n)$ time, for we need only scan over the tape a constant number of times. Later, we shall show that the asymptotics of the $0^k1^k$ problem cannot be improved on a single tape machine, so multi-tape turing machines are asymptotically faster than one tape machines.
\end{example}

We may take comfort in discovering that changing the model does not {\it drastically} affect the computation time of an algorithm. This is what this section sets out to address.

\begin{theorem}
    If a multi-tape turing machine has time complexity $f$, where $f \geq n$, then the multi-tape turing machine has an equivalent single-tape turing machine with time complexity in $O(f^2)$.
\end{theorem}
\begin{proof}
    We have already described a procedure which simulates a $k$-tape turing machine. We shall compute an asymptotic analysis of this simulation. Suppose the multitape turing machine takes $g(n)$ steps before terminating on a particular input of size $n$. Let us analyze each step
    %
    \begin{enumerate}
        \item The algorithm first takes the input $w_1 \dots w_n$, and manipulates it into the form
        %
        \[ \# \dot{w_1} \dots w_n \# \dot{\visspace} \# \dot{\visspace} \# \dot{\visspace} \# \dots \# \dot{\visspace} \]
        %
        where we have $k$ hashtags. This takes $O(n + k)$ steps.

        \item For each of the $g(n)$ steps the multitape machine takes, we must pass through our simulation tape, recognizing the current state. We then perform a second pass to move dots left or right when needed. In each step, we add at most $k$ new elements to our tape, so the size of the tape is always bounded by $n + k g(n)$. Therefore the number of steps in the first pass is in $O(n + kg(n))$. The second pass moves through the tape, and pushes at most $k$ new symbols into the tape, otherwise just moving the dots in the tape back and forth. A push moves at most $n + kg(n)$ symbols to the right, and thus the number of steps we take is in $O(k(n + kg(n)))$.
    \end{enumerate}
    %
    Step 2 is applied $g(n)$ times, so overall, the speed of the algorithm is in
    %
    \begin{align*}
        O(n + k) + O(n + kg(n)) + g(n) O(n + kg(n)) = O(ng(n) + g^2(n))
    \end{align*}
    %
    assuming that $n \in O(g)$, then the speed is in $O(g^2)$.
\end{proof}

Thus, though multitape machines may compute faster, they only introduce do things quadratically faster than single tape machines. One of the biggest issues with complexity theory is that this is not true of non-deterministic machines.

First off, we must debate how long a non-deterministic machine takes to compute. We cannot simply count all steps the non-deterministic machine takes, because the machine takes many branches, some of which may be infinite. Since we can see non-deterministic computation as some sort of parallel processing, we could define the time to be the length of the shortest branch of processesing which yields termination. Mathematically, however, we will see that it is more convenient to define the run time to be the length of the longest branch. We are then able to define the time complexity of a non-deterministic automata.

\begin{theorem}
    If a non-deterministic machine runs in $O(f)$ time, then there is an equivalent deterministic automata which runs in $2^{O(f)}$ time.
\end{theorem}
\begin{proof}
    Perform a time analysis on the turing machine which simulates a non-deterministic machine.
\end{proof}

Thus non-deterministic algorithms are fundamentally connected to exponential deterministic algorithms. This makes sense, for in general, if a problem can be divided into exponentially many cases to check, each verifiable in a linear amount of time, then a non-deterministic algorithm can split into each possible case, exponentially dividing, and then check each case in a polynomial amount of time, giving us a polynomial time algorithm. Thus the time of Non-deterministic turing machines is bounded by the time it takes to verify a single case.

Now we get to the fun stuff. Define the complexity class
%
\[ \textbf{P} = \bigcup_{k = 0}^\infty \textbf{TIME}(n^k) \]
%
these are all problems computable in polynomial time on a deterministic automata.













\chapter{Pseudorandomness}

Consider a binary string of $n$ bits. What does it mean for this string to be random, when a random string of $n$ bits takes every value with equal probability? What it means for a string to be random is realized by the modern theory of \emph{algorithmic randomness}.

\section{Statistical Testing}

Let us come up with conditions `random strings' should satisfy from a statistical approach. Consider an infinite string, i.e. an element $S = \{ S_i \} \in \Sigma^{\NN}$, where $\Sigma$ is some finite alphabet. If $\{ S_i \}$ were a family of independent random variables, uniformly distributed on $\Sigma$, then the law of large numbers tells us that, almost surely, for each $\sigma \in \Sigma$, if $N_S(\sigma,n)$ is the number of times that $\sigma$ occurs in the first $n$ digits of the sequence $S$, then by the law of large numbers, almost surely,
%
\[ \lim_{n \to \infty} \frac{N_S(\sigma,n)}{n} = \frac{1}{\#(\Sigma)}. \]
%
Such a sequence is called \emph{simply normal}. More generally, for \emph{any} finite string $s \in \Sigma^*$ of length $m$, if we let $N_S(s,n)$ denote the number of occurences of $s$ as a substring of the initial first $n$ characters of $S$, then the Kolmogorov zero-one law implies that almost surely,
%
\[ \lim_{n \to \infty} \frac{N_S(s,n)}{n} = \frac{1}{\#(\Sigma)^m}. \]
%
Such a sequence is called \emph{normal}. A \emph{number} $x \in \RR$ is \emph{normal in base $b$} if it's expansion in base $b$ is a normal sequence, and \emph{normal} if it is normal in every base. Almost all numbers are normal, but it is very difficult to prove that particular numbers, like $\sqrt{2}$, $e$, or $\pi$, are normal.

Certainly a `random sequence' should be normal. More generally, one might expect that a `random sequence' $\{ S_i \}$ has the property that for any increasing function $f: \NN \to \NN$, the sequence $\{ S_{f(i)} \}$ is normal. However, there is an obvious problem here. The pidgeonhole principle implies that there exists $\sigma \in \Sigma$ and an increasing function $f: \NN \to \NN$ such that $X_{f(i)} = \sigma$ for all $i$. Certainly, the sequence $\{ S_{f(i)} \}$ is not normal. To fix this problem, we should choose $f$ in a way that does not depend on $X$, in some sense. An approach suggested by Von Mises, and later established by Alonso Church, was to restrict to the study of \emph{computable functions}. But in 1939, Ville showed that von Mises' approach would not give `fully random' sequences - he constructed, for any countable set of functions $\mathcal{F}$, a binary sequence $\{ S_i \}$ such that $\{ S_{f(i)} \}$ is normal for all $f \in \mathcal{F}$, but such that for every $n$, the subsequence $S_1 \dots S_n$ has more zeroes than ones - this never happens since almost surely,
%
\[ \liminf_{n \to \infty} \frac{N_S(s,n) - n}{\sqrt{2n \log \log n}} = -1 \quad\text{and}\quad \limsup_{n \to \infty} \frac{N_S(s,n) - n}{\sqrt{2n \log \log n}} = 1. \]
%
One can immediately fix this by declaring a number to be random if it passes the law of the iterated logarithm in addition to the law of large numbers. But how do we know there is not another random test that could cause us problems here.

A fix to this problem emerged in the 1960s, due to Martin-L\"{o}f. In probability theory, a $\sigma$-algebra is defined on $\Sigma^{\NN}$ by taking the Borel topology, which is that topology generated by the open sets $[s]$, for each $s \in \Sigma^*$, which is the set of all infinite strings which have $s$ as an initial string. We define a Borel probability measure $\mu$ on $\Sigma^{\NN}$, such that
%
\[ \mu([s]) = \frac{1}{\#(\Sigma)^n}. \]
%
We have previously tried to define a sequence $S$ to be random by finding sets $E \subset \Sigma^{\NN}$ with $\mu(E) = 0$, and declaring that $S$ should not be an element of $E$. This is of course not possible for all such $E$, since $\{ S \}$ is a null set containing $S$ for any sequence $S$, so we instead specialize to the \emph{effectively null sets}.

Recall that a set $S \subset \Sigma^*$ is \emph{computably enumerable} if there is a Turing machine which is able to list out all elements of $S$ (and only elements of $S$). The set is \emph{computable} if there is a Turing machine with an input tape such that, on input $n \in \Sigma^*$, the Turing machine is able to decide in finite time whether $n$ is an element of $S$, or not an element of $S$. These notions are not equivalent: the family of all descriptions of Turing machines which halt is not a computable set (Turing's famous proof), but the set is computably enumerable - simply set up a growing queue which eventually enumerates all possible Turing machines, and iteratively swap between adding a new Turing machine to this queue, and simulating a single step of all Turing machines in the queue, removing them when they eventually halt). These notions are closely related, however. A set is computable if and only if it is computably enumerable \emph{in increasing order of length}.

Now let $S$ be an arbitrary sequence. Now we say a sequence $\{ T_n \}$ of open subsets of $\Sigma^{\NN}$ is \emph{uniformly computably enumerable} if there exists a computably enumerable set $G \subset \NN \times \Sigma^*$ such that for each $n$,
%
\[ T_n = \bigcup \{ [\sigma] : (n,\sigma) \in G \}. \]
% [0] [10] [110] [1110] does not contain 1111111111...
In addition, if a uniformly computably enumerable sequence $\{ T_n \}$ satisfies $\mu(T_n) \leq 2^{-n}$ for all $n$, then the intersection is an \emph{effectively null set}.

The method should now be clear. For any effectively null set $E \subset \Sigma^{\NN}$, we consider a \emph{Martin-L\"{o}f test} associated with $E$; a sequence $S$ passes a Martin-L\"{o}f" test if $S \not \in E$. If a sequence passes \emph{all possible} Martin-L\"{o}f tests, it is called \emph{Martin-L\"{o}f random}. Since there are only countably many Martin-L\"{o}f tests, and any particular Martin-L\"{o}f test is almost surely passed by a random sequence of symbols, we conclude that \emph{almost every} infinite sequence is Martin-L\"{o}f random.

\begin{remark}
    Roughly speaking, this is how a Martin-L\"{o}f test: we have a Turing machine that outputs a sequence of tuples $\{ (n_k,s_k) : n_k > 0\ \text{and}\ s_k \in \Sigma^{\NN} \}$, with the a-priori guarantee that
    %
    \[ \PP \left( \bigcup_{n_k = n} [\sigma_k] \right) \leq 2^{-n}. \]
    %
    We begin by writing down each positive integer. Every time a tuple $(n_k,s_k)$ emerges from the Turing machine, we check whether $s_k$ is the initial string of $S$ - if it is, we cross off the integer $n_k$. The string $S$ passes the test if there exists a number which is not eventually crossed off in the running of the algorithm.
\end{remark}

As an example, consider the Von Mises test, i.e. testing, for a given string $s \in \Sigma^*$ of length $m$, whether a given infinite sequence $S$ satisfies
%
\[ \lim_{n \to \infty} \frac{N_S(s,n)}{n} = \frac{1}{\#(\Sigma)^m}. \]
%
Here's a Martin-L\"{o}f test which actually gives a strong statement. If $\{ S_n \}$ are independent random variables, then we can write $N_S(s,n) = Z_1 + \dots + Z_m$, where each $Z_i$ is the sum of $m$ different random quantities, each a sum of $n/m$ independent $\text{Ber}(p)$ random variables, where $p = 1/\#(\Sigma)^m$. Hoeffding's inequality guarantees that
%
\[ \left\| \frac{N_S(s,n)}{n} - \frac{1}{\#(\Sigma)^m} \right\|_{\psi_2} \leq n^{-1} \sum_i \| Z_i - \EE[Z_i] \|_{\psi_2} \lesssim (m/n)^{1/2}. \]
%
Thus there is some universal computable constant $c_1 > 0$ such that for any $t > 0$,
%
\[ \PP \left( \left| \frac{N_S(s,n)}{n} - \frac{1}{\#(\Sigma)^m} \right| \geq t \right) \leq 2 e^{- c_1 n t^2/m}. \]
%
In particular, there exists a universal computable constant $c > 0$ such that for $t \gtrsim 1$,
%
\[ \PP \left( \left| \frac{N_S(s,n)}{n} - \frac{1}{\#(\Sigma)^m} \right| \geq c m^{1/2} n^{-1/2} \log(t/2)^{1/2} \right) \leq 1/t. \]
%
Now consider a computable sequence of positive numbers $\{ \delta_k \}$ that is summable, and suppose there exists a computable sequence $\{ k_n \}$ such that
%
\[ \sum_{k_n \leq k < k_{n+1}} \delta_n \leq 2^{-n}. \]
%
Consider the following Martin-L\"{o}f test; we enumerate all pairs $(n,s)$, where $s$ is a string of length $k \geq k_n$, and
%
\[ \left| \frac{N_s(s,n)}{n} - \frac{1}{\#(\Sigma)^m} \right| \geq c m^{1/2} n^{-1/2} \log(1/2 \delta_n)^{1/2}. \]
%
It is clear that the associated open set $T_n$ has $\mu(T_n) \leq 2^{-n}$ because of the choice of $k_n$. But this means that if $S$ is Martin-L\"{o}f randm, then there exists $n_0$ such that for $n \geq n_0$,
%
\[ \left| \frac{N_s(s,n)}{n} - \frac{1}{\#(\Sigma)^m} \right| = c m^{1/2} n^{-1/2} \log(1/2 \delta_n)^{1/2}. \]
%
Thus we have a quantitative rate of convergence of these averages, as compared to the qualitative test of Von Mises.

\section{Martingales}

Suppose $S$ is a random sequence of symbols. A \emph{Martingale} is an assign of fair betting odds to each outcome of the sequence, i.e. a function $f: \Sigma^* \to [0,\infty)$ such that for any string $s$,
%
\[ f(s) = \frac{1}{\#(\Sigma)} \sum_{\sigma \in \Sigma} f(s \sigma). \]
%
If $S = \{ S_i \}$ is a random sequence of symbols, then $\EE(f(S_1 \dots S_n)) = \EE(f(\varepsilon))$ for any $n > 0$. The fact that $f$ is non-negative allows us to apply the Martingale convergence theorem to conclude that there exists a function $g: \Sigma^{\NN} \to [0,\infty)$ such that, almost surely, $f(S_1 \dots S_n) \to g(S)$. In particular, almost surely, we know that
%
\[ \limsup_{n \to \infty} f(S_1 \dots S_n) < \infty. \]
%
In this case, we say $f$ \emph{suceeds} on the sequence $S$. We say a sequence $S$ is \emph{computably random} if there does not exist a computable Martingale $f$ which succeeds on $S$.






\begin{thebibliography}{9}

\bibitem{mendelson}
    Mendelson,
    \emph{Introduction to Mathematical Logic},
    Chapman and Hall,
    1997.

\bibitem{tao}
    Terrence Tao,
    \emph{The Completeness and Compactness Theorems of First Order Logic},
    2009.

\bibitem{hofstader}
    Douglas Hofstadter,
    \emph{G\"{o}del, Escher, Bach: an Eternal Golden Braid},
    1979.

\bibitem{russelinspiration}
    I, Grattan-Guinness,
    \emph{How Bertrand Russell Discovered His Paradox},
    1978.

\end{thebibliography}


\end{document}
















\section{Multivalued Logics and Independence}

The most basic assumption of boolean logic is that there are two truth values, $\bot$ and $\top$. Everything is either true or false. But what about the statement ``God exists''? Until the skys open up, it is impossible to determine whether this statement is true or false. If we model the statement with boolean values, we must decide whether the statement is true and false. It is the principle of three-valued logic, invented by Jan \L ukasiewicz, who decided that some statements have a third `truth value', which indicates that the classical truth of the statement is unknown. After this introduction, there is nothing stopping us from considering a system with even more `truth values', which evaluate statements based on the degree of truth, representing false, possible, justifiable, likely, and so on.

In classical logic, proof systems are built to `preserve truth'. All axioms are tautologies, and anything deduced from tautologies are tautologies. How then, are we to evaluate proof systems for multi-valued logics? When creating proof systems for multi-valued logics, we hope the value of a statement which follows from other statements is less than or equal to the value of the original statement. We shouldn't be able to prove some `more true' than what we know. For instance, consider Sorites paradox
%
\begin{quote}
    (1) ``A grain of sand is not a heap of sand''.\\
    (2) ``Adding a grain of sand to something which is not a heap does not make it into a heap''\\
    (3) ``A grain cannot be turned into a heap by adding grains.''
\end{quote}
%
We fix this argument by considering it in a multi-valued logic system. For each $k$, consider the statements
%
\begin{quote}
    (1.$k$) ``k grains of sand do not make a heap''.\\
    (2.$k$) ``Adding a grain to $k$ grains of sand doesn't make $k+1$ grains a heap''\\
    (1.$k+1$) ``$k+1$ grains do not make a heap.''
\end{quote}
%
we are only allowed to conclude $(1.k+1)$ from $(1.k)$ and $(2.k)$ when $(1.k+1)$ is `less true' than $(1.k)$ and $(2.k)$. As $k \to \infty$, if we continue the argument, then we find the statement eventually becomes completely false, which is true, for eventually a certain number of grains do become a heap. This book is not intended to talk about non-standard logics in detail. Instead, we use the notion of multivalued logic to attack the notion of independance. Are the axioms of our Hilbert system minimal -- do some of the axioms apply some of the other axioms? The trick to finding whether an axiom is independent is to find a truth system in which the other axioms are suitable, but the removed axiom is not. It follows that any statements which follows from the axioms are not removed.

Let us start by discarding (A1) from our axiom system. We may only derive results from (A2), (A3), and (MP). Can we derive (A1) using these rules? We shall try and set a value system on this proof system which satisfies the decreasing property of proofs. We shall consider `truth assignments' based on the table
%
\begin{center}
    \begin{tabular}{| c | c | c | c | }
        \hline $x$ & $y$ & $H_\Rightarrow(x,y)$ & $H_\neg(x)$ \\
        \hline $0$ & $0$ & $0$ & $1$ \\
        $0$ & $1$ & $2$ & $1$ \\
        $0$ & $2$ & $2$ & $1$ \\
        $1$ & $0$ & $2$ & $1$ \\
        $1$ & $1$ & $2$ & $1$ \\
        $1$ & $2$ & $0$ & $1$ \\
        $2$ & $0$ & $0$ & $0$ \\
        $2$ & $1$ & $0$ & $0$ \\
        $2$ & $2$ & $0$ & $0$ \\
        \hline
    \end{tabular}
\end{center}
%
With this definition, we may consider truth values of formulae. Note that $x \Rightarrow (y \Rightarrow x)$ evaluates to 2 when $x$ is 1 and $y$ is 2. We shall show any provable formula from (A2) and (A3) always takes the value zero. One calculates that this is true of any instance of (A2) and (A3). Furthermore, if $s$ and $s \Rightarrow w$ is always zero, then $w$ is always zero. But this shows that $x \Rightarrow (y \Rightarrow x)$ can never be proven using (A2) and (A3).

To prove that (A2) is independent of (A1) and (A3), take the truth table
%
\begin{center}
    \begin{tabular}{| c | c | c | c | }
        \hline $x$ & $y$ & $H_\Rightarrow(x,y)$ & $H_\neg(x)$ \\
        \hline $0$ & $0$ & $0$ & $1$ \\
        $0$ & $1$ & $2$ & $1$ \\
        $0$ & $2$ & $1$ & $1$ \\
        $1$ & $0$ & $0$ & $0$ \\
        $1$ & $1$ & $2$ & $0$ \\
        $1$ & $2$ & $0$ & $0$ \\
        $2$ & $0$ & $0$ & $1$ \\
        $2$ & $1$ & $0$ & $1$ \\
        $2$ & $2$ & $0$ & $1$ \\
        \hline
    \end{tabular}
\end{center}
%
Then everything proved from( (A1) and (A3) takes the value zero always, yet (A2) takes the value $2$ when $s$ is 0, $w$ is 0, and $u$ is 1.

We prove the independence of (A3) using a different trick, related to the removal of (A3) in intuitionistic logic. If $s$ is a statement, consider $h(s)$, which is defined by
%
\[ h(x) = x\ \ \ \ \ h((x \Rightarrow y)) = (h(x) \Rightarrow h(y))\ \ \ \ \ h((\neg x)) = x \]
%
Then $h$ removes all negations signs from $s$. If $s$ is an instance of (A1) or (A2), then $s$ is a tautology, and $h(s)$ is a tautology. Furthermore, if $h(w \Rightarrow u)$, and $h(w)$ are tautologies, then $h(w \Rightarrow u) = h(w) \Rightarrow h(u)$ is a tautology. Yet
%
\[ h((\neg s \Rightarrow \neg w) \Rightarrow ((\neg s \Rightarrow w) \Rightarrow s)) = (s \Rightarrow w) \Rightarrow((s \Rightarrow w) \Rightarrow w) \]
%
which is not a tautology, and is thus an instance of (A3) which cannot be proved by (A1) and (A2).








































\begin{lemma}
    If $x \neq y$, and $x$ is not a free variable of $L$, then
    %
    \[ M[N/x][L/y] = M[L/y][N[L/y]/x] \]
    %
    where we assume ahead of time that these terms are safe for substitution.
\end{lemma}
\begin{proof}
    We can prove this by structural induction on $M$. The base cases are
    %
    \[ x[N/x][L/y] = N[L/y] = x[N[L/y]/x] \]
    %
    since $x$ is not a free variable of $L$,
    %
    \[ y[N/x][L/y] = L = y[L/y][N/x] \]
    %
    and for $z \neq x,y$,
    %
    \[ z[N/X][L/y] = z = z[L/y][N[L/y]/x] \]
    %
    Now we consider the inductive cases.
    %
    \begin{itemize}
        \item If $M = M_0 M_1$, where
        %
        \[ M_0[N/x][L/y] = M_0[L/y][N[L/y]/x]\ \ \ \ \ M_1[N/x][L/y] = M_1[L/y][N[L/y]/x] \]
        %
        then the same is clearly true for $M$ because
        %
        \[ M[N/x][L/y] = M_0[N/x][L/y] M_1[N/x][L/y] \]

        \item If $M = \lambda x.M_0$, then
        %
        \[ M[N/x][L/y] = \lambda x.M_0[L/y] = [\lambda x.M_0][L/y][N[L/y]/x] \]

        \item If $M = \lambda y.M_0$, then since $N$ is free to substitute for $y$ in $M$, we find that either $N$ has no free occurrences of $y$, or $M_0$ has no free occurences of $x$, in which case $M_0[N/x] = M_0[N[L/y]/x]$, hence
        %
        \[ M[N/x][L/y] = \lambda y.M_0[N/x] = \lambda y.M_0[N[L/x]/x] = M[L/y][N[L/y]/x] \]

        \item If $M = \lambda z.M_0$, then
        %
        \begin{align*}
            M[N/x][L/y] &= \lambda z.M_0[N/x][L/y]\\
            &= \lambda z.M_0[L/y][N[L/y]/x]\\
            &= M[L/y][N[L/y]/x]
        \end{align*}
    \end{itemize}
    %
    This induction completes the proof.
\end{proof}

\begin{theorem}
    The syntax equivalence of the $\lambda$ calculus is preserved under substitution.
    %
    \begin{enumerate}
        \item[(i)] If $\lambda \vdash M_0 = M_1$, then $\lambda \vdash M_0[N/x] = M_1[N/x]$.
        \item[(ii)] If $\lambda \vdash N_0 = N_1$, then $\lambda \vdash M[N_0/x] = M[N_1/x]$.
        \item[(iii)] If $\lambda \vdash M_0 = M_1$ and $\lambda \vdash N_0 = N_1$, then $\lambda \vdash M_0[N_0/x] = M_1[N_1/x]$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    It is clear that (i) and (ii) imply (iii), so it suffices to prove (i) and (ii) separately. First, note that if $\lambda \vdash M_0 = M_1$, then $\lambda \vdash (\lambda x.M_0) = (\lambda x.M_1)$, and hence $\lambda \vdash (\lambda x.M_0)N = (\lambda x.M_1)N$, and reducing both equations, we find $\lambda \vdash M_0[N/x] = M_1[N/x]$. This proves (i). Similarily, if $\lambda \vdash N_0 = N_1$, then $\lambda \vdash (\lambda x.M)N_0 = (\lambda x.M)N_1$, and by reduction we conclude that (ii) is true.
\end{proof}

This theorem is powerful, yet not sufficiently powerful to deal with all substitutions. For instance, suppose we want to prove that if $\lambda \vdash N_0 = N_1$, then $\lambda \vdash \lambda x.x(\lambda y.N_0) = \lambda x.x(\lambda y.N_1)$. We cannot use the last theorem to prove this, for we cannot substitute $N_0$ and $N_1$ into the equation $\lambda x.x(\lambda y.z)$ if $N_0$ or $N_1$ contains $x$ or $y$ as a free variable. In order to prove these sorts of theorems easily, we need to add an additional tool to our metacalculus.

Given the variables and constants of the $\lambda$ calculus, and a new symbol $X$ define a \emph{context} to be an element of the context free grammar
%
\[ \mathbf{C} = \mathbf{V} | X | (\mathbf{C} \mathbf{C}) | (\lambda x.\mathbf{C}) \]
%
If $C$ is a context, then we will let $C(M)$ be the term of the $\lambda$ calculus obtained by replacing all occurences of $X$ with $M$. It is important that this is considered without any notion of safety of substitution -- we can introduce both free and bound variables to $C$ by the substitution.

\begin{theorem}
    If $C$ is a context, then if $\lambda \vdash N_0 = N_1$, $\lambda \vdash C(N_0) = C(N_1)$.
\end{theorem}
\begin{proof}
    Just do a structural induction, like always...
\end{proof}

\begin{lemma}
    For any context $C$, and variables $x = x_1, \dots, x_n$, there is a term $M$ such that for all terms $N$ containing only free variables associated with $x$, then $\lambda \vdash C(N) = M(\lambda x.N)$.
\end{lemma}
\begin{proof}
    We prove by structural induction on $C$. If $C = X$, then $C(N) = N$, and if we pick $M = (\lambda y.yy)$, we find
    %
    \[ \lambda \vdash M(\lambda x.N) = (\lambda y.yy)(\lambda x.N) = (\lambda x.N)(\lambda x.N) = N = C(N) \]
    %
    If $C$ is a variable $x$, then $C(N) = x$, and if we pick $M = (\lambda y.x)$, then
    %
    \[ \lambda \vdash (\lambda y.x)(\lambda x.N) = x = C(N) \]
    %
    If $C = (C_1 C_2)$, then by induction we can find $M_1$ and $M_2$ such that
    %
    \[ \lambda \vdash C_1(N) = M_1(\lambda x.N)\ \ \ \ \ \lambda \vdash C_2(N) = M_2(\lambda x.N) \]
    %
    and if we let $M = \lambda y.(M_1 y)(M_2 y)$, we find
    %
    \[ \lambda \vdash M(\lambda x.N) = (M_1 (\lambda x.N))(M_2 (\lambda x.N)) = C_1(N) C_2(N) \]
    %
    If $C = \lambda y.C_1$, then by induction we can find $M_1$ such that
    %
    \[ \lambda \vdash C_1(N) = M_1(\lambda x.N) \]
    %
    and we then just let $M = (\lambda xy. M_1 x)$, so that
    %
    \[ \lambda \vdash M(\lambda x.N) = \lambda y.M_1(\lambda x.N) = \lambda y.C_1(N) = C(N) \]
    %
    (This is where we have to use the fact that all the free variables of $N$ are contained in $x$, for otherwise $N$ might contain $y$ as a free variable). Hence the inductive steps have been proven.
\end{proof}

This essentially means that contexts are essentially a fancy way of carefully applying $\beta$ reduction over closed terms.

\begin{corollary}
    For any context $C$ and term $M$, there are variables $x = x_1 \dots x_n$ and a term $N$ such that $\lambda \vdash C(M) = N(\lambda x.M)$.
\end{corollary}
\begin{proof}
    If $M$ has free variables $x$, then using the last theorem we find that there is a universal $N$ with $\lambda \vdash C(M) = N(\lambda x.M)$.
\end{proof}

If $M = M_1 \dots M_n$, and $x = x_1 \dots x_n$, then $M$ \emph{fits in} $x$ if the $x_i$ do not occur as free variables in the $M_j$. Given some other vector of terms $N = N_1 \dots N_n$, this means that we can define $N[M/x] = N[M_1/x_1] \dots [M_n/x_n]$. This is a well defined substitution because $M$ fits in $x$, for otherwise we might not have the variables free for substitution. Given two vectors of terms $M = M_1 \dots M_n$ and $N = N_1 \dots N_n$, then we write $\lambda \vdash M = N$ if $\lambda \vdash M_i = N_i$ for each $i$. If a term $M$ has free variables $x_1, \dots, x_n$, it is often useful to write the term as $M(x_1, \dots, x_n)$ or $M(x)$, and we often write $M(N) = M(N_1, \dots, N_n)$ for the substitition of $N_i$ for each $x_i$. In order for this substitution to be well defined, we often require that $N$ fits into $x$. Using the substitution theorem we proved before, we find that if $N_1,N_2$ fit in $x$, and if $\lambda \vdash M_1(x) = M_2(x)$, and if $\lambda \vdash N_1 = N_2$, then $\lambda \vdash M_1(N_1) = M_2(N_2)$. The nice part about $\alpha$ reduction is that if $M$ has the same number of terms as $x$ has variables, there is always some $M_0$ obtained from $M$ by $\alpha$ reduction such that $M_0$ fits in $x$.

\begin{lemma}
    If $x$ is a vector of variables, then $\lambda \vdash (\lambda x.M)x = M$.
\end{lemma}
\begin{proof} Exercise \end{proof}

\begin{theorem}[Combinatory Completeness]
    Given a term $M(x)$, there is a term $N$ such that $\lambda \vdash NL = M(L)$ for any $L$ which fits in $x$.
\end{theorem}
\begin{proof}
    Let $N = \lambda x.M$. Then $\lambda \vdash NL = M(L)$ for any $L$, because $\lambda \vdash Nx = M(x)$, by the last lemma.
\end{proof}

Thus any `operation' in the $\lambda$ calculus can be sufficiently extracted to a \emph{combinator} -- a term containing no free variables. Thus $\lambda$ calculus could be done sufficiently accurately where we assume function application only occurs over combinators. It was the idea of Sch\"{o}nfinkel that one could construct a functional calculus without any variables at all!