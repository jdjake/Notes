\input{../../style.tex}

\title{Vector Calculus}
\author{Jacob Denson}
\date{2015}

\begin{comment}

\documentclass[12pt]{amsbook}

% Used to make text margins smaller.
% \usepackage[a4paper]{geometry}

% Adds a bunch more symbols to use in latex.
\usepackage{mathabx}
\usepackage{arydshln}

% Create cool chapter headings.
\usepackage[compact]{titlesec}
\titleformat{\chapter}[display]{}
    {\flushright\fontsize{14.4}{14.4}\selectfont{\MakeUppercase{\chaptertitlename}\hspace{2ex}}\fontsize{24.88}{24.88}\selectfont{\bf\thechapter}}{-20pt}{\huge\bfseries}
\titlespacing*{\chapter}{0pt}{0ex}{4ex}

\titleformat{\section}[block]
{\normalfont\sffamily}
{\thesection}{.5em}{\titlerule\\[.8ex]\bfseries}


% Adjusts itemize and enumerate to better fit the margin.
\usepackage{enumitem}
\setlist[enumerate, 1]{leftmargin=20pt}
\setlist[itemize, 1]{leftmargin=10pt}


% Creates Theorem Environments that look better
\usepackage{thmtools}
\makeatletter
\def\thm@space@setup{
  \thm@preskip=2mm
  \thm@postskip=0mm
}
\makeatother
\makeatother

\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{exercise}{Exercise}
\newtheorem{remark}{Remark}

\theoremstyle{definition}
\makeatletter  
\def\@endtheorem{\qed\endtrivlist\@endpefalse } % insert `\qed` macro
\makeatother
\newtheorem*{proof}{Proof}


\usepackage{etoolbox}
\usepackage{needspace}
\AtBeginEnvironment{definition}{\Needspace{5\baselineskip}}


\usepackage{tikz}
\usepackage{tkz-berge}
\usetikzlibrary{arrows,chains,matrix,positioning,scopes,calc}


\renewcommand*\contentsname{\hfill Table Of Contents \hfill}


\setlength\parindent{0pt} % sets indent to zero
\setlength{\parskip}{10pt} % changes vertical space between paragraphs


% Indices added for specific things in text.
\usepackage{hyperref} 
\hypersetup{
    colorlinks = true,
    linkcolor = black,
}
\makeindex


\usepackage{mathtools}
\DeclareMathOperator{\dom}{dom}
\DeclarePairedDelimiter{\inner}{\langle}{\rangle}
\newcommand{\pder}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\tripnorm}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}


\newcommand*{\plogo}{\fbox{$\mathcal{JD}$}} % Generic publisher logo

%----------------------------------------------------------------------------------------
%   TITLE PAGE
%----------------------------------------------------------------------------------------

\newcommand*{\maketitlepage}{\begingroup % Create the command for including the title page in the document
\centering % Center all text
\vspace*{\baselineskip} % White space at the top of the page

\rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{2pt} % Thick horizontal line
\rule{\textwidth}{0.4pt}\\[\baselineskip] % Thin horizontal line

{\bf \Huge Vector Calculus}\\[0.2\baselineskip] % Title

\rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt} % Thin horizontal line
\rule{\textwidth}{1.6pt}\\[\baselineskip] % Thick horizontal line

\scshape % Small caps
\Large
Analytical Properties of $\real^n$\\[\baselineskip] % Tagline(s) or further description

\large Edmonton, Alberta, Canada\par % Location and year

\vspace*{2\baselineskip} % Whitespace between location/year and editors

{\Huge Jacob Denson\par} % Editor list

\vfill % Whitespace between editor names and publisher logo

\plogo \\[0.3\baselineskip] % Publisher logo
{\scshape 2015} \\[0.3\baselineskip] % Year published

\endgroup}

\tikzset{
    right angle quadrant/.code={
        \pgfmathsetmacro\quadranta{{1,1,-1,-1}[#1-1]}     % Arrays for selecting quadrant
        \pgfmathsetmacro\quadrantb{{1,-1,-1,1}[#1-1]}},
    right angle quadrant=1, % Make sure it is set, even if not called explicitly
    right angle length/.code={\def\rightanglelength{#1}},   % Length of symbol
    right angle length=2ex, % Make sure it is set...
    right angle symbol/.style n args={3}{
        insert path={
            let \p0 = ($(#1)!(#3)!(#2)$) in     % Intersection
                let \p1 = ($(\p0)!\quadranta*\rightanglelength!(#3)$), % Point on base line
                \p2 = ($(\p0)!\quadrantb*\rightanglelength!(#2)$) in % Point on perpendicular line
                let \p3 = ($(\p1)+(\p2)-(\p0)$) in  % Corner point of symbol
            (\p1) -- (\p3) -- (\p2)
        }
    }
}







\newcommand{\optionalsection}[1]{\section[* #1]{(Important) #1}}
\newcommand{\deriv}[3]{\left. \frac{\partial #1}{\partial #2} \right|_{#3}} % partial derivative involving numerator and denominator.
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\bint}{\mathbf{Z}}
\newcommand{\gen}[1]{\langle #1 \rangle}

\newcommand{\End}{\operatorname{End}}
\newcommand{\Mor}{\operatorname{Mor}}
\newcommand{\Id}{\operatorname{id}}
\newcommand{\visspace}{\text{\textvisiblespace}}
\newcommand{\Gal}{\text{Gal}}

\newcommand{\xor}{\oplus}
\newcommand{\ft}{\wedge}
\newcommand{\ift}{\vee}

\newcommand{\prob}{\mathbb{P}}
\newcommand{\expect}{\mathbb{E}}
\DeclareMathOperator{\Var}{\mathbb{V}}
\newcommand{\Ber}{\text{Ber}}
\newcommand{\Bin}{\text{Bin}}

\DeclareMathOperator{\sech}{sech}
\newcommand{\cadlag}{c\'{a}dl\'{a}g}
\newcommand{\caglad}{c\'{a}dl\'{a}d}

\newcommand{\loc}[1]{#1_{\text{loc}}}

%\newcommand{\widecheck}[1]{{#1}^{\ft}}

\DeclareMathOperator{\diam}{\text{diam}}

\DeclareMathOperator{\QQ}{\mathbb{Q}}
\DeclareMathOperator{\ZZ}{\mathbb{Z}}
\DeclareMathOperator{\RR}{\mathbb{R}}
\DeclareMathOperator{\HH}{\mathbb{H}}
\DeclareMathOperator{\BB}{\mathbb{B}}
\DeclareMathOperator{\CC}{\mathbb{C}}
\DeclareMathOperator{\AB}{\mathbb{A}}
\DeclareMathOperator{\PP}{\mathbb{P}}
\DeclareMathOperator{\MM}{\mathbb{M}}
\DeclareMathOperator{\VV}{\mathbb{V}}
\DeclareMathOperator{\TT}{\mathbb{T}}
\DeclareMathOperator{\LL}{\mathcal{L}}
\DeclareMathOperator{\DD}{\mathcal{D}}
\DeclareMathOperator{\SW}{\mathcal{S}}
\DeclareMathOperator{\EC}{\mathcal{E}}
\DeclareMathOperator{\AC}{\mathcal{A}}

\DeclareMathOperator{\EE}{\mathbb{E}}
\DeclareMathOperator{\NN}{\mathbb{N}}

\DeclareMathOperator{\II}{\mathbb{I}}

\DeclareMathOperator{\DQ}{\mathcal{Q}}


\DeclareMathOperator{\IA}{\mathfrak{a}}
\DeclareMathOperator{\IB}{\mathfrak{b}}
\DeclareMathOperator{\IC}{\mathfrak{c}}
\DeclareMathOperator{\IP}{\mathfrak{p}}
\DeclareMathOperator{\IQ}{\mathfrak{q}}
\DeclareMathOperator{\IM}{\mathfrak{m}}
\DeclareMathOperator{\IN}{\mathfrak{n}}
\DeclareMathOperator{\IK}{\mathfrak{k}}
\DeclareMathOperator{\ord}{\text{ord}}
\DeclareMathOperator{\Ker}{\textsf{Ker}}
\DeclareMathOperator{\Coker}{\textsf{Coker}}
\DeclareMathOperator{\emphcoker}{\emph{coker}}
\DeclareMathOperator{\pp}{\partial}
\DeclareMathOperator{\tr}{\text{tr}}
\DeclareMathOperator{\Ree}{\text{Re}}


\DeclareMathOperator{\BL}{\text{BL}}

\DeclareMathOperator{\dstrike}{//}

\DeclareMathOperator{\supp}{\text{supp}}

\DeclareMathOperator{\codim}{\text{codim}}

\DeclareMathOperator{\minkdim}{\dim_{\mathbb{M}}}
\DeclareMathOperator{\hausdim}{\dim_{\mathbb{H}}}
\DeclareMathOperator{\sobdim}{\dim_{\mathbb{S}}}
\DeclareMathOperator{\lowminkdim}{\underline{\dim}_{\mathbb{M}}}
\DeclareMathOperator{\upminkdim}{\overline{\dim}_{\mathbb{M}}}
\DeclareMathOperator{\lhdim}{\underline{\dim}_{\mathbb{M}}}
\DeclareMathOperator{\lmbdim}{\underline{\dim}_{\mathbb{MB}}}
\DeclareMathOperator{\packdim}{\text{dim}_{\mathbb{P}}}
\DeclareMathOperator{\fordim}{\dim_{\mathbb{F}}}

\DeclareMathOperator{\CT}{ {{\otimes}^\wedge} }

\DeclareMathOperator{\msupp}{\text{$\mu$-supp}}
\DeclareMathOperator{\singsupp}{\text{sing-supp}}
\DeclareMathOperator{\Char}{\text{Char}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\ssm}{\smallsetminus}


\end{comment}











\begin{document}

\pagenumbering{gobble}

%\maketitlepage
\tableofcontents

\chapter*{Prelude: Vectors and Space}

On a map, we identify a position by a pair of coordinates, which we call the lattitude and longitude. Via this method, description of the earth's surface is made precise by a simple number pair. It is simple to extend this coordinate method to three dimensions by taking triples of numbers, enabling the statement of precise, quantitative results about the geometry of three-dimensional space. Permitting some imagination, we can extend this method to an arbitrary dimensional space.

\theoremstyle{definition}
\begin{definition}
  For any positive integer $n$, the $n$-dimensional coordinate space $\mathbf{R}^n$ is defined to be the set of all $n$ tuples whose elements reside in the real numbers. In the language of set theory,
  %
  \[ \mathbf{R}^n = \{ (v_1, v_2, \dots, v_n) : v_1, v_2, \dots, v_n \in \mathbf{R} \} \]
  %
  It is common to speak of the component elements $v_k$ as coordinates. If $v$ is an arbitrary symbol for a vector in $\mathbf{R}^n$, then, for any integer $i$ between 1 and $n$, we let $v_i$ will denote the $i$'th coordinate that $v$ is composed of.
\end{definition}
%
It is of course brash to state that we may reduce all of space to the algebra of vectors without some explanation. One can develop the mathematical knowledge of space directly from the axioms of Euclid, though it is only with the introduction of a real number system that space is affirmed to be a real vector space with the introduction of an origin. Given the current atomic view of physics, how does our relation to the infinitismal nature of the real number rectify with the quantized orthodoxy? The idea is that the quantization of space is so small that the real numbers are indistinguishably accurate approximations to actual space. Experimental evidence will confirm that the theory developed in this book is accurate to any description of mechanics in the classical nature.

The inventor of the ingenious `method of coordinates' gloated that he had reduced every problem in geometry to algebra, the language of numbers. Complex geometric constructions could be reduced to simple symbolic manipulations of numbers. In the past 400 years, this boasted `analytic geometry' has shown to be perhaps the most useful invention to physics and mathematics since numbers were discovered in themselves.

Inspired by the ideas of the new geometry, the two mathematicians Sir Isaac Newton and Gottfried Leibnitz used the two dimensional coordinate method to construct the infamous infinitismal calculus. In a largely unrelated chain of events, the study of analytic geometry was further extended to higher dimensional cousins of the cartesian space, and the field was renamed linear algebra. Vector calculus realizes the common ancestry of linear algebra and calculus, and attempts to integrate the two field's geometric insights.

Human intuition gives hints that the spatial properties of the real numbers have obvious analogues in three dimensional space. The aim of this report is to provide the rigorous construction that makes the intuition concrete. Assuming the properties of differential and integral calculus in `one dimensional space', as well as some elementary linear algebra, we show how the properties of calculus on the real line naturally extend to arbitrary coordinate spaces.

Our study begins with a brief, and what should be familiar description of the algebraic operations that define $\mathbf{R}^n$. Vectors are used to describe collections of magnitude. Given two vectors $v$ and $w$, we can then identify a vector whose magnitude combines the magnitudes of both $v$ and $w$.

\begin{definition}
  Let $v$ and $w$ be two vectors in the same vector space $\mathbf{R}^n$. The sum of $v$ and $w$, denoted $v + w$, is then defined as the coordinate tuple
  %
  \[ (v_1 + w_1, v_2 + w_2, \dots, v_n + w_n) \]
  %
  which is, of course, also in $\mathbf{R}^n$.
\end{definition}

Summation is useful, but we may also wish to `scale' a vector by a magnitude.

\begin{definition}
  Given a vector $v$, and a real number $\lambda$, we define the multiplication of $v$ by $\lambda$, denoted $\lambda v$, as
  %
  \[ (\lambda v_1, \lambda v_2, \dots, \lambda v_n) \]
  %
  together with addition, multiplication establishes fully the algebraic structure of $\mathbf{R}^n$.
\end{definition}

The set $\mathbf{R}^n$ with algebraic operations defined becomes an $n$ dimensional vector space over the field of real numbers. The canonical basis is the set of vectors $e_1, e_2, \dots, e_n$, where
%
\[ e_i = \underbrace{(0, \dots, 0, 1, 0, \dots, 0)}_\text{1 is in the i'th coordinate} \]
%
In this text, the algebraic knowledge of $\mathbf{R}^n$ that results from the definitions above is assumed. We focus our efforts on discovering analytical properties which have an affinity with the methods of the infinitismal calculus.





% \mainmatter

\chapter{Analytical space}

It could be said that the most basic geometric property is distance. With it, we can define a multitude of other geometric properties. The importance of distance lies in the fact that distance gives a number to quantize a relationship between a pair of objects. As a testament to the importance of distance to calculus, we note that one dimensional calculus even began by introducing the concept of distance to $\mathbf{R}$, hidden behind the term `absolute value'.

The key observation to adding distance to $\mathbf{R}^n$ results from the following consideration. We are well experienced with length in $\mathbf{R}$ from calculus, and length in $\mathbf{R}^2$ from analytic geometry. Length in $\mathbf{R}^n$, denoted by $\| \cdot \|$, should be consistant with these systems, in the sense of satisfying the following four properties:

\begin{enumerate}
  \item The geometry of $\mathbf{R}^1$ should be consistent with $\mathbf{R}$. If $v$ is a vector in $\mathbf{R}^1$ representing the tuple $(v_1)$, then
  %
  \[ \|v\| = |v_1| \]
  \item Since the basis elements are such that all other vectors are composed of, they should have a unit length. For any coordinate basis element $e_i$ in a space $\mathbf{R}^n$,
  %
  \[ \|e_i\| = 1 \]
  \item Geometries of $\mathbf{R}^n$ and $\mathbf{R}^m$ should be consistent. Let $v$ be a vector in $\mathbf{R}^k$ satisfying the equation
  %
  \[ v = \lambda_1 e_{n_1} + \lambda_2 e_{n_2} + \dots + \lambda_m e_{n_m} \]
  %
  where each $\lambda_k$ is a real scalar, each $e_{n_i}$ is a basis element of $\mathbf{R}^k$, and $m < k$. Then we should have that
  %
  \[ \|v\| = \| \lambda_1 e_1 + \lambda_2 e_2 + \dots + \lambda_m e_m \| \]
  %
  where $e_k$ are the coordinate vectors of $\mathbf{R}^m$, and hence lie in $\mathbf{R}^m$ rather than $\mathbf{R}^n$.
  \item If $v$ can be written as the sum of two vectors $w$ and $u$, where
  %
  \[ w = \sum_{k = 1}^n \lambda_k e_{n_k} \]
  %
  \[ u = \sum_{k = 1}^n \lambda_k e_{n'_k} \]
  %
  and the sets $\{ e_{n_k} \}$ and $\{ e_{n'_k} \}$ are disjoint, then
  %
  \[ \|v\| = \sqrt{ \|w\|^2 + \|u\|^2 } \]
  %
  In $\mathbf{R}^2$, this property establishes the pythagorean theorem. More generally, the property states that if we project the two dimensional plane into a space of any dimension, vectors on this plane should also obey the pythagorean property.
\end{enumerate}

It turns out that these four properties are all we need to axiomatically establish a system of length in $\mathbf{R}^n$.

\begin{theorem}
  There is a unique function that satisfies the four properties above, namely, for any vector $v$ in $\mathbf{R}^n$, the length of $v$ is
  %
  \[ \sqrt{\sum_{k = 1}^n v_k^2} \]
  %
  where the square root is non-negative.
\end{theorem}
\begin{proof}
  Let us prove the uniqueness of the function by induction. The uniqueness of length in $\mathbf{R}$ is obvious - it must be the absolute value function. We note that
  %
  \[ \sqrt{x^2} = x \]
  %
  if $x \geq 0$, and
  %
  \[ \sqrt{x^2} = -x \]
  %
  if $x \leq 0$. In $\mathbf{R}^2$, we simply apply the pythagorean theorem. If a vector $v$ can be written with coordinates $(v_1, v_2)$, then by taking the vectors $(0,v_2)$ and $(v_1,0)$ and associating them with $\mathbf{R}$ by Property 3., we see that
  %
  \[ \|v\| = \sqrt{\sqrt{v_1^2}^2 + \sqrt{v_2^2}^2} \]
  %
  Hence our definition is unique in $\mathbf{R}^2$. We prove the rest of the cases by induction. Suppose the norm is established uniquely in $n-1$ dimensions. We extend the distance to $n$ dimensions as follows. Take an arbitrary vector $v$ in $\mathbf{R}^n$. We can write $v$ as the sum of the two vectors
  %
  \begin{align*}
    (v_1, v_2,\dots, v_{n-1}, 0) && (0, 0, \dots, 0, v_n)
  \end{align*}
  %
  By Property 3. and induction,
  %
  \[ \|(v_1, v_2, \dots, v_{n-1}, 0)\| = \sqrt{ \sum_{k = 0}^{n-1} v_k^2 } \]
  %
  \[ \|(0,0,\dots,0,v_n)\| = \sqrt{v_n^2} \]
  %
  And since the two vectors have disjoint bases, by Property 4. we conclude that,
  %
  \begin{align*}
    \|v\| &= \| (v_1, v_2, \dots, v_{n-1}, 0) + (0,0,\dots,0,v_n) \|\\
      &= \sqrt{\bigg(\sqrt{v_1^2 + v_2^2 + \dots + v_{n-1}^2}\bigg)^2 + \bigg(\sqrt{v_n^2}\bigg)^2}\\
      &= \sqrt{v_1^2 + v_2^2 + \dots + v_{n-1}^2 + v_n^2}
  \end{align*}
  %
  Therefore the length makes sense in $n$ dimensions, and is the unique definition in $n$ dimensions.
\end{proof}

This theorem is more than enough motivation to make the following definition.

\begin{definition}
  The {\bf length}, or {\bf euclidean norm} of a vector $v$ in $\mathbf{R}^n$, denoted $\|v\|$, is defined to be
  %
  \[ \sqrt{v_1^2 + v_2^2 + \dots + v_n^2} \]
  %
  where the square root is the positive root of the sum of squares. The {\bf distance} between two points $x$ and $y$ in $\mathbf{R}^n$ is $\| x - y \|$. Thus the length of a vector $v$ is precisely its distance from the zero vector.
\end{definition}

It is easy to see that the length of an arbitrary vector is uniquely defined in $\mathbf{R}$ by the formula above. A sum of squares is always non-negative, and a non-negative number has a unique positive root in $\mathbf{R}$. We obviously need this in any definition of distance. Imagine the havoc that would be caused by a vector which has `no distance' in comparison to other vectors, or multiple possible distances!

Some immediate geometrical properties arise from the definition of the euclidean norm above. The statements are obvious to intuitive statements in the real world. Of course this results because the four properties which we established to define distance are so obvious to us.

\begin{lemma}
  For every vector $v \in \mathbf{R}^n$, $\|v\| \geq 0$. The length of $v$ is 0 only when the vector itself is the zero vector.
\end{lemma}
\begin{proof}
  Since our definition takes the unique non-negative square root of a number in $\mathbf{R}$, our norm must be non-negative. If, for some vector $v \in \mathbf{R}^n$, $\| v \| = 0$, then $v_1^2 + \dots + v_n^2 = 0$. As this is the sum of squares, which all have non-negative values, we must have $v_i^2 = 0$ for all coordinates $v_i$. This is true if and only if $v_i = 0$ for all coordinates $v_i$, hence $v = 0$.
\end{proof}

\begin{lemma}
  For every vector $v \in \mathbf{R}^n$, and every scalar $\lambda \in \mathbf{R}$, $\| \lambda v \| = | \lambda |\ \| v \|$.
\end{lemma}
\begin{proof}
  Because then $\lambda v = (\lambda v_1, \lambda v_2, \dots, \lambda v_n)$, and thus it follows that
  %
  \begin{align*}
    \| \lambda v \| &= \sqrt{(\lambda v_1)^2 + (\lambda v_2)^2 + \dots + (\lambda v_n)^2}\\
                    &= \sqrt{\lambda^2(v_1^2 + \dots + v_n^2)}\\
                    &= \sqrt{\lambda^2} \sqrt{v_1^2 + \dots + v_n^2}\\
                    &= | \lambda |\ \| v \|
  \end{align*}
\end{proof}

Intuitively, if we scale a vector by the number, we should scale the length by the same proportional value.

\begin{center}

\theoremstyle{definition}\begin{tikzpicture}
    \coordinate (A) at (0,0);
    \coordinate (B) at (1,0.5);
    \coordinate (C) at (4,2);

    \draw[->, >=triangle 45] (A)--(B) node[midway,sloped,below] {$v$};
    \draw[dashed, ->, >=triangle 45] (A)--(C) node[midway,sloped,above] {$\lambda v$};
\end{tikzpicture}
\end{center}

\begin{corollary}
  For every vector $v$, $\| v \| = \| -v \|$.
\end{corollary}

The corollary means that, if we mirror a vector about the x and y axis, the size of a vector stays the same.

\begin{center}
\begin{tikzpicture}
    \coordinate (A) at (0,0);
    \coordinate (B) at (1.5,1);
    \coordinate (C) at (-1.5,-1);
    \coordinate (D) at (0,1);
    \coordinate (E) at (0,-1);
    \coordinate (F) at (1.5,0);
    \coordinate (G) at (-1.5,0);

    \draw[->, >=triangle 45] (A)--(B) node[midway,sloped,below] {$v$};
    \draw[->, >=triangle 45] (A)--(C) node[midway,sloped,above] {$-v$};
    \draw[dashed] (D)--(E);
    \draw[dashed] (F)--(G);
\end{tikzpicture}
\end{center}

We now introduce an important concept from linear algebra, which we will motivate after it has been defined.

\begin{definition}
Given two vectors $v$ and $w$ in the vector space $\mathbf{R}^n$, we define the {\bf euclidean inner product} of the two vectors $v$ and $w$, denoted $\inner{v, w}$, as
%
\[ v_1w_1 + v_2w_2 + \dots + v_nw_n = \sum_{k = 1}^n v_k w_k \]
\end{definition}

What is the inner product? In order to provide a satisfying answer, we must discover some resultant properties of the definition. For a little motivation of its use, see that for any vector $v$, $\|v\|^2 = \inner{v, v}$.

\begin{lemma}
  For two vectors $v$ and $w$,
  %
  \[ \inner{v, w} = \inner{w, v} \]
\end{lemma}
\begin{proof}
  \begin{align*}
    \inner{v, w} &= \sum_{k=1}^n v_k w_k\\
                 &= \sum_{k=1}^n w_k v_k\\
                 &= \inner{w, v}
  \end{align*}
  %
  The idea of the proof rests on the commutativity of real numbers.
\end{proof}

\begin{lemma}
  For two vectors $v$ and $w$, and a scalar $\lambda$,
  %
  \[ \inner{\lambda v, w} = \inner{v, \lambda w} = \lambda \inner{v, w} \]
\end{lemma}
\begin{proof}
  This proof rests on the distributive property of $\mathbf{R}$.
  \begin{align*}
    \inner{\lambda v, w} &= \sum_{k = 1}^n v_k (\lambda w_k)\\
                         &= \lambda \sum_{k = 1}^n v_k w_k\\
                         &= \lambda \inner{v, w}
  \end{align*}

  We obtain the converse by noting that $\inner{v, \lambda w} = \inner{\lambda w, v} = \lambda \inner{w, v} = \lambda \inner{v, w}$.
\end{proof}

\begin{lemma}
  For three vectors $v$, $w$, and $u$,
  %
  \[ \inner{v + u, w} = \inner{w, v + u} = \inner{v,w} + \inner{u,w}  \]
\end{lemma}
\begin{proof}
  \begin{align*}
    \inner{v + u, w} &= \sum_{k = 1}^n (v_k + u_k)(w_k)\\
                     &= \sum_{k = 1}^n v_kw_k + \sum_{k = 1}^n u_kw_k\\
                     &= \inner{v,w} + \inner{u,w}
  \end{align*}
\end{proof}

To see intuitively what the dot product is, we rigorously define another geometric property, angles between two vectors. We require only one elementary property to show angles are unique.
%
\begin{itemize}
  \item The law of cosines should hold. That is, if $v$ and $w$ are two vectors, then the angle $\theta$ between them, should be such that
  %
  \[ \|v - w\|^2 = \| v \|^2 + \| w \|^2 - 2\|v\|\|w\| \cos(\theta) \]
  %
  Which is just an extension of the property that holds for vectors in cartesian space.
\end{itemize}
%
This is the only property to define the angle between two angles uniquely.

\begin{theorem}
  The {\bf angle} $\theta$ between two vectors $v$ and $w$ is the unique angle between 0 and $\pi$ such that
  %
  \[ \cos(\theta) = \frac{\inner{v, w}}{\| v \| \| w \|} \]
  %
  This definition corresponds to the usual definition in plane geometry.
\end{theorem}
\begin{proof}
  Take two arbitrary vectors $v$ and $w$. We know that the angle $\theta$ between them satisfies the equality.
  %
  \[ \| v - w \|^2 = \|v\|^2 + \|w\|^2 - 2\|v\|\|w\| \cos(\theta) \]
  %
  Noting that for any vector $u$, $\|u\|^2 = \langle u, u \rangle$, we obtain that
  %
  \begin{align*}
    \|v - w\|^2 &= \inner{v - w, v - w}\\
                &= \inner{v, v} - 2 \inner{v, w} + \inner{w, w}\\
                &= \|v\|^2 - 2 \inner{v, w} + \|w\|^2
  \end{align*}
  %
  Rearranging the previous equation and substituting our new value of $\|v - w\|^2$, we conclude that $\theta$ is the angle defined by the equation
  %
  \begin{align*}
    \cos(\theta) &= \frac{\|v\|^2 + \|w\|^2 - \|v - w\|^2}{2\|v\|\|w\|}\\
                 &= \frac{\|v\|^2 + \|w\|^2 - \|v\|^2 + 2\langle v, w \rangle - \|w\|^2}{2\|v\|\|w\|}\\
                 &= \frac{\langle v, w \rangle}{\|v\|\|w\|}
  \end{align*}
\end{proof}

Are angles well defined for all vectors? If either of the vectors are zero, then there magnitude is zero, hence the angle is not well defined (a zero vector has no `direction' to define an angle on). For any angle $\theta$,
%
\[ -1 \leq \cos(\theta) \leq 1 \]
%
Thus the angle between two vectors in $\mathbf{R}^n$ is only well defined if
%
\[ \frac{|\langle v, w \rangle|}{\|v\|\|w\|} \leq 1\]
%
The following theorem shows this holds for any pair of non-zero vectors in $\mathbf{R}^n$.

\begin{theorem}[The Cauchy-Schwarz Inequality]
  For any two vectors $v,w \in \mathbf{R}^n$,
  %
  \[ |\inner{v, w}| \leq \|v\|\|w\| \]
  %
  hence
  %
  \[ \frac{|\inner{v, w}|}{\|v\|\|w\|} \leq 1 \]
  %
  and thus the angle between the two vectors is well defined. In addition,
  %
  \[ \inner{v, w} = \|v\|\|w\| \]
  %
  if and only if $v$ and $w$ are linearly dependent.
\end{theorem}
\begin{proof}
  If $v$ and $w$ are linearly dependent, equality holds by the following calculation. Let $\lambda$ be the scalar such that $w = \lambda v$.
  %
  \begin{align*}
    \langle v, w \rangle &= \langle v, \lambda v \rangle\\
                         &= \lambda \langle v, v \rangle\\
                         &= \lambda \| v \|^2\\
                         &= \| v \| \| w \|
  \end{align*}
  %
  If $v$ and $w$ are not linearly dependent, $v - \lambda w \neq 0$ for any scalar $\lambda$ (this is precisely the converse of linear dependence). It then follows that $\| v - \lambda w \|^2 > 0$ for all $\lambda$. Expanding what this value means, by algebraic manipulations, we obtain that
  %
  \begin{align*}
    \| v - \lambda w \|^2 &= \langle v - \lambda w, v - \lambda w \rangle\\
                          &= \|v\|^2 - 2 \lambda \langle v, w \rangle + \lambda^2 \| w \|^2
  \end{align*}
  %
  This can be considered a quadratic function of $\lambda$ with no solutions in $\mathbf{R}$. Hence the discriminant is negative. That is,
  %
  \[ (2 \langle v, w \rangle)^2 - 4 \|v\|^2 \|w\|^2 < 0 \]
  %
  Rearranging the equation, we obtain that
  %
  \[ \langle v, w \rangle^2 < \|v\|^2\|w\|^2 \]
  %
  Hence
  %
  \[ |\langle v, w \rangle| <|\|v\|\|w\|| = \|v\|\|w\| \]
\end{proof}

\begin{corollary}
  The angle between two linearly independant vectors is 0.
\end{corollary}
\begin{proof}
  When two vectors $v$ and $w$ are linearly independant, from the above equality, we know for the angle $\theta$ between them,
  %
  \[\cos(\theta) = \frac{\inner{v, w}}{\|v\|\|w\|} = \frac{\|v\|\|w\|}{\|v\|\|w\|} = 1\]
  %
  hence $\theta = 0$.
\end{proof}

\begin{corollary}
  Two vectors are at right angles or orthogonal to one another, if and only if the inner product between them is 0.
\end{corollary}
\begin{proof}
  Let $v$ and $w$ be two vectors such that the angle $\theta$ between them is a right angle. It 
  \theoremstyle{definition}follows that $\cos(\theta) = 0$. By definition of the angle, we then know that
  %
  \[ \frac{\inner{v, w}}{\|v\|\|w\|} = 0 \]
  %
  hence $\langle v, w \rangle = 0$. Conversely, if $\langle v, w \rangle = 0$, we know that $\cos(\theta) = 0$, which happens if and only if $\theta$ is a right angle.
\end{proof}

Now we can intuitively explain the inner product. Take two vectors $v$ and $w$. Project vector $v$ onto vector $w$. What is projection? Precisely, take the vector $u$ in the span of $w$ such that $v - u$ is orthogonal to $w$. Scale $w$ by the length of the projection $u$, letting $w' = \|u\| w$. Then $\langle v, w \rangle = \| w' \| = \| u \| \| w \|$. Let us show this rigorously. If $v - u$ is at a right angle to $w$, $\langle v - u, w \rangle = 0$. As $u \in \text{span}(w)$, $u = \lambda w$ for some scalar $\lambda \in \mathbf{R}$. Then by calculation,
%
\begin{align*}
  \langle v - u, w \rangle &= \langle v - \lambda w, w \rangle\\
                           &= \langle v, w \rangle - \lambda \langle w, w \rangle\\
                           &= \langle v, w \rangle - \lambda \|w\|^2
\end{align*}
%
Hence $\lambda = \langle v, w \rangle/\|w\|^2$. It follows that
%
\[ \|u\| = \lambda \| w \| = \frac{\langle v, w \rangle}{\|w\|} \]
%
so $\|u\|\|w\| = \langle v, w \rangle$.

\begin{center}
\begin{tikzpicture}
    \coordinate (A) at (0,0);
    \coordinate (B) at (3.9,0.65);
    \coordinate (C) at (1,2);
    \coordinate (D) at (1.297,0.216);

    \draw[dashed, ->, >=triangle 45] (A)--(B) node[midway,sloped,below] {$v$};
    \draw[->, >=triangle 45] (A)--(C) node[midway,sloped,above] {$w$};
    \draw[->, >=triangle 45] (A)--(D) node[midway,sloped,below] {$u$};
    \draw[dashed] (D)--(C);
    \draw[right angle symbol={C}{D}{B}];
\end{tikzpicture}
\end{center}

\begin{center}
\begin{tikzpicture}
    \coordinate (A) at (0,0);
    \coordinate (B) at (3.9,0.65);
    \coordinate (C) at (1,2);
    \coordinate (D) at (1.297,0.216);

    \draw[dashed, ->, >=triangle 45] (A)--(B) node[midway,sloped,below] {$v$};
    \draw[->, >=triangle 45] (A)--(C) node[midway,sloped,above] {$w$};
    \draw[->, >=triangle 45] (A)--(D) node[midway,sloped,below] {$u$};
    \draw[dashed] (D)--(C);
    \draw[right angle symbol={C}{D}{B}];
\end{tikzpicture}
\end{center}

If you understood the above paragraph, you should see that the Cauchy Schwarz inequality is then intuitively true. The length of the projection of a vector is always less than or equal to the length of the vector itself. Since the inner product is the multiplication of the length of this projection by another vector, it is obvious that this is less than the original vector's length multiplied by the other vector.

The most important vector inequality results naturally from the Cauchy-Scharz-Inequality. We know it as the Triangle-Inequality, as should become clear once the statement of the proof is understood.

\begin{theorem}[The Triangle Inequality]
  For any vectors $v$ and $w$,
  %
  \[ \| v + w \| \leq \| v \| + \| w \| \]
\end{theorem}

\begin{proof}
  Let $v$ and $w$ be arbitrary vectors. We prove the statement by a simple calculation.

  \begin{align*}
    \| v + w \|^2 &= \langle v + w, v + w \rangle\\
                  &= \|v\|^2 + 2\langle v,w \rangle + \|w\|^2\\
                  &\underbrace{\leq \|v\|^2 + 2\|v\|\|w\| + \|w\|^2}_\text{By Cauchy-Schwarz}\\
                  &= (\|v\| + \|w\|)^2
  \end{align*}

  Hence $| \| v + w \| | \leq |\|v\| + \|w\| |$. As both are always non-negative, we obtain the inequality needed.
\end{proof}

\begin{center}
\begin{tikzpicture}
    \coordinate (A) at (0,0);
    \coordinate (B) at (3,0.5);
    \coordinate (C) at (1,2);
    \coordinate (D) at (4,2.5);

    \draw[->, >=triangle 45] (A)--(B) node[midway,sloped,below] {$v$};
    \draw[->, >=triangle 45] (B)--(D) node[midway,sloped,above] {$w$};
    \draw[->, >=triangle 45] (A)--(D) node[midway,sloped,above] {$v+w$};
\end{tikzpicture}
\end{center}

The Triangle inequality states that, if we want to go from a point $a$ to a point $b$. The direct route is always shorten than some other route.

\begin{corollary}
  For any vectors $v$ and $w$, and for any third vector $u$, we have that $\|v - w\| \leq \|v - u\| + \|u - w\|$.
\end{corollary}
\theoremstyle{definition}
\begin{proof}
  This follows as $\| v - w \| = \| (v - u) + (u - w) \|$, which by the triangle inequality, is less than or equal to $\| v - u \| + \| u - w \|$.
\end{proof}

\begin{corollary}
  For any vector $v = (v_1, v_2, \dots, v_n)$ in $\mathbf{R}^n$,
  %
  \[ \|v\| \leq \sum_{k = 1}^n |v_k| \]
\end{corollary}
\begin{proof}
  \[ \| v \| = \| \sum_{k = 1}^n v_k e_k \| \leq \sum_{k = 1}^n \| v_k e_k \| = \sum_{k = 1}^n | v_k | \]
\end{proof}

\begin{lemma}
  For any vector $v$ with a coordinate $v_i$,
  %
  \[ |v_i| \leq \| v \| \]
\end{lemma}
\begin{proof}
  If $x \leq y$, $\sqrt{x} \leq \sqrt{y}$. As $v_i^2 \leq \sum_{k=1}^n v_k^2$ ($v_i$ is in the sum), we know that
  %
  \theoremstyle{definition}
  \[ \sqrt{v_i^2} \leq \sqrt{\sum_{k=1}^n v_k^2} \]
  %
  and hence $|v_i| \leq \| v \|$.
\end{proof}

These definitions justify the geometric properties of vector spaces $\mathbf{R}^n$. With it, we can analyse almost all of the arguments of Euclid analytically. However, we are severely limited by specifying only equalities and inequalities, with which we can only analyze finite sets of points in detail. To extend our notions to the precedence that calculus requires, we require precise notions of geometry in infinite sets. We call these properties developed topological properties. Before this, let us briefly discuss the generalization of topics in this chapter.

We end our introduction to distance by establishing the fact that our original properties that defined distance were perhaps too strict. The only properties we actually need for calculus are some of the theorems above. We could really use any definition of distance provided they follow the above properties. We call any such definition a norm.

\begin{definition}
  Let $V$ be a vector space with scalars in $\mathbf{R}$. a norm on $V$ is a function $f:V \to \mathbf{R}^+$ satisfying three properties.
  %
  \begin{enumerate}
    \item For any vector $v$, $f(v) = 0$ if and only if $v$ is the zero vector.
    \item If $\lambda$ is a real number, $f(\lambda v) = |\lambda|f(v)$.
    \item For any two vectors $v$ and $w$, $f(v + w) \leq f(v) + f(w)$.
  \end{enumerate}
  %
  Any vector space with a norm we call a norm space, and we commonly write the function $f$ as $\| \cdot \|$.
\end{definition}

Generalized norm spaces are more useful when studying infinite dimensional spaces, like in functional analysis. For now, we stick with euclidean distance, but we note there are other examples of distance in $\mathbf{R}^n$:

\begin{itemize}
  \item For any real number $p$, The $l_p$ norm $\| \cdot \|_p$, defined by
  %
  \[ \|v\|_p = \sqrt[p]{\sum_{k = 1}^n |v_k|^p} \]
  %
  Note that the euclidean norm is simply the $l_2$ norm. The Manhatten norm, defined by
  %
  \[ \|v\| = \sum_{k = 1}^n |v_k| \]
  %
  which is named after the lateral distances one must walk to navigate a city, is the $l_1$ norm.
  \item The uniform norm $\| \cdot \|_\infty$, defined as
  %
  \[ \| v \|_\infty = \max(|v_1|, |v_2|, \dots, |v_n|) \]
  %
  This norm is maximalized in the sense that, as $p \to \infty$, $\|v\|_p \to \|v\|_\infty$, for any vector $v$.
\end{itemize}

Though it will not be useful to us, in the function space of continuous functions on an interval $[a,b]$, the norm of a function $f$ can be defined as
%
\[ \sqrt{\int_a^b f^2} \]
%
which is effectively a way to `sum' up the uncountable many coordinates of which the function is composed.

Most norms are established by an inner product. These are simply generalizations of the euclidean inner product.

\begin{definition}
  An inner product on a vector space $V$ on $\mathbf{R}$ is a function $f:V^2 \to \mathbf{R}$ such that
  %
  \begin{enumerate}
    \item For any vectors $v$ and $w$, $f(v,w) = f(w,v)$.
    \item $f(v,w+u) = f(v,w) + f(v,u)$.
    \item For any scalar $\lambda$, $f(\lambda v,w) = f(v,\lambda w) = \lambda f(v,w)$
    \item $f(v,v) \geq 0$, and $f(v,v) = 0$ if and only if $v = 0$.
  \end{enumerate}
  %
  an inner product is normally written $\inner{\cdot, \cdot}$
\end{definition}

We obtain a norm from any inner product $\inner{\cdot, \cdot}$ by defining $\|v\|^2 = \inner{v, v}$. The euclidean norm is defined this way. If, for two functions $f$ and $g$, we define $\inner{f,g}$ to be $\int_a^b fg$, then the function norm is obtained from an inner product.

A full extension of these concepts is more fitting in a course on `functional analysis'. We have only provided a little motivation for what one can do with the abstract notion of distances in vector spaces. In this report we will deal mainly with the euclidean inner product and norm -- this results from the fact that all norms are equivalent in finite dimensional space.





\chapter{A Taste of Topology}

Topology deals with the global properties of a space. Though distances will be important, they deal with only pairs of elements. Topology provides ways of considering space where infinitely many spaces are considered. It is much more intuitive to show the topological properties of $\mathbf{R}^n$ that result from distance is generality for objects that satisfy a distance. What properties of $\mathbf{R}^n$ do we need to talk in general about distance. The following definition encapsulates the definition.

\begin{definition}
  A metric space is a set $M$ together with a distance function $d$ which takes two objects in $M$ and returns a non-negative real value, satisfying the following values for any three elements $x$, $y$, $z$ in $M$:
  %
  \begin{enumerate}
    \item $d(x,y) = 0$ if and only if $x = y$.
    \item $d(x,z) \leq d(x,y) + d(x,z)$
    \item $d(x,y) = d(y,x)$
  \end{enumerate}
  %
  the elements in $M$ are commonly called points.
\end{definition}

The first property is known as non-negativity. The second property is a generalized version of the triangle inequality for any distance function. We call the third property symmetry.

Of course, $\mathbf{R}^n$ is a metric space where the distance between two vectors $v$ and $w$ is defined to be $\|v - w\|$. For any norm space $V$, we can define a distance function in this manner.

In order to justify the analytical properties in the last chapter, we used our intuition of polyhedra. It thus makes sense that we can define infinite properties of distances in turns of shapes which cannot be defined by finitely many staight lines, the most basic of which is a circle.

\begin{definition}
  Given a point $x$ in a metric space $M$, and a positive real number $r$, define the {\bf open ball} centred at $x$ with radius $r$, denoted $B(x,r)$, as the set
  %
  \[ \{ y \in M : d(x,y) < r \} \]
  %
  The {\bf closed ball}, denoted by $\overline{B}(x,r)$ is defined by
  %
  \[ \{ y \in M : d(x,y) \leq r \} \]
  %
  The {\bf circle}, denoted by $S(x,r)$, is
  %
  \[ \{ y \in M : d(x,y) = r \} \]
  %
  The {\bf punctured ball}, denoted by $\mathring{B}(x,r)$, is just $B(x,r) - \{x\}$
\end{definition}

Calculus in $\mathbf{R}$ starts with defining properties of sequences. In $\mathbf{R}^n$, this is no different.

\begin{definition}
  Let $(a_i)$ be a sequence of points in a metric space $A$. We say that $(a_i)$ converges to a point $a$ in $A$, written $a_i \to a$, or $\lim_{i \to \infty} a_i = a$, if any of the following equivalent statements hold.
  %
  \begin{enumerate}
    \item Every open ball centered at the point $a$ contains a tail of the sequence $(a_i)$.
    \item The sequence defined by $\| a_i - a \|$ converges to 0 in the real numbers.
    \item When $M$ is a vector space $\mathbf{R}^n$ with the euclidean norm, every coordinate sequence $([a_i]_k)$ of $a_i$ converges to $a_k$.
  \end{enumerate}
  %
  If a sequence does not converge to any point in $M$, we say the sequence diverges.
\end{definition}
\begin{proof} We prove multiple implications that map out a web of equivalences of the definitions. We leave it to the reader to show the proof provides all the implications needed.

  \begin{itemize}
  \item $(1) \implies (2)$: Let $(a_i)$ be a sequence such that every ball $B(a,r)$ contains a tail $(a_i)_{i \geq k}$ for some $k$. In particular, consider the sequence $d(a_i, a)$ in the real numbers. To show this converges to 0, we must use the calculus of the real numbers. Let $\varepsilon > 0$. Then, by considering $B(a,\varepsilon)$, we gain a tail such that $d(a_i, a) < \varepsilon$, which directly implies that the limit converges to 0.

  \item $(2) \implies (1)$: Let $(a_i)$ be a sequence such that $d(a_i, a) \to 0$. Then, for every $\varepsilon > 0$, there is a tail for some integer $k$ such that for any value in $(a_i)_{k \geq i}$, $d(a_i, a) < \varepsilon$. This means precisely that the tail $(a_i)_{k \geq i}$ is contained in $B(a,\varepsilon)$. As this statement holds for every $\varepsilon$, it holds for any open ball centered at $a$, and thus we obtain (1).

  \item $(2) \implies (3)$: Let $a_i$ be a sequence such that $\| a_i - a \| \to 0$. By lemma (2.12), we know that for any coordinate $x_k$, $|x_k| \leq \| x \|$. Thus it follows that for any coordinate $(a_i)_k$,
  %
  \[ | (a_i)_k - a_k | \leq \| a_i - a \| \]
  %
  As the second sequence dominates the first, and are both positive sequences, $|(a_i)_k - a_k| \to 0$, which means precisely that $(a_i)_k \to a_k$. This works for arbitrary coordinates, so we obtain (3).

  \item $(3) \implies (2)$: By Corollary (2.11), $\| x \| \leq \sum_{k = 1}^n |x_k|$ for any $x \in \mathbf{R}^n$. We use the same strategy as in the last paragraph. Suppose $|a_{i_k} - a_k| \to 0$ for every $k$. We then know that the finite sum of sequences $(\sum_{k = 1}^n |(a_i)_k|)$ converges to 0 also. But this is a dominating sequence of $\| a_k - a \|$, hence $\| a_k - a \| \to 0$, and we obtain (2).
  \end{itemize}
  %
  We leave it to the reader to show that these prove the equivalence of all of the statements.
\end{proof}

We obtain some elementary results from facts from real-valued calculus result from the correspondence of definition (2) of coordinates with convergence of real numbers. We state the facts in $\mathbf{R}$ without proof, and assume the reader can prove the corresponding theorem in $\mathbf{R}^n$ using definition (2).

\begin{theorem}
  If $(a_i)$ and $(b_i)$ are two sequences in $\mathbf{R}$, and $c$ is some fixed constant in $\mathbf{R}$, such that $a_i \to a$ and $ b_i \to b$, then
  %
  \[ a_i + b_i \to a + b \]
  %
  \[ \lambda a_i \to \lambda a \]
\end{theorem}

\begin{corollary}
  If $(v_i)$ and $(w_i)$ are two sequences in $\mathbf{R}^n$ such that $v_i \to v$, $w_i \to w$, then
  %
  \[v_i + w_i \to v + w\]
  %
  \[ \lambda v_i \to \lambda v \]
  %
  \[ \| v_i \| = \| v \| \]
\end{corollary}

Now we have stated the meaning of sequences, we can define one of the first relatively deep theorems of vector calculus, an extension of the Bolzano Weirstra\ss\ theorem for $\mathbf{R}$. We assume the result in $\mathbf{R}$ to prove our theorem.

\begin{theorem}[The Bolzano Weirstra\ss\ Theorem in $\mathbf{R}^k$]
  Every bounded sequence in $R^n$ contains a convergent subsequence.
\end{theorem}
\begin{proof}
  Consider a sequence $(a_i)$ in $R_n$. Define a new sequence in $\mathbf{R}$ $([a_i]_1)$ by taking the first coordinate of every point in the sequence. By the Bolzano Weirstra\ss\ theorem in $\mathbf{R}$, we know that there is a convergent subsequence $([a_{n_i}]_1)$, a sequence such that $[a_{n_i}]_1 \to a_1$ for some real value $a_1$. Now, suppose we have a sequence $(a_i)$ that converges in the first through $n-1$'th coordinate. Consider the sequence $([a_i]_n)$. By another application of Bolzano Weirstra\ss\ in $\mathbf{R}$, we obtain a subseqence that converges in the $n$'th coordinate. It follows that we can define a subsequence that converges in every coordinate and thus converges in $\mathbf{R}^n$.
\end{proof}

Like with Bolzano Weirstra\ss, it is not too difficult to extend Cauchy's theorem to $\mathbf{R}^k$ as well.

\begin{theorem}[Cauchy's Theorem in $\mathbf{R}^k$]
  If a sequence $(a_i)$ satisfies `Cauchy's Criterion', then it converges. Cauchy's Criterion is that, for any $\varepsilon$, there exists a positive integer $N$ such that for all elements $a$ and $b$ in the tail $(a_i)_{i \geq N}$,
  %
  \[ \| a - b \| < \varepsilon \]
\end{theorem}
\begin{proof}
  Suppose $(a_i)$ is a sequence satisfying the property. Let $\varepsilon$ be arbitrary, with corresponding tail $(a_i)_{i \geq N}$. Since $|x_i| \leq \|x\|$ for any coordinate $x_i$ of a vector $x$ by lemma (2.12), we have that for any vectors $a$ and $b$ in the tail, $|a_k - b_k| \leq \|a - b\| < \varepsilon$. Hence by Cauchy's theorem in $\mathbf{R}$, the coordinates converge. It follows that the entire vector sequence converges.
\end{proof}

Not all metric spaces have this property.

\begin{definition}
  A metric space which satisfies Cauchy's convergence criterion is called a complete metric space.
\end{definition}

\begin{definition}
  A point $x$ is a limit or accumulation point of a set $A$ if there exists a sequence $(a_i)$ such that every element in the sequence is in the set $A$, and $a_i \to x$. Equivalently, a point $x$ is a limit point if every ball around $x$ contains some point in $A$.
\end{definition}

\begin{definition}
  Given a set $A$ in a metric space $M$, the {\bf closure} is defined to be the set
  %
  \[ \overline{A} = \{ x \in M : \text{$x$ is an accumulation point of $A$} \} \]
  %
  We say a set is {\bf closed} if $\overline{A} = A$.
\end{definition}

\begin{lemma}
  For any set $A$, $\overline{A}$ is closed
\end{lemma}
\begin{proof}
  Proving $\overline{\overline{A}} = \overline{A}$ is equivalent to showing every accumulation point of $\overline{A}$ is an accumulation point of $A$. Let $a$ be an accumulation point of $\overline{A}$, so that there exists a sequence $(a_i)$ with every element $a_i$ in $\overline{A}$ such that $a_i \to a$. Define a new sequence $(a'_i)$ in $A$ as follows. Let $a'_k$ be an arbitrary element such that $d(a_k, a'_k) < 1/k$. This is always possible as $a_k$ is a limit point of $A$, and thus every ball around $a_k$ contains some point in $A$. We claim $a'_i \to a$. Let $\varepsilon > 0$ be arbitrary. Let $M$ be the integer such that $(a_i)_{i \geq M}$ is contained in $B(a,\varepsilon/2)$. Consider the tail $(a'_i)_{i \geq \max(M,2/\varepsilon)}$. Then, for any $a'_i$ in the tail, $d(a'_i, a_i) \leq \varepsilon/2$, and since $a_i$ is in the specified by $M$, $d(a_i, a) < \varepsilon/2$ we use the triangle inequality to conclude that
  %
  \[ d(a'_i, a) \leq d(a'_i, a_i) + d(a_i, a) \leq \varepsilon/2 + \varepsilon/2 = \varepsilon \]
  %
  Hence $a'_i \to a$, and thus $a$ is a limit point of $A$. It follows that $\overline{A}$ is closed.
\end{proof}

\begin{definition}
  A point $x$ is on the {\bf boundary} of a set $A$ if $x$ is a limit point of $A$ and $A^c$. The set of all boundary points of a set $A$ is denoted $\partial A$. The {\bf interior} of $A$ is $A^\circ = A - \delta A$.
\end{definition}

We leave it to the reader to prove the following properties. The boundary of an open ball is the sphere. The boundary of the complement of a set is the same as the boundary of the set. Finally, the closure of a set is precisely the set combined with its boundary, hence a closed set is precisely one that contains its boundary.

\begin{definition}
  A set $A$ is {\bf open} if $\partial A$ is disjoint from $A$, so $A^\circ = A$.
\end{definition}

\begin{theorem}
  A set $A$ is open if and only if $A^c$ is closed
\end{theorem}
\begin{proof}
  If $A^c$ is closed, $\partial A^c \subset A^c$. In exercise (4), it was proved that $\partial A^c = \partial A$, thus $\partial A \subset A^c$, and hence $\partial A \cap A = \emptyset$, so $A$ is open. If $A$ is open, $\partial A \cap A = \emptyset$, hence $\partial A \cap A^c = \partial A$ so that $\partial A \subset A^c$. Using exercise (4) again, it follows that $\partial A^c \subset A^c$, so that $A^c$ is closed by exercise (5). 
\end{proof}

In Mathematics, many groups of objects also have something called a `dual' set, a group such that almost every theorem of the first group has a corresponding theorem with the second. The dual of a closed set is an open set, and thus we will see many theorems about closed sets have immediate corollaries about open sets, and vice versa.

It is an understandable question to ask why these topological properties were not made explicit when studying one dimensional calculus. The answer to this is a simple case of degeneracy. Open sets in $\mathbf{R}$ are just unions of open intervals $(a,b)$, and closed sets are just unions and intersections of closed intervals $[a,b]$. In single variate calculus you were just using the innate topological properties of open and closed intervals without naming them in a general case. For $\mathbf{R}^n$ and other metric spaces, we need to state more rigorously these properties; they have a much deeper meaning in a general space.

\begin{lemma}[The Open Set Test]
  A set $A$ is open if and only if for every point $a \in A$, there is a ball $B(a,r)$ which is a subset of $A$.
\end{lemma}
\begin{proof}
  We prove by contraposition. Let $A$ be an arbitrary set. Suppose there is a point $a \in A$ such that every ball $B(a,r)$ contains points in $A^c$. Then $a \in \partial A$, as it is a limit point of $A^c$. We conclude that $A$ is not open, as it contains parts of its boundary. Thus by contraposition, a set is open if there is a ball $B(a,r)$ for every point $a$ in the set which is contained in the set. The converse follows the same argument strategy, and is left to the reader.
\end{proof}

\begin{theorem}
  Let $\mathcal{J}$ be an arbitrary index set, and $(A_j)_{j \in \mathcal{J}}$ a set of open sets. Then $\bigcup_{j \in \mathcal{J}} A_j$ is open.
\end{theorem}
\begin{proof}
  We prove by the open set test. Let $a$ be an arbitrary element in $\bigcup_{j \in \mathcal{J}} A_j$. Then there is some specific $A_k$ for which $a \in A_k$, and since this set is open, there is some ball $B(a,r)$ such that $B(a,r) \subset A_k$. As $A_k \subset \bigcup_{j \in \mathcal{J}} A_j$, the same ball must be contained in the union. Thus the union is open.
\end{proof}

\begin{corollary}
  If $(A_j)_{j \in \mathcal{J}}$ is a set of closed sets, then $\bigcap_{j \in \mathcal{J}}$ is closed.
\end{corollary}
\begin{proof}
  For then $(A_j^c)_{j \in \mathcal{J}}$ is a family of open sets, and
  %
  \[ \bigcap_{j \in \mathcal{J}} A_j = \bigg( \bigcup_{j \in \mathcal{J}} A_j^c \bigg)^c \]
\end{proof}

\begin{theorem}
  If $(A_1, A_2, \dots, A_n)$ is a finite collection of open sets, then $\bigcap_{k = 1}^n A_k$ is open.
\end{theorem}
\begin{proof}
  Let $a$ be in $\bigcap_{k = 1}^n A_k$. Then $a$ is in every set $A_k$, and for each $A_k$ there is a ball $B(a,r_k)$ contained in $A_k$, as $A_k$ is open. Then, since $B(a,\min(r_1,r_2, \dots, r_k))$ is a subset of every ball in $A_k$, it is contained in the intersection. Thus the intersection is open.
\end{proof}

\begin{corollary}
  The finite union of closed sets is closed.
\end{corollary}

It is not in general true that the arbitrary union of open sets is open, and the intersection of closed sets is closed. Take the set of $(B(0,r))_{r \in \mathbf{R}}$. Each of these sets is open, but the intersection is a single point 0, and is not open.

\begin{definition}
  A set $A$ is bounded if, for some point $x$, there is a radius $r$ such that $A \subset B(x,r)$.
\end{definition}

We should specify that $A$ is bounded at the point $x$, but the point is arbitrary, by the lemma below.

\begin{lemma}
  A set $A$ which is bounded at some point $x$ in a metric space $M$ is bounded at every point.
\end{lemma}
\begin{proof}
  Let $A$ be a set which is bounded at a point $x$ in $M$. Then there is a radius $r$ such that $A \subset B(x,r)$. Let $y$ be another arbitrary point. Take a new radius $r + d(v,w)$, and consider the ball $B(y,r + d(v, w))$. Let $z$ be an arbitrary point in $B(x,r)$. Then $d(x, z) < r$. By the triangle inequality, $d(y, z) \leq d(y, x) + d(x, z) = d(v, w) + r$. Hence $y \in B(y,r + d(y, x))$, and thus $B(x,r) \subset B(y,r + d(y, x))$. By the transitive property of subsets, $A \subset B(y,r + d(y, r))$. It follows that $A$ is bounded at $y$, for any $y \in \mathbf{R}^n$.
\end{proof}

\begin{definition}
  A set $A$ is compact if every sequence in $A$ contains a convergent subsequence that converges to a point in $A$.
\end{definition}

It is important to point out that both parts of the following theorem works only in $\mathbf{R}^n$. In arbitrary metric spaces it may not hold at all.

\begin{theorem}[The Heine-Borel theorem (part 1) ]
  A set in $\mathbf{R}^n$ is compact if and only if it is closed and bounded.
\end{theorem}
\begin{proof}
  Let $A$ be a set that is compact. It is closed because any sequence in $A$ must converge in $A$ (the sequence contains a convergent subsequence that must converge in $A$). Suppose a set $B$ is unbounded. Then we form a sequence $(b_i)$ that has no convergent subsequence as follows. Let $b_1$ be arbitrary. Given $(b_1, \dots, b_{n-1})$, define $b_n$ to be a point such that $\| b_n - b_i \| > n$ for all $i$. This must be possible, as otherwise the set is bounded. No subsequence of $(b_i)$ can possibly converge by construction. Thus $B$ cannot be compact, showing this for all sets as $B$ was arbitrary. By contraposition, $A$ must be bounded.
\end{proof}

\begin{definition}
  Let $A$ be a set. An open cover of $A$ is a collection $(A_j)_{j \in \mathcal{J}}$ of open sets such that $A$ is a subset of $\bigcup_{j \in \mathcal{J}} A_j$.
\end{definition}

An open cover does not need to be finite or even countable. However, some familiar sets have a property that we may always select finite amounts of a cover to cover the entire set. This shown below by the remaining part of the Heine Borel Theorem, one of the jewels of mathematical analysis.

\begin{theorem}[Heine-Borel Theorem (part 2)]
  A set $A$ is compact if and only if every open cover of $A$ contains a finite subcover.
\end{theorem}
\begin{proof}
  Suppose $A$ is a set such that every open cover contains a finite subcover. Then $A$ is bounded, as the collection $\{B(0,r)\}_{r \in \mathbf{R}}$ forms an open cover of $A$, and thus must contain a finite subcover, in other words a minimum ball that contains $A$. We prove that $A^c$ is open, and hence $A$ is closed, by a similar strategy to above. Let $a$ be an arbitrary element in $A^c$. Consider the set of closed ball complements $\{(\overline{B}(a,r))^c\}_{r \in \mathbf{R}}$. The set of all these forms an open cover of $A$, and thus must contain a finite subcover. We can then take a ball that is the complement of the smallest radius complement in that set, and this ball centered at $a$ becomes a subset of $A^c$. Thus $A^c$ is open, so $A$ is closed. As $A$ is closed and bounded, $A$ is compact, by the first part of the Heine-Borel theorem.

  Suppose $A$ is a compact subset of $\mathbf{R}^n$, and hence closed and bounded. As it is bounded, $A$ is contained in a ball. Every ball is contained in a cube, denoted $Q_0$. Let $l_0$ be the length of the diagonal of the cube. Suppose that there is a cover $\mathcal{C}$ of $A$ with no finite subcover. Divide $Q_0$ into $2^n$ subcubes. One of these must not have a finite subcover. Denote this cube $Q_1$. Continue defining these cubes by this method to form a chain $Q_0 \subset Q_1 \subset \dots$. The diagonal of cube $Q_k$ is $l_k = l_1/2^k$. Each cube $Q_k$ is non-empty, hence we may pick some $q_k$ from the cube to form a sequence $(q_k)$. We claim $q_k$ converges to some point $q$, proving the claim by Cauchy's criterion. Given $\varepsilon > 0$, pick an integer $M$ such that $M \geq \lg(l_1/\varepsilon)$. Then the tail $(q_k)_{k \geq M}$ is contained in the cube $Q_M$, whose diagonal $l_M = l_1/2_M$. As $M \geq \lg(l_1/\varepsilon)$, $2^M \geq l_1/\varepsilon$, hence $l_M = l_1/2^M \leq l_1 \varepsilon/l_1 = \varepsilon$. It follows that for any points $q'$ and $q''$ in the sequence, $\| q' - q' \| \leq \varepsilon$, as the diagonal is the longest distance between any two points in the square. Thus the sequence converges to some point $q$. There is some open set $C$ in $\mathcal{C}$ such that $q \subset C$. Take the ball $B(q,r)$ such that this ball is contained in $C$. Then there is some square $Q_n$ which is contained in this ball, yet it contains no finite subcover. By contradiction, this cover $\mathcal{C}$ could not have existed.
\end{proof}

The statement of a compact set in terms of open covers is often taken to be the definition of a compact set in most textbooks. We chose our original definition as it is most intuitive.

We need two more types of sets to finish off the topology of $\mathbf{R}^n$, the first being the quality of connectedness. Connectedness is an easy thing to see. A circle is connected, two separate circles are not. Like many intuitive concepts, connectedness becomes a very difficult concept to formalize.

\begin{definition}
  A set $A$ is {\bf disconnected} if it can be partitioned into two sets $A_1$ and $A_2$, in a way that there are two disjoint open sets $U_1$ and $U_2$ such that $A_1$ is contained in $U_1$ and $A_2$ is contained in $U_2$. A set is {\bf connected} if it is not possible to disconnect it.
\end{definition}

Our penultimate topological property, defined only in a vector space is convexity. Intuitively, a set is convex if, for any two points, the line between those two points remains in the set. How do we formalize this? Well, for any point $c$ between two points $a$ and $b$, $c = \lambda a + (1 - \lambda)b$ for some value $\lambda \in [0,1]$. This motivates the following definition.

\begin{definition}
  A set $A$ is convex, if, for any two points $a$ and $b$ in the set, and for any scalar $\lambda \in [0,1]$, $\lambda a + (1 - \lambda) b \in A$.
\end{definition}

What is nice about this definition is it involves no notion of distance. We can consider this for all vector spaces over the real numbers.

\begin{theorem}
  Any convex set is connected.
\end{theorem}
\begin{proof}
  We prove by contraposition. Suppose a set $A$ is disconnected. Then $A = A_1 \cup A_2$ for two disjoint sets $A_1$ and $A_2$, where there are open sets $U_1$ and $U_2$ such that $A_1 \subset U_1$, $A_2 \subset U_2$. Take two points $a_1 \in A_1$, $a_2 \in A_2$. We claim there is a point on the line between $a_1$ and $a_2$ that is not contained in $A$. Take the supremum of the set $\{ \lambda \in [0,1] : \lambda a_1 + (1 - \lambda) a_2 \in A_1 \}$, and denote it $\lambda'$. We claim $x = \lambda' a_1 + (1 - \lambda') a_2$ cannot be an element of $A_1$. If it was, it is contained in an open set $U_1$, and hence there is a ball $B(x,r)$ which is contained in $U_1$, and hence not in $A_2$. Take the value $\lambda = min(\lambda' + r/2\|a_1\|, \lambda + r/2\|a_2\|, 1)$. Then,
  %
  \begin{align*}
  \| \lambda a_1 + (1 - \lambda) a_2 - x \| &= \| (\lambda - \lambda') a_1 + (\lambda' - \lambda) a_2 \|\\
      &\leq |\lambda - \lambda'| \| a_1 \| + |\lambda' - \lambda \|a_2\|\\
      &\leq |r/2\|a_1\|| \|a_1\| + |r/2\|a_2\|| \|a_2\|\\
      &= r
  \end{align*}
  %
  Thus $\lambda'$ is not the supremum, and by contradiction. It cannot be in $A_1$. For similar reasons, it also cannot be in $A_2$, hence it is not in $A$ and thus the set is not convex.
\end{proof}

We introduce a final topological notion before we can start studying the familiar notions of calculus.

\begin{definition}
  Given a subset $A$ of $\mathbf{R}^n$, we say $c$ is an accumulation or cluster point if $c \in \overline{A - \{c\}}$. This is equivalent to the notion that, for any radius $r$, $\mathring{B}(c,r) \cap A \neq \emptyset$.
\end{definition}

Though we have analysed the topological properties of $\mathbf{R}^n$ in a fairly abstract setting, and have not established much more than a mountain of definitions. It is promised that all established properties will become useful as we establish a deeper understanding of differentiation and integration, the main goals of mathematics.





\chapter{Functions and Continuity}

After enough mathematics to understand this report, you should know the formal definition of a function. In this book, we deal with functions from a subset of a metric space to another metric space, specifically, from $\mathbf{R}^n$ to $\mathbf{R}^m$. In this case, we can consider this function to be a function of $n$ variables, mapped to $m$ variables. Really, this is just the composition of $m$ functions $f_i: \text{dom}(f) \to \mathbf{R}$, defined by the equation
%
\[ f(v) = (f_1(v), f_2(v), \dots, f_m(v)) \]
%
Since each of the component functions above maps into $\mathbf{R}$, the definition of limits of functions in $\mathbf{R}^n$ can be intrinsically connected with functions to $\mathbf{R}$. We simply need to ensure that as vectors in the domain get close to a point, the mapped points get very close to the limit.

\begin{definition}
  Consider a function $f$, mapping a subset of a metric space $M$ to a metric space $N$, and suppose $m$ is an accumulation point of $\text{dom}(f)$. We say that $f$ approaches a vector $n$ in $N$ as its domain approaches $m$, and we write $\lim_{x \to m} f(x) = n$, if any one of the equivalent notions is defined.

  \begin{enumerate}
    \item For every $\varepsilon > 0$, there exists a number $\delta > 0$ such that, for any point $x \in \text{dom}(f)$ such that $0 < d(x, m) < \delta$, $d(n, f(x)) < \varepsilon$.
    \item For every open ball $B(n,\varepsilon)$, there exists a punctured ball $\mathring{B}(m,\delta)$ such that $f(\mathring{B}(m,\delta))$ is a subset of $B(n,\varepsilon)$.
    \item For every sequence $(x_i)$ with elements in $\text{dom}(f) - \{ c \}$ such that $x_i$ tends to $m$, $f(x_i)$ tends to $n$.
  \end{enumerate}
\end{definition}
\begin{proof}
  The equivalence of (1) and (2) is obvious, found by expanding the definitions of (2). We prove the other equivalences below.

  $(1) \implies (3)$. Suppose $\lim_{x \to m} f(x) = n$ in the sense of the first definition. Take a sequence $(a_i)$ such that $a_i \to m$. Fix any $\varepsilon > 0$. There is some $\delta$ such that if $0 < d(x, m) < \delta$, $d(f(x), w) < \epsilon$. There is some integer $M$ such that the tail $(a_i)_{i \geq M}$ is contained in $B(c,\delta)$. But then the tail $(f(a_i))_{i \geq M}$ is contained in the ball $B(w,\varepsilon)$, so that the third definition holds as a result.

  $(3) \implies (1)$. We prove by contraposition. Suppose $\lim_{x \to m} f(x) \neq n$. Then there is $\varepsilon > 0$ such that, for any number $\delta$, there is a point $x$ such that $0 < d(m,x) < \delta$ but $d(x,n) \geq \varepsilon$. Define a sequence $(a_i)$ such that $d(a_i, m) < 1/i$, but $d(f(a_i),n) \geq \varepsilon$. Then $a_i \to m$, but $f(a_i) \not \to w$. By contraposition, we obtain that the third property implies the first.
\end{proof}

As the equivalence of sequences of vectors to sequences of real numbers allowed us to prove theorems, the equivalence of limits of functions to sequences implies many theorems on par with ones you have already seen.

\begin{corollary}
  The limit of a function is unique.
\end{corollary}
\begin{proof}
  Let $f$ be a function that converges at an accumulation point $u$ to $v$ and $w$. Then for every sequence $(a_i)$ such that $a_i \to c$, $a_i \to v$ and $a_i \to w$, hence $v = w$. As $u$ is an accumulation point, there must be a sequence with this property, hence $v = w$, and the limit is unique.
\end{proof}

We leave the rest of these arguments to the reader, which hold in vector spaces.

\begin{corollary}
  If $\lim_{v \to u} f(v) \to l$ and $\lim_{v \to u} f(v) \to m$, then
  %
  \[ \lim_{v \to u} (f \pm g)(x) = l \pm w \]
  %
  \[ \lim_{v \to u} \langle f, g \rangle = \langle l, w \rangle \]
  %
  \[ \lim_{v \to u} \| f \| = \| l \| \]
\end{corollary}

\begin{corollary}
  If $f$ maps from a subset of a metric space $M$ to a metric space $N$
  %
  \[ \lim_{v \to u} f(v) = l \]
  %
  and $g$ maps from a subset of $N$ to another metric space $L$ such that
  %
  \[ \lim_{w \to l} g(w) = m \]
  %
  Then it follows that
  %
  \[ \lim_{v \to u} (g \circ f)(v) = m \]
\end{corollary}

Before we move along to continuity, let us talk about some deceptive intuitive definitions of limits in vector spaces that do not have the properties that are wanted.

\begin{definition}
  The directional limit of a function $f$ at a point $a$ along a vector $v \neq 0$ is defined by
  %
  \[ \lim_{t \to 0} f(a + tv) \]
\end{definition}

It is clear that, if the actual limit of the function exists at a point, then all directional limits exist and are equal. The converse, surprisingly, is not true, as the following example illustrates.

Consider the function $f$, defined by
%
\[ f(x,y) = \left\{
     \begin{array}{lr}
       x^2/y & : y \neq 0\\
       0 & : y = 0
     \end{array}
   \right.\]
%
Let $v = (v_1, v_2)$ be an arbitrary vector. If $v_2 = 0$, the directional limit of $f$ at 0 in the direction of $v$ is
%
\[ \lim_{t \to 0} f(tv) = \lim_{t \to 0} 0 = 0 \]
%
otherwise,
%
\[ \lim_{t \to 0} f(tv_1, tv_2) = \lim_{t \to 0} (tv_1)^2/tv_2 = \lim_{t \to 0} t v_1^2/v_2 = 0 \]
%
Hence all directional limits exist and are equal. Consider the entire limit.
%
\[ \lim_{v \to 0} f(v) \]
%
For any value of $\delta$, consider the vector $v = (\delta - \varepsilon, \varepsilon)$, for $\delta > \varepsilon > 0$, such that $\|v\| \leq |\delta - \varepsilon| + \varepsilon = \delta$. Then
%
\[ f(v) = (\delta - \varepsilon)/\varepsilon = \delta/\varepsilon - 1 \]
%
As $\varepsilon$ tends to 0 from above, the value of this vector tends to infinity, and hence the limit of $f(v)$ as $v$ approaches 0 cannot exist. Regardless of how small we make $\delta$, we still have a sequence of vectors in $\delta$ that tend to infinity.

We note the limit of a function at a point may not approach the value of a function. We have a special name for functions with this property.

\begin{definition}
  Suppose we have a function $f$ from a subset of $M$ to $N$. Then, for a point $u \in \text{dom}(f)$, we say that $f$ is continuous at $u$ if, for any $\varepsilon > 0$, there is a $\delta$ such that, for any vector $v$ such that $d(v, u) < \delta$, $d(f(v), f(u)) < \varepsilon$. If $u$ is an accumulation point, this is equivalent to the fact that
  %
  \[ \lim_{v \to u} f(v) = f(u) \]
  %
  We say that, for a subset $C$ of the domain of $f$, $f$ is continuous on $C$ if $f$ is continuous at every point $c \in C$.
\end{definition}

Intuitively, continuity means we can draw the function without taking pen off paper. The limit of a function is precisely the point that make the function continuous. If $u$ is not an accumulation point, then the function is continuous at that point, since we may pick a $\delta$ such that $u$ is the only element in the ball. Continuous functions have many useful properties. We leave the proofs to the reader as they follow immediately from statements about limits of functions.

\begin{lemma}
  For any sequence $(v_i)$, such that $v_i \to v$ and any function $f$ continuous at $v$
  %
  \[ \lim_{i \to \infty} f(v_i) = f(\lim_{i \to \infty} v_i) \]
\end{lemma}

\begin{lemma}
  Let $f$ and $g$ be functions from subsets of $\mathbf{R}^m$ both continuous at a point $u$, and $h$ maps from a subset of $\mathbf{R}^n$, that is continuous at $f(c)$:

  \begin{enumerate}
    \item $f \pm g$ is continuous at $c$.
    \item $\langle f, g \rangle$ is continuous at $c$.
    \item $\|f\|$ is continuous at $c$.
    \item Every component function of $f$ is continuous at $c$.
    \item $h \circ f$ is continuous at $c$.
  \end{enumerate}
\end{lemma}

The following theorem is very important in the field of topology.

\begin{theorem}
  A function $f$ from a subset of $\mathbf{R}^n$ to $\mathbf{R}^m$ is continuous on its domain if and only if, for every open set $C$ in $\mathbf{R}^m$, there exists an open set $U$ such that $U \cap \text{dom}(f) = f^{-1}(C)$.
\end{theorem}
\begin{proof}
  Let $f$ be as above, continuous on its domain. We prove the statement for open balls, from which the entire theorem follows as an arbitrary open set is the union of open balls. Take a ball $B(x,r)$ for some point $x$ and some radius $r$. Then $f^{-1}(B(x,r))$ is defined to be the set
  %
  \[ \{ a \in \text{dom}(f) : \| f(a) - x \| < r \} \]
  %
  Since $f$ is continuous, there is $\delta$ such that
  %
  \[ B(a,\delta) \cap \text{dom}(f) \subset f^{-1}(B(f(a),r - d(f(a), x))) \]
  %
  This set is a subset of $B(x,r)$, as if
  %
  \[ d(y, f(a)) < r - d(f(a), x) \]
  %
  then
  %
  \[ d(y, x) \leq d(y, f(a)) + d(x, f(a)) < r - d(f(a), x) + d(f(a), x) = r \]
  %
  It follows that the set $B(a,\delta)$ is the set we require.

  Suppose for any open set $C$ in $\mathbf{R}^m$, there is an open set $U$ such that
  %
  \[ U \cap \text{dom}(f) = f^{-1}(C) \]
  %
  Then for any $\varepsilon > 0$, and for any point $x$, $B(f(x), \varepsilon)$ is open, hence there is an open set $U$ such that
  %
  \[ U \cap \text{dom}(f) = f^{-1}(B(f(x), \varepsilon)) \]
  %
  As $x \in f^{-1}(B(f(x), \varepsilon))$, we know there is a ball $B(x, \delta)$ such that $f(B(x, \delta) \cap \text{dom}(f)) \subset B(f(x), \varepsilon)$. Thus we obtain continuity of the function $f$.
\end{proof}

\begin{corollary}
  A function is continuous if and only if, for every closed set $C$ in $\mathbf{R}^m$, there exists a closed set $D$ such that $D \cap \text{dom}(f) = f^{-1}(C)$.
\end{corollary}

Some simple practical applications result from this theorem. Here we list one such example.

\begin{lemma}
  The set of solutions to the equation $\cos(x^2 + y^2) > 1/2$ is open.
\end{lemma}
\begin{proof}
  Let $f(x,y) = \cos(x^2 + y^2)$. This function is continuous as it is the composition of continuous functions. If $(x,y)$ is a solution to the inequality, this means exactly that $f(x,y) > 1/2$, which is true if and only if $f(x,y) \in f^{-1}((1/2, \infty))$. As this set is open, the inverse of that set is open.
\end{proof}

The interests of continuous functions in the field of Topology rely in the fact that the functions preserve the topological properties of a space in some way. Here is one such useful preservation, which results in what should be a well known corollary.

\begin{theorem}
  Let $f$ be a continuous function, and $A \subset \text{dom}(f)$ a compact set. Then $f(A)$ is compact.
\end{theorem}
\begin{proof}
  Let $U$ be an open cover on $f(A)$. For each open set $u$ in $U$, there is an open set $M$ such that $f^{-1}(u) = M \cap \text{dom}(f)$. Form the set of $M$'s for each $u$. This is an open cover of $A$, and hence contains a finite subcover $U'$. For each $u \in U'$, take the corresponding open set in $f(A)$. This forms a finite subcover, hence $f(A)$ is compact.
\end{proof}

\begin{corollary}[The Extreme Value Theorem]
  If $f$ is a mapping from $\mathbf{R}^n$ to $\mathbf{R}$, and $\text{dom}(f)$ is compact, then $f(\text{dom}(f))$ is bounded, and attains its minimum and maximum.
\end{corollary}
\begin{proof}
  Since $\text{dom}(f)$ is compact, $f(\text{dom}(f))$ is compact by continuity, which is hence closed and bounded. The maximum and minimum are the supremum and infinum, and are thus limit points of the set. It follows that as $f(\text{dom}(f))$ is closed, the maximum and minimum are in the set.
\end{proof}

Connectedness is also maintained by a continuous map, but in the other direction.

\begin{theorem}
  If $f$ is a continuous mapping, and $A$ is a separable subset of the range of $f$, then $f^{-1}(A)$ is separable. Hence by contraposition, if $B$ is a connected subset of $\text{dom}(f)$, $f(B)$ is also connected.
\end{theorem}
\begin{proof}
  Assume $A$ is a non-empty separable subset of the range of $f$, so
  %
  \[ A = A_1 \cup A_2 \]
  %
  for non-empty subsets $A_1$ and $A_2$ such that
  %
  \[ A_1 \cap \overline{A_2} = \overline{A_1} \cap A_2 = \emptyset \]
  %
  Consider the two sets $f^{-1}(A_1)$, and $f^{-1}(A_2)$. Then
  %
  \[ f^{-1}(A_1 \cup A_2) = f^{-1}(A_1) \cup f^{-1}(A_2) \]
  %
  and each is non-empty. Since $A_1 \subset \overline{A_1}$, we obtain that $f^{-1}(A_1) \subset f^{-1}(\overline{A_1})$, and the same for $A_2$. These sets are closed as their image is closed. It follows that the closure of $f^{-1}(A_1)$ is a subset of $f^{-1}(\overline{A_1})$, as for $A_2$, and
  %
  \[ f^{-1}(\overline{A_1}) \cap f^{-1}(A_2) = f^{-1}(\overline{A_1} \cap A_2) = f^{-1}(\emptyset) = \emptyset \]
  %
  The proof is similar for $A_2$, showing that $f^{-1}(A)$ is separable.
\end{proof}

\begin{corollary}[The Intermediate Value Theorem]
  If $f$ is a continuous mapping from a connected set to $\mathbf{R}$, and for a number $r$, two points $x$ and $y$, $f(x) \leq r \leq f(y)$, then there is some point $z$, $f(z) = r$.
\end{corollary}
\begin{proof}
  For otherwise the range of $f$ would be a separated set.
\end{proof}

\begin{lemma}Any norm on $\mathbf{R}^n$ is a continuous function.\end{lemma}
\begin{proof}
  Let $\tripnorm{\cdot}$ be an arbitrary norm on $\mathbf{R}^n$. Pick some arbitrary vector $v$. By an easy corollary of the triangle inequality,
  %
  \[ |\tripnorm{v} - \tripnorm{w}| \leq \tripnorm{v - w} \]
  %
  Hence if $\tripnorm{v - w} \leq \varepsilon$, $|\tripnorm{v} - \tripnorm{w}| \leq \varepsilon$ by transitivity, showing the function is continuous.
\end{proof}

Before we finish our discussion of continuity and limits, notice something about the norms on $\mathbf{R}^n$. Since
%
\[ \sqrt{\sum_{k = 0}^n v_k^2} \leq \sum_{k = 0}^n |v_k| \]
%
if a function converges in the $l_1$ norm it converges in the $l_2$ norm. Conversely, since, for each $v_k$,
%
\[ |v_k| \leq \|v\| \]
%
we obtain that
%
\[ \sum_{k = 0}^n |v_k| \leq n \sqrt{\sum_{k = 0}^n v_k^2} \]
%
Since $n$ is contant, a function converges in the $l_1$ norm if and only if it converges in the $l_2$ norm. The topologies of $l_1$ and $l_2$ are thus equivalent, and we call the norms equivalent if this occurs. It turns out that in any finite dimensional vector, space, two norms generate the same topology.

\begin{theorem}All norms on $\mathbf{R}^n$ are equivalent\end{theorem}
\begin{proof}
  Consider the norm $\| \cdot \|_{\infty}$ in $\mathbf{R}^n$ and $\tripnorm{ \cdot }$ an arbitrary other norm. We prove there are two constants $a$ and $b$ such that, for any vector $v$,
  %
  \[ a\|v\|_{\infty} \leq \tripnorm{v} \leq b\|v\|_{\infty} \]
  %
  thereby proving the norms are equivalent. To find a value for $b$ is just a simple calculuation.
  %
  \begin{align*}
    \tripnorm{v} &\leq \sum_{k = 1}^n \tripnorm{v_ke_k}\\
                 &= \sum_{k = 1}^n |v_k| \tripnorm{e_k}\\
                 &\leq \sum_{k = 1}^n |v_k| \max_{t} \tripnorm{e_t}\\
                 &= \big( \max_{t} \tripnorm{e_t} \big) \sum_{k = 1}^n |v_k|\\
                 &= \big ( \max_{t} \tripnorm{e_t} \big) \| v \|_{\infty}
  \end{align*}
  %
  To find $a$ is a bit more complicated. We first note that if we can find a value $a$ that bounds the norm below on the unit sphere $S(0,1)$ with respect to $\tripnorm{\cdot}$, then this value bounds every vector below. The reason is as follows, take an arbitrary vector $v \neq 0$. Then $v/\tripnorm{v}$ is on the unit sphere, hence $a\|v/\tripnorm{v}\| \leq \tripnorm{v/\tripnorm{v}}$, but we can take out the scalar value $a/\tripnorm{v} \|v\| \leq (1/\tripnorm{v}) \tripnorm{v}$, and by multiplying both sides by the positive value $\tripnorm{v}$, we obtain that $a \| v \| \leq \tripnorm{v}$.

  Thus we need only find an $a$ that applies in the unit sphere. We note that $\| \cdot \|/\tripnorm{\cdot}$ is a continuous function on the unit sphere, and thus on the unit sphere, by the extreme value theorem the function obtains a minimum value $c$. This minimum value $c$ cannot be 0, for this value only results from the vector $0$, which is not on the unit sphere. Thus for any vector $v$ on $S(0,1)$, $\| v \| \leq c \tripnorm{v} $, hence $\| v \| / c \leq \tripnorm{v}$. Thus we obtain a bound below for the norm.
\end{proof}

In infinite dimensional vector spaces, this theorem does not hold. We leave the rest of this subtle discussion in a more general form to a course in topology. For now, lets get to the actual calculus of $\mathbf{R}^n$: differentiability in $\mathbf{R}^n$.







\chapter{Linearity and Differentiation}

How do we study differentiability in $\mathbf{R}^n$. What does it mean for a function to be differentiable? There are a number of generalizations, and we begin with the weakest. Notice that, given a function $f:\mathbf{R}^n \to \mathbf{R}$ described by
%
\[ f(v_1, v_2, \dots ,v_n) \]
%
Now consider the function that keeps all coordinates of a vector fixed but one. That is, a function of the form
%
\[ f(v_1, v_2, \dots, \cdot, \dots, v_n) \]
%
If we fix all values in the vector but one value $v_i$, we obtain a real valued function, and thus we may differentiate normally. If we can do this for all values $v_i$, we can in some sense say the function is diffentiable as a whole. This motivates the following definition.

\begin{definition}
  A function $f: \mathbf{R}^n \to \mathbf{R}^m$ is partially differentiable at a point $v$ with respect to some basis element $e_j$ if the limit
  %
  \[ \lim_{k \to 0} \frac{f(v + ke_j)}{k} \]
  %
  exists. We call each limit a partial derivative and denote the value by $\pder{f}{x_j}f(v)$.
\end{definition}

Technically, though $\partial_jf(v)$ is a value for any $v$, the derivative determines a function on the subset of the domain of $f$ on which the $j$'th partial derviative exists. We describe this function as $\partial_jf$ to be consistent with the notation above. Some people think it helps them think about calculus, where $dx$ represents an infinitismal change in $x$ quotiented with a corresponding infinitismal change in $f(x)$. They take joy in seeing equations like the chain rule in one dimensional calculus, that
%
\[ \frac{df}{dx} = \frac{dg}{df} \frac{df}{dx} \]
%
can be obtained by `cancelling out' the $df$. Remember that partial derivatives do not represent fractions in any capacity!

\begin{theorem}
  Let $f$ be a function mapping a subset of $\mathbf{R}^n$ to $\mathbf{R}^m$, and let $n$ component functions $f_k$ be defined. Then the partial derivative $\partial_j f$ is equal to
  %
  \[ (\partial_j f_1, \partial_j f_2, \dots, \partial_j f_m) \]
  %
  In the sense that also, one exists at a point if and only if the other exists.
\end{theorem}
\begin{proof}
  Let $a$ be an arbitary point in the domain of $f$.
  %
  \begin{align*}
    \lim_{t \to 0} \frac{f(a + te_j) - f(a)}{t} &= \lim_{t \to 0} \sum_{k = 0}^m \frac{f_k(a + e_j) - f_k(a)}{t}e_m\\
    &= \sum_{k = 0}^m \lim_{t \to 0} \sum_{k = 0}^m \frac{f_k(a + e_j) - f_k(a)}{t}\\
    &= (\partial_j f_1, \partial_j f_2, \dots, \partial_j f_m)
  \end{align*}
  %
  We know the limit of the entire function exists if and only if each coordinate limit exists, and hence the function exists in total.
\end{proof}

The partial derivative can easily be found using one dimensional calculus. For any function $f$ from $\mathbf{R}^n$ to $\mathbf{R}^m$ defined at a point $a$ with a partial derivative $\partial_j f(a)$, define a function $g$ from $\mathbf{R}$ to $\mathbf{R}^m$ by
%
\[ g(t) = f(a + te_j) \]
%
Then $g' = \partial_j f$.

A simple generalization of partial derivatives results from noticing that the use of basis elements is quite arbitrary in the definition of a derivative. For any vector $v$, we can consider the value of
%
\[ \lim_{t \to 0} \frac{f(a + tv) - f(a)}{t} \]
%
and we obtain no problems.

\begin{definition}
  For a vector $v$ and a function $f$ with a point $a$ in its domain, we define the directional limit $\partial_v f(a)$ to be
  %
  \[ \lim_{t \to 0} \frac{f(a + tv) - f(a)}{t} \]
\end{definition}

We can recursively define directional derivatives. If $\partial_v f$ is defined on a region around a point, we may define $\partial_w \partial_v f$ to be the directional derivative of the partial derivative of $f$ with respect to $y$.

We now turn to a study of topological properties of linear operators. Recall the following definition.

\begin{definition}
  A function $T:\mathbf{R}^n \to \mathbf{R}^m$ is a linear function/operator/transformation if, for any two vectors $v$ and $w$, and any scalar $\lambda$ in $\mathbf{R}$:

  \begin{itemize}
    \item $T(v + w) = T(v) + T(w)$
    \item $T(\lambda v) = \lambda T(v)$
  \end{itemize}
\end{definition}

Every linear transformation between $\mathbf{R}^n$ and $\mathbf{R}^m$ can be represented by an $m$ by $n$ matrix $M$, such that, for any vector $v \in \mathbf{R}^n$:
%
\[ T(v) = Mv \]

\begin{lemma}
  For any linear transformation $T:\mathbf{R}^n \to \mathbf{R}^m$, there is a number $m$ such that, for any vector $v$:
  %
  \[ \| T(v) \| \leq m \| v \| \]
\end{lemma}
\begin{proof}
  For any linear transformation $T$, there is a matrix $M$ such that $T(v) = Mv$. Let $M_i$ be the $i'$th row of the matrix. Then
  %
  \[ T(v) = \begin{bmatrix} \inner{M_1, v} \\ \inner{M_2, v} \\ \vdots \\ \inner{M_m, v} \end{bmatrix} \]
  %
  Then it follows that, by Corallary 2.12,
  %
  \begin{align*}
    \| T(v) \| &\leq \sum_{k = 1}^m | \inner{M_k, v} |\\
               &\leq \sum_{k = 1}^m \| M_k \| \| v \|\\
               &\leq \sum_{k = 1}^m \max_{k} \| M_k \| \| v \|\\
               &= m \max_{k} \| M_k \| \| v \|
  \end{align*}
  %
  and we have constructed a fixed bound for $\| T(v) \|$ in terms of $\| v \|$.
\end{proof}

\begin{corollary}
  All linear functions are continuous
\end{corollary}
\begin{proof}
  The proof is a simple calculation, using the lemma above. We note that, as in one-dimensional calculus, $\lim_{x \to a} f(x)$ is equivalent to the $\lim_{k \to 0} f(a + k)$. For any linear transformation $T$,
  %
  \[ \lim_{k \to 0} T(k) = 0 \]
  %
  We know that $\| T(k) \| \leq m \| k \|$ for some value $m > 0$. Thus for any $\varepsilon$, choose a value $\delta$ equal to $\varepsilon/m$. Then, if $\| k \| < \varepsilon/m$,
  %
  \[ \| T(k) \| \leq m \| k \| \leq m \varepsilon/m = \varepsilon \]
  %
  Then we get a simple computation of the limit at any position $a$:
  %
  \begin{align*}
    \lim_{k \to 0} T(a + k) &= \lim_{k \to 0} T(a) + T(k)\\
                            &= T(a) + \lim_{k \to 0} T(k)\\
                            &= T(a)
  \end{align*}
\end{proof}

Consider, for each linear transformation $T$, the value
%
\[ \max_{v \in \overline{B}(0,1)} \|T(v)\| \]
%
The maximum is obviously attained on each set as $\|\cdot\|$ and $T$ are continuous, and $\overline{B}(0,1)$ is compact. One can verify that this is in fact a norm on the vector space of linear transformations, so that we may talk about spatial properties of these linear operators.

Linear transformations are the key to understanding full on differentiation in $\mathbf{R}^n$. Notice in the one dimensional case,
%
\[ \lim_{h \to 0} \frac{f(c + h) - f(c)}{h} = m \]
%
can be rewritten as
%
\[ \lim_{h \to 0} \frac{f(c + h) - f(c) - mh}{h} = 0 \]
%
and $mh$ can really be seen as a function on $h$, turning it into a linear transformation on $\mathbf{R}$.

\begin{definition}
  Let $f$ be a function from $\mathbf{R}^n$ to $\mathbf{R}^m$. The derivative of $f$ at a point $a$, denoted $Df(a)$, is the linear transformation $\lambda$ from $\mathbf{R}^n$ to $\mathbf{R}^m$ such that
  %
  \[ \lim_{v \to 0} \frac{\|f(a + v) - f(a) - \lambda(v)\|}{\|v\|} = 0 \]
  %
  where $v$ is a vector, and hence can approach from any direction.
\end{definition}

This definition is better than the partial derivative definition because it results in many of the same properties that the derivative in one dimension. For instance, if the vector derivative of a function exists, then that function is continuous. The same is not true if all partial derivatives exist -- they only attack the problem from one direction. Even when all directional derivatives exist the full derivative may not exist.

\begin{theorem}
  For a function $f$, if $Df(a)$ exists, it is unique.
\end{theorem}
\begin{proof}
  Let $\lambda$ and $\mu$ be two linear transformations such that
  %
  \[ \lim_{v \to 0} \frac{\|f(a + v) - f(a) - \lambda(v)\|}{\|v\|} = \lim_{v \to 0} \frac{\|f(a + v) - f(a) - \mu(v)\|}{\|v\|} = 0 \]
  %
  By adding both equalities together,
  %
  \[ \lim_{v \to 0} \frac{\|f(a + v) - f(a) - \lambda(v)\| + \|f(a + v) - f(a) - \mu(v)\|}{\|v\|} = 0 \]
  %
  Now by the triangle inequality, and as
  %
  \[ \|f(a + v) - f(a) - \mu(v)\| = \|\mu(v) - f(a + v) + f(a)\| \]
  %
  We obtain that
  %
  \[ \lim_{v \to 0} \frac{\|\mu(v) - \lambda(v)\|}{\|v\|} = 0 \]
  %
  Which states specifically that
  %
  \[ \lim_{v \to 0} \|\mu(\frac{v}{\|v\|}) - \lambda(\frac{v}{\|v\|})\| = 0 \]
  %
  We note that, for any specific direction of $v$, this function is constant, and thus the values of the transformation are equal in the unit sphere. But this implies that the functions are equal everywhere, as the basis elements of the vector space lie on the unit interval.
\end{proof}

Like with partial derivatives, we let $Df$ denote the function that maps a point $a$ in $\mathbf{R}^n$ to the linear transformation $Df(a)$. This function exists exactly because of the uniqueness of the derivative at a point.

\begin{theorem}[The Chain Rule]
  Let $f$ be a function from $\mathbf{R}^n$ to $\mathbf{R}^m$, and $g$ a function from $\mathbf{R}^k$ to $\mathbf{R}^n$. Suppose $g$ is differentiable at a point $c$, and $f$ is differentiable at $g(c)$. Then the function $h = f \circ g$ is differentiable at $c$, and the derivative satisfies the equation
  %
  \[ Dh(c) = Df(g(c)) \circ Dg(c) \]
\end{theorem}
\begin{proof}
  \[ \lim_{v \to 0} \frac{\|f(g(c) + v) - f(g(c)) - \lambda(v)\|}{\|h\|} = 0 \]
  \[ \lim_{v \to 0} \frac{\|g(c + v) - g(c) - \mu(v)\|}{\|h\|} = 0 \]

  \[ \lim_{v \to 0} \frac{\| f(g(c + v)) - f(g(c)) - \lambda \circ \mu (v)\|}{\|h\|} = 0 \]
\end{proof}










\chapter{Optimization}

Half of mathematics consists of trying to find the extrema of one situation of another. Interesting problems hardly ever result in the milieu between the minimization and maximization of some quantity. In one dimension, we were able to calculate minima and maxima of functions with relative ease. This chapter extends this process to the multidimensional case.

We start with a relatively simple case. Let $f:U \to V$ be a function differentiable at every point on its domain. Suppose $p \in U$ is a local extrema of the function. If we consider any partial derivative around this point, we obtain the derivative of a one dimensional function, attaining a local minima at some corresponding point to $p$. This justifies the fact that the partial derivative must be zero at this point. Therefore, in order to discover those points which are possible candidates for extrema, we must first calculate partial derivatives and find the points at which the derivatives even out. Of course, this is not sufficient to determine whether a function is actually an extrema at this point, nor will an exploration of a single second partial derivative give us much information -- we can quite easily have `saddle' or `pringle' points which is a maxima for one partial derivative, but a minima for another.

% PRINGLE PICTURE HERE

Nonetheless, sometimes there are two many zero points to check every single one. Thus we must ascend to a more complex process. If a function $f:\mathbf{R}^n \to \mathbf{R}$ has sufficient smoothness at a point $c$, such that all 2nd order partial derivatives exist, we may form the $n$ by $n$ Hessian matrix $H(c)$, where the netry at the $i$'th row and $j$'th column is $\frac{\partial^2 f}{\partial x_i \partial x_j}$. This matrix will be the key to finding maxima and minima of a function across its domain. We note that when 2nd order partial derivatives are continuous in an open set containing $c$, we can conclude that for any $x_i$ and $x_j$,
%
\[ \frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i} \]
%
Hence $H(c)$ will be a symmetric matrix.

Now let $A$ be any symmetric matrix. Elementary linear algebra tells us that there exists a basis of orthonormal eigenvectors which span the matrix's domain. Let $(v_1, \dots, v_n)$ be such a basis, and $(\lambda_1, \dots, \lambda_n)$ a corresponding set of eigenvalues. Then it follows that
%
\begin{align*}
    \langle A \sum a_k v_k, \sum a_k v_k \rangle &= \langle \sum \lambda_k a_k v_k, \sum a_k v_k\\
    &= \sum_{i,j} \langle \lambda_i a_i v_i, a_j v_j \rangle\\
    &= \sum_i \lambda_i a_i^2
\end{align*}
%
This follows for any vector due to the basis' universal span. If we select the maximal and minimal eigenvalues $\lambda_{min}$ and $\lambda_{max}$, we obtain that
%
\[ A \sum a_k v_k, \sum a_k v_k \rangle = \sum_i \lambda_i a_i \leq \lambda_{max} \| \sum a_i \|^2 \]
%
Similarly, $\langle Av, v \rangle \geq \lambda_{min} \|v\|$. On the unit sphere we obtain that
%
\[ \lambda_{min} \leq \langle Av, v \rangle \leq \lambda_{max} \]
%
These bounds are attained, as $\langle Av_{max}, v_{max} \rangle = \lambda_{max}$, and $\langle Av_{min}, v_{min} \rangle = \lambda_{min}$.

A matrix $A$ is positive definite if all eigenvalues are positive. From what we have deduced earlier, if we have an orthonormal eigenvector span, that for any vector $v$, $\langle Av, v \rangle \geq \lambda_{min} > 0$. A matrix is positive semidefinite if eigenvalues are non-negative. Negative definity and semi-definity are defined similarly, and obvious results like the one above follow for these matrices. A matrix that does not fall into any of these groups is indeterminate.

These mathematical prerequisites enable us to state the `second derivative test' for multivariate functions. Suppose $f:\mathbf{R}^n \to \mathbf{R}$ has continuous second derivatives. Consider the Hessian matrix $H(c)$ at a point $c$. The smoothness of $f$ implies $H(c)$ is symmetric. Firstly,

\begin{lemma}
    For every $\varepsilon > 0$, there is a neighborhood around $c$ such that for any point $x$ in this neighbourhood, and for any point $z$ on the unit circle,
    %
    \[ | \langle H(x)z,z \rangle - \langle H(c)z, z \rangle| < \varepsilon \]
\end{lemma}
%
\begin{proof}
    For any $x$,
    %
    \begin{align*}
        |\langle H(x)z, z \rangle - \langle H(c)z, z \rangle | &= | \langle (H(x) - H(c))z, z \rangle |\\
        &\leq  \| (H(x) - H(c)) z \|\ \| z \|\\
        &\leq \|H(x) - H(c)\|\ \|z\|\\
        &\leq \sqrt{ \sum_{i,j} h(x)_{i,j} - h(c)_{i,j}}\\
        &= \sqrt{ \sum_{i,j} \frac{\partial^2 f}{\partial x_i \partial x_j} (x) - \frac{\partial^2 f}{\partial x_i \partial x_j} (c) }
    \end{align*}
    %
    And this converges to zero since the partials ware continuous, hence the theorem follows.
\end{proof}

\begin{lemma}
    If $H(c)$ is positive definite, there exists a neighborhood around $c$ such that every Hessian in that neighborhood is positive definite.
\end{lemma}
%
\begin{proof}
    Suppose $H(c)$ is positive definite. Then 
    %
    \[ \varepsilon = \min_{z \in S(0,1)} \langle H(c)z, z \rangle > 0 \]
    %
    A simple trick shows that
    %
    \[ \langle H(x)z, z \rangle = [\langle H(x)z, z \rangle - \langle H(c)z, z \rangle] + \langle H(c)z, z \rangle \]
    %
    Applying lemma 1, we find a neighbourhood where the square bracket is between $-\varepsilon$ and $\varepsilon$. Hence $\langle H(x)z, z \rangle > -\varepsilon + \varepsilon = 0$. Since $z$ is arbitrary, $H(x)$ is positive definite.
\end{proof}

Now we may derive the test.

\begin{theorem}
    Suppose $f:\mathbf{R}^n \to \mathbf{R}$ is continuous of second order, and $c$ is a point such that the gradient of $f$ at that point is zero. Then
    %
    \begin{enumerate}
        \item If $H(c)$ is positive definite, $c$ is a local minimum.
        \item If $(H(c)$ is negative definite, $c$ is a local maximum.
        \item If $H(x)$ is indefinite, $c$ is a saddle point.
        \item If $H(c)$ is semidefinite, we may derive no conclusion.
    \end{enumerate}
\end{theorem}
\begin{proof}
    Consider Taylor's formula for $n = 1$ at $c_i$:
    %
    \[ f(x) = f(c) + \sum_{i = 1}^n (x_i - c_i) \frac{\partial f}{\partial x_i}(c) + R(x) \]
    %
    Since the gradient is zero,
    %
    \[ f(x) = f(c) + R(x) \]
    %
    \[ f(x) - f(c) = R(x) = \frac{1}{2!} \sum_{i,j} (x_i - c_i)(x_j - c_j) \frac{\partial^2 f}{\partial x_i \partial x_j} (y) \]
    %
    for some $y$ in $(x,c)$. Carrying on,
    %
    \[ f(x) - f(c) = \frac{1}{2} \langle H(y)v, v \rangle \]
    %
    And we see why our previous discussion is important.
    %
    \begin{enumerate}
        \item If $H(y)$ is positive definite, let $B$ be a positive definite neigbourhood. Hence $\langle H(y)v, v \rangle > 0$, so for each $x$ in $B$, $f(x) > f(b)$.
        \item Apply (1) to $-f$.
        \item Suppose $H(c)$ is indefinite. Consider an orthonormal basis $\{ v_k \}$ with eigenvectors $\{ \lambda_k \}$. Consider $\varepsilon > 0$, such that $|\varepsilon| < |\lambda_{min}|,|\lambda_{max}|$. Let $V$ be an arbitrary neighborhood of $c$, and $B$ a closed ball as in lemma 1. Without loss of generality, we may assume $B$ is a subset of $V$ with a radius $r$. Let $x = c + r v_{min}$. Then
        %
        \[ \|x - c\| = \|r v_{min} \| = r \]
        %
        so that $x$ is in the ball. Then
        %
        \begin{align*}
            f(x) - f(c) &= \frac{1}{2} \langle H(y)r v_{min}, r v_{min}\\
            &= \frac{r^2}{2} \langle H(y) v_{min}, v_{min} \rangle\\
            &= \frac{r^2}{2} [\langle H(y) v_{min}, v_{min} \rangle - \langle H(c) v_{min}, v_{min} \rangle + \langle H(c) v_{min}, v_{min} \rangle]\\
            &< \frac{r^2}{2} (\varepsilon + \lambda_{min}) < 0
        \end{align*}
        %
        for small enough $\varepsilon$. A similar method shows there are points in the ball with a difference greater than zero. Hence the point is a saddle point.
    \end{enumerate}
\end{proof}

In $\mathbf{R}^2$, the Hessian matrix is two by two, and the determinant of this matrix is $\lambda_1 \lambda_2$, where these are the two eigenvalues. If $\lambda_1 \lambda_2 < 0$, they have differing signs, so the Hessian is indefinite. If $\lambda_1 \lambda_2 = 0$, one of the eigenvalues is 0, so the matrix is semidefinite. If $\lambda_1 \lambda_2 > 0$, both have the same sign so the matrix is definite. If we look along some axis at this point, we may determine by the second derivative test in one dimension whether the function is minimal or maximal at the point.

The problem with the above optimality discussion is that the boundary of any set we are working with is not considered. In one dimension, working with an interval $[a,b]$, we need only check two boundary points. On a general closed ball, there are infinitely many points. With nice sets, the boundary forms a curve or a surface, so we need to study finding optimal points on these geometric objects to discover points on the closed surface.

The easiest way is to parameterize the curve or surface. For instance, on the unit circle, we can use the function
%
\[ \varphi(t) = (cos(t), sin(t)) \]
%
Any function on the unit circle then becomes a one dimensional function upon which we can find the maximum and minimum points quite simply. A more interesting method is to use `Lagrange multipliers', If $(a,b)$ is a point on the circle, then $a^2 + b^2 = 1$. Thus the circle is a level curve, the set of all points such that the function $F(x,y) = x^2 + y^2$ has value one. More generally, given a function $f:\mathbf{R}^n \to \mathbf{R}$ and a number $r$, the level curve of $f$ at $r$ is $f^{-1}(r)$, which we denote $L_r$. For our function $F$, $L_1$ is the unit circle, $L_r$ for $r > 0$ is the circle with radius $\sqrt{r}$, and the level set is empty for $r < 0$.

We shall first consider $\mathbf{R}^2$. Let $f$ be a differentiable function from $\mathbf{R}^2$ to $\mathbf{R}$, and consider a point $c$ in $L_r$ for some $r$, where $f$ is smooth on a neighborhood around $c$ in the subspace topology of $L_r$. Assume that we may parametrize $L_r$ in this neighborhood by a differentiable function $\varphi$, such that $\varphi'$ is never zero. We know $f \circ \varphi$ is constant since the mapping is from a level curve, hence $D(f \circ g) = 0$. By the chain rule,
%
\[ D(f \circ \varphi) = \langle \nabla (f \circ \varphi)(x), D\varphi(x) \rangle \]
%
Hence the gradient and derivative of $\varphi$ are perpendicular. This can be done similarly in any dimension (we never really used the properties of $\mathbf{R}^2$. Another thing to notice is that this theorem does not just hold for level curves.

\begin{lemma}
    Suppose $f:\mathbf{R}^n \to \mathbf{R}$ is smooth, and $\varphi$ parametrizes a smooth curve or srface. If $f$ obtains a minimum or maximum on $\varphi$ at a point $c$, then $\nabla f(c)$ is orthogonal to the curve parameterized by $\varphi$.
\end{lemma}
\begin{proof}
    $(f \circ \varphi$ attains a local extremum at $c$, hence
    %
    \[ (f \circ \varphi)' = \langle f(\varphi^{-1}(c), \varphi'(\varphi^{-1}(c))) = 0 \]
    %
    This states exactly that the two values are perpendicular.
\end{proof}

The reason this is true everywhere on a level curve is that every point on the curve is a local minimum or maximum.

We now have the prerequisite knowledge to explain the method of Lagrange multipliers. Let $f$ be a function from $\mathbf{R}^n$ to $\mathbf{R}$, and $\varphi$ a parameterization of a surface or curve in $\mathbf{R}^n$. Let $g$ be a function from $\mathbf{R}^n$ to $\mathbf{R}$ such that $\varphi$ parameterizes $L_r(g)$ for some value $r$. Then $\nabla g(c)$ is orthogonal to $\varphi$ at every point, and if an extrema occurs at a point $c$, then $\nabla f(c)$ is also orthogonal. In $\mathbf{R}^2$, this means $\nabla f(c)$ is parallel to $\nabla g(c)$, so $\nabla f(c) = \lambda \nabla g(c)$ for some value $\lambda$. This can be seen as a system of equations to be solved. This holds in $\mathbf{R}^n$ as well, but the details of verifying this is true is left for a more extensive course in differential geometry.

As an example, let us find the extrema of the function $g(x,y) = x + xy$ on $\mathbf{R}^2$. We may take the level curve of the function $f(x,y) = x^2 + y^2$ at radius one. We have
%
\begin{align*}
    \nabla f(x,y) = (2x\ 2y) && \nabla g(x,y) = (1 + y\ x)
\end{align*}
%
and these must be parallel at any extrema on the circle, so we are solving the system of equations
%
\begin{align*}
    1 + y &= 2 \lambda x\\
    x &= 2 \lambda y
\end{align*}
%
such that $x^2 + y^2 = 1$. Since $y(1 + y) = \lambda 2xy$, and $x^2 = \lambda 2xy$, $y + y^2 = x^2$. Since $x^2 + y^2 = 1$, $y + 2y^2 = 1$. This is a quadratic equation, whose roots are the coordinates $(0, \pm 1)$, and $(1/2, \pm \sqrt{3/2})$. On $(0,\pm 1)$ the values of $g$ are 0, and at $(1/2, \sqrt{3/2})$ the values of $g$ are $1 + \sqrt{3/2}$. These are the extrema we need.





\chapter{Implicit Functions}

We now move onto the implicit function theorem, one of the fundamental results of advanced calculus. A standard definition of the unit circle is the set of points $(x,y)$ such that $x^2 + y^2 = 1$. We will not be able to find a function $f:[-1,1] \to \mathbf{R}$ such that every point on the circle is $(x,f(x))$ for some number $x$, since both $(x, \sqrt{1 - x^2})$ and $(x, -\sqrt{1 - x^2})$ lie on circle for any number $x \in [-1, 1]$. Nonetheless, there are still two differentiable functions on $(-1,1)$ whose graphs form a subset of the circle, namely the maps $x \mapsto \sqrt{1 - x^2}$ and $x \mapsto -\sqrt{1 - x^2}$. This section will attempt to explain when we can find these functions in an arbitrary point set in $\mathbf{R}^n$. An important way we will generalize the function is to notice that for any point $(x,y)$ on the unit circle, where $x \neq \pm 1$, we have found a differentiable function on some open interval $(x - \varepsilon, x + \varepsilon)$ whose range lies on the entire unit circle.

One key observation to make about the definition of the unit circle is that it is defined by the set of points that vanish on some function $f:\mathbf{R}^n \to \mathbf{R}$. In the case of the circle itself, the function is $(x,y) \mapsto x^2 + y^2 - 1$. Provided that the partial derivative $\frac{\partial f}{\partial y} (a,b) \neq 0$ at a point $(a,b)$, we will conclude that we can solve for $y$ at each $x$ in a neighbourhood around $a$. To prove the implicit function theorem in its full generality, we require the inverse function theorem, a way to invert a function around specific points in $\mathbf{R}^n$. Our progression will consist of a series of lemmas that extract $f^{-1}$ from the differentiability of $f$.

\begin{lemma}
    If $f:A \to \mathbf{R}^n$ is continuously differentiable, where $A$ is an open convex set in $\mathbf{R}^n$, and there is some number $M$ such that for any indices $i$, $j$, and for any points $x$ in $A$,
    %
    \[ | \frac{\partial f_i(x)}{\partial x_j}(x) | \leq M \]
    %
    then we may conclude that
    %
    \[ \| f(x) - f(y) \| \leq n^2M \| x - y \| \]
    %
    for all $x$ and $y$ in $A$.
\end{lemma}
\begin{proof}
    Our proof consists of the standard trick of shifting axes one-by-one and applying the mean value theorem.
    %
    \begin{align*}
        | f_i(y) - f_i(x) | &= \big| \sum_{k = 1}^n f_i(x_1, \dots, x_{k-1}, y_k, \dots, y_n) - f_i(x_1, \dots, x_k, y_{k+1}, \dots, y_n) \big|\\
        &\leq \sum_{k = 1}^n \big|f_i(x_1, \dots, x_{k - 1} y_k, \dots, y_n) - f_i(x_1, \dots, x_k, y_{k+1}, \dots, y_n) \big|
    \end{align*}
    %
    Using the mean value theorem, for each $k$, if $y_k \neq x_k$ there is some $z = (x_1, \dots, z_k, y_{k+1}, \dots, y_n)$ where $x_k \leq z_k \leq y_k$ such that
    %
    \[ \frac{f_i(x_1, \dots, y_k, \dots, y_n) - f_i(x_1, \dots, y_{k+1}, \dots, y_n)}{y_k - x_k} = \frac{\partial f_i}{\partial x_k} (z) \]
    %
    and because $z$ is in $A$ by construction (since $A$ is convex),
    %
    \[ | f_i(x_1, \dots, y_k, \dots, y_n) - f_i(x_1, \dots, y_{k+1}, \dots, y_n) | = | y_k - x_k | | \frac{\partial f_i}{\partial x_k} (z) | \leq |y - x| M \]
    %
    Of course, if $y_k = x_k$, then the difference between the $f_i$ values is zero, so is definitely less than or equal to $\|y - x\| M$. Combining all these inequalities together,
    %
    \begin{align*}
    | f_i(y) - f_i(x) | &= \sum_{k = 1}^n f_i(x_1, \dots, y_k, \dots, y_n) - f_i(x_1, \dots, y_{k+1}, \dots, y_n) |\\
    &\leq \sum_{k = 1}^n \| y - x\| M = nM\|y - x\|
    \end{align*}
    %
    It then follows that
    %
    \[ \| f(y) - f(x) \| \leq \sum_{k = 1}^n |f_i(x) - f_i(y)| \leq n^2M\|y - x\| \]
\end{proof}

\begin{corollary}
    If $A$ is open, $f: A \to \mathbf{R}^n$ is continuously differentiable, and $Df(a)$ is invertible at a point $a$, then there is $\alpha > 0$ such that, for any two points $x$ and $y$ in an open neighbourhood of $a$,
    %
    \[ \| f(x) - f(y) \| \geq \alpha \| x - y \| \]
    %
    It follows that $f$ is injective on this neighbourhood, since if $f(x) = f(z)$, $0 = \|f(x) - f(z)\| \geq \alpha \|x - y\|$, so $\|x - y\| = 0$, and thus $x = y$.
\end{corollary}
\begin{proof}
    Since $f$ is continuously differentiable, pick a convex open neighbourhood $U$ containing $a$ such that, for all $x$ in this neighbourhood, and for all indices $i, j$,
    %
    \[ | \frac{\partial f_i}{\partial x_j}(x) - \frac{\partial f_i}{\partial x_j}(a) | < \frac{\|Df(a)\|}{2n^2} \]
    %
    Now take the function $g(x) = f(x) - Df(a)x$. Then
    %
    \[ | \frac{\partial g_i}{\partial x_j}(x) | = | \frac{\partial f_i}{\partial x_j}(x) - \frac{\partial f_i}{\partial x_j}(a) | < \frac{\|Df(a)\|}{2n^2} \]
    %
    Applying the previous lemma,
    %
    \begin{align*}
        \frac{\|Df(a)\|}{2} \|x - y\| &\geq \| g(x) - g(y) \| = \| Df(a)[x - y] - [f(y) - f(x)] \|\\
        &\geq \| Df(a) [x - y]\| - \|f(y) - f(x)\| \\
    &\geq \|Df(a)\|\ \|x - y\| - \|f(x) - f(y)\|
    \end{align*}
    %
    We therefore can conclude that
    %
    \[ \|f(x) - f(y)\| \geq \frac{\|Df(a)\|}{2} \|x - y\| \]
\end{proof}

\begin{lemma}
    If $A$ and $B$ are sets in $\mathbf{R}^n$, with $A$ open, and $f:A \to B$ is a bijective continuously differentiable function such that $Df(x)$ is invertible for all $x$ on $A$, then $B$ is open, and $f^{-1}$ is continuous.
\end{lemma}
\begin{proof}
    Let $b$ be an element of $B$, and let $a = f^{-1}(b)$. Consider some open ball $U$ containing $a$. The boundary of this ball is closed and bounded, hence compact. Since $f$ is one-to-one, $f(\partial U)$ is a compact set disjoint from $b$, so there is some $\varepsilon$ such that every point in $f(\partial U)$ is a distance $\varepsilon$ from $b$. Consider a ball $V$ of radius $\varepsilon/2$ around $b$. This set contains no points in the boundary of $U$. We claim that for each $y \in V$, there is some $x \in U$ such that $f(x) = y$. Consider the function on the closure of $U$, defined by
    %
    \[ g: c \mapsto \|f(c) - y \|^2 \]
    %
    This function is continuous, and therefore obtains a minimum point on the closure of $U$, since this set is compact. Let $x$ be an arbitrary point on the boundary. Then
    %
    \[ \| f(x) - y \| \leq \| f(x) - b \| + \| y - b \| \leq \varepsilon + \varepsilon/2 \]
    %
    and $b$ itself is a distance smaller than this, so $x$ cannot be the minimum value. Therefore, it must be true that $g$ is differentiable at the minimum point $x$, and at this point, the derivative is zero. For each $i$,
    %
    \[ \frac{\partial g}{\partial x_i}(x) = \sum_{k = 1}^n 2(y_k - f_k(x)) \frac{\partial f_k}{\partial x_i}(x) = 0 \]
    %
    Collectively, these equations can be written
    %
    \[ Df(x) \begin{pmatrix} 2(y_1 - f_1(x)) \\ 2(y_2 - f_2(x)) \\ \vdots \\ 2(y_n - f_n(x)) \end{pmatrix} = 2 Df(x) (y - f(x)) = 0 \]
    %
    But $Df(x)$ is invertible by hypothesis, so $y - f(x) = 0$, and thus $f(x) = y$. Since $y$ was arbitrary, we have shown that around every $b$ in $B$ there is a ball contained in $f(A)$, so that $B$ is open, as was desired. But this shows that $f^{-1}$ is continuous, since if $U \subset A$ is open, then we may repeat the above argument again to conclude $f(U)$ is open.
\end{proof}

\begin{lemma}
    If $A$ is an open set in $\mathbf{R}^n$, and $f:A \to B$ is a bijective continuously differentiable function such that $Df(x)$ is invertible for all $x$ in $A$, then $f^{-1}$ is differentiable, and $Df^{-1}(f(x)) = [Df(x)]^{-1}$.
\end{lemma}
\begin{proof}
    Let $y = f(x)$ be a given point in $B$. Let $\mu = Df(x)$. We need show that $Df^{-1}(y) = \mu^{-1}$. For $a$ in $A$, we may write
    %
    \[ f(a) = f(x) + \mu(a - x) + \varphi(a - x) \]
    %
    where $\varphi$ maps a number $a$ to $f(a) - f(x) - \mu(x - a)$. Because $Df(x)$ exists,
    %
    \[ \lim_{a \to x} \frac{\|\varphi(a - x)\|}{\|a - x\|} = 0 \]
    %
    it is also true that
    %
    \[ x - a - \mu^{-1}(f(a) - f(x)) = \mu^{-1}(\varphi(x - a)) \]
    %
    Since every $b$ in $B$ can be written $f(a)$ for some $a$ in $A$, this equation can be written
    %
    \[ f^{-1}(y) - f^{-1}(b) - \mu^{-1}(b - y) = \mu^{-1}(\varphi(f^{-1}(b) - f^{-1}(y))) \]
    %
    and to get the derivative, we need to show
    %
    \[ \lim_{a \to y} \frac{\| \mu^{-1}(\varphi(f^{-1}(b) - f^{-1}(y))) \|}{\|b - y\|} = 0 \]
    %
    equivalently, since $\mu^{-1}$ is a linear transformation, that
    %
    \[ \lim_{b \to y} \frac{\| \varphi(f^{-1}(b) - f^{-1}(y)) \|}{\|b - y\|} = 0 \]
    %
    But now
    %
    \begin{align*}
        \lim_{b \to y} \frac{\| \varphi(f^{-1}(b) - f^{-1}(y)) \|}{\|b - y\|} &= \lim_{b \to y} \frac{\| \varphi(f^{-1}(b) - f^{-1}(y)) \|}{\|f^{-1}(b) - f^{-1}(y) \|} \frac{\|f^{-1}(b) - f^{-1}(y)\|}{\|b - y\|}\\
        &= \lim_{b \to y} \frac{\| \varphi(f^{-1}(b) - f^{-1}(y)) \|}{\|f^{-1}(b) - f^{-1}(y) \|} \lim_{b \to y} \frac{\|f^{-1}(b) - f^{-1}(y)\|}{\|b - y\|}\\
        &= \lim_{a \to x} \frac{\| \varphi(a - x) \|}{\|a - x \|} \lim_{b \to y} \frac{\|f^{-1}(b) - f^{-1}(y)\|}{\|b - y\|}\\
    \end{align*}
    %
    The first limit converges to zero, and the second factor is bounded in some neighbourhood around $b$ by Corollary (2.1). Therefore, the entire function converges to zero, and we have differentiated our function.
\end{proof}

\begin{remark}
    If $f$ is a $C^m$ function in the last proof, then so if $f^{-1}$.
\end{remark}
\begin{proof}
    By the last theorem, we know at each point $y$, $Df^{-1}(y) = [Df(f^{-1}(x))]^{-1}$. We can see then that the derivative operator is the composition of three functions $Df^{-1} = \cdot^{-1} \circ Df \circ f^{-1}$, where $\cdot^{-1}$ inverts a matrix by an inverse function. We'll proceed by induction. Suppose $f$ is $C^1$. Then $Df$ is a continuous operator. Because $f^{-1}$ and $\cdot^{-1}$ are invertible, we conclude $Df^{-1}$ is also $C^1$. Now suppose that $f$ is $C^m$. Then $f^{-1}$ is at least $C^{m-1}$ by induction, and $Df$ is the composition of $C^{m-1}$ functions, and is hence $C^{m-1}$. We conclude therefore that $f$ is $C^m$.
\end{proof}

Now we can prove the inverse function theorem quite simply.

\begin{theorem}[Inverse Function Theorem]
    Suppose $f:A \to \mathbf{R}^{n}$ is a $C^m$ function. If $Df(a)$ is invertible at some point $a$, then there is a neighbourhood $U$ of $A$ such that $f$ is invertible on $U$, and $f^{-1}$ is $C^m$.
\end{theorem}
\begin{proof}
    By Corollary (2.1), there is a neighbourhood around $a$ on which $f$ is one-to-one. Because $\det \circ Df$ is a continuous function, we may also assume that for any $x$ in this neighbourhood $Df(x)$ is invertible. Then the hypothesis of all the above discussion are satisfied, and $f$ is invertible on this neighbourhood, with $f^{-1}$ a $C^m$ function.
\end{proof}

It is interesting to note that the invertibility of the total derivative is a necessary and sufficient condition for a function to be locally diffeomorphic. Surely, a function can be inverted without the determinant of the derivative being equal to zero (take $f(x) = x^3$, at $x = 0$), but the inverse ($f(x) = \sqrt[3]{x}$) can not be differentiated at the point whose derivative is not invertible. Suppose that $Df^{-1}(p)$ did exist at a point $p$ where $\det Df(p) = 0$. Then
%
\begin{align*}
    1 &= \det I = \det D(f^{-1} \circ f)(p) = \det \big[ Df^{-1}(f(p)) \circ Df(p) \big]\\
    &= \det Df^{-1}(f(p))\ \cdot \det Df(p) = 0 \cdotp \det Df^{-1}(f(p)) = 0
\end{align*}
%
By reduction ad absurdum, $Df^{-1}(p)$ cannot exist.

\begin{corollary}[Implicit Function Theorem]
    Suppose $f:\mathbf{R}^n \times \mathbf{R}^m \to \mathbf{R}^m$ is continuously differentiable in a neighbourhood around a point $(a,b)$, and $f(a,b) = 0$. Let $M$ be the $m \times m$ matrix whose entry at the ith row and jth column is
    %
    \[ \frac{\partial f_i}{\partial x_{n + j}} (a,b) \]
    %
    If $\det(M) \neq 0$, there is an open set $A \subset \mathbf{R}^n$ containing $a$ and an open set $B \subset \mathbf{R}^m$ containing $b$ such that for each $x$ in $A$ there is a unique $g(x)$ in $B$ such that $f(x,g(x)) = 0$. The function $g$ is differentiable.
\end{corollary}
\begin{proof}
    Consider the function $F: \mathbf{R}^n \times \mathbf{R}^m \to \mathbf{R}^n \times \mathbf{R}^m$ by the mapping $(a,b) \mapsto (a,f(a,b))$. Clearly $DF(a)$ is invertible, since $M$ is invertible, and 
    %
    \[ DF(a) = \begin{pmatrix} I_n & 0 \\ 0 & M \end{pmatrix}. \]
    %
    By the inverse function theorem, there is a differentiable inverse $F^{-1}$. Clearly $F^{-1}$ maps $(x,y)$ to $(x,k(x,y))$ for some differentiable function $k$. Define a function $\pi$ mapping $(x,y)$ to $y$. Then
    %
    \[ f(x,k(x,y)) = f(F^{-1}(x,y)) = [\pi \circ F \circ F^{-1}] (x,y) = \pi \circ (x,y) = y \]
    %
    Therefore $f(x,k(x,0)) = 0$, and we may define $g(x) = k(x,0)$.

    Let us now show that $g$ is a unique parameterization around $a$. Let $g'$ be another function satisfying $f(x,g'(x)) = 0$. We may as well assume that $g'$ and $g$ are defined on the same neighbourhood since we can always take the intersection, which is non-empty since it contains $a$. We will prove that $g$ and $g'$ are equal on a neighbourhood around $a$. Since $g'$ is continuous. Since $g'$ is continuous, we may also restrict the domain such that the range of $g'$ is contained within the domain of $F^{-1}$. Then
    %
    \[ F(x,g'(x)) = (x,0) \]
    %
    so
    %
    \[ (x,g'(x)) = F^{-1}(x,0) = (x,k(x,0)) \]
    %
    and thus $g'(x) = k(x,0) = g'(x)$.
\end{proof}

Our proof tells us that $g$ is differentiable, but not how to find the value of the derivative. Fortunately, this is not too hard to calculate. Since $f_i(x,g(x)) = 0$ for all values $x$, by defining $h: x \mapsto (x,g(x))$, we obtain, by taking derivatives of both sides of the equation, for each $i$,
%
\[ \frac{\partial (f_i \circ h)}{\partial x_j} (x) = 0 \]
%
and by the chain rule for partial derivatives,
%
\[ \frac{\partial (f_i \circ h)}{\partial x_j} (x) = \frac{\partial f_i}{\partial x_j}(x,g(x)) + \sum_{k = 1}^{m} \frac{\partial f_i}{\partial x_{n + k}}(x,g(x)) \frac{\partial g_k}{\partial x_j} (x) = 0 \]
%
This equation looks a lot scary than its one dimensional version, where $g: \mathbf{R} \to \mathbf{R}$, By writing $g(x) = y$ and $g'$ as $\frac{dy}{dx}$,
%
\[ (f \circ h)'(x) = \frac{\partial f}{\partial x}(x,y) + \frac{\partial f}{\partial y}(x,y) \frac{dy}{dx}(x) \]
%
Taking $f(x,y) = x^2 + y^2$,
%
\[ (f \circ h)'(x) = 2x + 2y \frac{dy}{dx}(x) = 0 \]
%
so that $\frac{dy}{dx}(x) = -x/y$. This is just the method of `implicit differentiation' which you would have learned about in a basic course of one dimensional calculus. The idea of implicit differentation is completely rigorous, but without the implicit function theorem to guarentee that there exists some differentiable function $g$ that satisfies an equation, we cannot with implicit differentation guarentee that there exists a function that satisfies an equation. The implicit function theorem tells us that our intuition is right, and that there does exist a function of this character.





\chapter{Areas under Functions}

The elementary theory of integration is so similar to that of one-dimensional integration that we shall brush over it fairly fast. First, we require some of the theory of nets, that generalization of sequences from which convergence of partition rests. Recall that a preordered set is a set $A$ together with a relation $\leq$ that is reflexive and transitive. $A$ is directed upwards if, given two elements $x$ and $y$ in $A$, there is an element $z$ in $A$ such that $x \leq z$, $y \leq z$. A net is a function $f:A \to B$, where $B$ is an arbitrary set, and $A$ is a partially ordered set directed upwards. If $U$ is a subset of $B$, then $f$ is eventually in $U$ if there is $a \in A$ such that, for all $x \geq a$, $f(a) \in U$. If $B$ is a topological space, we say that $f$ converges to a point $b \in B$ if $f$ is eventually in every neighbourhood $U$ of $b$. Of course, in the topological spaces we want to talk about ($\mathbf{R}^n$), we need only show this for open balls whose center is $b$.

Elementary properties of sequences to $\mathbf{R}^n$ carry naturally onto nets. For instance, that if $f$ converges to $a$, and $g$ converges to $b$, then $f + g$ converges to $a + b$, and that a bounded monotone net converges to its supremum. The proofs of these, and further statements, are so similar to the proofs in the language of sequences that we leave all of them for the reader to prove.

With nets in our hand, we are ready to capture the idea of integration. Recall that a partition of an interval $[a,b]$ is an increasing sequence $P = t_0, t_1, \dots, t_n$, with $t_0 = a$, and $t_n = b$. A partition $P$ is finer than a partition $P'$, written $P \geq P'$, if $P'$ is a subsequence of $P$. A partition of a rectangle $A = A[a_1, b_1] \times [a_2, b_2] \times \dots \times [a_n \times b_n]$ is a sequence of partitions $P = P_1, \dots, P_n$, where $P_i$ is a partition of $[a_i, b_i]$. As in the one-dimensional case, we say a partition $Q = Q_1, \dots, Q_n$ is finer that a partition $P = P_1, \dots, P_n$ on this rectangle if $Q_i$ is finer than $P_i$. This defines a preorder on the set of partitions on the rectangle, where $Q \geq P$ if $Q$ is finer than $P$.

Now suppose we have a bounded function $f:A \to \mathbf{R}$. We define two nets from the set of partitions of the rectangle in $\mathbf{R}^n$ to $\mathbf{R}$. The lower sum $L(P)$ is defined
%
\[ L(P) = \sum_{R \in P} \inf_{x \in R} f(x) v(R) \]
%
\[ U(P) = \sum_{R \in P} \sup_{x \in R} f(x) v(R) \]
%
It is simple to show that $L$ and $U$ are bounded monotonic functions such that $L(P) \leq U(P)$ for all $P$, and hence $L$ and $U$ are convergent nets. We call the converged values of $L$ and $U$ the lower integral and upper integrals of $f$ on $R$, denoted
%
\begin{align*}
    \mathbf{L} \int_R f && \mathbf{U} \int_R f
\end{align*}
%
If $\mathbf{L} \int_R f = \mathbf{U} \int_R f$, then we say $f$ is integrable on $R$, and we denoted the value of the integral to be simply
%
\[ \int_R f \]
%
An equivalent condition for establishing integrability is to show that for any $\varepsilon$, there are two partitions $P$ and $P'$ such that $U(P') - L(P) < \varepsilon$.

We are almost ready for a study of integration. First we must discuss a few more prerequisites. Suppose $\varphi$ is a function frmo $\mathbf{R}^n$ to $\mathbf{R}^n$, whose derivative at every point is invertible. The inverse function theorem guarentees that $\varphi$ has a differentiable inverse function everywhere. We define the Jacobian determinant of $\varphi$ at $x$, denoted $\mathcal{J}_\varphi(x)$, to be the absolute value of the determinant of $D\varphi(x)$. This value tells us how much space is stretched by $\varphi$.

Consider an affine transformation $v \mapsto Av + b$. The Jacobian determinant here tells us the area of the parallelogram stretched from the unit circle. The transformation into polar coordinates tells us the area of the stretch in all directions. Here, the jacobian is non constant.

The reason we discuss this is to discuss a change of variables in integration, which may simplify a formula and make it easy for us to determine. The Jacobian of the change will tell us how to stretch and squash the integral to keep the area constant.

We require some elementary differential geometry as well for out study. A differential curve is a subset of $\mathbf{R}^n$ such that every neighbourhood around a point can be locally parametrized by a function mapped from an interval $(a,b)$ whose derivative is never zero. Similarily, a differentiable surface is a function locally parametrized by a function from an open set in $\mathbf{R}^2$ whose gradient is never zero. Even more generally, a differentiable manifold of dimension $k$ is a subset of $\mathbf{R}^n$ which locally behaves like $\mathbf{R}^k$ locally.










\chapter{Integration of Vector Fields}

In one dimension, we have three interpretations of the integral of a function on the line, which we might summarize via three slightly different notations we use for the one-dimensional integral:
%
\begin{align*}
    \underbrace{\int f}_{(1)} && \underbrace{{\int_{[a,b]} f}}_{(2)} && \underbrace{\int_a^b f}_{(3)}
\end{align*}
%
\begin{enumerate}
    \item The antiderivative of a function $f$.
    \item The area under the curve defined by $f$ on the interval from $a$ to $b$.
    \item The work required to push a particle from $a$ to $b$ against a force $f$.
\end{enumerate}
%
There really is only so many degrees of freedom in one dimension, so these three ideas naturally correspond. But they diverge when studying higher dimensional variants of problems, which we might describe via three different areas of analysis:
%
\begin{enumerate}
    \item The Study of Solutions to Differential Equations.
    \item Measure Theory.
    \item Vector Integration.
\end{enumerate}
%
In this chapter we shall discuss the basic aspects of (3), which has been incredibly important in the evolution of modern differential geometry, as well as the study of physics, especially the theory of electromagnetism. The main difficulties will be explaining complicated physical concepts with rigorous mathematical precision.





\section{Scalar Integrals on Curves and Surfaces}

We begin with a basic problem. Given a curve $c$ in $\RR^n$ (an image of a continuous map from an interval in $\RR$), how do we measure it's length. Whatever definition we make should tell us that the length of the straight line from a point $x$ to a point $y$ is $|x - y|$. In general, we might estimate the length of a curve by approximating it by a piecewise linear curve. Given a parameterization $c: [a,b] \to \RR^n$ of a curve, let us take a partition $P = \{ t_0, t_1, \dots, t_n \}$ of $[a,b]$, where $t_0 = a$ and $t_n = b$, and define
%
\[ \Lambda(c,P) = \sum_{k = 1}^n |c(t_k) - c(t_{k-1})|. \]
%
Then $\Lambda$ is an \emph{increasing net} on the space of all partitions (the triangle inequality tells us $\Lambda$ increases as partitions are refined). And thus the net has a limit, which we define to be the \emph{length} of the curve $x$.

% LINEAR ESTIMATION DIAGRAM HERE

Seems simple, but as is often the case in the analysis of general functions, we can conjure up some pathological examples of curves that do not behave nicely. Continuity is a mysterious beast, and there are many examples of continuous curves that contradict intuition. We can, in fact, construct a continuous curve $c: [0,1] \to [0,1]^2$ that is \emph{surjective}; we can fill up the entire unit square with a bounded curve. The idea of constructing this horrific curve begins by drawing a simple `jag' that covers very little of the square. By coiling the curve up recursively, we may take a curve `in the limit' which covers the entire space. Our definition will assign this curve an `infinite length', and so here we will only focus on `rectifiable curves', i.e. those curves with finite length. In particular, any piecewise $C^1$ curve will have finite length.

% DRAW PEANO CURVE DIAGRAMS HERE

Using the basic definition of arc length is too daunting for real life work. Fortunately, a theorem will simplify our discussion to the most basic of cases. Notationally, it will simplify to define the derivative of a vector valued function. If $f: \RR \to \RR^n$, then we define
%
\[ \int_a^b f = \left( \int_a^b f_1, \int_a^b f_2, \dots, \int_a^b f_n \right) \]
%
First, a lemma:
%
\begin{lemma}
    If $f:\RR \to \RR^n$ is a function, then
    %
    \[ \left| \int_a^b f \right| \leq \int_a^b |f|. \]
\end{lemma}
%
\begin{proof}
  Assume $\int_a^b f \neq 0$, the result being trivial otherwise. Let $c_k = \int_a^b f_k$, and $c = (c_1, c_2, \dots, c_n)$. Then
    %
    \begin{align*}
        \left| \int_a^b f \right|^2 &= \sum_{k = 1}^n \left( \int_a^b f_k\; dx \right)^2\\
        &= \int_a^b \sum_{k = 1}^n c_k f_k\\
        &= \int_a^b c \cdot f\; dx \leq |c| \int_a^b |f|\; dx\\
        &= \left| \int_a^b f \right|\; \int_a^b |f|.
    \end{align*}
    %
    We then simply divide both sides by a factor.
\end{proof}

\begin{theorem}
    Consider a curve parameterizable by a continuously differentiable function $c$ whose domain is an interval $[a,b]$. Then
    %
    \[ \Lambda(c) = \int_a^b |c'| \]
    %
    Thus every $C^1$ curve is rectifiable.
\end{theorem}
\begin{proof}
  For any numbers $t_0$ and $t_1$
    %
    \[ |c(t_1) - c(t_0)| = \left| \int_{t_0}^{t_1} c' \right| \leq \int_{t_0}^{t_1} |c'| \]
    %
    This inequality implies that $\Lambda(c) \leq \int_a^b |c'|$. Conversely, since $c'$ is continuous on $[a,b]$, it is uniformly continuous. Thus for each $\varepsilon$ there is $\delta$ such that if $| t_1 - t_0 | < \delta$, then $|c'(t_1) - c'(t_0)| < \varepsilon$. Let $P = \{ t_0, \dots, t_n \}$ be a partition small enough that each segment is smaller than $\delta$. Then it follows that if $t_k \leq t \leq t_{k+1}$, then $|c'(t)| \leq |c'(t_k)| + \varepsilon$. If we let $\delta_k = t_k - t_{k-1}$, then
    %
    \begin{align*}
    \int_{t_{k-1}}^{t_k} |c'| &\leq \delta_k |c'(t_k)| + \delta_k \varepsilon\\
    &= \bigg| \int_{t_{k-1}}^{t_k} c'(t_k) \bigg| + \delta_k \varepsilon\\
    &= \bigg| \int_{t_{k-1}}^{t_k} c'(t_k) + c'(t) - c'(t)\ dt \bigg| + \varepsilon\\
    &\leq \bigg| \int_{t_{k-1}}^{t_k} c'(t)\ dt \bigg| + \bigg| \int_{t_{k-1}}^{t_k} c'(t_k) - c'(t)\ dt \bigg| + \delta_k \varepsilon\\
    &\leq | c(t_k) - c(t_{k-1}) | + 2 \delta_k \varepsilon
    \end{align*}
    %
    Summing up the telescoping error terms, we conclude that
    %
    \begin{align*}
      \int_a^b |c'| &= \sum_{k = 1}^n \int_{t_{k-1}}^{t_k} |c'|\\
      &\leq \sum_{k = 1}^n |c(t_k) - c(t_{k-1})| + 2 \delta_k \varepsilon\\
      &= \Lambda(c,P) + 2\varepsilon (b - a)\\
      &\leq \Lambda(c) + 2 \varepsilon (b - a)
    \end{align*}
    %
    Letting $\varepsilon$ tend to zero gives
    %
    \[ \int_a^b |c'| \leq \Lambda(c), \]
    %
    which completes the proof.
\end{proof}

A similar argument works more generally to determine a \emph{scalar integral}
%
\[ \int_c f \]
%
of a curve $c$ between two points $a$ and $b$. The value of this integral is precisely
%
\[ \int_a^b (f \circ c) |c'|. \]
%
The length of a curve is then precisely $\int_c 1$.

\begin{example}
  Let us compute the arclength of the archimidean spiral, the curve described in polar coordinates by the equation $r = \theta$. Via our method, we take the parameterization
%
\[ x(t) = t(\cos(t), \sin(t)) \]
%
The derivative at some point $t$ is
%
\[ x'(t) = (\cos(t) - t\sin(t), \sin(t) + t\cos(t)) \]
%
\[ |x'| = \sqrt{(\cos(t) - t\sin(t))^2 + (\sin(t) + t\cos(t))^2} = \sqrt{t^2 + 1} \]
%
Suppose we want to find the arclength of the subset of the curve bounded in some ball of radius $r$. This is just the curve parameterized by $x$ on $[0,r]$. The length is therefore
%
\begin{align*}
    \int_x ds &= \int_0^r \|x'\| = \int_0^r \sqrt{t^2 + 1}\ dt\\
    &= \left. \frac{t\sqrt{1 + t^2} + \ln(t + \sqrt{1 + t^2})}{2} \right|_0^r\\
    &= \frac{r\sqrt{1 + r^2} + \ln(r + \sqrt{1 + r^2})}{2}
\end{align*}
%
For large $r$, this quantity is $r^2/2 + O(r)$, which is intuitive given that the archimidean spiral travels around the origin roughly $r + O(1)$ times, and more than half of these orbits is at a radius $\geq r/2$.
\end{example}

\begin{example}
You may have already learnt to calculate the length of the graph $\Gamma(f)$ of a differentiable function $f:[a,b] \to \RR$ via the formula
%
\[ \Lambda(\Gamma(f)) = \int_a^b \sqrt{1 + (f')^2}. \]
%
This is precisely the formula we get via the definition we have given here. Taking the parameterization $x(t) = (t,f(t))$ on $[a,b]$, our arclength is
%
\[ \Lambda(x) = \int_a^b \|x'\| = \int_a^b \sqrt{1 + (f')^2} \]
%
One may show the equivalence between these definitions via infinitisimals as follows:
%
\[ \int_a^b \sqrt{1 + \bigg(\frac{dy}{dx}\bigg)^2} = \int_a^b \sqrt{dx^2 + dy^2} = \int_a^b \sqrt{\bigg(\frac{dx}{dt}\bigg)^2 + \bigg(\frac{dy}{dt}\bigg)^2}\ dt \]
%
However, it is difficult to interpret this pnuemonic rigorously.
\end{example}

Our next specialized integral is restricted to $\mathbf{R}^3$. Suppose we want to calculate the surface area of a surface $\Sigma$ parameterized by a function $s: \Omega \to \RR^3$, where $\Omega \subset \RR^2$. We could begin defining the surface area in terms of linear approximations as above, and then simplify to a formula via derivatives. We provide the gestures of how to do this here. If $z = (x,y)$, then the area of the part of $\Sigma$ given by the image of a small box $I = [x + \delta_x] \times [y + \delta_y]$ should be approximated by the area of the rectangle with vertices
%
\[ \{ s(z), s(z) + (\partial_x s)(z) \delta_x, s(z) + (\partial_y s)(z) \delta_y, s(z) + (\partial_x s)(z) \delta_x + (\partial_y s)(z) \delta_y \}, \]
%
and this area is precisely
%
\[ \delta_x \delta_y \det( \partial_x s(z), \partial_y s(z) ) = \delta_x \delta_y |\partial_x s(z) \times \partial_y s(z)|. \]
%
Breaking things down into infinitisimals, we are thus lead to believe that the area of $\Sigma$ is
%
\[ \int_{\Omega} \left| \frac{\partial s}{\partial x} \times \frac{\partial s}{\partial y} \right|, \]
%
and we take this as the definition of the area. When we weight the surface area integral by a scalar factor $f$, we obtain the scalar surface integral
%
\[ \int_\Sigma f\; dA = \int_{\Omega} (f \circ s) \left| \frac{\partial s}{\partial x} \times \frac{\partial s}{\partial y} \right|. \]
%
Of course, the surface area of $\Sigma$ is then $\int_\Sigma dA$.

\begin{example}
  What is the surface area of the sphere of radius $r$? We have a parameterization
%
\[ f(\theta, \phi) = (r\sin(\phi)\cos(\theta), r\sin(\phi)\sin(\theta), r\cos(\phi)) \]
%
Where $0 \leq \theta \leq 2\pi$, and $0 \leq \phi \leq \pi$. The partial derivatives are
%
\[ \frac{\partial f}{\partial \theta} = r\sin(\phi)(-\sin(\theta), \cos(\theta), 0) \]
%
\[ \frac{\partial f}{\partial \phi} = r(\cos(\phi)\cos(\theta), \cos(\phi)\sin(\theta), -\sin(\phi)) \]
%
\[ \frac{\partial f}{\partial \theta} \times \frac{\partial f}{\partial \phi} = -r^2 \sin(\phi)(\sin(\phi)\cos(\theta), \sin(\phi)\sin(\theta), \cos(\phi)) \]
%
\[ \bigg\|\frac{\partial f}{\partial \theta} \times \frac{\partial f}{\partial \phi} \bigg\| = r^2 |\sin(\phi)| \]
%
And thus the surface area is
%
\begin{align*}
    \int_0^{2\pi} \int_0^\pi \bigg\|\frac{\partial f}{\partial \theta} \times \frac{\partial f}{\partial \phi} \bigg\|\ d\phi\ d\theta &= \int_0^{2\pi} \int_0^\pi r^2 |sin(\phi)|\ d\phi\ d\theta = 2\pi r^2 \int_0^\pi sin(\phi)\\
    &= 2\pi r^2 [\cos(0) - \cos(\pi)] = 2\pi r^2 [1 + 1] = 4\pi r^2
\end{align*}
%
Surface areas of other differentiable shapes can be calculated in a similar fashion.
\end{example}

The integrals here are still really just ways to talk about averages of quantities on surfaces. You can do all we do here using the machinery of measure theory, i.e. by definining measures on curves and surfaces that allow us to interpret the integrals as Lebesgue integrals. In the next section we'll talk about integrals of vector fields, which allow us to define genuinely different quantities.

\section{Vector Field Integrals}

The most natural explanation of a push and pull over a curve and surface is to identify the map $v: \RR^n \to \RR^n$. Physically, we interpret the field as given a family of `pushes' and `pulls' that live in some at each point. If a particle lies at a point $x$, a force $v(x)$ will act on it.

Now suppose we have a particle moving through space in a curve parameterized by $x:[a,b] \to \mathbf{R}^n$ against a vector field $F$. We want to determine the work required to move this particle through space. If this particle is moving in a straight line from $x(a)$ to $x(b)$ against a constant force $F$, then the work required to move the particle is simply $F \cdot (x(b) - x(a))$. As with calculating arclength, we can use this formula to obtain a more general definition for more complicated curves. It will simply be
%
\[ \int_a^b F \cdot x'. \]
%
We denote this quantity by
%
\[ \int_x F \cdot dx = \int_x F_1\; dx + F_2\; dx_2 + \dots + F_n\; dx_n. \]
%
Implicit to our discussion here is the concept of \emph{orientation}. On its own, a geometric curve has no sense of direction; Nonetheless, when we assign some parameterization to the curve we introduce a sense of direction. If we parameterize the curve in the other direction, we will obtain the opposite sign. Whereas before we had to push against the force, in the other direction we allow the force to push us forward, so energy gained is lost in the other direction, and vice versa. To compute a line integral on a geometric curve, we require an orientation to the shape -- an implicit direction we are meant to follow around the curve. We will only integrate over `oriented' parameterizations $x:[a,b] \to \RR^n$ -- curves that go in the positive direction. Without some complex notions of differential geometry, this technical detail is hard to describe, so we appeal to intuition: When you parameterize your curve, make sure it goes in the `right' direction!

The line integral allows us to calculate the force that is being pushed against an object as it moves along a curve from one point to another. The corresponding vector field for surfaces should be the amount of force pushed through the area of the surface. We must integrate over `the value of the push' at each point. One manner of deriving the formula for this vector surface integral begins (like many constructions in this chapter) by analysing a simple case.

If a force at a point is completely tangential to the surface at a point, it does not push anything through the surface at that point; the force just glances off. Since we can split any force into its separate components, we can separate the tangential parts of the force to obtain only the component normal to the surface. If we choose some normal vector $n(x)$ at every point $x$ on the surface, which faces outward from the surface (picking a particular orientation in order to do so), the value of the force pushing against the object will have as a value $\langle F(x), n(x) \rangle$. We then obtain a scalar surface integral. If $S$ is the surface, our integral will be
%
\[ \int_S \langle F, n \rangle\ dA \]
%
If we parameterize our surface by a function $S:D \to \mathbf{R}^3$ such that $\| \frac{\partial S}{\partial x} \times \frac{\partial S}{\partial y} \|$ faces the same direction as $n(x)$ at each point (so that the parameterization is oriented correctly), then
%
\[ n(x) = \frac{\frac{\partial S}{\partial x} \times \frac{\partial S}{\partial y}}{\| \frac{\partial S}{\partial x} \times \frac{\partial S}{\partial y} \|} \]
%
so then
%
\begin{align*}
    \int_S \langle F, n \rangle\ dA &= \int_D \langle F \circ S, n \circ S \rangle \bigg\| \frac{\partial S}{\partial x} \times \frac{\partial S}{\partial y} \bigg\| = \int_D \left\langle F \circ S, \frac{\frac{\partial S}{\partial x} \times \frac{\partial S}{\partial y}}{\| \frac{\partial S}{\partial x} \times \frac{\partial S}{\partial y} \|} \right\rangle \bigg\| \frac{\partial S}{\partial x} \times \frac{\partial S}{\partial y} \bigg\|\\
    &= \int_D \left\langle F \circ S, \frac{\partial S}{\partial x} \times \frac{\partial S}{\partial y} \right\rangle
\end{align*}
%
We have therefore justified defining the surface integral to be
%
\[ \int_S \langle F, n \rangle\ dA = \int_D \left\langle F \circ S, \frac{\partial S}{\partial x} \times \frac{\partial S}{\partial y} \right\rangle \]
%
In physics one can use the integral to calculate the flux of electricity through a surface, the force of a sail from the wind, or the light intensity upon a photographic lens. In generality, the surface integral describes elegantly any kind of `joint force on a body', whose locality cannot be modelled by a single point particle.

When we defined the line integral, we implicitly assumed we were measuring the force pushed against a particle moving along the curve. But how would we calculate, instead the force being pushed through the curve , as with the surface integral? The trick to doing this is as follows. If $x' = (a,b)$ is the derivative of a paramerization of the curve, then the normal vector to the curve is in the direction $(-b,a)$, and we want to calculate the inner product of the force against this vector. If our force is $F = (P,Q)$, we therefore want to integrate
%
\[ \int_x -P\ dx + Q\ dy \]
%
and the result will turn out as we need.

Some may be disheartened by the lack of theorems in this chapter, but the second purpose of mathematics is not only to discover new profound truths, but also to be the great explainer. Now that we have explained vector integrals, we can begin to discover some interesting theorems that deal with calculating them.





\section{Flux}

Imagine that you have a continuous medium in $\RR^n$, whose density $\rho$ fluctuates over time, such that the portion of the medium existing at a point $x \in \RR^n$ moves through space at a velocity $v(x)$. We call this vector field the \emph{flux density}. Given an oriented hypersurface $\Sigma$ in $\RR^n$, the \emph{flux} is the rate at which this medium passes `out of' $\Sigma$ (where `outwardness' is defined by the orientation of the surface). This is denoted by
%
\[ \int_\Sigma \rho [v \cdot n]\; d\sigma. \]
%
It is the primary example of a \emph{surface integral of a vector field}. The density $\rho$ is a scalar-valued function on $\RR^n$, $v$ is a vector field, and $n$ is a vector field on $\Sigma$ giving the \emph{outward pointing} normal vector to the surface. Thus the integrand above is actually \emph{scalar valued} on $\Sigma$, and thus can be defined by previous methods. But lets see intuitively why this scalar integral gives the flux.

Let us proceed as in the case of curves, and approximate a general hypersurface into a tiling of polygons. Given a small polygon $\pi_{x,\delta}$ given by the convex hull of
%
\[ \{ x, x + \delta w_1, \dots, x + \delta w_{n-1} \}, \]
%
where $w_1,\dots,w_{n-1}$ are orthogonal to one another, then in a time interval of length $t$, the magnitude of the medium passing through $\pi_{x,\delta}$ should be proportional to $\rho(x)$, multiplied by the volume of the region formed from the convex hull of the points
%
\[ \{ x, x + \delta w_1, \dots, x + \delta w_{n-1}, x + t v(x) \}. \]
%
But this quantity is equal to
%
\[ t \delta^{n-1} \rho(x) |\det(w_1,\dots,w_{n-1}, v(x))|, \]
%
and taking $t \to 0$, we see that the rate of flow through the polyhedron is precisely
%
\[ \delta^{n-1} \rho(x) |\det(w_1,\dots,w_{n-1},v(x))|. \]
%
If $n(x)$ is a unit vector orthogonal to $w_1,\dots,w_{n-1}$, then we can write this quantity as
%
\[ \delta^{n-1} \rho(x) |v(x) \cdot n(x)|. \]
%
If $n(x)$ is the \emph{outward} pointing unit vector, then the flow should be positive if $v(x)$ is pointing outward, i.e. if $n(x) \cdot v(x)$ is positive. Conversely, the flow should be negative if $v(x)$ is pointing inward, if $n(x) \cdot v(x)$ is negative. But this means that the `infinitisimal flow' should be
%
\[ \delta^{n-1} \rho(x) v(x) \cdot n(x). \]
%
If we break a surface into a union of different polyhedrons, apply this method, sum up, and then take limits as $\delta \to 0$, we see we get precisely the scalar integral on a surface we described above.






\section{The Fundamental Theorems of Calculus}

In one dimension, the fundamental theorem of calculus tells us that
%
\[ \int_a^b f'(x)\ dx = f(b) - f(a) \]
%
Integrating the derivative of a function over a segment is the same as integrating the function over the `boundary' of this segment -- in the one dimensional case, this consists of two points: the start and end of an interval.

This chapter will be concerned with a collection of theorems that generalize the fundamental theorem of calculus to higher integrals. In general, our theorems will be of the form
%
\[ \int_M d\omega = \int_{\partial M} \omega \]
%
To integrate the `derivative' of a function over some shape, it suffices to integrate the function itself over the boundary. The fundamental theorem of calculus is just one of a very important set of theorems of this variety. Of course, half of the battle is defining what exactly it means to differentiate over a vector field.

The first theorem to generalize this notion is little more than an extension of the fundamental theorem of calculus to integration of higher dimensional curves.

\begin{theorem}[Green's Theorem]
    If $M \subseteq \mathbf{R}^2$ is a region whose boundary is a $C^2$ curve $\partial M$ in $\mathbf{R}^2$, and $\alpha$ and $\beta$ are $C^1$, then
    %
    \[ \int_{\partial M} \alpha\ dx + \beta\ dy = \int_M \frac{\partial \beta}{\partial x} - \frac{\partial \alpha}{\partial y} \]
\end{theorem}
\begin{proof}
    We shall prove the theorem assuming $\alpha = 0$. The proof assuming $\beta = 0$ is similar, and together they prove the theorem in general. For now, assume $M = [0,1]^2$. Then, by the Newton Leibnitz formula,
    %
    \[ \int_0^1 \int_0^1 \frac{\partial \beta}{\partial x}(x,y)\ dx\ dy = \int_0^1 \beta(1,y) - \beta(0,y)\ dy \]
    %
    On the other side, we may define a boundary of the square as follows
    %
    \begin{align*}
        A(t) = (t,0) && B(t) = (1,t) && C(t) = (t,1) && D(t) = (0,t)
    \end{align*}
    %
    these parameterize the boundary of the square, and
    %
    \[ \int_{\partial [0,1]^2} \beta\ dy = \int_A \beta\ dy + \int_B \beta\ dy - \int_C \beta\ dy - \int_D \beta\ dy = \int_B \beta\ dy - \int_D \beta\ dy \]
    %
    Since the other values cancel out. The right side of the equation then continues,
    %
    \[ \int_0^1 (\beta \circ B)\ B_2' - (\beta \circ D)\ D_2' = \int_0^1 \beta(1,y) - \beta(0,y)\ dy \]
    %
    And the two sides are equal, so we have proved the theorem for the square. We shall now also assume we have proved the theorem for $\alpha$ as well (left as an exercise).

    Now consider an arbitrary shape $M$. We assume that $M$ is a shape parameterized by a function $f$ from $[0,1]^2$ (oriented from left to right, bottom to top). By a change of variables,
    %
    \begin{align*}
        \int_M \frac{\partial \beta}{\partial x} &= \int_{[0,1]^2} \bigg( \frac{\partial \beta}{\partial x} \circ f \bigg) \bigg( \frac{\partial f_1}{\partial x} \frac{\partial f_2}{\partial y} - \frac{\partial f_1}{\partial y} \frac{\partial f_2}{\partial x} \bigg)\\
    \end{align*}
    %
    By definition of the line integral,
%
\begin{align*}
\int_{\partial M} \beta\ dy &= \int_{\partial [0,1]^2} (\beta \circ f) \frac{\partial f_2}{\partial x}\ dx + (\beta \circ f) \frac{\partial f_2}{\partial y}\ dy\\
&= \int_{[0,1]^2} (\beta \circ f) \left[\frac{\partial^2 f_2}{\partial x  \partial y} - \frac{\partial^2 f_2}{\partial y \partial x}\right] + \left( \frac{\partial \beta}{\partial x} \circ f \right) \left[ \frac{\partial f_1}{\partial x} \frac{\partial f_2}{\partial y} - \frac{\partial f_1}{\partial y} \frac{\partial f_2}{\partial x}\right]\\
&+ \left( \frac{\partial \beta}{\partial y} \circ f \right) \left[ \frac{\partial f_2}{\partial x} \frac{\partial f_2}{\partial y} - \frac{\partial f_2}{\partial y} \frac{\partial f_2}{\partial x} \right]\\
&= \int_{[0,1]^2} \left( \frac{\partial \beta}{\partial x} \circ f \right) \left[ \frac{\partial f_1}{\partial x} \frac{\partial f_2}{\partial y} - \frac{\partial f_1}{\partial y} \frac{\partial f_2}{\partial x}\right]
\end{align*}
%
    We thus have extrapolated from the simple case of the square to a general parameterizable region in $\mathbf{R}^2$. In general, if your region can be broken into finitely many piecewise parameterizable `squarish' regions, this proof will also show that green's theorem holds, since interior edges will cancel each other out.
\end{proof}

One way to visualize this theorem is as follows. Suppose we are integrating a force through a curve $\partial M$. Then the value of the integral (if $F = (P,Q)$), is, using green's theorem
%
\[ \int_{\partial M} -P\ dx + Q\ dy = \int_M \frac{\partial Q}{\partial x} + \frac{\partial P}{\partial y} \]
%
This formula measures how fast the force leaves the region $M$. If we see $M$ as a body, then saying that
%dA
\[ \frac{\partial Q}{\partial x} + \frac{\partial P}{\partial y} = 0 \]
%
states that matter is incompressible. Of course, if this is true, then the force cannot push through the body at any point! The boundary integral must be zero.

While we're talking about integrating along normals, we should consider the next fundamental theorem, which describes integrating over a surface. First, we need to define a operation that converts a vector field to another vector field. Given some field $F$, we want to determine the measure of how much the field `spins' at a point. The next theorem will justify why we define the `spin' of the field, called the curl, and defined
%
\[ (\nabla \times F) = \left(\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y}, \frac{\partial F_3}{\partial y} - \frac{\partial F_2}{\partial z}, \frac{\partial F_1}{\partial z} - \frac{\partial F_3}{\partial x}\right) \]
%
If we consider the field $F(x) = -\frac{x}{\|x\|}$ (a central field), then $\nabla \times F = 0$ at all points; the force doesn't spin at all -- it extends straight outward in every direction.

\begin{theorem}[Stoke's Theorem]
    If $F$ is $C^1$, and $M$ is a `nice' surface, then
    %
    \[ \int_M \langle \nabla \times F, n \rangle\ dA = \int_{\partial M} \langle F, dx \rangle \]
\end{theorem}
\begin{proof}
    We will reduce Stoke's theorem to Green's theorem. First we shall convert the 3D line integral to a 2 dimensional one. We will assume $M$ be parameterizable by a map $s$ from $[0,1]^2$. Then
    %
    \[ \int_{\partial M} \langle F, dx \rangle = \int_{\partial [0,1]^2} \langle F \circ s, \frac{\partial s}{\partial x} \rangle\ dx + \langle F \circ s, \frac{\partial s}{\partial y} \rangle\ dy \]
    %
    Now take the integral of the curl,
    %
    \[ \int_M \langle \nabla \times F, n \rangle\ dA = \int_{[0,1]^2} \langle \nabla \times F \circ s, \frac{\partial s}{\partial x} \times \frac{\partial s}{\partial y} \rangle \]
    %
    If you extend these computations a bit (a trivial exercise, of course), we notice that applying Green's theorem to the line integral equation, results in the right hand side of the last equation. Thus Stoke's theorem results quite naturally.
\end{proof}

It is nice to see that while Stoke's theorem is a small extension of Green's theorem, Green's theorem is also a special case of Stoke's theorem. Stoke's theorem says
%
\[ \int_{\partial M} \langle F, dx \rangle = \int_M \langle \nabla \times F, n \rangle\ dA \]
%
Green's theorem says
%
\[ \int_{\partial M} A\ dy + B\ dx = \int_M \frac{\partial B}{\partial y} - \frac{\partial A}{\partial x} \]
%
But in this case, we can view $M$ as a surface in $\mathbf{R}^3$ by adding an extra zero coordinate. The corresponding vector field in $\mathbf{R}^3$ is $(A,B,0)$. Then
%
\[ \nabla \times F = (0,0,\frac{\partial B}{\partial x} - \frac{\partial A}{\partial y}) \]
%
and therefore, by Stoke's theorem,
%
\[ \int_{\partial M} A\ dx + B\ dy = \int_M \frac{\partial B}{\partial x} - \frac{\partial A}{\partial y} \]
%
Since the surface normal is always $(0,0,1)$, because our surface is flat on the $z$-axis. In differential geometry, one can prove all of the fundamental theorems in this chapter by one generalized stokes theorem; this is one resulting instance proving a very special case from another very special case of the generalized form.

Turning back to our definition of curl, Stoke's theorem says that in order to find the spin of the vector field across the entire surface $M$, we need only consider how the force bends around the boundary (which is kind of like one `spin' around the surface).

The final fundamental theorem involves one more vector space construction. Given a vector field $F$, we want to measure how fast $F$ `spreads' from a point. This is the divergence, defined
%
\[ \langle \nabla, F \rangle = \frac{\partial F_1}{\partial x} + \frac{\partial F_2}{\partial y} + \frac{\partial F_3}{\partial z} \]
%
We therefore obtain a scalar function from the vector field. If the scalar function is positive, we have a source, pushing out force. If the function is negative, we have a sink, sucking in force. The divergence theorem states that, in order to find the force entering a surface, we need only determine the divergence of the interior -- that is, we need only sum up the sinks and sources of the field.

\begin{theorem}[The Divergence Theorem]
    \[ \int_{\partial M} \langle F, n \rangle\ dA = \int_M \langle \nabla, F \rangle\ dV \]
\end{theorem}
\begin{proof}
\end{proof}

We shall conclude our talk of vector analysis by talking about three operations we have constructed. Given a function $f$, we obtain a vector field $\nabla f$. Given a vector field $F$, we can obtain another vector field $\nabla \times F$, or obtain a scalar function $\langle \nabla, F \rangle$. After some nasty computation, we can deduce that for all functions $f$ and vector fields $F$,
%
\begin{align*}
    \nabla \times \nabla f = 0 && \langle \nabla, \nabla \times F \rangle = 0
\end{align*}
%
In other words, to the delight of algebraists, the diagram below is exact.
%
\[ \text{Scalar Functions} \xrightarrow{\nabla} \text{Vector Fields} \xrightarrow{\nabla \times \cdot} \text{Vector Fields} \xrightarrow{\langle \nabla, \cdot \rangle} \text{Scalar Functions} \]
%
Now suppose we want to integrate a closed surface over a field $F$ such that $F = \nabla f$ for some scalar function $f$, or if $F = \nabla \times G$ for some other vector field $G$. By Stoke's theorem,
%
\[ \int_{\partial M} \langle F, dx \rangle = \int_M \langle \nabla \times F, dx \rangle = \int_M \langle \nabla \times \nabla f \rangle = \int_M 0 = 0 \]
%
By the Divergence theorem,
%
\[ \int_{\partial M} \langle F, n \rangle\ dA = \int_M \langle \nabla, F \rangle\ dV = \int_M \langle \nabla, \nabla \times G \rangle = \int_M 0 = 0 \]
%
If we can identify a function or a field whose curl or gradient is our field, it becomes remarkably easy to calculate integrals over these fields. It is therefore in our interest to determine when a field is the gradient of some function, and if it is, we call that field conservative, due to physical considerations.

Of course, if $\nabla \times F \neq 0$, then $F$ cannot be the gradient of any function $f$. If $\langle \nabla, F \rangle \neq 0$, then $F$ cannot be the curl of any other vector field. On well behaved sets, however, this is the worst that can happen.

\begin{theorem}
    If $F$ is $C^1$ on an convex set in $\mathbf{R}^n$, the following are convex.
    \begin{enumerate}
        \item $F$ is conservative.
        \item $\int_C \langle F, dx \rangle = 0$ for every closed $C^1$ curve.
        \item Any two curves who have the same beginning and endpoint have equal line integrals.
        \item $\nabla \times F = 0$ (of course, this only is equivalent in $\mathbf{R}^3$).
    \end{enumerate}
\end{theorem}
\begin{proof}
    (1) to (2) is justified by the above discussion. If (2) is true, and we have two curves $x$ and $y$ with the same beginning and endpoints. Then we may daisy chain them together, forming a closed curve. Therefore, by assumption
    %
    \[ 0 = \int_{x - y} \langle F, dx \rangle = \int_x \langle F, dx \rangle - \int_y \langle F, dx \rangle  \]
    %
    and we conclude that (3) is true as well. To obtain (1) from (3), fix some point $a \in \mathbf{R}^n$. For each $y \in \mathbf{R}^n$, pick a curve $x_y$ by $x(t) = t(y) + (1-t)a$. Define a function $f:\mathbf{R}^n \to \mathbf{R}$ by
    %
    \[ f(y) = \int_{x_y} \langle F, dx \rangle \]
    %
    We claim that $\nabla f = F$. For simplicitly, we shall prove that $\frac{\partial f}{\partial x} = F_1$, and leave the rest of the deductions to the reader. We have
    %
    \[ \frac{\partial f}{\partial x_k} = \lim_{h \to 0} \frac{1}{h} \left[ \int_{x_{y+he_k}} \langle F, dx \rangle - \int_{x_y} \langle F, dx \rangle \right] \]
    %
    Define a curve by $x(t) = y + te_k$. Then, due to the assumption of (3),
    %
    \[ \int_{x_{y + he_k}} \langle F, dx \rangle = \int_{x_y} \langle F, dx \rangle + \int_0^h F_k(y + te_k)\ dt \]
    %
    Let $g$ be a function such that $g'(t) = F_k(y + te_k)$. Then
    %
    \[ \frac{\partial f}{\partial x_k} = \lim_{h \to 0} \int_0^h \frac{1}{h} F_k(y + te_k)\ dt = \lim_{h \to 0} \frac{g(h) - g(0)}{h} = g'(0) = F_k(y) \]
    %
    We have thus constructed the function needed.

    A long tedious calculation shows that (4) is true from (1). To show that (2) is true based on the assumption of (4), one can simply use Stoke's theorem. We have shown all we needed to show.
\end{proof}

One can, in the same manner, prove the same kind of theorem for fields $F$ such that $F = \nabla \times G$. We leave it to the reader to formulate the details.

\section{A Sample of Differential Geometry: Generalizing Vector Calculus}

Vector Calculus is pretty nice, but most of our discussion above applies only to $\mathbf{R}^3$, perhaps somewhat blas\'{e} to those mathematicians who talk only about $\mathbf{R}^n$. If you are that kind of person, then perhaps differential geometry will give you something to look forward to when learning the more advanced topic of differential geometry. For now, we shall develop the general method of extending the results above in an abstract manner without appeal to the differential geometry that the symbols represent. Without some small amounts of intuition, the mechanics we will develop our axiomatic in definition.

The idea of the differential form, the generalization of line and surface integrals to a higher dimension, extends from the guilt that we still feel today from the barbaric manner in which our mathematical ancestors treated analysis, battering down problems with a horde of infinitisimals that, formally, were nonsense. Nonetheless, true results were obtained by dividing out and integrating away infinitisimal factors to obtain an exact quantity, and so people continued to use what are now called `psuedomathematical concepts'. The idea of a differential form becomes clear when we realize that, though the manifestation of an infinitisimal push, pull, area, volume, etc, can never be realized, we can still obtain a relative slope of this quantity, and a relative direction. From this slope and direction, we can construct ourselves a linear transformation on the original space. From this the idea of a differential form arises naturally.

We shall begin by considering the simplest example of an infinitisimal, an infinitisimal push or pull in some direction. At each point $p$ in the space $\mathbf{R}^n$ we are working in, we obtain an infinitisimal push or pull. This can be extended to a linear function on the space by extending the slope to the displacement at a specific point. This is just an example of a differential one-form. Formally, a one form is therefore a map from $\mathbf{R}^n$ to linear functions on $\mathbf{R}^n$. Right now, we can think of every one-form $\omega$ as a formal sum
%
\[ \omega = f_1 dx_1 + f_2 dx_2 + \dots + f_n dx_n \]
%
where the coefficients are scalar functions. Each function measures the specific infinitisimal displacement in some direction.

If one-forms are infinitisimal pushes and pulls in some direction, then two forms are infinitisimal areas, three forms are infinitisimal volumes, etc. We form these forms from the product of one-forms. If we have an infinitisimal area, it must of course be composed of two infinitisimal lengths. We write the product of two one forms $\omega$ and $\eta$ (formally, without regard to what it actually means) by the wedge product
%
\[ \omega \wedge \eta \]
%
We have a few axiomatic rules for working with wedge products on one forms.
%
\begin{enumerate}
    \item $(\alpha + \beta) \wedge \omega = \alpha \wedge \omega + \beta \wedge \omega$.
    \item $(f \alpha) \wedge \omega = \alpha \wedge (f \omega) = f \alpha \wedge \omega$.
    \item $\alpha \wedge \beta = - \beta \wedge \alpha$. (The infintitisimal area is signed).
    \item $\alpha \wedge \alpha = 0$.
    \item $(\alpha \wedge \beta) \wedge \omega = \alpha \wedge (\beta \wedge \omega)$
\end{enumerate}
%
The set of all two forms is the set of all $\omega \wedge \eta$, where $\omega$ and $\eta$ are one forms. From the rules above, we can conclude that a basis for two forms is the set of $dx_i \wedge dx_j$, where $i < j$. In general, the set of $n$-forms can be defined to be generated from the set of $dx_{i_1} \wedge \dots \wedge dx_{i_n}$, where $i_1 < i_2 < \dots < i_n$. We conclude that the set of all $n$ forms in $\mathbf{R}^m$ has dimension $\frac{m!}{n!(m - n)!}$. The set of all zero forms is just the set of all scalar functions.

Now we want to find a way to differentiate forms. We shall write this `exterior' derivative of some $n$ form $\omega$ by $d\omega$, and it should give us an $n + 1$ form. Since 0-forms are just functions, the derivative of a zero form should correspond in someway to a derivative -- and it shall. We define the derivative of a function $f$ by
%
\[ df = \sum_{k = 1}^n \frac{\partial f}{\partial x_n} dx_n \]
%
If $\omega = f dx_{i_1} \wedge \dots \wedge dx_{i_n}$, then we define $d\omega = df \wedge \dots \wedge dx_{i_n}$. We can then extend this definition to any $n$-form. It is nice to note that the following correspondences hold.
%
\[ \nabla f \cong df \]
\[ \nabla \times F \cong d(F_1\ dx + F_2\ dy + F_3\ dz) \]
\[ \langle \nabla, F \rangle \cong d(F_1\ dy \wedge dz + F_2\ dz \wedge dz + F_3\ dx \wedge dy) \]
%
This is some case by case examples of the so called `Hodge dual' is differential geometry. Now suppose we have some parameterization $x:[0,1]^n \to \mathbf{R}^m$, and some $n$ form on this set. We want to define the integral of this form over this set. We let
%
\[ \int_x f dx_{i_1} \wedge \dots \wedge dx_{i_n} = \int_{[0,1]^n} (f \circ x) \det(\frac{\partial x}{\partial x_{i_1}}, \dots, \frac{\partial x}{\partial x_{i_n}}) \]
%
Then we extend this linearly to any other $n$-form. Integration of one-forms corresponds to line integrals, and integrals of two forms corresponds to surface integrals.

We may now state the generalized stoke's theorem (though we do not have the capacity to prove it at this time).
%
\begin{theorem}[Generalized Stoke's Theorem]
    If $\omega$ is an $n$ form, and $x$ some parameterization of an $n+1$ dimensional set, then
    %
    \[ \int_{\partial x} \omega = \int_x d\omega \]
\end{theorem}
%
Green's theorem corresponds to integrating over closed 1-forms in $\mathbf{R}^2$. Stoke's theorem corresponds to integrating over closed 2-forms in $\mathbf{R}^3$. Integrating over closed 3-forms in $\mathbf{R}^3$ corresponds to the divergence theorem. While notationally, it may be simpler to calculate integrals with differential form notation, we will not expand any more into differential geometry. Consider it a taste of many more beautiful ideas to come.

\begin{thebibliography}{12}

\bibitem{taoforms} Terrence Tao
\emph{Differential Forms and Integration}

\bibitem{babrudin} Walter Rudin
\emph{Principles of Mathematical Analysis}

\bibitem{compdiff} Michael Spivak
\emph{A Comprehensive Introduction to Differential Geometry}

\bibitem{mani} Michael Spivak
\emph{Calculus on Manifolds}

\end{thebibliography}

\end{document}