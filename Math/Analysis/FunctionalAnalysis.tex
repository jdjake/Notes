\input{../../style.tex}

%\newcommand{\vvvert}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
%    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}

\title{Functional Analysis}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}
\maketitle
\tableofcontents
\pagenumbering{arabic}

\part{Vector Spaces}

Functional analysis is the interlace of algebra and analysis, in which algebraic structures are endowed with topological structure. The approach's utility counts for the rapid growth of applications over the past century, be it in partial differential equations, quantum mechanics, statistics, or computing science. The idea of functional analysis emerges when we realize that in mathematics, we are often rarely concerned with a single mathematical object, like a function, a random variable, or a measure, but instead we are concerned with a large class of such objects. To handle all these objects at once, we add both algebraic and analytic structure to this class to make it a vector space with topological structure.

\begin{example}
    We rarely analyze a measurable function $f$ in isolation. Instead, we prove theorems about a class of measurable functions defined on a common measure space. If $f$ and $g$ are measurable, then we may consider their addition $f + g$, their multiplication $fg$, and scaling $\lambda f$, which are all measurable. Thus the space of measurable functions with a common domain is a vector space. Similarily, the sum and product of two continuous functions on a topological space is continuous, so, we may consider $C[0,1]$, the space of all continuous scalar-valued functions on the unit interval $[0,1]$, as a vector space.
\end{example}

The most common symmetry operations in analysis are scaling, rotation, and reflection. As a result, the most common vector spaces which occur in functional analysis are defined over the real and complex numbers, and we will restrict ourselves to the study of vector spaces over these fields.
%More specifically, real vector spaces $X$ are those spaces equipped with scaling operations $x \mapsto \lambda x$ for $\lambda > 0$, and a `reflection' operators $x \mapsto -x$. If $X$ is a real vector space with a specified `oriented twisting', i.e. a linear map $J: X \to X$ with $J^2(x) = -x$ for all $x \in X$, then there is a natural way to turn $X$ into a complex vector space by defining $(a + ib)x = ax + b(Jx)$. The geometry of the complex numbers then enables us to define an oriented twist by any angle $\theta$, by the operation $x \mapsto e^{i\theta} x$. Thus we have a natural operation of the complex numbers over the space.





\chapter{Banach Spaces}

The most natural way to associate a space with topological structure is by equipping the space with a metric. On a vector space, there is an even more powerful way of equipping the space with a metric. We can think of elements of a vector space as `arrows' extending from the origin. Thus a natural way to add a topology is to give these arrows a `length in themselves'. A function associating a vector with it's length is known as a {\it norm}. More precisely, a \emph{seminorm} on a vector space $X$ is a map $\| \cdot \|: X \to [0,\infty)$ which is {\it homogenous}, in the sense that for each $\alpha \in \mathbf{K}$ and $x \in X$, $\| \alpha x \| = |\alpha| \| x \|$, and satisfies the {\it triangle inequality} $\| x_1 + x_2 \| \leq \| x_1 \| + \| x_2 \|$ for each $x_1,x_2 \in X$. If in addition, we have $\| x \| = 0$ only when $x = 0$, then we say $\| \cdot \|$ is a {\it norm}. A vector space $X$ equipped with a norm is called a {\it norm space}. Then $X$ is naturally a metric space if we define $d(x_1,x_2) = \| x_1 - x_2 \|$. If $X$ is a \emph{complete} metric space, we say $X$ is a \emph{Banach space}.

On finite dimensional spaces $X$, there is essentially a unique way to define the size of a function. We say two norms $\| \cdot \|_1$ and $\| \cdot \|_2$ on a vector space $X$ are {\it comparable} if for each $x \in X$, $\| x \|_1 \sim \| x \|_2$. This is why it suffices for the most part to focus solely on the Euclidean norm when doing analysis on $\RR^d$.

\begin{theorem}
    If $X$ is a finite dimensional vector space, then all norms on $X$ are comparable.
\end{theorem}
\begin{proof}
    If $X$ is finite dimensional, with basis $\{ e_1,\dots,e_n \}$, then for any norm $\| \cdot \|$ on $X$,
    %
    \[ \| x \| = \| x_1 e_1 + \dots = x_n e_n \| \leq |x_1| \| e_1 \| + \dots + |x_n| \| e_n \| \lesssim \left( |x_1|^2 + \dots + |x_n|^2 \right)^{1/2}. \]
    %
    It follows that, if we equip $X$ with the Euclidean topology with respect to the basis $\{ e_i \}$, and consider the set
    %
    \[ S = \{ x \in X: \| x \| = 1 \}, \]
    %
    then $S$ is closed and bounded, and thus by the Heine-Borel theorem, compact. If we define $f: S \to (0,\infty)$ by setting
    %
    \[ f(x) = \left( |x_1|^2 + \dots + |x_n|^2 \right)^{1/2}, \]
    %
    then $f$ is continuous in the Euclidean topology and non-vanishing. Thus there is $C > 0$ such that $f(x) \geq C$ for all $x \in S$. But rescaling then shows that for any $x \in X$,
    %
    \[ \left( |x_1|^2 + \dots + |x_n|^2 \right)^{1/2} \geq (1/C) \| x \|. \]
    %
    Thus the Euclidean norm and the norm $\| \cdot \|$ are comparable. Since this norm was arbitrary, this completes the proof.
\end{proof}

On the other hand, there are many different ways to define the `size' of an infinite dimensional vector. For instance, for $f \in C^\infty[0,1]$, we can consider:
%
\begin{itemize}
    \item The $L^\infty$ norm
    %
    \[ \| f \|_{L^\infty[0,1]} = \max_{t \in [0,1]} |f(t)|. \]

    \item The $L^1$ norm
    %
    \[ \| f \|_{L^1[0,1]} = \int_0^1 |f(t)|\; dt. \]

    \item The $L^2$ norm
    %
    \[ \| f \|_{L^2[0,1]} = \left( \int_0^1 |f(t)|^2\; dt \right)^{1/2}. \]

    \item The Sobolev norms
    %
    \[ \| f \|_{H^n[0,1]} = \sum_{i = 0}^n \left( \int_0^1 |D^i f(t)|^2\; dx \right)^{1/2}. \]
\end{itemize}
%
None of these norms is comparable, and they give rise to a rich family of qualitatively different norm spaces.

\begin{example}
    Let $X$ be a measure space. For each $1 \leq p < \infty$, let $\mathcal{L}^p(X)$ denote the space of all measurable scalar-valued functions $f$ such that
    %
    \[ \| f \|_{L^p(X)} = \left( \int |f(x)|^p\; dx \right)^{1/p}. \]
    %
    is finite. Minkowski's inequality tells us that $\| \cdot \|_{L^p(X)}$ is a seminorm on $\mathcal{L}^p(X)$. Notice that if $u$ is a function such that $u(x) = 0$ for almost every $x$, then $\| f \|_{L^p(X)} = \| f + u \|_{L^p(X)}$. In particular, this means that $\| \cdot \|_{L^p(X)}$ is a well defined operation on the quotient space of $\mathcal{L}^p(X)$ by the vector space of functions which are equal to zero almost everywhere. We denote this quotient space by $L^p(X)$. Since $\| f \|_{L^p(X)} = 0$ if and only if $f$ is equal to zero almost everywhere, this implies that $L^p(X)$, equipped with the induced norm $\| \cdot \|$, is a norm space.
\end{example}

\begin{example}
    Given a measurable function $f$ on a measure space $X$, we can define the $L^\infty$ seminorm
    %
    \[ \| f \|_{L^\infty(X)} = \inf \{ t > 0: |f(x)| \leq t\ \text{for almost every $x \in X$} \}. \]
    %
    The space of functions with finite $L^\infty$ seminorm is denoted by $\mathcal{L}^\infty(X)$, and the quotient space by the class of functions equal to zero almost everywhere is denoted $L^\infty(X)$. The space $L^\infty(X)$ is then a norm space.
\end{example}

\begin{example}
    On a compact topological space $K$, we can consider the family of all continous, scalar-valued functions with domain $K$, denoted $C(K)$. The quantity
    %
    \[ \| f \|_{L^\infty(K)} = \sup \{ |f(x)|: x \in K \}, \]
    %
    gives $C(K)$ the structure of a norm space. More generally, if $X$ is any norm space, we can consider the family $C(K,X)$ of all bounded continuous functions from $K$ to $X$, with associated norm
    %
    \[ \| f \|_{C(K,X)} = \sup \{ \| f(x) \|: x \in K \}. \]
    %
    If $X$ is a Banach space, then $C(K,X)$ is a Banach space.
\end{example}

\begin{example}
    The most important norm spaces for intuition are the sequence spaces $l^p$, defined for $1 \leq p < \infty$, and consisting of all sequences $a = \{ a_n \}$ such that the norm
    %
    \[ \| a \|_{l^p} = \left( \sum_{n = 1}^\infty |a_n|^p \right)^{1/p} \]
    %
    is finite, as well as the space $l^\infty$, consisting of all bounded sequences $a$, with the norm
    %
    \[ \| a \|_{l^\infty} = \sup_{n \geq 1} |a_n|. \]
    %
    These possess many of the interesting properties of the more general norm spaces $L^p(X)$, but possess fewer of the analytic technicalities required to work with these more general spaces.
\end{example}

\begin{remark}
    We have performed a particular example of a general construction when moving from the spaces $\mathcal{L}^p(X)$ to the spaces $L^p(X)$. If $X$ is a vector space equipped with a seminorm $\| \cdot \|$, then it is simple to verify that the set
    %
    \[ X_0 = \{ x \in X: \| x \| = 0 \} \]
    %
    is a subspace of $X$, and for any $x \in X$ and $x_0 \in X_0$, $\| x \| = \| x + x_0 \|$. Thus the seminorm $\| \cdot \|$ is well defined on the quotient space $X/X_0$. Moreover, the seminorm is now a \emph{norm} on this space. Thus one can reduce many questions about seminormed spaces to questions about normed spaces. For the spacial case where $X = \mathcal{L}^p(X)$, the set $X_0$ is precisely the family of measurable functions $f$ which are equal to zero almost everywhere, and the quotient $X/X_0$ is precisely the space $L^p(X)$.
\end{remark}

It is proved in a first course in real analysis that the spaces $C(K)$ are complete, i.e. since the uniform limit of a family of continuous functions is continuous. Thus $C(K)$ is a Banach space. Let us now that the spaces $L^p(X)$ are complete norm spaces, i.e. Banach spaces.

\begin{theorem}
    For any measure space $X$, $L^p(X)$ is a Banach space.
\end{theorem}
\begin{proof}
    We deal for $1 \leq p < \infty$, the case $p = \infty$ being left to the reader. Let $\{ f_k \}$ be a Cauchy sequence in $L^p(X)$. Then we can find a subsequence $\{ f_{k_i} \}$ such that for each $i$,
    %
    \[ \| f_{k_{i + 1}} - f_{k_i} \|_{L^p(X)} \leq 1/2^i. \]
    %
    By the monotone convergence theorem, we find that the infinite sum
    %
    \[ F(x) = \sum_{i = 1}^\infty | f_{k_{i+1}}(x) - f_{k_i}(x)| \]
    %
    is absolutely convergent for almost every $x \in X$. Thus we can define a measurable function $f$ almost everywhere by the formula
    %
    \[ f(x) = \lim_{i \to \infty} f_{k_i}(x) = \lim_{i \to \infty} \sum_{j = 1}^i (f_{k_{j+1}} - f_{k_j}). \]
    %
    Fatou's lemma and Minkowski's inequality imply that
    %
    \begin{align*}
        \| f - f_{k_i} \|_{L^p(X)} &= \left( \int \left| \lim_{j \to \infty} f_{k_j}(x) - f_{k_i}(x) \right|^p\; dx \right)^{1/p}\\
        &\leq \liminf_{j \to \infty} \left( \int \left| f_{k_j}(x) - f_{k_i}(x) \right|^p \right)^{1/p}\\
        &\leq \liminf_{n \to \infty} \| f_{k_j} - f_{k_i} \|_{L^p(X)} \leq 1/2^{i-1}.
    \end{align*}
    %
    Thus the sequence $\{ f_{k_i} \}$ converges to $f$ in the $L^p(X)$ norm. But since $\{ f_k \}$ is Cauchy, this means that $\{ f_k \}$ converges to $f$ in the $L^p(X)$ norm as well. Thus Cauchy sequences converge in $L^p(X)$, and this means $L^p(X)$ is complete.
\end{proof}

\begin{remark}
    Studying Banach spaces is not too much more general than studying norm spaces. Similar to the fact that every metric space has a completion, given any norm space $X$, we can always find a Banach space $\tilde{X}$ containing $X$ as a dense subset, which is unique up to isometry. One can view the elements of $\tilde{X} - X$ as `asymptotic' elements of $X$, since they correspond to the limits of Cauchy sequences of $X$. For instance, if we equip $C[0,1]$ with the $L^1$ norm induced by the Lebesgue measure, then $C[0,1]$ is an incomplete norm space. The completion of $C[0,1]$ with respect to this norm is the space $L^1[0,1]$ of integrable functions. Thus the behaviour of integrable functions can be thought of as representing the asymptotic integrability properties of a family of continuous functions, which we are familiar with integrating from our first years of university.
\end{remark}

The examples above provide most of the intuition we will need for the basic Banach space theory. But it is useful to know other examples for applications in other areas of analysis.

\begin{example}
    For any Hausdorff space $X$, let $M(X)$ denote the family of finite Borel measures on $X$. We can define the \emph{total variation norm} on $X$ by setting, for a measure $\mu$ on $X$, $\| \mu \|_{M(X)} = |\mu|(X)$. Then $M(X)$ becomes a Banach space under this norm, which can be proved in a manner similar to the proof that $L^1(X)$ is complete, by taking a rapidly decaying subsequence and proving it converges to a measure in the total variation norm.
\end{example}

\begin{example}
    Let $K$ be a compact metric space. For each $0 < p \leq 1$, we say a scalar-valued function $f$ on $K$ is {\it Lipschitz of order $p$} if $|f(x) - f(y)| \lesssim d(x,y)^p$ for each $x,y \in K$. The set of such functions forms a dense subspace $\text{Lip}_p(K)$ of $C(K)$. For each $f \in \text{Lip}_p [0,1]$, we define the Lipschitz norm
    %
    \[ \| f \|_{\text{Lip}_p(K)} = \| f \|_{L^\infty(K)} + \sup \frac{|f(x) - f(y)|}{d(x,y)^p}. \]
    %
    This norm makes $\text{Lip}_p(K)$ into a Banach space. To see this, consider a Cauchy sequence $\{ f_n : n \geq 1 \}$ in $\text{Lip}_p(K)$. Then the completeness of $L^\infty(K)$ shows there exists $f \in L^\infty(K)$ such that $f_n \to f$ in the $L^\infty$ norm, and it is not too difficult to argue from this that $f \in \text{Lip}_p(K)$, and that $f_n \to f$ in the $\text{Lip}_p(K)$ norm.
%    . Since $\{ f_n \}$ is Cauchy,  there exists a constant $M$ such that for all $n$, $|f_n(x) - f_n(y)| \leq M |x - y|^p$. In particular, the uniform convergence of $f_n$ to $f$ implies that $|f(x) - f(y)| \leq M |x - y|^p$, so $f \in \text{Lip}_p(K)$. Now
    %
%    \begin{align*}
%        \lim_{n \to \infty} &\sup_{x,y \in K} \frac{|[f(x) - f_n(x)] - [f(y) - f_n(y)]|}{d(x,y)^p}\\
%        &= \lim_{n \to \infty} \sup_{x,y \in K} \lim_{m \to \infty} \frac{[f_m(x) - f_n(x)] - [f_m(y) - f_n(y)]}{d(x,y)^p}\\
%        &\leq \lim_{n \to \infty} \limsup_{m \to \infty} \sup_{x,y \in K} \frac{[f_m(x) - f_n(x)] - [f_m(y) - f_n(y)]}{d(x,y)^p}\\
%        &\leq \lim_{n \to \infty} \limsup_{m \to \infty} \| f_m - f_n \|_{\text{Lip}_p(K)} = 0.
%    \end{align*}
    %
%    Thus $\{ f_n \}$ converges to $f$ in the $\text{Lip}_p(K)$ metric as well as in $C(K)$. Thus $\text{Lip}_p(K)$ is a complete metric space.
\end{example}

\begin{example}
    If $K$ is a compact subset of $\RR^d$, then we define $C^n(K)$ to be the space of all $n$-times continuously differentiable functions on $K$, with the norm
    %
    \[ \| f \|_{C^n(K)} = \max_{|\alpha| \leq n} \| D^\alpha f \|_{L^\infty(K)}. \]
    %
    Then $C^n(K)$ is a Banach space, the proof of this fact being very similar to the proof that $\text{Lip}_p(K)$ is a Banach space.
\end{example}

These are the main examples of norm spaces measuring smoothness where a completion is unnecessary. When measuring the smoothness of a function using some integrability condition, a completion is necessary, and interpreting this completion leads to the discussion of weak smoothness.

\begin{example}
    Let $K$ be a compact subset of $\RR^d$, and define $\mathcal{W}^{n,p}(K)$ to be the space of all $f \in C^n(K)$ such that for any multi-index $\alpha$ with $|\alpha| \leq n$, $D^\alpha f \in L^p(K)$, and equip $\mathcal{W}^{n,p}(K)$ with the norm
    %
    \[ \| f \|_{W^{n,p}(K)} = \sup_{|\alpha| \leq n} \| D^\alpha f \|_{L^p(K)}. \]
    %
    Then $\mathcal{W}^{n,p}(K)$ is a norm space, but is \emph{not} complete. The completion of this space is the \emph{Sobolev space} $W^{n,p}(K)$, and elements of this space can be identified with measurable functions possessing \emph{weak}, or \emph{distributional} derivatives in $L^p(K)$ up to order $n$.
\end{example}

Let $X$ be a norm space. It is easy to see that the addition map is jointly continuous in each variable, as is scaling. The norm itself it seen to be a continuous $[0,\infty)$ valued function. For a fixed $x_0 \in X$, the translation map $x \mapsto x + x_0$ is a homeomorphism of $X$, and for $\lambda \neq 0$, $x \mapsto \lambda x$ is also a homeomorphism of $X$.

We can develop the theory of series in a norm space, i.e. a sum converges if it's partial sums converge. Given a sequence $\{ x_n \}$. We say the sequence is absolutely summable if $\sum \| x_n \| < \infty$. The same proof as for $\RR$ shows that such a sequence is {\it unconditionally summable}, i.e. that for each permutation $\pi: \mathbf{N} \to \mathbf{N}$,
%
\[ \sum_{n = 1}^\infty x_{\pi(n)} = \sum_{n = 1}^\infty x_n. \]
%
Instead of defining a Banach space in terms of Cauchy sequences, we could also define a Banach space as a space in which every absolutely summable series converges.

\begin{remark}
    Note that, unlike sequences in $\RR^n$, in \emph{any} infinite dimensional Banach space $X$ there are sequences which are unconditionally summable, but \emph{not absolutely summable}. This is a result of A. Dvoretzky and C.A. Rogers. For a particular example, consider the sequence $\{ e_n \}$ in $l^\infty$, where $e_n$ is the sequence equal to zero everywhere except the $n$th position, where it takes the value one. Set $x_n = e_n / n$. Then the sequence $\{ x_n \}$ is not absolutely summable, yet it is unconditionally summable.
\end{remark}

Before we move on to more interesting concepts, let us introduce some notation and describe some easily proved properties of Banach spaces. For any norm space $X$, we let $B_X$ denote the closed unit ball in $X$, and $S_X$ the unit sphere, i.e.
%
\[ B_X = \{ x \in X: \| x \| \leq 1 \}\quad\text{and}\quad S_X = \{ x \in X: \| x \| = 1 \}. \]
%
Given two sets $A$ and $B$, we let $A + B = \{ a + b : a \in A, b \in B \}$. Then for each $a \in X$, $a + B_X$ is the closed unit ball centered at $a$. Similarily, we can define $A - B = \{ a - b : a \in A, b \in B \}$, and for each $\alpha \in k$, $\alpha A = \{ \alpha a : a \in A \}$.

\section{Subspaces}

If $X_0$ is a vector subspace of a norm space $X$, then $X_0$ inherits the property of being a norm space from $X$. But if $X$ is a Banach space, then $X_0$ will only be complete with this norm when $X_0$ is a \emph{closed} subspace of $X$. Here are some examples.

\begin{example}
    Let $c_0$ be the subspace of $l^\infty$ consisting of the family of all sequences $\{ x_n \}$ such that $\lim_{n \to \infty} x_n = 0$. Then $c_0$ is a closed subspace of $l_\infty$, and is thus a Banach space. The space $c_{00}$ of all sequences $\{ x_n \}$ which are only non-zero for finitely many values is a subspace of $c_0$, but it is not closed, and therefore not a Banach space. In fact, $c_{00}$ is dense in $l^p$ for all $1 \leq p < \infty$, and dense in $c_0$, but not dense in $l^\infty$ (it's closure is $c_0$).
\end{example}

\begin{example}
    Let $\mathbf{T}$ be the interval $[-\pi,\pi)$ equipped with the Lebesgue measure. For each $f \in L^p(\mathbf{T})$, and any integer $n \in \mathbf{Z}$, we define
    %
    \[ \widehat{f}(n) = \frac{1}{2\pi} \int_{-\pi}^\pi f(x) e^{-nix}\; dx. \]
    %
    H\"{o}lder's inequality shows that
    %
    \[ |\widehat{f}(n)| \leq \| f \|_{L^1(\TT)} \leq \| f \|_{L^p(\TT)} \]
    %
    for each $n \in \mathbf{Z}$. In particular, this means the set
    %
    \[ H^p(\mathbf{T}) = \{ f \in L^p(\mathbf{T}) : \widehat{f}(n) = 0\ \text{if $n < 0$} \} \]
    %
    is closed in $L^p(\TT)$, and is therefore a Banach space. These are the {\it Hardy spaces} on $\mathbf{T}$. They can be identified with a family of holomorphic functions on the unit disk under the identification of $f$ with the analytic function
    %
    \[ u(z) = \sum_{n = 0}^\infty \widehat{f}(n) z^n, \]
    %
    which converges for $|z| < 1$ since $\{ \widehat{f}(n) \}$ is a bounded sequence. The family of all analytic functions obtained in this way can be described as the space
    %
    \[ H^p(\DD) = \left\{ f \in \mathcal{O}(\mathbf{D}^\circ) : \left( \sup_{0 < r < 1} \int_{|z| = r} |f(w)|^p\; dw \right)^{1/p} < \infty \right\}, \]
    %
    though proving these spaces are identified requires some more powerful tools in harmonic analysis. But one can prove quite simply that $H^p(\DD)$ is a Banach space with the norm
    %
    \[ \| f \|_{H^p(\DD)} = \sup_{0 < r < 1} \left( \int_{|z| = r} |f(w)|^p\; dw \right)^{1/p}, \]
    %
    if we take for granted Cauchy's integral formula.
\end{example}

If $X_0$ is a subspace of $X$, then $\overline{X_0}$ will be a \emph{closed subspace} of $X$. For a set $E \subset X$, we let $\langle E \rangle$ denote the smallest subspace of $X$ containing $E$, the \emph{span} of $E$, and $[E]$ the smallest {\it closed} subspace, the \emph{closed span} of $E$.

\section{Convexity}

A critical notion in functional analysis is {\it convexity}. A set $E \subset X$ is {\it convex} if for each $x,y \in E$ and $\lambda \in (0,1)$, $\lambda x + (1 - \lambda) y \in E$. A set $E$ is {\it balanced} if $\alpha E \subset E$ for each $|\alpha| \leq 1$, and {\it symmetric} if $-E = E$. And a set $E$ is {\it absorbing} if, for each $x \in X$, there exists $t$ such that for $|\alpha| \geq t$, $x \in \alpha E$. As an example, the unit ball $B_X$ is closed, convex, and absorbing. In the future, we will make heavy use of the following result.

\begin{theorem}
    Every closed, convex, absorbing set in a Banach space contains a neighbourhood of the origin.
\end{theorem}
\begin{proof}
    Let $E \subset X$ be a closed, convex, absorbing set. Then $E \cap (-E)$ is closed, convex, and absorbing, so we may assume without loss of generality that $E$ is symmetric. It suffices to show $E^\circ$ is nonempty, for then $E^\circ/2 + (-E^\circ)/2$ is a neighbourhood of the origin contained in $E$. Now assume $E^\circ = \emptyset$. This means that for each $n$, $F_n = (nE)^c$ is an open, dense set. Thus $\bigcap F_n$ is dense. But this is impossible by the Baire category theorem, since $\bigcup nE = X$. Thus we conclude $E^\circ \neq \emptyset$.
\end{proof}

For each set $E \subset X$, we let $\text{co}(E)$ denote the smallest convex subset of $X$ containing $E$. This is the {\it convex hull} of $E$.

\section{Bounded Linear Operators}

A natural object of study in linear algebra is to understand the family of linear maps between two vector spaces $X$ and $Y$. In the theory of norm spaces, we want to understand the family of {\it continuous} linear maps. As the last part of the next theorem shows, it is also natural to call these linear maps \emph{bounded} linear maps.

\begin{theorem}
    Let $X$ and $Y$ be norm spaces, and $T: X \to Y$ a linear map. Then the following are equivalent:
    %
    \begin{itemize}
        \item $T$ is continuous.
        \item $T$ is continuous at $0 \in X$.
        \item $T$ is uniformly continuous.
        \item There exists $M \geq 0$ such that $\| Tx \| \leq M \| x \|$ for all $x \in X$.
    \end{itemize}
\end{theorem}
\begin{proof}
    We leave the proof of the equivalence of (1), (2), and (3) to the reader. If $T$ is continuous at the origin, then in particular, since $T(0) = 0$, for each $\varepsilon > 0$, there exists $\delta > 0$ such that if $\| x \| \leq \delta$, $\| Tx \| \leq \varepsilon$. But this means that $\| Tx \| \leq (\varepsilon / \delta) \| x \|$ if $\| x \| \leq \delta$, and by scaling an arbitrary $x$, one sees that this inequality actually holds for all $x \in X$. Conversely, if $M$ exists such that $\| Tx \| \leq M \| x \|$, and $\varepsilon > 0$, and we set $\delta = \varepsilon / M$, then if $\| x \| \leq \delta$, $\| Tx \| \leq \varepsilon$, so $T$ is continuous at the origin.
\end{proof}

We let $B(X,Y)$ denote the class of bounded linear operators from $X$ to $Y$. For each such operator, we define the {\it operator norm}
%
\[ \| T \| = \sup_{x \neq 0} \frac{\| Tx \|}{\| x \|}. \]
%
This quantity is finite if and only if the operator is continuous. One can verify that this definition gives $B(X,Y)$ the structure of a norm space, and moreover, if $Y$ is a Banach space, then $B(X,Y)$ is a Banach space. Moreover, given $T \in B(X,Y)$ and $S \in B(Y,Z)$, we can form the composition operator $S \circ T \in B(X,Z)$, and one verifies that
%
\[ \| S \circ T \| \leq \| S \| \| T \|. \]
%
Thus spaces of operators have even more algebraic and analytic structure than norm spaces. This structure is heavily exploited in the theory of {\it operator algebras}, covered later in these notes.

\begin{example}
    If $Y$ is not a Banach space, then $B(X,Y)$ can fail to be a Banach space. For instance, if $Y$ is $c_{00}$ equipped with the same norm as $l_1$, then $B(l_1,Y)$ is not a Banach space. For instance, if
    %
    \[ T_n(a) = \sum_{k = 1}^n \frac{1}{k^2} \left( \sum_{i = k}^\infty a_i \right) e_k, \]
    %
    then $\| T_n - T_{n + m} \| \leq \sum_{k = n}^\infty (1/k)^2$, so $\{ T_n \}$ is Cauchy. However, the limit of these sequence of operators is the map
    %
    \[ T(a) = \sum_{k = 1}^\infty \frac{1}{k^2} \left( \sum_{i = k}^\infty a_i \right) e_k \]
    %
    which is an element of $B(l_1,l_1)$, but not an element of $B(l_1,Y)$.
\end{example}

If $\{ T_n \}$ is a sequence of operators in $B(X,Y)$ which coverges in the operator norm to an operator $T$, then it is obvious that $Tx = \lim_{n \to \infty} T_nx$ for each $x \in X$. Pointwise convergence does not imply convergence in the operator norm, as the next example shows.

\begin{example}
    Recall the Banach space $c_0$, and define a sequence of operators $T_n: c_0 \to \mathbf{K}$ by letting $T_n(a) = a_n$. Then each $T_n$ is a bounded operator, with $\| T_n \| = 1$. Thus the sequence $\{ T_n \}$ does not converge to zero in the operator norm. However, for each $a \in c_0$, $\lim_{n \to \infty} T_n(a) = 0$, so the sequence $\{ T_n \}$ does converge pointwise to zero.
\end{example}

In the example above, we would say $\{ T_n \}$ converges in the \emph{strong operator topology}, but does not converge in the \emph{norm topology}, a concept we return to later in these notes.

The theory of bounded operators for finite dimensional norm spaces is effectively trivial.

\begin{theorem}
    Every linear map from a finite dimensional norm space $X$ to a norm space $Y$ is bounded.
\end{theorem}
\begin{proof}
    Let $\{ e_1, \dots, e_d \}$ be a basis for $X$, for which, without loss of generality, we may assume $\| e_i \| = 1$ for each $i$. Define, for each $x \in X$ with $x = a_1e_1 + \dots + a_d e_d$,
    %
    \[ |x| = (|a_1|^2 + \dots + |a_d|^2)^{1/2}. \]
    %
    Notice that the triangle inequality implies that
    %
    \[ \| x \| \leq |a_1| + \dots + |a_d| \leq \sqrt{d} \left( |a_1|^2 + \dots + |a_d|^2 \right)^{1/2}. \]
    %
    Thus $\| \cdot \|$ is continuous with respect, to the topology induced by $|\cdot|$. If $S$ is the unit sphere with respect to the norm $|\cdot|$, then the Heine-Borel theorem implies that $S$ is compact, and in particular, $\| \cdot \|$ attains a maximum and a minimum on $S$. In particular, since $\| x \| \neq 0$ for each $x \in S$, this implies that there is $\varepsilon > 0$ such that $\| x \| \geq \varepsilon$ for each $x \in S$. And rescaling thus shows that $\| x \| \geq \varepsilon |x|$ for each $x \in X$. In particular, this means that $|\cdot|$ and $\| \cdot \|$ are comparable. In particular, $T$ is continuous with respect to $\| \cdot \|$ if and only if it is continuous with respect to $| \cdot |$. And we then find that for any $x = a_1e_1 + \dots + a_de_d$, the triangle inequality implies
    %
    \begin{align*}
        \| Tx \| &\leq |a_1| \| Te_1 \| + \dots + |a_d| \| Te_d \|\\
        &\leq (|a_1| + \dots + |a_d|) \max \| Te_i \|\\
        &\leq \sqrt{d} |x| \| Te_i \|.
    \end{align*}
    %
    Thus $T$ is a continuous linear map.
\end{proof}

Conversely, if $X$ is infinite dimensional, and $Y \neq 0$, then there exist discontinous linear maps from $X$ to $Y$. For instance, if we consider a basis $\mathcal{B}$ for $X$ containing at least countably many elements $\{ e_1, e_2, \dots \}$, such that $\| e_i \| = 1$ for each $i$, and we find $y \neq 0$ is in $Y$, then we can define a linear map such that $T(e_i) = i \cdot y$, and $T(f) = 0$ for each $f \in \mathcal{B}$ with $f \neq e_i$ for each $i$. Then $T$ is unbounded since $\| T e_i \| \geq i \| e_i \|$ for each $i > 0$.

\begin{remark}
    A key component of the proof above was the fact that $X$ possessed a norm whose respective topology satisfied the Heine-Borel property. If $X$ is infinite dimensional, no such norm can exist. This can be shown by showing that for any norm $\| \cdot \|$ on an infinite dimensional space $X$, and any $n$, there is a countable collection $\{ x_1, x_2, \dots \} \subset S_X$ such that $\| x_i - x_j \| \geq 1$ for $i \neq j$. Clearly this sequence can have no accumulation points. We can define such a sequence inductively. Given $\{ x_1, \dots, x_n \}$, let $X_n = \langle x_1, \dots, x_n \rangle$. Then $X_n$ is a finite dimensional norm space, hence a Banach space, hence closed in $X$. In particular, if $y \not \in X_n$, then $d(X_n,y) > 0$. If $\alpha = d(X_n,y)$, then $d(X_n,y/\alpha) = 1$. But this means that $\| x_i - y/\alpha \| \geq 1$ for each $i \in \{ 1, \dots, n \}$, so we can set $x_{n+1} = y/\alpha$.
\end{remark}

We say a map $T: X \to Y$ between two norm spaces is an {\it isomorphism} if for each $x \in X$, $\| Tx \| \sim \| x \|$. In particular, this implies $T$ is continuous and injective, but importantly, \emph{not necessarily surjective}. We say $X$ is {\it embedded} in $Y$. If $T$ is surjective, we can define an inverse map $T^{-1}: Y \to X$, which is continuous, and we then say $X$ and $Y$ are isomorphic. Note that if $X$ is a Banach space, then $T(X)$ is also a Banach space. In most other areas of math, an isomorphism has to be surjective, but unfortunately, the definition of isomorphism above has become standardized in Banach space theory. A stronger notion than an isomorphism is an {\it isometry}, which is a map $T: X \to Y$ such that $\| Tx \| = \| x \|$ for all $x \in X$. Again, $T$ need not be surjective, and we say $X$ has been {\it embedded} in $Y$. If $T$ is surjective, $T^{-1}$ is also an isometry, and we say $X$ and $Y$ are isometrically isomorphic. Isomorphic spaces have many of the same properties, but in order to transfer all properties about norm spaces, one really needs an isometry.

\begin{example}
    Let $c$ be the subspace of $l^\infty$ consisting of all convergent, bounded sequences. Then $c$ is a closed subspace, and thus a Banach space. In fact, $c$ is isomorphic to $c_0$, since we can define an operator $T: c_0 \to c$ by setting
    %
    \[ T(a) = (a_0 + a_1,a_0 + a_2,\dots). \]
    %
    We have $\| T a \|_\infty \leq 2 \| a \|_\infty$, so $T$ is continuous. Furthermore, $T$ is invertible, with
    %
    \[ T^{-1}(b) = \left( \lim_{n \to \infty} b_n, b_0 - \lim_{n \to \infty} b_n, b_1 - \lim_{n \to \infty} b_n, \dots \right), \]
    %
    and $\| T^{-1}(b) \|_\infty \leq 2 \| b \|_\infty$. Thus we conclude
    %
    \[ (1/2) \| a \|_\infty \leq \| Ta \|_\infty \leq 2 \| a \|_\infty, \]
    %
    and so $c$ and $c_0$ are isomorphic. Nonetheless, these spaces are \emph{not} isometric. One property that $c_0$ has, but $c$ does not have, is that if $a \in S_{c_0}$, then there are $b, c \in S_{c_0}$ with $b \neq c$ such that $a = (b + c)/2$. In the language of convexity, we would say that $S_{c_0}$ has {\it no extreme points}. To see why, we define
    %
    \[ b_n = \begin{cases} a_n &: |a_n| \geq 1/2 \\ a_n/2 &: |a_n| < 1/2 \end{cases}\quad\text{and}\quad c_n = \begin{cases} a_n &: |a_n| \geq 1/2 \\ 3a_n/2 &: |a_n| < 1/2 \end{cases}. \]
    %
    Clearly $a = (b+c)/2$, and $\| b \|_\infty = \| c \|_\infty = 1$. Since $\lim_{n \to \infty} a_n = 0$, eventually $|a_n| \leq 1/2$, and so $b_n \neq c_n$. On the other hand, the sequence $a = (1,1,\dots)$ lies in $S_c$, yet it $b,c \in S_c$ satisfy $a = (b + c)/2$, then $b = c = a$.
\end{example}

Linear operators into $\mathbf{K}$ are given a special name, they are known as \emph{linear functionals}, and form the simplest class of linear operators. The space $B(X,\mathbf{K})$ is then known as the {\it dual space} of $X$, denoted by $X^*$. Given $x^* \in X^*$ and $x \in X$, we often denote $x^*(x)$ by $\langle x^*, x \rangle$, and the definition of the operator norm implies that
%
\[ |\langle x^*, x \rangle| \leq \| x^* \| \| x \|. \]
%
More generally, for two Banach spaces, the existence of a bounded linear map $T: Y \to X^*$ is equivalent to the existence of a bilinear scalar-valued map from $X \times Y$, also called a \emph{pairing} of $X$ and $Y$, the image of $x \in X$ and $y \in Y$ we might denote as $\langle x, y \rangle$, such that
%
\[ |\langle x, y \rangle| \lesssim \| x \| \| y \|. \]
%
Thus in the sequel we will define maps into $X^*$ by bilinear maps.

\begin{example}
    Let $X$ be a measure space. For any $1 \leq p \leq \infty$, if $q$ is the conjugate of $p$, then we have an isometry from $L^q(X) \to L^p(X)^*$ induced by the bilinear pairing
    %
    \[ \langle f, g \rangle = \int_X f(x) g(x)\; dx \]
    %
    for $f \in L^p(X)$ and $g \in L^q(X)$. H\"{o}lder's inequality justifies that $\| Tg \|_{L^p(X)^*} = \| g \|_{L^q(X)}$, so that the correspondence is an isometry.

    In many cases, this isometry is surjective, which means we can identify $L^p(X)^*$ with $L^q(X)$. To begin with, assume first that $X$ is finite, and $1 \leq p < \infty$. Then for any linear functional $\lambda \in L^p(X)^*$, we can define a complex / signed measure $\mu$ on $X$ such that for any set $E \subset X$,
    %
    \[ \mu(X) = \lambda(1_E). \]
    %
    The measure $\mu$ is absolutely continuous with respect to the measure on $X$, and so by the Radon Nikoydym theorem, there exists $g \in L^1(X)$ such that $\mu = dg$. But this means that for any simple function $s$,
    %
    \[ \lambda(s) = \int s(x) g(x)\; dx. \]
    %
    Since simple functions are dense, by continuity, we find that for any $f \in L^p(X)$,
    %
    \[ \lambda(f) = \int f(x) g(x)\; dx. \]
    %
    This guarantees that we actually have $g \in L^q(X)$, and thus the isometry is surjective.

    This result continues to be true if $X$ is $\sigma$-finite instead of being just finite, i.e. assuming that there is an increasing family of finite subsets $\{ X_i \}$ of $X$ with $X = \lim_i X_i$. For each $i$, we have a natural decomposition of $L^p(X)$ into $L^p(X_i) \oplus L^p(X_i^c)$ and so any linear functional $\lambda \in L^p(X)^*$ restricts to a linear function on $L^p(X_i)^*$, and thus corresponds to a function $g_i \in L^p(X)$ supported on $X_i$ with $\| g_i \|_{L^q(X)} \leq \| \lambda \|$ for all $i$. Clearly $g_{i+1}$ is an extension of $g_i$ to $X_{i+1}$, and so we see that we can define $g = \lim_i g_i$ on $X$. By monotone convergence we have $g \in L^q(X)$, and for any $f \in L^p(X)$,
    %
    \[ \lambda(f) = \int f(x) g(x)\; dx. \]
    %
    Thus if $X$ is $\sigma$ finite and $1 \leq p < \infty$, then $L^p(X)^*$ is isometric to $L^q(X)$.

    Finally, suppose $X$ is a general measure space, and $1 < p < \infty$. For each $\lambda \in L^p(X)^*$ and any measurable set $A \subset X$, we have a functional $\lambda_A \in L^p(X)^*$ given by
    %
    \[ \lambda_A(f) = \lambda(f \cdot 1_A). \]
    %
    Then if $1 \leq p < \infty$, then
    %
    \[ \| \lambda_{A \cup B} \| = \left( \| \lambda_A \|^q + \| \lambda_B \|^q \right)^{1/q}. \]
    %
    The density of functions with finite measure support in $L^p(X)$ for $1 \leq p < \infty$ justifies that in that range,
    %
    \[ \| \lambda \| = \sup_{|A| < \infty} \| \lambda_A \|. \]
    %
    For $1 < p < \infty$, if $\| \lambda_A \| \geq \| \lambda \| - \varepsilon$, then
    %
    \[ \| \lambda \| = \left( \| \lambda_A \|^q + \| \lambda_{A^c} \|^q \right)^{1/q} \]
    %
    which means that
    %
    \[ \| \lambda - \lambda_A \| = \| \lambda_{A^c} \| = \left( \| \lambda \|^q - \| \lambda_A \|^q \right)^{1/q} \leq q \| \lambda \|^{1-1/q} \cdot \varepsilon^{1/q}. \]
    %
    But this means that we may find $X_0 \subset X$, where $X_0$ is $\sigma$ finite, such that $\lambda = \lambda_{X_0}$, and we can apply the $\sigma$ finite case to conclude that $\lambda$ is induced by some element of $L^q(X_0)$.

    On the other hand, if $X$ is not sigma finite, then $L^1(X)^*$ need not be isometric to $L^\infty(X)$. One can remedy this by defining a larger class of functions, called the \emph{locally measurable functions}, and consider those functions that are \emph{locally bounded almost everywhere}. But this is technical, and does not often arise in practice.

    On the other hand, the space $L^\infty(X)^*$ is almost always crazy. It contains $L^1(X)$. But if $\mu$ is a finite measure absolutely continuous with respect to the measure on $X$, then we can define a continuous functional on $L^\infty(X)$ by setting
    %
    \[ f \mapsto \int_X f(x)\; d\mu(x). \]
    %
    For $\sigma$-finite measure spaces $X$, we can identify $L^\infty(X)^*$ with the family of \emph{finitely additive} set functions on $X$ which are absolutely continuous with respect to the measure on $X$. For non $\sigma$-finite measure spaces, we can obtain a similar identification by considering locally absolutely continuous finitely additive set functions. Since finitely additive set functions can behave in many nasty ways, the space $L^\infty(X)^*$ is not a space people like to spend a lot of time in.
\end{example}

\begin{example}
    The last example justifies the existence of an isometry $l^1 \to (l^\infty)^*$, which is \emph{not surjective}. It restricts to a map $l^1 \to c_0^*$, which remains an isometry. Moreover, this isometry is actually surjective. Indeed, if $\lambda \in c_0^*$, then we can define a sequence $\{ b_n \}$ such that $b_n = \lambda(e_n)$. Then $\{ b_n \}$ lies in $l^1$, since
    %
    \begin{align*}
        |b_1| + \dots + |b_n| &= \lambda \left( \overline{b_1} / |b_1| \cdot e_1 + \dots + \overline{b_n} / |b_n| \cdot e_n \right)\\
        &\lesssim \| \overline{b_1} / |b_1| \cdot e_1 + \dots + \overline{b_n} / |b_n| \|_{l^\infty}\\
        &= 1,
    \end{align*}
    %
    and since this bound is independent of $n$, we can take $n \to \infty$. Now certainly $\lambda(a) = \langle a, b \rangle$ for any $a \in c_{00}$. But $c_{00}$ is dense in $c_0$, which justifies that $\lambda(a) = \langle a, b \rangle$ for any $a \in c_0$.
\end{example}

\begin{example}
    The Riesz representation theorem says that for any compact Hausdorff space $K$, the space $C(K)^*$ is isometric to the space of finite Radon measures $M(K)$ on $K$, where for $\mu \in M(K)$ and $f \in C(K)$, we have
    %
    \[ \langle f, \mu \rangle = \int f(x) d\mu(x). \]
    %
    It is clear that if $\lambda \in C(K)^*$ is induced by some measure $\mu$, then for any $U \subset K$, we have
    %
    \[ \mu(U) = \inf \{ \lambda(f): f \in C(K) \quad\text{and}\quad 1_U \leq f \}. \]
    %
    The Riesz representation theorem then rests on justifying that for any $\lambda \in C(K)^*$, the definition above gives a finite Radon measure $\mu$.
\end{example}

Since $d(x,0) = \| x \|$, the Hahn-Banach theorem justifies that there is $x^* \in B_{X^*}$ such that
%
\[ \langle x, x^* \rangle = \| x \|. \]
%
Thus one can measure the norm of an element of $X$ by testing it against linear functionals, a fact that is often useful since linear things are often easier to analyse than sublinear things. On the other hand, it is not necessarily true that for any $x^* \in X^*$, there \emph{exists} $x \in B_X$ with
%
\[ \langle x, x^* \rangle = \| x^* \|, \]
%
though certainly we have
%
\[ \sup_{\| x \| \leq 1} \langle x, x^* \rangle = \| x^* \|. \]
%
Such functionals are called \emph{norm attaining}. But not all functionals are norm attaining.

\begin{example}
    Let $x^* \in c_0^*$ be given by
    %
    \[ x^*(a) = \sum_{n = 1}^\infty 2^{-n} a_n. \]
    %
    Then $\| x^* \| = 1$, yet $|x^*(a)| < 1$ for any $a \in c_0$ with $\| a \| \leq 1$. Thus $x^*$ is not a norm attaining linear functional.
\end{example}

Given a bounded linear map $T: X \to Y$, we have a natural linear map $T^*: Y^* \to X^*$ given by the relation
%
\[ \langle T^* y^*, x \rangle = \langle y^*, Tx \rangle. \]
%
The map $T^*$ is called the \emph{adjoint} of $T$. If $T$ is surjective, then it is easy to see that $T^*$ is injective. Similarily, if $T^*$ is surjective, then $T$ is injective. But the converse of either of these statements is not true in general. But if $T$ is an isomorphism, then $T^*$ will also be an isomorphism, and if $T$ is an isometry, then $T^*$ will also be an isometry.

The duals of direct sums are easy to analyze.

\begin{theorem}
    If $X_1, \dots, X_n$ are norm spaces, then $(X_1 \oplus \dots \oplus X_n)^*$ is isomorphic to $X_1^* \oplus \dots \oplus X_n^*$. If we give $X_1 \oplus X_n$ the norm $l^p(X_1 \oplus \dots \oplus X_n)$, then $(X_1 \oplus \dots \oplus X_n)^*$ is isometric to $l^q(X_1 \oplus \dots \oplus X_n)$, where $p$ and $q$ are conjugate duals.
\end{theorem}
\begin{proof}
    Certainly any $\gamma \in (X_1 \oplus \dots \oplus X_n)^*$ induces unique $\gamma_1 \in X_1^*, \dots, \gamma_N \in X_n^*$ such that
    %
    \[ \gamma(x_1 \oplus \dots \oplus x_n) = \gamma_1(x_1) + \dots + \gamma_n(x_n), \]
    %
    so that we have a natural linear bijection from $(X_1 \oplus \dots \oplus X_n)^*$ to $X_1^* \oplus \dots \oplus X_n^*$. We have $\| \gamma_1 \|, \dots, \| \gamma_n \| \leq \| \gamma \|$, so this bijection is an isomorphism. By H\"{o}lder's inequality,
    %
    \begin{align*}
        |\gamma_1(x_1) + \dots + \gamma_n(x_n)| &\leq \| \gamma_1 \| \| x_1 \| + \dots + \| \gamma_n \| \| x_n \|\\
        &\leq \left( \| \gamma_1 \|^q + \dots + \| \gamma_n \|^q \right)^{1/q} \left( \| x_1 \|^p + \dots + \| x_n \|^p \right)^{1/p}\\
        &\leq \| \gamma_1 \oplus \dots \oplus \gamma_n \|_{l^q(X_1^* \oplus \dots \oplus X_n^*)} \| x_1 \oplus \dots \oplus x_n \|_{l^p(X_1 \oplus \dots \oplus X_n)}.
    \end{align*}
    %
    Thus $\| \gamma \| \leq \| \gamma_1 \oplus \dots \oplus \gamma_n \|_{l^q(X_1^* \oplus \dots \oplus X_n^*)}$. Conversely, picking $x_1 \in X_1, \dots, x_n \in X_n$ such that for each $i \in \{ 1, \dots, n \}$, $\| x_i \| = \| \gamma_i \|^{q-1}$, and $|\gamma_i(x_i) - \| \gamma_i \|^q| \leq \varepsilon$, gives
    %
    \begin{align*}
        \frac{|\gamma_1(x_1) + \dots + \gamma_n(x_n)|}{\left( \| x_1 \|^p + \dots + \| x_n \|^p \right)^{1/p}} &\geq \frac{\| \gamma_1 \|^q + \dots + \| \gamma_n \|^q}{\left( \| \gamma_1 \|^q + \dots + \| \gamma_n \|^q \right)^{1/p}} - O(\varepsilon)\\
        &= \left( \| \gamma_1 \|^q + \dots + \| \gamma_n \|^q \right)^{1/q} - O(\varepsilon).
    \end{align*}
    %
    Taking $\varepsilon \to 0$ proves the isometry.
\end{proof}

The duals of quotient spaces require some additional technology to analyze. Given a norm space $X$ and $A \subset X$, we let
%
\[ A^\perp = \{ x^* \in X^* : \langle a, x^* \rangle = 0\ \text{for all $a \in A$} \}, \]
%
and for $B \subset X^*$, we set
%
\[ {}^{\perp} B = \{ x \in X: \langle x, b \rangle = 0\ \text{for all $b \in B$} \}. \]
%
These are the \emph{annihilators} of $A$ and $B$ respectively.

\begin{theorem}
    Let $X$ be a norm space, let $A \subset X$, and let $B \subset X^*$. Then $A^\perp$ and ${}^{\perp} B$ are closed subspaces of $X$ and $X^*$, and ${}^{\perp} (A^\perp)$ is equal to $[A]$, the closed lienar space of $A$.
\end{theorem}
\begin{proof}
    We verify that ${}^{\perp} (A^\perp) = [A]$. Certainly $[A] \subset {}^{\perp} (A^\perp)$. Conversely, since $[A]$ is closed and convex, for any $x \in X - A$, we can find $x^* \in X^*$ containing $[A]$ in it's kernel, but with $x^*(x) \neq 0$. But then $x^* \in A^\perp$, but $x$ does not annihilate $x^*$, so $x \not \in {}^{\perp} (A^\perp)$.
\end{proof}

\begin{theorem}
    Let $X$ be a norm space, and $X_0 \subset X$ a subspace. Then $X_0^*$ is isometric to $X^* / X_0^\perp$.
\end{theorem}
\begin{proof}
    Certainly we have a map from $T: X^* \to X_0^*$, whose kernel is $X_0^\perp$, which is surjective thanks to the Hahn-Banach theorem, and thanks to that same theorem, also satisfies
    %
    \[ \| Tx^* \| = \inf_{x_0^* \in X_0^\perp} \| x^* + x_0^* \|, \]
    %
    which completes the proof.
\end{proof}

If $X_0 \subset X$ is a \emph{closed subspace}, we can consider $X / X_0$, and in particular, $(X/X_0)^*$.

\begin{theorem}
    Let $X$ be a norm space, and $X_0 \subset X$ a closed subspace. Then $(X/X_0)^*$ is isometric to $X_0^\perp$.
\end{theorem}
\begin{proof}
    The linear quotient map $\pi: X \to X / X_0$ has an adjoint $\pi^*: (X/X_0)^* \to X^*$, and for any $\lambda \in (X/X_0)^*$, $\pi^* \lambda \in X_0^\perp$ since if $x_0 \in X_0$,
    %
    \[ (\pi^* \lambda)(x_0) = \lambda(\pi(x_0)) = \lambda(0) = 0. \]
    %
    Moreover, $\pi^*$ is an isometry. Conversely, the first isomorphism theorem tlels us that the image of $\pi^*$ is $X_0^\perp$, so we conclude that $(X/X_0)^*$ is isomorphic to $X_0^\perp$.
\end{proof}

\section{Three Fundamental Theorems}

We now turn to fundamental results which are a consequence of the completeness of a Banach space, in particular, the Baire category theorem. These are the \emph{open mapping theorem}, the \emph{uniform boundedness principle}, and the \emph{closed graph theorem}. We will prove these three theorems as a consequence of a lemma called Zabreiko's Lemma.

\begin{lemma}
    Every countably subadditive seminorm on a Banach space is continuous.
\end{lemma}
\begin{proof}
    Let $X$ be a Banach space, and $\rho: X \to [0,\infty)$ a countably subadditive seminorm. Our goal is to find $s > 0$ and $\varepsilon > 0$ such that $\rho(x) < s$ whenever $\| x \| < \varepsilon$, because it would then follow by homogeneity that for any $x \in X$,
    %
    \[ \rho(x) < (s/\varepsilon) \| x \| \lesssim \| x \|, \]
    %
    which shows $\rho$ is continuous by virtue of the fact that
    %
    \[ |\rho(x) - \rho(y)| \leq \rho(x - y) \lesssim \| x - y \|. \]
    %
    Set
    %
    \[ G = \{ x \in X: \rho(x) < 1 \} \]
    %
    then $G$ is convex and absorbing, so $\overline{G}$ is closed, convex, and absorbing, and thus contains a neighborhood of the origin. Thus there is $\varepsilon > 0$ such that $x \in \overline{G}$ if $\| x \| < \varepsilon$. Given $x$ with $\| x \| < \varepsilon$, we can therefore find $x_0 \in G$ with $\| x - x_0 \| < \varepsilon / 2$. But this means that $x - x_0 \in \overline{G/2}$, so that we may find $x_1 \in G/2$ such that $\| x - x_0 - x_1 \| < \varepsilon / 4$. Continuing this process gives a sequence $\{ x_1, x_2, \dots \}$, with $x_i \in G/2^i$ for each $i \geq 0$, such that
    %
    \[ x = \sum_{i = 0}^\infty x_i. \]
    %
    But then by countably subadditivity, we conclude that
    %
    \[ \rho(x) \leq \sum_{i = 0}^\infty \rho(x_i) < \sum_{i = 0}^\infty 1/2^i \leq 2. \]
    %
    Thus setting $s = 2$ completes the proof of what was required.
\end{proof}

Now we prove the three important theorems of functional analysis, which relate \emph{qualitative properties} of an operator with \emph{quantitative properties}. We have already seen this kind of relation before, where we saw a linear operator was continuous (a qualitative property), if and only if it is bounded (a quantitative property), though the three theorems we now describe offer a deeper relation of this form. The first result is also known as the Banach-Steinhaus Theorem.

\begin{theorem}[The Uniform Boundedness Theorem]
    Let $\{ T_\alpha: X \to Y \}$ be a family of bounded operators from a Banach space $X$ to a norm space $Y$. Then the following are equivalent:
    %
    \begin{itemize}
        \item (Uniform Boundedness): $\sup_\alpha \| T_\alpha \| < \infty$.
        \item (Pointwise Boundedness): For any $x \in X$, $\sup_\alpha \| T_\alpha x \| < \infty$.
    \end{itemize}
\end{theorem}
\begin{proof}
    Let $\rho(x) = \sup_\alpha \| T_\alpha x \|$. If the family of operators is pointwise bounded, then $\rho$ is finite-valued, and countably subadditive. It follows that $\rho$ is continuous, i.e. we have $\rho(x) \leq M \| x \|$ for all $x \in X$. But this means that for all $x \in X$
    %
    \[ \sup_\alpha \| T_\alpha x \| \leq M \| x \|, \]
    %
    and thus $\sup_\alpha \| T_\alpha \| \leq M$.
\end{proof}

\begin{corollary}
    Let $\{ T_\alpha: X \to Y \}$ be a net of bounded linear operators between two Banach spaces $X$ and $Y$. Then the following are equivalent:
    %
    \begin{itemize}
        \item For every $x \in X$, $\lim_\alpha T_\alpha x$ converges as $\alpha \to \infty$.
        \item There is a bounded operator $T: X \to Y$ such that $Tx = \lim_\alpha T_\alpha x$ for all $x \in X$.
        \item The operators $\{ T_\alpha \}$ are uniformly bounded, and there exists a dense subspace $X_0$ of $X$ such that $\lim_\alpha T_\alpha x$ exists for all $x \in X_0$.
    \end{itemize}
\end{corollary}
\begin{proof}
    Assume the first point. The conditions imply that the family $\{ T_\alpha \}$ is pointwise bounded, and thus uniformly bounded, which means that if we define $T: X \to Y$ by the limit, then
    %
    \[ \| Tx \| \leq \sup_\alpha \| T_\alpha x \| \leq \sup_\alpha \| T_\alpha \| \| x \|. \]
    %
    If $T$ is bounded, then the family $\{ T_\alpha \}$ is pointwise bounded, and thus uniformly bounded, which implies the third point. Finally, if the operators $\{ T_\alpha \}$ are uniformly bounded (let's say, with $\| T_\alpha \| \leq M$ for all $\alpha$), and a dense subspace $X_0$ of the form above exists, then we claim that for each $x \in X$, the net $\{ T_\alpha x \}$ is Cauchy, and thus converges. Indeed, for any $\varepsilon > 0$, there exists $x_0 \in X_0$ such that $\| x - x_0 \|_X \leq \varepsilon / 2M$. Since $T_\alpha x_0$ converges, there exists $\alpha_0$ such that for $\alpha_1,\alpha_2 \geq \alpha_0$, $\| T_{\alpha_1} x_0 - T_{\alpha_2} x_0 \| \leq \varepsilon / 2$. But this means that for those same $\alpha_1$ and $\alpha_2$,
    %
    \[ \| T_{\alpha_1} x - T_{\alpha_2} x \| \leq \| T_{\alpha_1} x - T_{\alpha_1} x_0 \| + \| T_{\alpha_2} x - T_{\alpha_2} x_0 \| + \| T_{\alpha_1} x_0 - T_{\alpha_2} x_0 \| \leq \varepsilon. \]
    %
    Thus we have shown that the net is Cauchy, and thus must converge.
\end{proof}

A classic example of this result being applied is in the theory of Fourier series. If $f \in L^2(\TT)$, then one can consider the partial summation operators $S_N: L^2(\TT) \to L^2(\TT)$ such that
%
\[ S_N f(t) = \sum_{n = -N}^N \widehat{f}(n) e^{nit}, \]
%
and orthogonality shows that $\| S_N f \|_{L^2(\TT)} \leq \| f \|_{L^2(\TT)}$ for all $N > 0$. What's more, we can verify that for $f \in C^\infty(\TT)$, $S_N f$ converges uniformly to $f$ as $N \to \infty$, and thus converges to $f$ in the $L^2(\TT)$ norm. The last corollary thus implies that for any $f \in L^2(\TT)$, $S_N f$ converges in $L^2(\TT)$ to $f$ as $N \to \infty$.

On the other hand, if we consider the analogous operators $S_N: C(\TT) \to C(\TT)$, then these operators are \emph{not} uniformly bounded, namely $\| S_N \| \gtrsim \log(N)$. By the uniform boundedness theorem, these operators cannot be \emph{pointwise bounded}, i.e. there exists $f \in C(\TT)$ such that $\{ S_N f \}$ is not bounded in $C(\TT)$. Thus we have proved (nonconstructively) the existence of an integrable function $f$ whose partial Fourier sums does not converge uniformly to $f$. It is quite difficult to actually construct a \emph{particular} example of a continuous function whose Fourier sums do not converge uniformly to $f$, which shows the power of nonconstructive techniques.

Now we move on to the open mapping theorem, which related qualitative and quantitative solvability.

\begin{theorem}[Open Mapping Theorem]
    Let $T: X \to Y$ be a bounded linear map between Banach spaces. Then $T$ is surjective if and only if $T$ is open.
\end{theorem}
\begin{proof}
    Suppose that $T$ is an open map. Then $T(B_X)$ contains an open neighborhood of $B_Y$, which must be absorbing, which implies $T(X) = Y$. Thus $T$ is surjective. Conversely, suppose $T$ is surjective. To show $T$ is open, it suffices to show that $U = T(B_X^\circ)$ is open. Let $\rho(y) = \inf \{ \| x \| : Tx = y \}$. Then $\rho$ is a countable subadditive seminorm on $Y$, and thus continuous. But this means $U = \{ y \in Y: \rho(y) < 1 \}$ is an open set, completing the proof.
\end{proof}

A bounded linear map $T: X \to Y$ is open if and only if there exists $M > 0$ such that for any $y \in Y$, there is $x \in X$ such that $Tx = y$, and $\| x \| \leq M \| y \|$. This formulation shows that a linear map is open precisely when the equation $Tx = y$ is solvable for each $y \in Y$ in a \emph{quantitatively-stable sense} for each $y \in Y$. The open mapping theorem says this quantitative solvability is equivalent to the qualitative property of being surjective, i.e. the equation $Tx = y$ merely being solvable for each $y \in Y$. Note, however, that working in the quantitatively-stable regime, like in the uniform boundedness theorem, has several advantages. For instance, we need only solve the equation stably for each $y$ lying in some dense subspace $Y_0$ of $Y$, since the quantitative result then follows for all elements of $Y$ by an approximation argument.

This technique is used all the time, for instance, in partial differential equations, where we show that if $Lf = g$ for smooth $f$ and $g$, then $\| f \|_X \leq \| g \|_X$ (using an appropriate norm space $X$ containing smooth functions), and then the open mapping justifies that the equation is solvable for any given $g \in X$, which is not necessarily smooth.

\begin{corollary}
    A bijective, bounded linear map $T: X \to Y$ between Banach spaces is an isomorphism.
\end{corollary}

If $T: X \to Y$ is a surjective map, then there exists a linear map $S: Y \to X$ such that $T \circ S$ is the identity. It is tempting to conclude that we can find a choice of $S$ which is bounded. But one must be careful, because there may not be a \emph{bounded} choice of $S$ when the problem is underdetermined (the corollary above shows that this is not a problem when the problem is determined, i.e. a unique $S$ exists).

If we take $X_1$ to be the kernel of $T$, then we see that the problem is essentially equivalent to finding a closed subspace $X_2$ of $X$ such that $X$ is the algebraic direct sum of $X_1$ and $X_2$, because then $T$ restricts to an isomorphism between $X_2$ and $Y$. Thus the problem is equivalent to finding a \emph{complementary subspace} of $X$ for $X_1$. We will see in the next chapter that finding complementary subspaces is easy when $X$ is a Hilbert space. But in general, complementary subspaces of Banach spaces do not exist (and by a result of Lindenstrauss-Tzafriri, which says any infinite dimensional Banach space which isn't a Hilbert space contains a closed subspace which is not complemented, we should expect finding these complements to be a nontrivial task in any other regime).

Now we turn to the closed graph theorem. Recall that a function $f: X \to Y$ between two topological spaces is closed when it's graph $\Gamma(f)$ is a closed subset of $X \times Y$. This is a weaker notion than continuity. If $X$ and $Y$ are metric spaces, then $f$ is closed precisely under the condition that whenever $\{ x_n \}$ is a sequence in $X$ converging to some $x \in X$, and $\{ f(x_n) \}$ converges to some $y \in Y$, then $Tx = y$.

\begin{theorem}[Closed Graph Theorem]
    Let $T: X \to Y$ be a linear map between Banach spaces. Then $T$ is bounded if and only if it is closed.
\end{theorem}
\begin{proof}
    Let $\rho(x) = \| Tx \|$. The continuity of $\rho$ would prove the claim, so it suffices to show that $\rho$ is countable subadditive. If $\{ x_n \}$ is a sequence in $X$ such that $\sum_{n = 1}^\infty x_n$ converges, then it suffices to show that
    %
    \[ \| T(\sum x_n) \| \leq \sum \| Tx_n \|. \]
    %
    We may therefore assume without loss of generality that $\sum \| Tx_n \| < \infty$, which means that $\sum Tx_n$ converges absolutely. But since $T$ is closed, we then know that $T(\sum_{n = 1}^\infty x_n) = \sum_{n = 1}^\infty Tx_n$, which gives the required subadditivity.
\end{proof}

One method to verify an operator is closed, separate from the direct method, is to find a Hausdorff topology on $Y$ which is coarser than the norm topology, but such that $T$ is continuous from $X$ to $Y$ in this new topology. If $\{ x_n \}$ converges to $x \in X$, then it follows that $\{ Tx_n \}$ converges to $Tx$ in this coarser topology. If it also converges to $y$ in the norm topology, then it must also converge to $y$ in the coarser topology, so $Tx = y$, i.e. $T$ is closed.

We might state this in a slightly more elegant way. Let $X$ and $Y$ be Banach spaces, and let $Z$ be a Hausdorff topological vector space containing $Y$ as a subspace. Let $T: X \to Z$ be a continuous linear map. Then $Tx \in Y$ for all $x \in X$ (qualitative regularity) if and only if we have a bound of the fomr $\| Tx \|_Y \lesssim \| x \|_X$ for all $x \in X$ (or only on a dense subclass of $X$).

This kind of approach is often used in harmonic analysis / partial differential equations, where we might only be able to justify that $T$ is a continuous map from some function space $X$ to, say, a space of distributions $Z$, and we must then work to justify that $T$ actually induces some additional regularity properties. For instance, one can easily justify that the Fourier transform acts as a continuous linear operator $\mathcal{F}: L^p(\RR^d) \to \mathcal{S}(\RR^d)^*$. Provided that we can show that for any $f \in \mathcal{S}(\RR^d)$, and $1 \leq p \leq 2$, the Hausdorff-Young inequality
%
\[ \| \mathcal{F} f \|_{L^{p^*}(\RR^d)} \leq \| f \|_{L^p(\RR^d)} \]
%
is true, the closed graph theorem shows that for \emph{any} $f \in L^p(\RR^d)$, we actually have $\mathcal{F} f \in L^{p^*}(\RR^d)$. Here is a somewhat analogous example.

\begin{theorem}
    Suppose $T: L^p(X) \to L^q(X)$ is a linear operator such that for any sequence $\{ f_n \}$ in $L^p(X)$ which converges almost everywhere to a function $f \in L^p(X)$, the sequence $\{ Tf_n \}$ converges almost everywhere to $Tf \in L^q(X)$. Then $T$ is a bounded operator.
\end{theorem}
\begin{proof}
    We apply the closed graph theorem. Suppose $\{ f_n \}$ is a sequence converging to some function $f$ in the $L^p$ norm, and $Tf_n$ converges to $g$ in the $L^q$ norm. Then $\{ f_n \}$ has a subsequence $\{ f_{n_k} \}$ converging almost everywhere to $f$, so $Tf_{n_k}$ converges almost everywhere to $Tf$. But this implies that $Tf = g$, since $\{ Tf_{n_k} \}$ must have a further subsequence converging almost everywhere to $g$.
\end{proof}

\section{Quotient Spaces and Direct Sums}

Let $X$ be a norm space, and $Y$ a closed subspace. We can then associate a natural norm on the quotient space $X/Y$. We define
%
\[ \| x + Y \| = \inf_{y \in Y} \| x + y \|. \]
%
Since $Y$ is closed, this is a norm, not merely a seminorm. If $X$ is a Banach space, then $X/Y$ is a Banach space. More generally, being a Banach space is a {\it three space property}, in the sense that if any two of the spaces $\{ X, Y, X/Y \}$ are Banach spaces, then the third space is also a Banach space. The only non trivial statement contained in this three space property is that if $Y$ and $X/Y$ are Banach spaces, then $X$ is a Banach space.

\begin{lemma}
    Let $X$ be a norm space. If $Y$ is a closed subspace of $X$, which is a Banach space, and the norm space $X/Y$ is also a Banach space, then $X$ is a Banach space.
\end{lemma}
\begin{proof}
    Suppose $\{ x_n \}$ is a Cauchy sequence in $X$, then $\{ x_n + Y \}$ is a Cauchy sequence in $X/Y$, and so converges to some $x + Y$. We may therefore select some sequence $\{ y_n \}$ in $Y$ and some $y \in Y$ such that $x_n + y_n$ converges in $X$ to $x + y$. But the fact that $\{ x_n \}$ is a Cauchy sequence then implies that $\{ y_n \}$ is also a Cauchy sequence, and thus converges to some $y_0 \in Y$. And then we find that the sequence $\{ x_n \}$ converges to $x + (y - y_0)$. Thus we have proven that $X$ is a Banach space.
\end{proof}

The following result is expected if one has dealt with quotients in other categories of mathematical objects before.

\begin{theorem}
    Let $T: X \to Y$ be a (not necessarily continuous) linear map, and let $Z$ be a closed subspace of the kernel of $T$. Then there is a unique map $S: X/Z \to Y$ such that $T = S \circ \pi$, $T$ is bounded if and only if $S$ is bounded, and $\| T \| = \| S \|$.
\end{theorem}
\begin{proof}
    The existence and uniqueness of $S$ is provided by linear algebra. If $T$ is bounded, then
    %
    \[ \| S(x + Z) \| = \| Tx \| \lesssim \| x \|, \]
    %
    Thus
    %
    \[ \| S(x + Z) \| \lesssim \inf_{z \in Z} \| x + z \| = \| x + Z \|, \]
    %
    which shows $S$ is bounded and $\| S \| \leq \| T \|$. Conversely, if $S$ is bounded, then $T = S \circ \pi$ is bounded, and $\| T \| \leq \| S \| \| \pi \| = \| S \|$.
\end{proof}

This theorem leads to a `first isomorphism theorem' for Banach spaces. If $T: X \to Y$ is a bounded linear map, and $K$ is the kernel then the induced map $S: X/K \to Y$ is an isomorphism. We conclude with an application to the theory of finite rank operators.

The quotient map $\pi: X \to X/Y$ is a continuous, surjective linear map, since for any $x \in X$,
%
\[ \| x + Y \|_{X/Y} \leq \| x \|_X. \]
%
The open mapping theorem thus implies that $\pi$ is an open map. We could see this in a more elementary way from the fact that $\pi(B_X^\circ) = \pi(B_{X/Y}^\circ)$. The theory of quotient spaces is closely connected to open linear maps, because if $T: X \to Y$ is any bounded, surjective linear map, then $T$ induces an isomorphism $T_0: X_0 \to Y$, where $X_0$ is the quotient of $X$ by the kernel of $T$. Thus we may view any surjective linear map as equivalent to a quotient map, up to an isomorphism of the underlying Banach spaces.

\begin{example}
    The sequence space $c_0$ forms a closed subspace of $l^\infty$, so we can consider the quotient space $l^\infty / c_0$. We claim that for any sequence $a \in l^\infty$,
    %
    \[ \| a + c_0 \| = \limsup_{n \to \infty} |a_n|. \]
    %
    Let $I_N: l^\infty \to \l^\infty$ be the bounded operator such that $(I_N a)_n = 0$ for $n \leq N$, and $(I_N a)_n = a_n$ for $n > N$. Then $a - I_N a \in c_0$ for all $a \in l^\infty$, and so for all $N$, $\| a + c_0 \| \leq \| I_N a \|_{l^\infty}$. But this means that
    %
    \[ \| a + c_0 \| \leq \inf_N \| I_N a \|_{l^\infty} = \limsup_{n \to \infty} |a_n|. \]
    %
    Conversely, if $b \in c_0$, then
    %
    \[ \| a + b \|_{l^\infty} = \sup_n |a_n + b_n| \geq \limsup_{n \to \infty} |a_n + b_n| = \limsup_{n \to \infty} |a_n|. \]
    %
    Thus we see that $\| a + c_0 \| = \limsup_{n \to \infty} |a_n|$.
\end{example}

\begin{theorem}
    Suppose $T: X \to Y$ is a finite rank linear map from $X$ to $Y$. Then $T$ is bounded if and only if it's kernel is closed in $X$.
\end{theorem}
\begin{proof}
    Suppose the kernel $K$ of $T$ is closed. Then $X/K$ is a finite dimensional norm space, and so $S: X/K \to Y$ is automatically continuous, hence $T$ is continuous.
\end{proof}

If $T$ is a linear functional, a much stronger statement can be made.

\begin{theorem}
    Let $T$ be a non-zero linear functional on $X$. Then $T$ is bounded if and only if it's kernel $K$ is not dense in $X$.
\end{theorem}
\begin{proof}
    The kernel $K$ of $T$ has codimension one in $X$. If $K$ is not dense in $X$, then $\overline{K}$ is a closed, proper subspace of $X$, which implies $K = \overline{K}$, and thus the last theorem implies that $T$ is continuous.
\end{proof}

Another application deals with sums of closed subspaces. If $M$ and $N$ are closed subspaces of a norm space $X$, then $M + N$ need not be closed. But in one special case, the sum is closed.

\begin{theorem}
    Let $M$ be a finite dimensional subspace of $X$, and let $N$ be a closed subspace. Then $M + N$ is closed.
\end{theorem}
\begin{proof}
    Since $N$ is closed, we can consider the quotient space $X/N$. For the resultant projection map $\pi: X \to X/N$, $\pi(M + N) = \pi(M)$ is a finite dimensional subspace of $X/N$, hence closed. By continuity, this implies that $\pi^{-1}(\pi(M)) = M + N$ is closed.
\end{proof}

The opposite of a quotient, which shrinks a space, is the \emph{direct sum}, which builds bigger spaces out of smaller ones. Given a family of norm spaces $X_1, \dots, X_N$, we can equip the algebraic direct sum $X_1 \oplus \dots \oplus X_N$ with various norms, e.g. by defining
%
\[ \| x_1 \oplus \dots \oplus x_n \|_{l^p(X_1 \oplus \dots \oplus X_n)} = \left( \sum \| x_i \|_{X_i}^p \right)^{1/p}. \]
%
The norms here are all equivalent since we are taking a finite direct sum, but the choice $p = 2$ is often the most natural, since if $X_1, \dots, X_N$ are Hilbert spaces, then the direct product will also be a Hilbert space. The induced topology on $X_1 \oplus \dots \oplus X_N$ is precisely the product topology. In particular, if each of the component spaces is a Banach space, the direct sum will also be a Banach space.

We can also take internal direct sums. If $X$ is a norm space, and contains $n$ closed subspaces $X_1, \dots, X_n$ such that every element of $X$ can be written uniquely as $x_1 + \dots + x_n$ for $x_1 \in X_1, \dots, x_n \in X_n$, then we say $X$ is the \emph{internal direct sum} of the space $\{ X_1, \dots, X_n \}$. We have a natural map $T: X_1 \oplus \dots \oplus X_n \to X$, which is continuous since
%
\[ \| T(x_1 \oplus \dots \oplus x_n) \| = \| x_1 + \dots + x_n \|_X \leq \| x_1 \| + \dots + \| x_n \| = \| x_1 \oplus \dots \oplus x_n \|_{l^1(X_1 \oplus \dots \oplus X_n)}. \]
%
Thus by the open mapping theorem, we actually have
%
\[ \| x_1 + \dots + x_n \|_X \sim \| x_1 \|_X + \dots + \| x_n \|_X. \]
%
Thus $X$ is the direct product of the space $X_1, \dots, X_n$.

\section{The Hahn-Banach Theorem}

A common problem in functional analysis is the problem of \emph{extension}. Given a bounded linear map $T: X_0 \to Y$, where $X_0$ and $Y$ are norm spaces and $X_0$ is a subspace of a larger norm space $X$, the problem is to extend $T$ to a bounded linear map $T: X \to Y$, in such a way that we do not drastically increase the operator norm in this exension. This is possible, for instance, if $X_0$ is dense in $X$.

\begin{lemma}
    Let $X$ be a norm space, $X_0$ a dense subspace of $X$, and $Y$ a Banach space. Then any bounded linear map $T: X_0 \to Y$ extends uniquely to a bounded linear map $T: X \to Y$ while preserving the operator norm of $T$.
\end{lemma}

Problem begin emerging as soon as we remove the assumption that $X_0$ is a dense subspace of $X$. For instance, it turns out that there is \emph{no way} to extend the identity map $T: c_0 \to c_0$ to a bounded linear operator from $l^\infty$ to $c_0$. The Hahn-Banach theorem shows that one can extend \emph{linear functionals} defined on proper subspaces without increasing the overall norm of the space. We begin with the `bare bones' version of the Hahn-Banach theorem, which does not even require we are working in a norm space.

\begin{theorem}[Hahn-Banach Theorem]
    Let $X$ be a real vector space, and let $\rho: X \to \RR$ be a sublinear functional on $X$, i.e. $\rho(tx) = t \rho(x)$ for $t > 0$, and $\rho(x_1 + x_2) \leq \rho(x_1) + \rho(x_2)$ for any $x_1,x_2 \in X$. Let $X_0$ be a subspace of $X$, and suppose $\lambda: X_0 \to \RR$ is a linear functional with the property that $\lambda(x_0) \leq \rho(x_0)$ for all $x_0 \in X_0$. Then $\lambda$ extends to a linear function from $X$ to $\RR$, such that $\lambda(x) \leq \rho(x)$ for all $x \in X$.
\end{theorem}
\begin{proof}
    We apply Zorn's Lemma. Let $\lambda: X' \to \RR$ be an extension of $\lambda$ to a linear functional upper bounded by $\rho$. Suppose $X'$ is not all of $X$. Then there is some vector $x_0 \in X - X'$. Now for any $x_1,x_2 \in X'$, we have
    %
    \begin{align*}
        \lambda(x_1) + \lambda(x_2) &= \lambda(x_1 + x_2)\\
        &= \lambda(x_1 - x_0) + \lambda(x_2 + x_0)\\
        &\leq \rho(x_1 - x_0) + \rho(x_2 + x_0).
    \end{align*}
    %
    Thus
    %
    \[ \lambda(x_1) - \rho(x_1 - x_0) \leq \rho(x_2 + x_0) - \lambda(x_2). \]
    %
    It follows that
    %
    \[ \sup_{x' \in X'} \lambda(x') - \rho(x' - x_0) \leq \inf_{x' \in X'} \rho(x' + x_0) - \lambda(x'). \]
    %
    Pick some value $t$ in between these values, and define $\lambda$ on $X' \oplus \langle x_0 \rangle$ by setting
    %
    \[ \lambda(x' + sx_0) = \lambda(x') + st. \]
    %
    Then for any $s \geq 0$,
    %
    \[ \lambda(x' + sx_0) = s(\lambda(x'/s) + t) \leq s \rho(x'/s + x_0) = \rho(x' + sx_0). \]
    %
    and
    %
    \[ \lambda(x' - sx_0) = s(\lambda(x'/s) - t) \leq s \rho(x'/s - x_0) = \rho(x' - sx_0). \]
    %
    Thus we see that $\lambda(x) \leq \rho(x)$ on $X' \oplus \langle x_0 \rangle$. Applying Zorn's lemma then shows that it is possible to extend $\lambda$ to all of $X$.
\end{proof}

Since any norm is a sublinear functional, we obtain an immediate consequence for norm spaces.

\begin{corollary}
    Let $X$ be a norm space with a subspace $X_0$, and let $\lambda \in X^*$ be a bounded linear functional on $X_0$. Then $\lambda$ extends to a linear functional on $X$ preserving the operator norm of $\lambda$.
\end{corollary}
\begin{proof}
    The proof is immediate if $X$ is a real norm space. If $X$ is a norm space over the complex numbers, then we may view $X$ as a real norm space. If we let $\gamma = \text{Re}(\lambda)$, then $\gamma$ is a real linear functional with $\| \gamma \| = \| \lambda \|$. Thus the real case gives us an extension of $\gamma$ to $X$ preserving the operator norm. But we may then define
    %
    \[ \lambda(x) = \gamma(x) - i \gamma(ix) \]
    %
    which is complex linear.
\end{proof}

\begin{remark}
    The extension here is certainly not unique, unless $X_0$ is a dense subspace of $X$.
\end{remark}

The powerful consequence of this result is that it ensures that any norm space has a large family of linear functionals, which allows us to show that the geometric features of $X^*$ reflect the geometric features of $X$. In particular, the functionals of $X^*$ \emph{separate} $X$.

\begin{theorem}
    Let $X$ be a norm space containing a closed subspace $X_0$, and suppose that $x \in X - X_0$. Then there is a linear functional $\lambda \in X^*$ such that $\lambda(x) = d(x,X_0)$, $\lambda$ vanishes on $X_0$, and $\| \lambda \| = 1$.
\end{theorem}
\begin{proof}
    Assume first that $X$ is a norm space over the real numbers. Define $\lambda$ initially on $X_0 + \langle x \rangle$ by setting $\lambda(x_0 + tx) = t d(x,X_0)$ for any $t \in \RR$ and $x_0 \in X$. We verify that
    %
    \[ | t d(x,X_0)| = |d(tx, X_0)| \leq \| tx + x_0 \| \]
    %
    so $\| \lambda \| \leq 1$. But also we can take a sequence $x_i \in X_0$ such that $\| x - x_i \| \to d(x,X_0)$, and then
    %
    \[ d(x,X_0) = \lambda(x - x_i) = \lim_{i \to \infty} \| x - x_i \| \]
    %
    which implies $\| \lambda \| = 1$. Hahn-Banach now extends $\lambda$ to all of $X$ while maintaining the operator norm, completing the proof.
\end{proof}

\begin{corollary}
    If $x_1 \neq x_2 \in X$, then there is $\lambda \in X^*$ with $\lambda(x_1) \neq \lambda(x_2)$.
\end{corollary}

\begin{corollary}
    If $X$ is a norm space, and $X_0$ a \emph{finite dimensional} subspace, then there exists a closed subspace $X_1$ of $X$ such that $X = X_0 \oplus X_1$.
\end{corollary}
\begin{proof}
    Let $\{ x_1, \dots, x_n \}$ be a basis for $X_0$. Hahn-Banach enables us to find $\lambda_1,\dots,\lambda_n \in X^*$ such that $\lambda_i(x_j) = \delta_{ij}$. Consider the linear operator $\lambda: X \to \mathbf{K}^n$ such that
    %
    \[ \lambda(x) = (\lambda_1(x), \dots, \lambda_n(x)). \]
    %
    Let $X_1$ be the kernel of $\lambda$. Then $X_0 \cap X_1 = \{ 0 \}$, and $X_0 + X_1 = X$, so $X = X_0 \oplus X_1$.
\end{proof}

The space $X_1$ is called a \emph{complementary subspace} of $X_0$. Thus we have argued that finite dimensional subspaces of norm spaces always have complementary subspaces. It is not always true that any subspace of a Banach space has a complementary subspace. For instance, if we could write $l^\infty = c_0 \oplus X$ for some closed subspace $X$ of $l^\infty$, then we could easily extend the identity map from $c_0$ to itself to $l^\infty$. Thus $c_0$ does not have a complementary subspace in $l^\infty$.

Historically, the Hahn-Banach theorem arose out of problems involving the solutions of linear equations, and the Hahn-Banach theorem has useful applications to such problems.

\begin{theorem}
    Let $X$ be a norm space, $A$ a nonempty subset of $X$, and $\{ c_x : x \in A \}$ a collection of scalars. Then there is a bounded linear functional $\lambda \in X^*$ such that $\lambda(x) = c_x$ for all $x \in A$, if and only if for any $\alpha_1,\dots,\alpha_n \in k$ and $x_1, \dots, x_n \in A$,
    %
    \[ |\alpha_1 c_{x_1} + \dots + \alpha_n c_{x_n}| \lesssim \| \alpha_1 x_1 + \dots + \alpha_n x_n \|. \]
\end{theorem}
\begin{proof}
    If the inequality holds, we can define $\lambda$ on $\langle A \rangle$ by setting
    %
    \[ \lambda(\alpha_1 x_1 + \dots + \alpha_n x_n) = \alpha_1 c_{x_1} + \dots + \alpha_n c_{x_n}. \]
    %
    This is well defined because if $\alpha_1 x_1 + \dots + \alpha_n x_n = \beta_1 y_1 + \dots + \beta_n y_n$,
    %
    \[ |\alpha_1 c_{x_1} + \dots + \alpha_n c_{x_n} - \beta_1 c_{y_1} - \dots - \beta_m c_{y_m} | \lesssim \| \alpha_1 x_1 + \dots + \alpha_n x_n - \beta_1 y_1 - \dots - \beta_m y_m \| = 0, \]
    %
    so $\alpha_1 c_{x_1} + \dots + \alpha_n c_{x_n} = \beta_1 c_{y_1} + \dots + \beta_m c_{y_m}$. We have $|\lambda(x)| \lesssim \| x \|$ and so we can now apply Hahn-Banach to extend $\lambda$ to everywhere.
\end{proof}

By interchanging the roles of points and linear functionals in this theorem, we obtain \emph{Helly's theorem}, though only under the assumption that we have a \emph{finite} collection of linear functionals, and the norm may be enlarged.

\begin{lemma}
    Let $\lambda_1, \dots, \lambda_n$ and $\lambda$ be linear functionals on the same vector space. Then $\lambda$ is a linear combination of the functions $\lambda_1,\dots,\lambda_n$ if and only if $\text{Ker}(\lambda_1) \cap \dots \cap \text{Ker}(\lambda_n) \subset \text{Ker}(\lambda)$.
\end{lemma}
\begin{proof}
    The condition is certainly necessary. Conversely, suppose the condition is true. We prove the result by induction. If $n = 1$, then the result is obvious. More generally, if we restrict $\lambda$ to the kernel of $\lambda_n$, then by induction, we can find $\alpha_1, \dots, \alpha_{n-1}$ such that $\lambda = \alpha_1 \lambda_1 + \dots + \alpha_{n-1} \lambda_{n-1}$ on the kernel of $\lambda_n$. But then the kernel of $\lambda_n$ is contained in the kernel of $\lambda - \alpha_1 \lambda_1 + \dots + \alpha_{n-1} \lambda_{n-1}$, so we can write $\lambda - \alpha_1 \lambda_1 - \dots - \alpha_{n-1} \lambda_{n-1} = \alpha_n \lambda_n$, and then $\lambda = \alpha_1 \lambda_1 + \dots + \alpha_n \lambda_n$.
\end{proof}

\begin{theorem}[Helly]
    Let $X$ be a norm space, and let $\lambda_1,\dots,\lambda_n \in X^*$ be linear functionals on $X$. Then for any $\alpha_1,\dots,\alpha_n \in k$, there exists $x \in X$ such that $\lambda_i(x) = \alpha_i$, if and only if there is $M > 0$ such that
    %
    \[ |\alpha_1 c_1 + \dots + \alpha_n c_n | \leq M \| \alpha_1 \lambda_1 + \dots + \alpha_n \lambda_n \|, \]
    %
    and then for any $\varepsilon > 0$, we can pick a choice of $x$ such that $\| x \| \leq M + \varepsilon$.
\end{theorem}
\begin{proof}
    It is clear that the inequality is necessary in order for $x$ to exist. Conversely, if the conditions hold, we may assume without loss of generality that $\lambda_1, \dots, \lambda_n$ are linear independant, and that at least one of the $\alpha_i$ is nonzero (or else we can set $x = 0$). For any $i$, we therefore know that $\text{Ker}(\lambda_i)$ does not contain $\bigcap_{j \neq i} \text{Ker}(\lambda_j)$, which means that there is $x_1, \dots, x_n \in X$ such that $\lambda_i(x_j) = \delta_{ij}$. In particular, if we define $Tx = (\lambda_1 x, \dots, \lambda_n x)$, then $T$ is surjective. In particular, there is $x_0 \in X$ such that $\lambda_i(x_0) = \alpha_i$ for each $i$. Now $x \not \in \bigcap \text{Ker}(\lambda_i)$, so we can find $\gamma \in X^*$ with $\| \gamma \| = 1$, $\gamma$ vanishing on $\bigcap_i \text{Ker}(\lambda_i)$, and with $\gamma(x_0) = d(x_0,\bigcap \text{Ker}(\lambda_i))$. Then $\bigcap_i \text{Ker}(\lambda_i) \subset \text{Ker}(\gamma)$, and so we can write $\gamma = \beta_1 \lambda_1 + \dots + \beta_n \lambda_n$ for some $\beta_1,\dots,\beta_n \in k$. Thus
    %
    \[ d(x_0, \bigcap_i \text{Ker}(\lambda_i)) = \gamma(x_0) = \beta_1 \alpha_1 + \dots + \beta_n \alpha_n \leq M \| \gamma \| = M. \]
    %
    Thus for any $\varepsilon > 0$, there is $z \in \bigcap_i \text{Ker}(\lambda_i)$ such that $\| x_0 - z \| \leq M + \varepsilon$. Setting $x = x_0 - z$ then gives the required choice of $x$.
\end{proof}

Finally, we deal with the \emph{geometric} interpretation of the Hahn-Banach theorem. Given two non-empty convex subsets $C_1$ and $C_2$ of $X$, a natural problem is to find a closed hyperplane which `separates' them. This is equivalent to finding a non-zero linear functional $\lambda \in X^*$ such that
%
\[ \sup_{x \in C_1} \lambda(x) \leq \inf_{x \in C_2} \lambda(x). \]
%
Separations of this form relate to the standard Hahn-Banach theorem because we have a kind of duality between positive homogeneous functions and convex sets. More precisely, given a non-negative positive homogeneous function $\rho$, we can associate the absorbing sets
%
\[ A_\rho = \{ x \in X: \rho(x) < 1 \} \quad \text{and} \quad A_\rho^* = \{ x \in X: \rho(x) \leq 1 \}. \]
%
Conversely, given an absorbing set $A$, we can associate the positive homogeneous function
%
\[ \rho_A(x) = \inf \{ t > 0 : x \in t \cdot A \}, \]
%
known as the \emph{Minkowski functional} of $A$. It is easy to see that for any absorbing set $A$, we have $A \subset A_{\rho_A}^*$. Conversely, we always have $\rho = \rho_{A_\rho}$. The properties of functionals and absorbing sets also reflect one another:
%
\begin{itemize}
    \item If $A$ is convex, then $\rho_A$ is sublinear, and $A_{\rho_A} \subset A \subset A_{\rho_A}^*$. Conversely, if $\rho$ is sublinear, then $A_\rho$ is convex.
    \item If $A$ is balanced, then $\rho_A$ is homogeneous. Conversely, if $\rho$ is homogeneous, then $A_\rho$ is balanced.
\end{itemize}
%
In particular, if $A$ is absorbing, convex, and balanced, then $\rho_A$ is a seminorm. And if $\rho$ is a seminorm, then $A_\rho$ is absorbing, convex and balanced.

\begin{theorem}
    Let $X$ be a real norm space, and let $C \subset X$ be a nonempty convex set. If $x_0 \in X$ satisfies $d(x_0,C) > 0$, then there is $\lambda \in X^*$ such that $\lambda(x_0) > \sup_{x \in C} \lambda(x)$.
\end{theorem}
\begin{proof}
    Assume first that $C$ is an open convex set containing the origin. Then $C$ is absorbing, and we can consider the Minkowski funcitonal $\rho_C$. Since $d(x,C) > 0$, there is $0 < s_0 < 1$ such that $sx_0 \not \in C$ for $s > s_0$. Thus
    %
    \[ \rho_C(x_0) \geq s_0^{-1} > \sup \{ \rho_C(x) : x \in C \}. \]
    %
    If we set $\lambda(t x_0) = t \rho_C(x_0)$, then Hahn-Banach allows us to extend $\lambda$ to a linear functional on $X$, such tht $\lambda(x) \leq \rho_C(x)$ for all $x$. But this means that
    %
    \[ \lambda(x_0) = \rho_C(x_0) > \sup \{ \rho_C(x): x \in C \} \geq \sup \{ \lambda(x): x \in C \}. \]
    %
    Since $C$ is open, $\rho_C(x) \lesssim_C \| x \|$, so $\lambda$ is a bounded linear functional.

    Now suppose $C$ is a general convex set. Let $B = 2^{-1} d(x_0,C) \cdot B_X^\circ$. Fix $x_1 \in C$, and consider the open convex set $C_1 = C + B - x_1$, which contains the origin. Now
    %
    \[ d(x_0 - x_1, C_1) = d(x_0, C + B) > 0. \]
    %
    This the last case of the theorem finds $\lambda \in X^*$ such that
    %
    \[ \lambda(x_0 - x_1) > \sup_{x \in C_1} \lambda(x) \geq \sup_{x \in C} \lambda(x - x_1). \]
    %
    But this means that
    %
    \[ \lambda(x_0) > \sup_{x \in C} \lambda(x), \]
    %
    which completes the proof.
\end{proof}

Next we turn to Mazur's separation theorem.

\begin{theorem}
    Let $X$ be a real topological vector space, and let $C$ be a convex set with non-empty interior, let $X_0 \subset X$ be a subspace and let $x_0 \in X - X_0$. Then if $C^\circ \cap (x_0 + X_0) = \emptyset$, there exists a quantity $s$, and a non-zero linear functional $x^* \in X^*$ such that
    %
    \begin{itemize}
        \item $x^*(x) = s$ for all $x \in x_0 + X_0$.
        \item $x^*(x) \leq s$ for all $x \in C$.
        \item $x^*(x) < s$ for all $x \in C^\circ$.
    \end{itemize}
\end{theorem}
\begin{proof}
    We may assume, without loss of generality, that $0 \in C$. Then $C$ is absorbing, and we can consider it's Minkowski functional $C$. Then the continuity of multiplication implies that
    %
    \[ C^\circ = \{ x \in X: \rho(x) < 1 \}. \]
    %
    For $x \in X_0$, define a linear functional $f$ on $X_0 + \langle x_0 \rangle$ by setting
    %
    \[ f(x + \alpha x_0) = \alpha. \]
    %
    Now $\alpha^{-1} x + x_0$ is not in $C^\circ$ by assumption, so it follows that
    %
    \[ f(x + \alpha x_0) = \alpha \leq \rho(x + \alpha x_0). \]
    %
    Now $\rho$ is a continuous seminorm on $X$, because $C$ has non-empty interior, and so $f$ is a continuous linear functional. Setting $s = 1$ completes the completes the proof.
\end{proof}

\begin{corollary}
    Let $X_0$ be a closed subspace of a locally convex space $X$. Suppose $x \in X - X_0$. Then there is $x^* \in X^*$ which vanishes on $X_0$, but with $x^*(x) = 1$.
\end{corollary}
\begin{proof}
    Since $x \in X - X_0$, there is a convex open neighborhood $U$ of $x$. Thus the last theorem applies with $x_0 = 0$, and we must have $s = 0$ in this case since then $0 \in x_0 + X_0$. Thus there is $x^* \in X^*$ such that $x^*(x) \neq 0$, but $x^*$ vanishes on $X_0$. Rescaling, we may assume $x^*(x) = 1$.
\end{proof}

Thus linear functionals in any locally convex space $X$ separate points in $X$. This is not the case in any topological vector space, since, for instance if $p < 1$, then $L^p[0,1]^* = \{ 0 \}$. On the other hand, pointwise evaluation is a continuous linear functional on $l^p$ every for $p < 1$, so $(l^p)^*$ separates $l^p$ even though it is not locally convex.

\begin{corollary}
    Let $X$ be a locally convex space, and $X_0$ a subspace. Then every continuous linear function in $X_0^*$ extends to a functional on $X$.
\end{corollary}
\begin{proof}
    Fix $x_0^* \in X_0^*$, and assume without loss of generality that $x_0^* \neq 0$. Then the kernel of $x_0^*$ is a hyperplane $H$ in $X_0^*$. Let $\overline{H}$ be the closure of $H$ in $X$. Then $\overline{H} \neq X$, and so we can find a nonzero linear functional $x^* \in X^*$ containing $\overline{H}$ in it's kernel. But then $x^*$ is a scalar multiple of $x_0^*$ on $X_0$, and we can rescale $x^*$ so it is equal to $x_0^*$ on $X_0$.
\end{proof}

Now we turn to Eidelhart's separation theorem, which works for pairs of convex sets, one of which has nonempty interior.

\begin{theorem}
    Let $X$ be a real topological vector space, and let $C_1$ and $C_2$ be two convex subsets, where $C_1$ has nonempty interior. If $C_1^\circ \cap C_2 = \emptyset$, then there is $x^* \in X^*$ and $s \in k$ such that
    %
    \begin{itemize}
        \item $x^*(x) \geq s$ for all $x \in C_1$.
        \item $x^*(x) \leq s$ for all $x \in C_2$.
        \item $x^*(x) < s$ for all $x \in C_2^\circ$.
    \end{itemize}
\end{theorem}
\begin{proof}
    Applying Mazur's theorem, since $X_0 = \{ 0 \}$ does not intersect $C_2^\circ - C_1$, we find $x^* \in X^*$ such that for any $x_1 \in C_1$ and $x_2 \in C_2^\circ$
    %
    \[ x^*(x_2) < x^*(x_1). \]
    %
    There there is $s$ such that
    %
    \[ \sup_{x \in C_2^\circ} x^*(x) \leq s \leq \inf_{x \in C_1} x^*(x). \]
    %
    Now if $x_1 \in C_1$ and $x_2 \in C_2^\circ$, then $t_0 x_1 + (1 - t_0) x_2 \in C_2^\circ$ for suitably small $t_0$, and thus
    %
    \[ x^*(x_2) < (1 - t_0) x^*(x_2) + t_0 x^*(x_1) \leq s. \]
    %
    Thus $x^*(x) < s$ for all $x \in C_2^\circ$. Now if $x \in C_2$, find some $x' \in C_2^\circ$. Then for all $t \in (0,1)$, $(1 - t)x + tx' \in C_2^\circ$, and so
    %
    \[ (1 - t) x^*(x) + t x^*(x') < s. \]
    %
    Taking $t \to 0$ gives $x^*(x) \leq s$.
\end{proof}

If neither $C_1$ and $C_2$ have any interior, and one of these sets is compact, we still have a separation theorem, though it is not as strong. This is Tukey's separation theorem.

\begin{theorem}
    Let $X$ be a locally convex vector space, let $C_1$ be a compact, convex subset of $X$, and $C_2$ a closed convex subset of $X$ such that $C_1 \cap C_2 = \emptyset$. Then there is $x^* \in X^*$ such that
    %
    \[ \sup_{x \in C_1} x^*(x) < \inf_{x \in C_2} x^*(x). \]
\end{theorem}
\begin{proof}
    Since $C_1$ is compact, $C_2 - C_1$ is closed and does not contain the origin. Because $X$ is locally convex, we can find a convex neighborhood $U$ of the origin, disjoint from $C_2 - C_1$. Thus $C_2$ is disjoint from $C_1 + U$. Thus applying Eidelhart's separation theorem, we can find $x^* \in X^*$ such that for any $x \in C_1 + U$,
    %
    \[ x^*(x) < \inf_{x \in C_2} x^*(x). \]
    %
    But this means that, because $C_1$ is compact,
    %
    \[ \sup_{x \in C_1} x^*(x) < \inf_{x \in C_2} x^*(x). \qedhere \]
\end{proof}

An important corollary of these separation results is that the dual space of a locally convex space $X$ determines the convex, closed sets of $X$.

\begin{theorem}
    Suppose $X$ is a vector space equipped with two locally convex topologies, turning it into two locally convex spaces $X_1$ and $X_2$. If $X_1^* = X_2^*$, then a convex set $C \subset X$ is closed in $X_1$ if and only if it is closed in $X_2$.
\end{theorem}
\begin{proof}
    Without loss of generality, assume $X$ is a real vector space, and that $C$ is a nonempty convex set in $X$ which is closed in $X_1$. Then for each $x_0 \in X - C$, produce a continuous linear functional $f_{x_0} \in X_1^*$ such that, if
    %
    \[ c_{x_0} = \inf_{x \in C} f_{x_0}(x_0), \]
    %
    then $f_{x_0}(x_0) < c_{x_0}$. Since $f_{x_0}$ is also a continuous linear functional on $X_2$, the set
    %
    \[ A_{x_0} = \{ x \in X: f_{x_0}(x) \geq c_{x_0} \} \]
    %
    is closed in $X_2$, contains $C$, and does not contain $x_0$. But then
    %
    \[ C = \bigcap_{x_0 \in X - C} A_{x_0} \]
    %
    is closed in $X_2$.
\end{proof}




\section{Reflexivity}

Given a norm space $X$, we can iterate the duality construction, obtaining the \emph{second dual} space $X^{**}$, the \emph{third dual} $X^{***}$, and so on. We have a natural isometry $i: X \to X^{**}$ induced for $x \in X$ and $x^* \in X^*$ by the natural pairing
%
\[ \langle x^*, i(x) \rangle = \langle x, x^* \rangle. \]
%
However, this isometry is \emph{not always} an isometric isomorphism. If it is, we say $X$ is \emph{reflexive}. Since $X^*$ is a Banach space for any norm space $X$, only Banach spaces are reflexive.

\begin{example}
    For any measure space $X$, and any $1 < p < \infty$, the space $L^p(X)$ is reflexive. On the other hand, except when they are finite dimensional, the spaces $L^1(X)$ and $L^\infty(X)$ are not reflexive.
\end{example}

\begin{example}
    We have seen the dual of $c_0$ is $l^1$, and the dual of $l^1$ is $l^\infty$, so the double dual of $c_0$ is $l^\infty$. Thus $c_0$ is not reflexive.
\end{example}

\begin{remark}
    It is \emph{not} true that if $X$ is isomorphic to $X^{**}$, then $X$ is reflexive. This was a conjecture for quite some time, until James constructed a non-reflexive Banach space $J$, known as the \emph{James space}, such that $J$ is isomorphic to $J^{**}$.
\end{remark}

If $T: X \to Y$ is an isomorphism, then we obtain an isomorphism $T^{**}: X^{**} \to Y^{**}$ forming a commuting square with $T$ and the inclusions $X \to X^{**}$ and $Y \to Y^{**}$. Thus if $X$ and $Y$ are isomorphic norm spaces, and $X$ is reflexive, then $Y$ is reflexive.

\begin{theorem}
    If $X$ is reflexive, all linear functionals on $X$ are norm attaining.
\end{theorem}
\begin{proof}
    For any norm space $X$ and $x^* \in X^*$, there is certainly $x^{**} \in X^{**}$ with $\| x^{**} \| = 1$ such that $\langle x^*, x^{**} \rangle = \| x^* \|$. But if $X$ is reflexive, then $x^{**}$ is actually the double dual of some $x \in X$, and so $\langle x, x^* \rangle = \| x^* \|$.
\end{proof}

It turns out that this property characterizes reflexive spaces, a result proved by Robert C. James. That is, if $X$ is a Banach space such that all linear functionals on $X$ are norm attaining, then $X$ is reflexive.

\begin{theorem}
    Every closed subspace of a reflexive space is reflexive.
\end{theorem}
\begin{proof}
    Let $X$ be reflexive, and let $X_0 \subset X$ be a closed subspace. Now the dual of $X_0$ is isometric to $X / X_0^\perp$, and so the double dual of $X_0$ is isometric to $X_0^{\perp \perp}$. But  for any $x \in X$, $x \in {}^{\perp} (X_0^\perp)$ if and only if $x^{**} \in X_0^{\perp \perp}$. If $X_0$ is closed, then this means that $x \in X_0$ if and only if $x^{**} \in X_0^{\perp \perp}$, which shows the double dual is an isomorphism for $X_0$, and so $X_0$ is reflexive.
\end{proof}

\begin{corollary}
    Let $X$ be a Banach space. Then $X$ is reflexive if and only if $X^*$ is reflexive.
\end{corollary}
\begin{proof}
    Suppose $X$ is reflexive. If $X$ is reflexive, then the natural inclusion $i: X \to X^{**}$ is a surjective isometry, so $i^*: X^{***} \to X^*$ is a surjective isometry. But $i^*$ is the inverse of the inclusion map $j: X^* \to X^{***}$ since for $x^* \in X^*$, and $x \in X$,
    %
    \[ \langle i^*(j x^*), x \rangle = \langle j x^*, i x \rangle = \langle x^*, x \rangle. \]
    %
    Thus $j$ is an isometry, and so $X^*$ is reflexive. Conversely, suppose $X^*$ is reflexive. Then $X^{**}$ is reflexive, and $X$ is isometric to a closed subspace of $X^{**}$, and so $X$ is reflexive.
\end{proof}

\begin{corollary}
    Let $X$ be a norm space, and $X_0$ a closed subspace. If $X$ is reflexive, then $X / X_0$ is reflexive.
\end{corollary}
\begin{proof}
    If $X$ is reflexive, $X^*$ is reflexive. The dual of $X / X_0$ is isometric to $X_0^\perp$, which is a closed subspace of $X^*$ and thus reflexive.
\end{proof}

\begin{theorem}
    Let $X$ be a norm space, and $X_0 \subset X$ a closed subspace. If $X_0$ and $X/X_0$ are reflexive, then $X$ is reflexive.
\end{theorem}
\begin{proof}
    Let $\lambda \in X^{**}$ be given. If $T: X_0^\perp \to (X/X_0)^*$ be the natural isometry, then we can consider $\lambda \circ T^{-1} \in (X/X_0)^{**}$. Since $X/X_0$ is reflexive, there is $x \in X$ such that for any $x^* \in X_0^\perp$,
    %
    \[ \langle x, x^* \rangle = \lambda(x^*). \]
    %
    Thus $x^{**} - \lambda \in X_0^{\perp \perp}$, which is isometric to the double dual of $X_0$. Since $X_0$ is reflexive, there is $x_0 \in X_0$ such that $x^{**} - \lambda = x_0^{**}$, and so $\lambda = x^{**} - x_0^{**}$, showing $X$ is reflexive.
\end{proof}

Thus reflexivity is a three space property.

\begin{corollary}
    Let $X_1, \dots, X_n$ be norm spaces. Then $X_1 \oplus \dots \oplus X_n$ is reflexive if and only if each of the spaces $X_1, \dots X_n$ are reflexive.
\end{corollary}

\begin{corollary}
    Let $X$ be a reflexive space, and $Y$ a Banach space. If $T: X \to Y$ is a surjective bounded linear operator, then $Y$ is reflexive.
\end{corollary}

\begin{example}
    Since we know $c_0$ is not reflexive, this provides a proof that $c_0^* \cong l^1$ and $(l^1)^* \cong l^\infty$ cannot be reflexive. If $X$ is a measure space such that $L^p(X)$ is infinite dimension, then we can embed $l^p$ in $L^p(X)$, which provides a rigorous proof that $L^1(X)$ and $L^\infty(X)$ are not reflexive whenever they are infinite dimensional.
\end{example}

\begin{example}
    Let $K$ be a compact Hausdorff space containing an infinite number of points. If $K$ contains $\{ x_1, \dots \}$, for each $n \in \{ 1, \dots \}$, define a Borel measure $\delta_n \in C(K)^*$ such that for $f \in C(K)$,
    %
    \[ \int f(x) d \delta_n(x) = f(x_n). \]
    %
    These measures induce an isometry $T: l^1 \to C(K)^*$ such that $T(e_n) = \delta_n$. Thus $C(K)^*$ is not reflexive, and so $C(K)$ is not reflexive. Alternatively, we can prove $C(K)$ is not reflexive by finding a linear functional on $C(K)$ which is not norm attaining, e.g. by taking an infinite sequence $\{ x_n \}$ of distinct points converging to some $x_0 \in K$, and considering the linear functional corresponding to the measure
    %
    \[ \mu = \sum_{n = 1}^\infty 2^{-n} \delta_n - \delta_0. \]
    %
    Then $\| \mu \| = 2$, but for any $f \in C(K)$ with $\| f \|_{L^\infty(K)} = 1$,
    %
    \[ \int f d\mu = \sum_{n = 1}^\infty 2^{-n} f(x_n) - f(x_0) < 2, \]
    %
    since we could only have equality here if $f(x_n) = 1$ for all $n > 0$, and with $f(x_0) = -1$, which cannot be possible if $f$ is continuous.
\end{example}

TODO: Read Megginson's section on James' Theorem.







\section{Separability}

We recall that a topological space is \emph{separable} if it has a countable dense subset. We recall that the image of any separable set under a continuous map is separable. If $X$ is a separable metric space, then it's completion is also separable. And any compact metric space is separable.

\begin{example}
    The function space $C[0,1]$ is separable, since rational coefficient polynomials are dense in $C[0,1]$ by the Weirstrass approximation theorem.
\end{example}

\begin{example}
    The spaces $L^p(\RR^n)$ is separable for $1 \leq p < \infty$, since step functions given by linear combinations of indicator functions supported on cubes with rational endpoints are dense. On the other hand, $L^\infty(\RR^n)$ is not separable, since it contains an uncountable family of points $\{ f_t : t \in \RR \}$ such that $\| f_t - f_s \| = 1$ for all $t \neq s$. One can define, for instance, $f_t = 1_{[0,t] \times [0,1]^{d-1}}$.
\end{example}

\begin{example}
    The space $M[0,1]$ is not separable, since the family of measures $\{ \delta_t : t \in [0,1] \}$ is uncountable, and $\| \delta_t - \delta_s \| = 2$ for $t \neq s$.
\end{example}

\begin{example}
    The space $c_0$ is separable since it is the closed span of the countable set $\{ e_n \}$. The spaces $l^p$ are separable for $1 \leq p < \infty$. But $l^\infty$ is not separable, since if $a$ and $b$ are any $\{ 0, 1 \}$ valued sequence, $\| a - b \|_{l^\infty} \geq 1$, and there are uncountably many such sequences in $l^\infty$.
\end{example}

\begin{theorem}
    Let $X$ be a norm space. Then the following properties hold:
    %
    \begin{itemize}
        \item If $A \subset X$ is separable, then so is $\overline{A}$, $\text{co}(A)$, $\overline{\text{co}}(A)$, $\langle A \rangle$, and $[A]$.
        \item If $X_0$ is a closed subspace of $X$, then $X/X_0$ is separable.
        \item If $X = X_1 \oplus \dots \oplus X_n$ for norm spaces $X_1,\dots,X_n$, then $X$ is separable if and only if each of the spaces $X_1, \dots, X_n$ is separable.
        \item Separability is a three space property.
    \end{itemize}
\end{theorem}
\begin{proof}
    We leave everything here as an exercise, except that if $X_0 \subset X$ is a closed, separable subspace of a norm space $X$, and $X / X_0$ is separable, then $X$ is separable. If we pick a countable set $S_1 \subset X$ such that $\{ s + X_0 : s \in S_1 \}$ is dense in $X / X_0$, and we pick a countable set $S_2 \subset X_0$ which is dense in $X_0$, then for any $x \in X$, we can find a sequence $\{ x_i \}$ in $S_2$ such that $d(x - x_i, X_0) \to 0$. But then we can find a sequence $\{ x_i^0 \}$ in $S_2$ such that $\| x - x_i - x_i^0 \| \leq 2 d(x - x_i,X_0) \to 0$. Thus the countable set $S_1 + S_2$ is dense in $X$.
\end{proof}

\begin{theorem}
    Let $X$ be a norm space. If $X^*$ is separable, then $X$ is separable.
\end{theorem}
\begin{proof}
    Let $S \subset X^*$ be a countable dense set. For each $\lambda \in S$, pick $x_\lambda \in X$ with $\| x_\lambda \| = 1$ such that $|\lambda(x_\lambda)| \geq \| \lambda \| / 2$. If $T = \{ x_\lambda \}$, then for any $\gamma \in X^*$, we can find $\lambda \in S$ such that $\| \lambda - \gamma \| \leq 0.1 \| \gamma \|$. Thus
    %
    \[ |\gamma(x_\lambda)| \geq |\lambda(x_\lambda)| - 0.1 \| \gamma \| \geq 0.9 \| \lambda \| - 0.1 \| \gamma \| \geq 0.8 \| \gamma \|. \]
    %
    But this means that $T^\perp = \{ 0 \}$, and so $X = [T]$, proving $X$ is separable.
\end{proof}

The space $l^1$ is separable, but it's dual, $l^\infty$ is not, so the converse of the last theorem is not true. On the other hand, if $X$ is \emph{reflexive}, then $X$ is separable if and only if $X^*$ is separable.

Now $l^1$ is separable, and for any closed subspace $M$ of $l^1$, $l^1 / M$ is separable. Up to isomorphism, these are the \emph{only} separable Banach spaces.

\begin{theorem}
    If $X$ is a separable Banach space, then there is a surjective linear map $T: l^1 \to X$.
\end{theorem}
\begin{proof}
    Let $\{ x_1, x_2, \dots \}$ be a countable, dense subset of $B_X$, and define
    %
    \[ Ta = \sum_{n = 1}^\infty a_n x_n. \]
    %
    Then $\| Ta \|_X \leq \| a \|_{l^1}$. Since the image of $T$ contains $\{ x_1, x_2, \dots \}$, the range is certainly dense. But for any $x \in B_X$, we can find successively find $x_{i_1}, x_{i_2}, \dots \in B_X$ such that for all $n > 0$,
    %
    \[ \| x - x_{i_1} - (1/2) x_{i_2} - \dots - (1/2^n) x_{i_n} \| \leq 2^{-n-1}. \]
    %
    Then
    %
    \[ T \left( \sum_{j = 0}^\infty 2^{-j} e_{i_j} \right) = x. \]
    %
    Thus the range of $T$ contains $B_X$, and thus $T$ is surjective.
\end{proof}

\begin{theorem}
    A norm space $X$ is separable if and only if there is a compact set $K \subset X$ such that $X = [K]$.
\end{theorem}
\begin{proof}
    Suppose $X$ is separable, and let $\{ x_n \}$ be a dense subset. Then set
    %
    \[ K = \{ 0 \} \cup \{ 2^{-n} \| x_n \|^{-1} x_n \} \]
    %
    is compact, and $[K] = X$.
\end{proof}






\section{Adjoints of Linear Operators}

Recall that if $T: X \to Y$ is a linear map between two vector spaces, then we can consider it's \emph{algebraic adjoint} $T^*$ mapping linear functionals on $Y$ to linear functionals on $X$ by the formula
%
\[ (T^* \lambda)(x) = \lambda(Tx). \]
%
We have $(T+S)^* = T^* + S^*$, $(TS)^* = S^* T^*$, and $(T^{-1})^* = (T^*)^{-1}$. If $T$ is a \emph{bounded} linear map between two Banach spaces, and $\lambda$ is a \emph{bounded} linear functional on $Y$, then $T^* \lambda$ will \emph{also} be a bounded linear functional, since for any $x \in X$,
%
\[ |T^* \lambda(x)| = |\lambda (Tx) | \lesssim \| Tx \|_Y \lesssim \| x \|_X. \]
%
Thus given any bounded linear map $T: X \to Y$ between two Banach spaces, we obtain a bounded linear operator $T^*: Y^* \to X^*$ between the dual spaces of $X$ and $Y$, called the \emph{adjoint} of the operator $T$. By using the fact that a map between two spaces is weakly bounded if and only if it is bounded, one can show that a linear map $T: X \to Y$ between norm spaces is bounded if and only if $y^* \circ T \in X^*$ for each $y^* \in Y^*$. The calculation above shows that
%
\[ \| T^* \lambda \|_{X^*} \leq \| T \| \| \lambda \|_{Y^*}. \]
%
Thus $\| T^* \| \leq \| T \|$. This means that $\| T^{**} \| \leq \| T^* \|$, and we have
%
\[ \| Tx \|_Y = \| T^{**} x^{**} \|_{Y^{**}} \leq \| T^* \| \| x^{**} \|_{X^{**}} = \| T^* \| \| x \|_X. \]
%
Thus we conclude that $\| T \| \leq \| T^* \|$, and so we actually have $\| T \| = \| T^* \|$, so the correspondence $T \mapsto T^*$ is a linear isometry from $B(X,Y)$ to $B(Y^*,X^*)$. We remark that this is not always an onto isometry unless $X$ and $Y$ are reflexive, but we have a characterization of the image of this isometry (it is the space of weak $*$ continuous maps, see the section on weak $*$ continuity).

If $T: X \to Y$ is a bounded linear map, then we obtain a natural linear map $T^{**}: X^{**} \to Y^{**}$, which is precisely and extension of the map $T$. In particular, if $X$ is reflexive, $T^{**} = T$.

For any norm spaces $X$ and $Y$, if $T: X \to Y$ is an isomorphism, then $T^*: Y^* \to X^*$ is an isomorphism. To see this result, we note that if $T: X \to Y$ is a bounded linear map, then
%
\[ \text{Ker}(T) = {}^\perp (T^*(Y^*)) \quad\text{and}\quad \text{Ker}(T^*) = T(X)^\perp. \]
%
It thus follows that $T$ is injective if and only if the image of $T^*$ is weak $*$ dense in $X^*$, and the image of $T$ is norm dense in $Y$ if and only if $T^*$ is injective.

\begin{theorem}
    A bounded linear operator $T: X \to Y$ is a surjective isomorphism if and only if $T^*: Y^* \to X^*$ is a surjective isomorphism.
\end{theorem}
\begin{proof}
    If $T$ is an isomorphism, then $T^*$ is an isomorphism because $(T^*)^{-1} = (T^{-1})^*$. Conversely, suppose $T^*$ is an isomorphism, with $\| (T^*)^{-1} \| = M^{-1}$. Then $T$ is injective, with dense range in $Y$. Then $T^*(B_{Y^*}) \subset M B_{X^*}$. Thus
    %
    \begin{align*}
        \| Tx \|_Y &= \sup_{y^* \in B_{Y^*}} |\langle Tx, y^* \rangle|\\
        &= \sup_{y^* \in B_{Y^*}} |\langle x, T^* y^* \rangle|\\
        &\geq \sup_{x^* \in M B_{X^*}} |\langle x, x^* \rangle|\\
        &= M \| Tx \|.
    \end{align*}
    %
    Thus the image of $T$ is closed in $Y$, from which it follows that $T$ is surjective, and thus an isomorphism.
\end{proof}

A bounded linear operator has closed range if and only if it's adjoint has weak $*$ closed range, as the next result shows.

\begin{theorem}
    Let $T: X \to Y$ be a bounded linear map between Banach spaces. Then $T(X)$ is closed in $Y$ if and only if $T^*(Y^*)$ is closed in $X^*$, which holds if and only if it is weakly $*$ closed in $X^*$.
\end{theorem}
\begin{proof}
    We leave it as an exercise to show, without loss of generality, that $T$ is injective and has dense image in $Y$. Then $T^*$ is injective, and has weak $*$ dense image also.
    %
    \begin{itemize}
        \item Suppose $T(X) = Y$. Then $T$ is an isomorphism, so $T^*$ is an isomorphism, hence $T^*(Y^*) = X$. Thus if $T(X)$ is closed in $Y$, $T^*(Y^*)$ is closd in $X^*$.
        \item Next, suppose $T^*(Y^*)$ is closed in $X^*$. We want to show $T^*(Y^*)$ is then also weak $*$ closed. Suppose that $\{ x_\alpha^* \}$ is a net in $T^*(Y^*) \cap B_{X^*}$ that converges in the weak $*$ topology to some $x^* \in B_{X^*}$. Let $y^*_\alpha = (T^*)^{-1}(x_\alpha^*)$. Since $T^*$ is an isomorphism from $Y^*$ onto it's image, $\{ y^*_\alpha \}$ is a bounded net, and thus has a weak $*$ convergent subnet. By thinning the net, we may assume without loss of generality that the net converges in the weak $*$ topology to some $y^* \in Y^*$. But $T^*$ is weak $*$ continuous, so $x^*_\alpha$ converges to $T^*(y^*) \in T^*(Y^*) \cap B_{X^*}$. Thus $T^*(Y^*) \cap B_{X^*}$ is weak $*$ closed, so by the Krein-Smulian theorem, it follows that $T^*(Y^*)$ is weak $*$ closed.

        \item Suppose that $T^*(Y^*)$ is weak $*$ dense. Then $T^*(Y^*) = X^*$, which implies $T^*$ is an isomorphism, and thus $T$ is an isomorphism, so $T(X) = Y$.
    \end{itemize}
    %
    This proves all the equivalences required.
\end{proof}




\section{Projections and Complementary Subspaces}

In our discussion of the open mapping theorem, we have already encountered the notion of a \emph{complementary subspace}. If $X$ is a norm space, and $X_0$ is a closed subspace, we say $X_0$ is \emph{complemented} in $X$ if there exists a closed subspace $X_1$ such that $X = X_0 \oplus X_1$. If $X$ is a Banach space, then it follows from the closed graph theorem that for any $x \in X$, we can write $x$ uniquely as $x_0 + x_1$ for $x_0 \in X_0$ and $x_1 \in X_1$, and
%
\[ \| x \| \sim \| x_0 \| + \| x_1 \|. \]
%
It thus follows that $X_1$ is isomorphic to $X/X_0$. A space $X_0$ is complemented if and only if every bounded linear operator $T:X_0 \to Y$ extends to a bounded linear operator $T: X \to Y$.

If we define $P: X \to X$ by setting $Px = x_0$, then $P$ is then a continuous operator onto $X_0$ such that $P^2 = P$. We call such an operator a \emph{bounded projection} with closed range $X_0$. Conversely, if $P: X \to X$ is a bounded projection, then it's range is closed because it's range is the kernel of $1 - P$. If $X_0$ is this range, then $X_1 = \text{Ker}(P)$ is a closed subspace which complements the space $X_0$.

It is simple to see a linear map is a projection if and only if it's adjoint is a projection. Since $T$ has closed range if and only if $T^*$ has closed range. If $P: X \to X$ is a projection with closed range $X_0$ and kernel $X_1$, then $P^*: X^* \to X^*$ is a projection with closed range $X_0^\perp$ and kernel $X_1^\perp$.

\begin{theorem}
    If $X$ is a topological vector space, and $X_0 \subset X$ is a closed subspace with either finite dimension, or finite codimension, then $X_0$ is complemented in $X$.
\end{theorem}
\begin{proof}
    If $X_0$ has finite codimension, take a basis $\{ x_1, \dots, x_n \}$ for $X/X_0$. Then the span of $\{ x_1, \dots, x_n \}$ is a closed subspace of $X$ complementing $X_0$. Conversely, if $X_0$ has finite dimension, with basis $\{ x_1, \dots, x_n \}$, one can use Hahn-Banach to produce a family of continuous linear functionals $x_1^*, \dots, x_n^* \in X^*$ such that $x_i^*(x_j) = \delta_{ij}$. The map $Px = (x_1^* x) x_1 + \dots + (x_n^* x) x_n$ is then a continuous projection onto $X_0$.
\end{proof}

TODO: Show $c_0$ is not complemented in $l^\infty$

TODO: Show $X^*$ is complemented in $X^{***}$ for any Banach space $X$. Thus $c_0$ is not the dual of any Banach space.






\section{Compact Operators}

We already know operators on an infinite dimensional Banach space are more tricky than there finite dimensional counterparts. Thus it is often useful to restrict our attention to operators which do not `spread themselves out' too much. An operator is \emph{compact} if the image of any bounded set is precompact. It suffices to verify that the image of the unit ball is precompact. Given two Banach spaces $X$ and $Y$, we shall let $K(X,Y)$ denote the space of all compact operators from $X$ to $Y$.

\begin{theorem}
    The space $K(X,Y)$ of compact operators forms a closed subspace of $B(X,Y)$ closed under multiplication on the left and right, in the sense that for any $M \in K(X,Y)$, and any pair of bounded operators $N \in B(Y,Y')$ and $L \in B(X',X)$, $NML \in K(X',Y')$.
\end{theorem}
\begin{proof}
    Suppose $M_0$ and $M_1$ are compact. Let $U$ be a bounded susbet of $X$. Then
    %
    \[ (\lambda M_0 + \gamma M_1)(U) = \lambda M_0(U) + \gamma M_1(U) \]
    %
    And the sum of two precompact sets is precompact. Thus $\lambda M_0 + \gamma M_1$ is precompact, so $K(X,Y)$ is a subspace of $B(X,Y)$. If $U$ is a bounded subset of $X'$, then $L(U)$ is a bounded subset of $X$, so $ML(U)$ is a precompact subset of $Y$, implying $NML(U)$ is a precompact subset of $Y'$, for if every net $\{ y_\alpha \}$ in $ML(U)$ has a convergent subnet, then the corresponding subnet of $\{ N(y_\alpha) \}$ must also converge.

    To prove $K(X,Y)$ is closed we rely on the fact that a subset of a complete metric space is precompact if and only if, for any $\varepsilon > 0$, the subset can be covered by finitely many balls of radius $\varepsilon$. Now suppose we have $M_1, M_2, \dots \in K(X,Y)$, and $M_i \to N$. We claim $N$ is compact. Let $U$ be a bounded subset of $X$, such that $\| x \| < K$ for each $x \in U$. Then for each $\varepsilon > 0$, there are finitely many balls of radius $\varepsilon$ which cover $M_i(U)$, for each $i$. If $\| M_i - N \| < \varepsilon / K$, then $\| (M_i - N)(x) \| < \varepsilon$ for each $x \in U$, so that if $M_i x$ is contained in some ball with center $y$ of radius $\varepsilon$, then $Nx$ is contained in the ball with center $y$ and radius $2 \varepsilon$. Thus $N(U)$ can be covered with finitely many balls of radius $2\varepsilon$, and is therefore precompact.
\end{proof}

\begin{corollary}
    $K(X)$ is a closed, two-sided ideal of $B(X)$.
\end{corollary}

\begin{remark}
    It is a theorem of Calkin that $K(H)$ is the only closed, two-sided ideal of $B(H)$ for any Hilbert space $H$. Thus we are lead to consider the \emph{Calkin Algebra} $B(H)/K(H)$, which is a simple algebra, and an interesting object of study in the $K$-theory of operator algebras.
\end{remark}

The most basic examples of compact operators are bounded \emph{finite rank} operators, i.e. operators whose range is finite dimensional. Indeed, bounded operators map bounded sets to bounded sets, and a closed bounded set in a finite dimensional space is compact. Because $K(X,Y)$ is a norm closed subset of $B(X,Y)$, any limit of a family of finite rank operators is compact. We say a Banach space $Y$ has the \emph{approximation property} if finite rank bounded operators are dense in $K(X,Y)$ for any Banach space $X$.

\begin{theorem}
    Any Hilbert space has the approximation property.
\end{theorem}
\begin{proof}
    Let $X$ be a Banach space, and $H$ a Hilbert space. If $T: X \to H$ is compact, then the image $H_0$ of $T$ must be separable. Let $\{ e_n \}$ be a basis for $H_0$, and let $T_{\leq N} = P_{\leq N} T$, where $P_N$ is the projection onto the first $N$ elements of the basis for $H_0$. If it was not true that $T_{\leq N}$ converged to $T$ in the operator norm, we could find $\varepsilon > 0$, and two sequences $\{ N_i \}$ and $\{ x_i \}$ such that $\| x_i \| = 1$ for all $i$, and $\| P_{\leq N_i} T x_i - T x_i \| \geq \varepsilon$. Since $T$ is compact, by thinning the subsequence we may assume that $Tx_i \to y$. Then
    %
    \[ \| P_{\leq N_i} y - y \| \geq \| P_{\leq N_i} Tx_i - Tx_i \| - \| P_{\leq N_i} (Tx_i - y) \| - \| Tx_i - y \| \geq \varepsilon - 2 \| Tx_i - y \|. \]

    %
    Now $P_{\leq N_i} y \to y$, so
    %
    \[ 0 = \limsup_{i \to \infty} \| P_{\leq N_i} y - y \| \geq \limsup_{i \to \infty} \varepsilon - 2 \| Tx_i - y \| = \varepsilon, \]
    %
    which gives a contradiction. Thus $T_{\leq N}$ converges to $T$.
\end{proof}

For similar reasons, the spaces $l^p$ for $1 \leq p < \infty$ and $c_0$ have the approximation property, since for any compact operator $T: X \to l^p$ or $T: X \to c_0$, the projections $P_{\leq N} T$ are finite rank operators converging in norm to $T$.

\begin{remark}
    Finding a Banach space \emph{without} the approximation property (i.e. finding a compact operator which cannot be approximated by finite rank operators) proved to be an incredibly difficult problem in 20th century research on functional analysis. Such an operator was only found by Per Enflo in 1972 using tools from the geometry of Banach spaces.
\end{remark}

\begin{corollary}
    Let $H$ be a Hilbert space. If $T$ has finite rank, then $T^*$ has finite rank. A bounded operator $T$ on $H$ is compact if and only if $T^*$ is compact.
\end{corollary}
\begin{proof}
    Suppose $T$ has finite rank. Then we can find an orthogonal basis $\{ e_n \}$ for $H$ such that
    %
    \[ Tx = \sum_{i,j = 1}^M a_{ij} \langle x, e_i \rangle e_j = \sum_{i,j = 1}^M a_{ij} S_{ij} x. \]
    %
    But
    %
    \[ \langle S_{ij} x, y \rangle = \langle x, e_i \rangle \langle e_j, y \rangle = \langle x, \langle y, e_j \rangle e_i \rangle = \langle x, S_{ji} y \rangle. \]
    %
    Thus $S_{ij}^* = S_{ji}$, and so by linearity,
    %
    \[ T^* y = \sum_{i,j = 1}^M \overline{a_{ij}} S_{ji} y. \]
    %
    Thus $T^*$ has finite rank.

    Now if $T$ is a compact operator on $H$, then $T = \lim T_n$, where $T_n$ is a finite rank operator. Then $T^* = \lim T_n^*$ is the limit of finite rank operators, so $T^*$ is also compact. The reverse follows by symmetry.
\end{proof}

\begin{example}
    The simplest operators on a Hilbert space are the diagonal operators, i.e. those operators $T: H \to H$ given in terms of an orthogonal basis $\{ e_n \}$ and $\{ \lambda_n \}$ such that $Te_n = \lambda_n e_n$. Then
    %
    \[ \| T \| = \sup_n |\lambda_n|, \]
    %
    If $S(\varepsilon) = \{ n : |\lambda_n| \geq \varepsilon \}$ is infinite for any $\varepsilon$, then $\{ e_n : n \in S \}$ is a bounded subset of $H$, but $Te_n = \lambda_n e_n$ does not have a convergent subsequence, and so $T$ is not a compact operator. It follows that if $T$ is compact, $\lambda_n \to 0$ as $n \to \infty$. Conversely, if $\lambda_n \to 0$ as $n \to \infty$, then the finite rank operators
    %
    \[ T_{\leq N} x = \sum_{n = 1}^N \lambda_n \langle x, e_n \rangle e_n \]
    %
    converge in operator norm to $T$ as $N \to \infty$, and so $T$ is a compact operator. Thus a bounded diagonalizable operator on a separable Hilbert space is compact precisely when it's eigenvalues converge to zero. More generally, for a diagonal operator on a non separable Hilbert space, every subsequence of eigenvalues must converge to zero, which implies all but countably many eigenvalues are zero.
\end{example}

Note that the operator above, despite it's range contains all the elements of a basis of $H$, \emph{is not surjective}. To see this, note that it does not contain any vector of the form $x = \sum a_n e_n$ such that $\sum (a_n / \lambda_n)^2 = \infty$. On the other hand, it's range is certainly a \emph{dense} subspace of $H$. This is a general phenomenon for compact operators.

\begin{lemma}
    Let $X$ and $Y$ be Banach spaces, and let $T: X \to Y$ be a compact operator. Then the range of $T$ cannot be closed unless $T$ is of finite rank.
\end{lemma}
\begin{proof}
    If the range $Y_0$ of $T$ is closed, then $T$ induces a surjective operator $T_0: X \to Y_0$ between two Banach spaces, and $T_0$ remains compact. By the open mapping theorem, $T_0$ is open. But this implies $Y_0$ is finite dimensional since $Y_0$ then contains an open, precompact set.
\end{proof}

The range of any compact operator is also \emph{separable}, even if it is not the limit of a family of finite rank operators.

\begin{lemma}
    Let $X$ and $Y$ be Banach spaces. Then the range of any compact operator $T: X \to Y$ is separable.
\end{lemma}
\begin{proof}
    Let $B_X$ be the unit ball in $X$. Then $T(B_X)$ is precompact, and therefore separable. But this implies the (non-closed) linear span of $T(B_X)$ is separable, and the linear span of $T(B_X)$ is equal to the range of $T$.
\end{proof}

A useful way to verify compactness in spaces of continuous functions is to use the Arzela-Ascoli criterion. A family of functions $\{ g_\alpha : X \to \mathbf{C} \}$ on a topological space $X$ is \emph{equicontinuous} at $x \in X$ if, for any $\varepsilon > 0$, there is an open set $U$ containing $x$ such that $\| g_\alpha(a) - g_\alpha(b) \| < \varepsilon$ for any $a,b \in U$ and any index $\alpha$.

\begin{theorem}[Arzela-Ascoli]
    If $X$ is a compact metric space, the precompact subsets of $C(X)$ are precisely those subsets which are bounded and equicontinuous.
\end{theorem}
\begin{proof}
    We recall that in a complete metric space, a set is precompact if and only if it has a finite $\varepsilon$ cover for any $\varepsilon > 0$ (i.e., it is \emph{totally bounded}). Now suppose $S \subset C(X)$ is precompact. It is simple to see from the total boundedness condition that $S$ is bounded. For any $\varepsilon > 0$, find a $\varepsilon$ cover $S_0$ of $S$. Since $X$ is compact, all functions in $C(X)$ are uniformly continuous, so we can find $\delta > 0$ such that for all $f_0 \in S_0$, if $d(x_1,x_2) < \delta$, $|f_0(x_1) - f_0(x_2)| < \varepsilon$. But this means that if $f \in C(X)$, there is $f_0 \in S_0$ such that $\| f - f_0 \|_{L^\infty} \leq \varepsilon$, and so if $d(x_1,x_2) < \delta$,
    %
    \[ |f(x_1) - f(x_2)| \leq 2\varepsilon + |f_0(x_1) - f_0(x_2)| \leq 3\varepsilon. \]
    %
    Thus $S$ is uniformly equicontinuous.

    Conversely, suppose $S$ is a bounded, uniformly equicontinuous set. Since $S$ is equicontinuous, for any $\varepsilon > 0$, there is $\delta > 0$ such that $|f(x_1) - f(x_2)| < \varepsilon$ if $d(x_1,x_2) < \delta$. Since $X$ is compact, we can find a finite cover of $X$ by balls of radius $\delta / 2$ centred at points $\{ x_1, \dots, x_N \}$. The function $T$ on $C(X)$ given by $Tf = (f(x_1), \dots, f(x_n))$ is continuous, and thus $T(S)$ is a bounded subset of Euclidean space, and thus precompact. Thus we can find a finite family of functions $\{ f_1, \dots, f_M \}$ such that if $f \in S$, then there is an index $i$ such that for any $j$, $|f(x_j) - f_i(x_j)| < \varepsilon$. But for any $x \in X$, there is $j$ such that $d(x,x_j) < \delta / 2$, and so
    %
    \[ |f(x) - f_i(x)| \leq \varepsilon + |f(x_j) - f_i(x_j)| \leq 2\varepsilon. \]
    %
    Thus $\| f - f_i \|_{L^\infty} \leq 2\varepsilon$, so we have found a finite $2 \varepsilon$ cover of $S$. Since $\varepsilon$ was arbitrary, this means $S$ is totally bounded, and thus precompact.
\end{proof}

Thus if $X$ is a Banach space, and $Y$ is a compact metric space, then a bounded operator $T: X \to C(Y)$ is compact precisely when $T$ maps bounded subsets of $C(X)$ to equicontinuous subsets of $C(Y)$.

\begin{example}
    The best examples of compact operators are given by integral operators with an appropriately regular kernel. Given two compact Hausdorff spaces $X$ and $Y$ equipped with finite Borel measures $\mu$ and $\nu$, and a continuous function $K: X \times Y \to \CC$, we can define an operator by the integral formula
    %
    \[ Tf(y) = \int_X K(x,y) f(x)\; dx. \]
    %
    Then for any $f \in L^1(X)$, $Tf \in C(X)$, which indicates a gain of regularity and a possible compactness of the operator $T$. We claim $T$ is compact from $L^p(X)$ to $L^q(Y)$ for any $1 \leq p,q \leq \infty$. To see this, we note that any integral operator with kernel
    %
    \[ \tilde{K}(x,y) = \sum a_{ij} f_i \otimes g_j \]
    %
    for some continuous functions $f_i \in C(X)$ and $g_j \in C(Y)$, has finite rank and is therefore compact. The Stone-Weirstrass theorem implies such functions are dense in $C(X \times Y)$. Thus we can approximate any $K \in C(X \times Y)$ by a kernel $\tilde{K}$ of the form above. But for any $1 \leq p,q \leq \infty$ we have
    %
    \[ \| T \|_{L^p \to L^q} \lesssim \| K \|_{L^\infty(X \times Y)}, \]
    %
    so it follows that $T$ is a limit in the operator norm of finite rank operators, and thus compact.
\end{example}

\begin{example}
    A \emph{Hilbert-Schmidt} operator between two Hilbert spaces $H_1$ and $H_2$ is a bounded operator $T: H_1 \to H_2$ such that in two orthogonal bases $\{ e_i \}$ and $\{ f_j \}$,
    %
    \[ \| T \|_{HS} = \left( \sum_{i,j} |\langle Te_i, f_j \rangle|^2 \right)^{1/2} = \left( \sum_i \| Te_i \|^2_{H_2} \right)^{1/2} < \infty. \]
    %
    This norm is independant of the choice of bases, as can be verified by a simple expansion. If $T$ is Hilbert Schmidt, so is $T^*$, and $\| T^* \|_{HS} = \| T \|_{HS}$. Moreover, we have a bound $\| T \| \leq \| T \|_{HS}$. Any Hilbert-Schmidt operator is compact, because if $P_{\leq N}$ is projection onto the first $N$ vectors in a basis $\{ e_k \}$, then $P_{\leq N} T$ has finite rank, and $P_{\leq N} T \to T$ as $N \to \infty$ in the operator norm, because it converges in the Hilbert-Schmidt norm. A basic example of a Hilbert-Schmidt operator is an integral operator $T$ between two measure spaces $X$ and $Y$ with kernel $K \in L^2(X \times Y)$, in which case $\| T \|_{HS} = \| K \|_{L^2(X \times Y)}$. To see this, we let $\{ e_i \}$ and $\{ f_j \}$ be orthogonal basis on $L^2(X)$ and $L^2(Y)$. Since $\overline{e_i} \otimes f_j$ is then an orthogonal basis for $L^2(X \times Y)$, we calculate that
    %
    \[ \| T \|_{HS}^2 = \sum |\langle Te_i, f_j \rangle|^2 = \sum |\langle K, \overline{e_i} \otimes f_j \rangle|^2 = \| K \|_{L^2(X \times Y)}. \]
    %
    The space of all Hilbert-Schmidt operators between two Hilbert spaces $H_1$ and $H_2$ forms a Hilbert space itself, $B_{HS}(H_1,H_2)$, where the inner product is
    %
    \[ \langle T_1, T_2 \rangle_{HS} = \sum \langle T_1e_i, f_j \rangle \overline{\langle T_2 e_i, f_j \rangle}. \]
    %
    One can also consider slightly different variants of Hilbert-Schmidt operators. For instance, the family of Hilbert-Schmidt operators between two measure spaces $X$ and $Y$ are the family of all operators $T: L^2(X) \to L^2(Y)$ induced by $K \in L^2(X \times Y)$ by setting
    %
    \[ Tf(x) = \int K(x,y) f(y)\; dy. \]
    %
    If $\{ e_n \}$ is an orthogonal basis for $L^2(X)$, and $\{ e_m \}$ is an orthogonal basis for $L^2(Y)$, then $e_n \otimes e_m$ is a basis for $L^2(X \times Y)$, and so
    %
    \[ \| K \|_{L^2(X \times Y)}^2 = \sum |\langle K, e_n \otimes e_m \rangle|^2 = \sum_{n,m} |\langle Te_n, e_m \rangle|^2. \]
    %
    Thus a Hilbert-Schmidt integral operator is a Hilbert Schmidt operator of the kind above. Conversely, if $T$ is a normal, compact operator from $L^2(X)$ to itself, then there exists an orthonormal basis $\{ e_n \}$ diagonalizing $T$ with eigenvalues $\{ \lambda_n \}$. If $K(x,y) = \sum_n \lambda_n e_n(x) e_n(y)$, then $K$ is a Hilbert-Schmidt kernel which gives the operator $T$. 
\end{example}

\begin{example}
    Let $B$ be the unit ball in $\RR^d$, and consider a kernel $K: B \times B \to \CC$ such that $|K(x,y)| \lesssim |x - y|^{-(d-s)}$, for some $s > 0$. Then
    %
    \[ \| K \|_{L^\infty_y(B) L^1_x(B)}\quad\text{and}\quad \| K \|_{L^\infty_x(B) L^1_y(B)} < \infty, \]
    %
    so Schur's Lemma implies that if $T$ is the integral operator corresponding to $K$, then $\| T f \|_{L^p(\RR^d)} \lesssim \| f \|_{L^p(\RR^d)}$ for $1 \leq p \leq \infty$. For $s > d/2$, this operator is a Hilbert-Schmidt operator, but not for $s \leq d/2$. Nonetheless, for any $s > 0$, $T$ is a compact operator. To see this, for each $n$, define $T_n$ to be the integral operator corresponding to the kernel $K_n(x,y) = \mathbf{I}(|x - y| \geq 1/n) K(x,y)$. Then $K_n$ is a Hilbert-Schmidt kernel, and so every operator in the family $\{ T_n \}$ is compact. Moreover, we have
    %
    \[ \lim_{n \to \infty} \| K - K_n \|_{L^\infty_y L^1_x} = \lim_{n \to \infty} \| K - K_n \|_{L^\infty_x L^1_y} = 0, \]
    %
    and so $T_n \to T$ in the operator norm. Thus $T$ is compact.
\end{example}

We now turn to a result of Schauder, which we have already proved in the case where $T: H \to H$ is a compact operator on a Hilbert space (and more generally, the same proof follows for any pair $X$ and $Y$ such that $B(X,Y)$ has the approximation property).

\begin{theorem}
    Let $X$ and $Y$ be Banach spaces. Then a bounded operator $T: X \to Y$ is compact if and only if it's adjoint $T^*: Y^* \to X^*$ is compact.
\end{theorem}
\begin{proof}
    Suppose first that $T$ is a bounded compact operator. We apply the Arzela-Ascoli theorem. Let $K$ be the compact set formed from the closure of $T(B_X)$, where $B_X$ is the unit ball in $X$. Let $R_1: Y^* \to C(K)$ and $R_2: X^* \to C_b(B_X)$ be the natural restriction operators. Then Arzela-Ascoli implies that $R_1$ is a compact linear operator. Thus $R(B_{Y^*})$ is a precompact subset of $C(K)$. But if $T^*_C: C(K) \to C_b(B_X)$ is the natural linear pullback operator correspoding to $T: B_X \to K$, then $R_2 \circ T^* = T^*_C \circ R_1$. Since $R(B_{Y^*})$ is precompact, so too is $R_2(T^*(B_{Y^*})) = T^*_C(R(B_{Y^*}))$. But this means that $T^*(B_{Y^*})$ is precompact.

    Conversely, suppose $T^*$ is a bounded, compact operator. Then $T^{**}$ is compact. If $\{ x_\alpha \}$ is a bounded net in $X$, then $\{ x_\alpha^{**} \}$ is a bounded net in $X^{**}$, so $T^{**} x_\alpha^{**} = (Tx_\alpha)^{**}$ has a convergent bounded subnet $\{ (Tx_{\alpha_\beta})^{**} \}$. But convergence of $\{ (Tx_{\alpha_\beta})^{**} \}$ is equivalent to convergence of $\{ Tx_{\alpha_\beta} \}$, so $\{ Tx_\alpha \}$ has a convergent subnet. Thus $T$ is a compact, bounded operator. 
\end{proof}

\begin{theorem}
    If the adjoint of a bounded linear operator $T: X \to Y$ between Banach spaces is continuous from the weak $*$ topology of $Y^*$ to the norm topology of $X^*$, then $T$ is compact.
\end{theorem}
\begin{proof}
    Let $\{ y_\alpha^* \}$ be a bounded net in $Y^*$. Then $\{ y_\alpha^* \}$ has a weak $*$ convergent subsequence by the Banach-Alaoglu theorem. But then by the continuity of $T^*$, it follows that $\{ T^* y_\alpha^* \}$ has a norm convergent subsequence. Thus $T^*$ is compact.
\end{proof}

TODO: Trace class operators.

TODO: Weak Compactness and Total Continuity (3.4 and 3.5) of Megginson.








\section{Bases in Banach Spaces}

In the theory of general infinite dimension vector spaces, the standard definition of a basis (which in the sequel we will call a \emph{Hamel basis}, expanding a vector in a finite linear combination of basis elements, does not necessarily lead to a useful theory, because of analytical problems involving finite sums.

\begin{theorem}
    No Banach space $X$ has a countably infinite Hamel basis.
\end{theorem}
\begin{proof}
    Suppose $X$ has a countable infinite basic $\{ e_1, e_2, \dots \}$. For each $n$, let $X_n = \langle e_1, \dots, e_n \rangle$. Then $X_n$ is closed and has non-empty interior (if it's interior was non-empty, it would be absorbing, and thus $X_n = X$. But then the Baire category theorem says that $\bigcup X_n$ is nowhere dense, which is impossible since $X = \bigcup X_n$.
\end{proof}

In the setting of Banach spaces, we can get around this by instead considering infinite linear combinations of vectors. An ordered sequence $\{ e_n \}$ is a \emph{Schauder basis} for a Banach space $X$ if such that for any $x \in X$, there is a unique sequence $\{ a_n \}$ of scalars such that $x = \sum_n a_n e_n$, where $\sum_n a_n e_n$ is defined as the limit of the net of finite sums corresponding to the sequence. In the sequel, by a basis, we will mean a Schauder basis. A sequence $\{ e_n \}$ in a Banach space is a \emph{Schauder basic sequence} if it is a Schauder basis for it's closed span. A basic sequence $\{ e_n \}$ is \emph{bounded} if $\inf \| e_n \| \neq 0$ and $\sup \| e_n \| \neq \infty$, and \emph{normalized} if $\| e_n \| = 1$ for all $n$. If $\{ e_n \}$ is a Schauder basis, then so is $\{ \lambda_n e_n \}$ for any family of nonzero scalars $\{ \lambda_n \}$, so we can always normalize a Schauder basis.

\begin{remark}
    Note that every Banach space with a Schauder basis must be separable.
\end{remark}

If $X$ is a Banach space with a Schauder basis $\{ e_n \}$, then we can view $X$ as a sequence space in the following natural way. We let $Y$ be the collection of all sequences $a = \{ a(n) \}$ such that $\sum_n a(n) e_n$ converges, and define $\| a \| = \| \sum_n a(n) e_n \|$. Then $Y$ is a Banach space, and the map $T: Y \to X$ given by $Ta = \sum a(n) e_n$ shows the spaces are isometric.

\begin{example}
    We now construct the \emph{classical Schauder basis} for $C[0,1]$, first described in Schauder's 1927 that introduced the concept. To obtain the basis, we define a family of \emph{functions} $\{ s_n \}$ with $\| s_n \|_{L^\infty} = 1$ for all $n$. We define $s_0(t) = 1$ for all $t \in [0,1]$, and $s_1(t) = t$ for all $t \in [0,1]$. Then for $n > 1$, if $2^{m-1} < n \leq 2^m$, we define $s_n$ to be the tent function supported on the length $1/2^{m-1}$ interval with left endpoint $l_n = n/2^{m-1} - 1 - 1/2^{m-1}$ and right endpoint $r_n = n/2^{m-1} - 1$.

    We now show these functions are a Schauder basis. To show that the span of functions $\{ s_n \}$ is dense in $C[0,1]$, for any $f \in C[0,1]$, we iteratively define a sequence $\{ a_n \}$ such that $f = \sum_{n = 0}^\infty a_n s_n$. If $f_N = \sum_{n = 0}^N a_n s_n$, then the sequence is recursively defined, such that $a_0 = f(0)$, $a_1 = f(1) - f_0(1)$, and for $n > 1$, $a_n = f(r_n) - f_{n-1}(r_n)$. Then the functions $\{ f_N \}$ are precisely linear interpolations of the function $f$ between an increasing family of points $S_N$, such that $S_{2^N} = \{ 0, 1/2^{N-1}, \dots, 1 - 1/2^{N-1}, 1 \}$. Because $\lim_{N \to \infty}$ is dense in $[0,1]$, it follows by the uniform continuity of $f$ that $\| f - f_N \|_{L^\infty} \leq 1$.

    It remains to show this expansion is unique. But this is actually easy to see, for if $f = \sum_{n = 0}^\infty b_n s_n$ for some sequence $\{ b_n \}$, then, setting $f_N = \sum_{n = 0}^N b_n s_n$, one can recover the sequence from the recursive formula $a_0 = f(0)$, $a_1 = f(1) - f_0(1)$, and so on, and it follows from induction that $b_n = a_n$ for all $n$, where $\{ a_n \}$ is the sequence defined above.
\end{example}

\begin{theorem}
    If $\{ e_n \}$ is a Schauder basis for a Banach space $X$, then there is $C > 0$ such that for $x \in X$, if $x = \sum a_n e_n$, then
    %
    \[ (1/C) \sup_N \| \sum_{n = 1}^N a_n e_n \| \leq \| x \| \leq \sup_N \| \sum_{n = 1}^N a_n e_n \|. \]
\end{theorem}
\begin{proof}
    We have
    %
    \[ \| x \| = \lim_{N \to \infty} \| \sum_{n = 1}^N a_n e_n \| \leq \sup_N \| \sum_{n = 1}^N a_n e_n \|. \]
    %
    It is simple to check the supremum $x \mapsto \sup_N \| \sum_{n = 1}^N a_n e_n \|$ defines a norm $\rho$ on $X$. It is actually a complete norm, since if $\{ x_k \}$ is a Cauchy sequence in $X$ with respect to this norm, and $x_k = \sum a_n(k) e_k$, then
    %
    \[ |a_n(k_1) - a_n(k_2)| \leq \| \sum_{m = 1}^n (a_n(k_1) - a_n(k_2)) e_n - \sum_{m = 1}^{n-1} (a_n(k_1) - a_n(k_2)) e_n \| \leq 2 \rho(x_n - x_m). \]
    %
    Thus $\{ a_n(k) \}$ is a Cauchy sequence for each $n$. Let $a_n = \lim a_n(k)$ for each $n$. Since $\{ x_k \}$ is Cauchy, it is bounded, so $\rho(x_k) \leq M$ for some $M > 0$ uniformly in $k$. For any $\varepsilon > 0$, we may pick $k_0$ such that for $k_1,k_2 \geq k_0$, and any $N$,
    %
    \[ \| \sum_{n = 0}^N [a_n(k_1) - a_n(k_2)] e_n \| \leq \varepsilon. \]
    %
    It follows that for any $m_1 \leq m_2$,
    %
    \[ \| \sum_{n = m_1}^{m_2} [a_n(k_1) - a_n(k_2)] e_n \| \leq 2 \varepsilon. \]
    %
    Thus
    %
    \[ \| \sum_{n = m_1}^{m_2} [a_n - a_n(k_0)] e_n \| = \lim_{k \to \infty} \| \sum_{n = m_1}^{m_2} [a_n(k) - a_n(k_0)] e_n \| \leq 2 \varepsilon. \]
    %
    In particular, if we pick $m_0$ such that for $m \geq m_0$,
    %
    \[ \| \sum_{n = m_0}^m a_n(k_0) e_n \| \leq \varepsilon, \]
    %
    then
    %
    \[ \| \sum_{n = m_0}^m a_n e_n \| \leq 3 \varepsilon. \]
    %
    Thus the partial sums of $\sum_n a_n e_n$ are Cauchy, and thus converge to some $x \in X$. And it is simple to see that
    %
    \[ \lim_{k \to \infty} \sup_N \| \sum_{n = 0}^N [a_n - a_n(k)] e_n \| = 0. \]
    %
    But the identity map is continuous from the standard norm on $X$ to the $\rho$ norm on $X$, so it follows by the open mapping theorem that the $C$ required by the theorem exists.
\end{proof}

\begin{corollary}
    Let $\{ e_n \}$ be a Schauder basis for a Banach space $X$. If for each $n$, we define a linear functional $\alpha_n$ such that $x = \sum \alpha_n(x) e_n$, then $\alpha_n$ is a continuous linear functional. Similarily, the projection maps $P_{\leq N} x = \sum_{n = 1}^N a_n e_n$ are also continuous. Both families are uniformly bounded.
\end{corollary}

We define $\sup_N \| P_{\leq N} \|$ to be the \emph{basis constant} for the Schauder basis. The basis is \emph{monotone}, or \emph{orthogonal}, if the basis constant is one. As an example, the standard bases for $c_0$ and $l^p$ for $1 \leq p < \infty$ are monotone.

\begin{theorem}
    The basis constant for a Schauder basis $\{ e_n \}$ for a Banach space $X$ is the smallest number $M > 0$ such that for $m_1 \leq m_2$,
    %
    \[ \| \sum_{n = 1}^{m_1} a_n e_n \| \leq M \| \sum_{n = 1}^{m_2} a_n e_n \|. \]
\end{theorem}
\begin{proof}
    If
    %
    \[ \| \sum_{n = 1}^{m_1} a_n x_n \| \leq M \| \sum_{n = 1}^{m_2} a_n e_n \| \]
    %
    for all $m_1 \leq m_2$, then setting $m_1 = 0$ and taking $m_2 \to \infty$ gives, for all $x \in X$,
    %
    \[ \| P_{\leq m_1} x \| \leq M \| x \|. \]
    %
    Thus $M \geq \sup_N \| P_{\leq N} \|$. Conversely, we have for any $m_1 \leq m_2$, if $x = \sum a_n e_n$, then
    %
    \[ \| \sum_{n = 1}^{m_1} a_n e_n \| = \| P_{\leq m_1} x \| = \| P_{\leq m_1} P_{\leq m_2} x \| \leq \| P_{\leq m_1} \| \| P_{\leq m_2} x \| \leq \sup_N \| P_{\leq N} \| \| P_{\leq m_1} x \|, \]
    %
    and this yields the result.
\end{proof}

\begin{corollary}
    A Schauder basis $\{ e_n \}$ is monotone if and only if for all $x \in X$,
    %
    \[ \| x \| = \sup_N \| P_{\leq N} x \|. \]
    %
    which holds if and only if $\| P_{\leq N} x \|$ is a monotone increasing sequence in $N$.
\end{corollary}

A Schauder basis is \emph{strictly monotone} if $\| \sum_{n = 1}^m a_n x_n \| < \| \sum_{n = 1}^{m+1} a_n x_n \|$ for all $a_1, \dots, a_{n+1}$ with $a_{n+1} \neq 0$. The standard Schauder basis for $l^p$ is strictly monotone, whereas the basis for $c_0$ is \emph{not} monotone.

\begin{theorem}
    A sequence $\{ e_n \}$ in a Banach space $X$ is a Schauder basis if it's span is dense in $X$, and for any $m_1 \leq m_2$, and any sequence of scalars $\{ a_n \}$, there is $M > 0$ such that
    %
    \[ \| \sum_{n = 1}^{m_1} a_n e_n \| \leq M \| \sum_{n = 1}^{m_2} a_n e_n \|. \]
\end{theorem}
\begin{proof}
    It certainly follows from the assumption that the sequence is linearly independent, and that each $x \in X$ can be expanded \emph{uniquely} as $\sum a_n e_n$ for some sequence $\{ a_n \}$, because if $\sum_n a_n e_n = 0$, then
    %
    \[ \| \sum_{n = 1}^N a_n e_n \| \leq M \lim_{N \to \infty} \| \sum_{n = 1}^N a_n e_N \| = 0, \]
    %
    so $a_i = 0$ for all $i$.
\end{proof}

\begin{corollary}
    Any subsequence of a basic sequence is basic.
\end{corollary}

\begin{example}
    TODO: The Haar basis on $L^p[0,1]$.
\end{example}

\begin{remark}
    It is very difficult to find a separable Banach space which does not have a basis, but there exists such a space. Per Enflo found a reflexive space with this property, the same space that does not have the approximation property.
\end{remark}

TODO: Every infinite dimenisonal Banach space has a basic sequence in it. 4.1 of Megginson.

TODO: Connections between approximation property and having a basis.

We now consider bases $\{ e_n \}$ having the condition that for any $x \in X$, the series $\sum_n a_n e_n$ converges \emph{unconditionally} to $x$, i.e. any rearrangement converges unconditionally.

\begin{lemma}
    Let $X$ be a Banach space, and let $\{ x_n \}$ be a sequence in $X$. If every permutation of the sequence $\sum_{n = 1}^\infty x_n$ converges, then they all converge to a common value.
\end{lemma}

\begin{lemma}
    A series $\sum_n x_n$ converges unconditionally if and only if every subseries converges.
\end{lemma}

\begin{remark}
    In particular, every subseries converges unconditionally.
\end{remark}

We leave these as exercises to the reader. For the first lemma, assume that two permutations do not have the same limiting value, and show that one can produce a third permutation that does not converge. For the second result, do something similar.

If $\sum_n x_n$ converges conditionally, then for any $A \subset \NN$, we define $\sum_A x_n$ to be the sum of the subseries corresponding to $A$. If $A = \emptyset$, we set $\sum_A x_n = 0$.

\begin{lemma}
    If $X$ is a closed subspace of $l^\infty$ containing all $\{ 0, 1 \}$ valued sequences, then $X = l^\infty$.
\end{lemma}
\begin{proof}
    Any positive real-number can be written uniquely as $\sum_{k = -N}^\infty a(k) / 2^k$ for some $a(n) \in \{ 0, 1 \}$ and some $N > 0$ where $N < \log |a|$. If $a \in l^\infty$, we can write $a_n = \sum_{k = -N} a_n(k) / 2^k$, where $N < \log \| a \|_{L^\infty}$ is independant of $n$. The sequences $b_n = \{ a_n(k) \}$ all lies in $X$, and thus so do the sequences $b_{\leq M} = \sum_{k = -N}^M b_n$ for any $M > 0$. But $b_{\leq M}$ converges to $a$ uniformly, so $a \in X$.
\end{proof}

Unlike in the finite dimensional setting, a series converging unconditionally need not converge absolutely. Nonetheless, if $\sum_n x_n$ converges unconditionally, then the series $\sum_n x^*(x_n)$ converges absolutely for any $x^* \in X^*$.

\begin{lemma}
    Let $X$ be a Banach space. Suppose that $\sum_n x_n$ is a series such that $\sum_n x^*(x_n)$ is absolutely convergent for any $x^* \in X^*$. Then there is $M > 0$ such that
    %
    \[ \sup_N \| \sum_{n = 1}^N a_n x_n \| \leq M \| a \|_{l^\infty} \]
    %
    for any $a \in l^\infty$.
\end{lemma}
\begin{proof}
    Define $T: X^* \to l^1$ by $T(x^*) = (x^* x_n)$. Then $T$ is linear, and by the closed graph theory, bounded. But if $a \in l^\infty$, then $T^*a = \sum a_n x_n$, and so the result follows.
\end{proof}

\begin{theorem}
    A series $\sum_n x_n$ in a Banach space $X$ converges unconditionally if and only if $\sum a_n x_n$ converges for any $a \in l^\infty$.
\end{theorem}
\begin{proof}
    If $\sum a_n x_n$ converges for any $a \in l^\infty$, then certainly any subseries of $\sum_n x_n$ converges, and so $\sum_n x_n$ converges unconditionally. Conversely, suppose $\sum_n x_n$ converges unconditionally. Then $\sum_n x^*(x_n)$ converges absolutely for all $x^* \in X^*$, so it follows that there is $M > 0$ such that
    %
    \[ \sup_N \| \sum_{n = 1}^N a_n x_n \| \leq M \| a \|_{l^\infty}, \]
    %
    for any $a \in l^\infty$, which means
    %
    \[ \sup_{m_1 \leq m_2} \| \sum_{n = m_1}^{m_2} a_n x_n \leq 2M \| a \|_{l^\infty}. \]
    %
    Now $\sum_n a_n x_n$ converges for any $a_n \in \{ 0, 1 \}$, so it suffices to show that the space $Y$ of all $\{ a_n \}$ in $l^\infty$ such that $\sum a_n x_n$ converges is closed. If $\sum a_n(k) x_n$ converges for all $k$, and $\{ a_n(k) \}$ converges to $\{ a_n \}$ uniformly, then
    %
    \[ \sup_N \| \sum_{n = m_1}^{m_2} (a_n - a_n(k)) x_n \| \leq 2M \| a - a(k) \|_{l^\infty}. \]
    %
    For any $\varepsilon > 0$, there is $k_0$ such that $\| a - a(k_0) \|_{l^\infty} \leq \varepsilon$. There is also $m_0$ such that for $m_0 \leq m_1 \leq m_2$,
    %
    \[ \| \sum_{n = m_1}^{m_2} a_n(k_0) x_n \| \leq \varepsilon. \]
    %
    But this means that
    %
    \[ \| \sum_{n = m_1}^{m_2} a_n x_n \| \leq (2M + 1) \varepsilon. \]
    %
    Since $\varepsilon$ was arbitrary, the partial sums of the sequence $\sum a_n x_n$ are Cauchy, and so the sum converges, which shows $Y$ is closed, hence $Y = l^\infty$, and this completes the proof.
\end{proof}

\begin{corollary}
    If a sum $\sum x_n$ in a Banach space $X$ converges unconditionally, then so does $\sum a_n x_n$ for any $a \in l^\infty$.
\end{corollary}

A basis $\{ e_n \}$ is \emph{unconditional} if for $x \in X$, there is a unique sequence of scalars $\{ a_n \}$ such that $x = \sum a_n x_n$, and this sum converges unconditionally. The standard bases on $l^p$ and $c_0$ are both unconditional, and it is clear that if $\{ e_n \}$ is an unconditional base, then so too is $\{ \lambda_n e_n \}$ for any non-zero sequence of scalars $\{ \lambda_n \}$.

\begin{theorem}
    If $\{ e_n \}$ is an unconditional base for a Banach space $X$, then there is $C > 0$ such that for any $x \in X$ with $\sum a_n x_n$,
    %
    \[ (1/C) \sup \{ \sum_n a_n x_n : \| a \|_{l^\infty} \leq 1 \} \leq \| x \| \leq \sup \{ \sum_n a_n x_n : \| a \|_{l^\infty} \leq 1 \}. \]
\end{theorem}
\begin{proof}
    Proof is analogous to the analogous Schauder basis theorem, i.e. it suffices to prove the supremum defines a norm under which $X$ is complete, and let as an exercise.
\end{proof}

Denote the supremum norm defined in the above theorem by $\| \cdot \|_{BMU}$.

\begin{lemma}
    For any $x = \sum a_n e_n$ in $X$,
    %
    \[ \| \sum b_n a_n e_n \|_{BMU} \leq \| b \|_{l^\infty} \| \sum a_n e_n \|_{BMU}. \]
\end{lemma}
\begin{proof}
    The proof follows immediately if $\| b \|_{l^\infty} = 1$. But then the result follows in general by rescaling.
\end{proof}

\begin{theorem}
    If $\{ e_n \}$ is an unconditional basis for a Banach space $X$, then for any sequences $\{ a_n \}$ and $\{ b_n \}$ with $|a_n| \leq |b_n|$ and such that $\sum a_n e_n$ and $\sum b_n e_n$ converge,
    %
    \[ \| \sum a_n e_n \|_{BMU} \leq \| \sum b_n e_n \|_{BMU}. \]
\end{theorem}
\begin{proof}
    Write $a_n = c_n b_n$ for $c$ with $\| c \|_{l^\infty} \leq 1$, and apply the last result.
\end{proof}

Given a Banach space with a normalized unconditional basis $\{ e_n \}$, we can give $X$ a commutative product structure by defining
%
\[ \left( \sum a_n e_n \right) \left( \sum b_n e_n \right) = \sum a_n b_n e_n. \]
%
The right hand side is well defined since if $\sum a_n e_n$ converges, and $\| e_n \| = 1$ for all $n$, then we must have $a \in c_0$. Moreover, this turns $X$ into a Banach algebra with the $\text{BMU}$ norm above. This algebra is never unital, however, since the unit would have to be the sum $\sum e_n$, which does not converge. If $X$ is a real vector space, then $X$ is also a \emph{Banach lattice}, if we equip it with the order structure such that $\sum a_n e_n \leq \sum b_n e_n$ if $a_n \leq b_n$ for all $n$, since if $x_1 = \sum a_n e_n$ and $x_2 = \sum b_n e_n$ converge unconditionally, then $\sum |a_n| e_n$ and $\sum |b_n| e_n$ converges unconditionally, and thus so does $\sum (|a_n| + |b_n) e_n$, and thus $\sum \min(a_n,b_n) e_n$ and $\sum \max(a_n,b_n) e_n$, and this is $x_1 \wedge x_2$ and $x_1 \vee x_2$.

The \emph{unconditional basis constant} for an unconditional basis $\{ e_n \}$ is the smallest $M > 0$ such that for any $A \subset \mathbf{N}$, and any $\{ a_n \}$ such that $\sum a_n e_n$ converges,
%
\[ \| \sum_{n \in A} a_n e_n \| \leq M \| \sum_n a_n e_n \|. \]
%
This exists by the uniform boundedness principle. If, for each $A \subset \mathbf{N}$, we define $P_A: X \to X$ by setting $P_A(\sum a_n e_n) = \sum_{n \in A} a_n e_n$, then $P_A$ is continuous, and $M = \sup_{A \subset \mathbf{N}} \| P_A \|$. We can also define a quantity called the \emph{unconditional constant}, which is the smallest quantity $M > 0$ such that for any $A \subset \mathbf{N}$,
%
\[ \| \sum_{n \in A} a_n e_n - \sum_{n \not \in A} a_n e_n \| \leq M \| \sum_n a_n e_n \|. \]
%
If, for a $\{ -1, 1 \}$ valued sequence $\sigma$, we define an isomorphism $T_\sigma: X \to X$ by setting $T_\sigma(\sum a_n e_n) = \sum \sigma_n a_n e_n$, then $M = \sup_\sigma \| T_\sigma \|$.

\begin{theorem}
    Suppose $\{ e_n \}$ is an unconditional basis with basic constant $K_b$, unconditional basis constant $K_{ub}$, and unconditional constant $K_u$. Then
    %
    \[ 1 \leq K_b \leq K_{ub} \leq 1/2 + K_u/2 \leq K_u \leq 2 K_{ub}. \]
    %
    If we use the $\| \cdot \|_{BMU}$ norm, then $K_b = K_{ub} = K_u = 1$.
\end{theorem}
\begin{proof}
    The only nontrivial inequality here is that $K_{ub} \leq 1/2 + K_u/2$. But this follows because if $\sum_n a_n x_n \in X$ and $A \subset \mathbf{N}$, then the triangle inequality implies that
    %
    \[ 2 \| \sum_{n \in A} a_n e_n \| - \| \sum_n a_n e_n \| \leq \| \sum_{n \in A} a_n e_n - \sum_{n \in \mathbf{N} - A} a_n e_n \| \leq K_u \| \sum a_n e_n \|. \]
    %
    Thus $\| \sum_{n \in A} a_n e_n \| \leq (1/2)(1 + K_u) \| \sum a_n e_n \|$, which proves the ienquality. If we are using the $\| \cdot \|_{BMU}$ norm, then $K_u = 1$, and the inequalities then imply that $K_b = K_{ub} = K_u = 1$.
\end{proof}

\begin{theorem}
    A sequence $\{ e_n \}$ is an unconditional basis for a Banach space $X$ if and only if
    %
    \begin{itemize}
        \item $e_n \neq 0$ for all $n$.
        \item The span of $\{ e_n \}$ is dense in $X$.
        \item There is $M > 0$ such that for any finite subsets $A \subset B \subset \mathbf{N}$,
        %
        \[ \| \sum_{n \in A} a_n e_n \| \leq M \| \sum_{n \in B} a_n e_n \|. \]
    \end{itemize}
\end{theorem}
\begin{proof}
    It is clear that such an $M$ should exist for any unconditional basis $\{ e_n \}$. Conversely, if the three properties above is satisfied, then certainly $\{ e_n \}$ is a Schauder basis for $X$, and so it suffices to show that $\sum a_n e_n$ converges unconditionally if it converges at all. But if $\sum a_{n_i} e_{n_i}$ is a subseries, then the subseries converges because
    %
    \[ \| \sum_{i = m_1}^{m_2} a_{n_i} e_{n_i} \| \leq M \| \sum_{n = n_{m_1}}^{n_{m_2}} a_n e_n \| \]
    %
    and so it follows from the fact that the partial sums of $\sum a_n e_n$ are Cauchy that the partial sums of the subseries $\sum a_{n_i} e_{n_i}$ are Cauchy.
\end{proof}

\begin{example}
    The bases for $l^p$ and $c_0$ are uncoditional. The classical Schauder basis $\{ s_n \}$ for $C[0,1]$, however, is \emph{not} an unconditional basis. To show the basis is unconditional, consider an infinite sequence of dyadic intervals $I_1, I_2, \dots$, when $I_k$ has length $1/2^k$, and $I_k$ shares the same left endpoints as $I_{k-1}$ for $k$ even, and the same right end points for $k$ odd. Let $t_k = s_{n_k}$, where $s_{n_k}$ is the unique tent function in the Schauder basis supported on $I_k$. With some work, one can show that $\| \sum_{j = 1}^n t_j \|_{l^\infty}$ is strictly increasing in $n$ and unbounded, whereas $\| \sum_{j = 1}^n (-1)^{j+1} t_j \|_{l^\infty} \leq 2$ for all $n$. If the basis was unconditional, we would have, for all $n > 0$,
    %
    \[ \| \sum_{j = 1}^n t_j \|_{l^\infty} \leq K_u \| \sum_{j = 1}^n (-1)^{j+1} t_j \|_{l^\infty} \leq 2K_u. \]
    %
    But taking $n \to \infty$ shows that $K_u$ cannot be bounded. In fact, $C[0,1]$ does not have an unconditional basis.
%    Thus $n_1 = 2$, $n_2 = 3$, and so on. Let $\{ c_k \}$ be the centres of the intervals $\{ I_k \}$, and let $a_n = \sum_{j = 1}^n t_j(v_n)$. Then $a_1 = 1$, $a_2 = 3/2$, and more generally,
    %
%    \[ a_n = 1 + \frac{a_{n-1}+a_{n-2}}{2}. \]
    %
%    Thus
    %
%    \[ a_n - a_{n-1} = 1 - \frac{a_{n-1} - a_{n-2}}{2}. \]
    %
%    Since $a_2 - a_1 = 1/2$, for $n \geq 2$ we have $a_n - a_{n-1} = 2/3 (1 - (-1/2)^n) \geq 1/2$, so the sequence $\{ a_n \}$ is strictly increasing and unbounded. Moreover, we have $\| \sum_{j = 1}^n t_n \|_{l^\infty} = a_n$. Now if $b_n = (\sum_{j = 1}^n (-1)^{j+1} t_j(v_n))$, then one can show that $\| \sum_{j = 1}^n (-1)^{j+1} t_j \|_\infty \leq 2$.
    % 2a_n - a_{n-1} - a_{n-2} = 1$.
    % f(z) = \sum_{k = 1}^\infty a_k z^k$.
    % 2f - zf - z^2 f = 2a_1z + 2a_2 z^2 - a_1 z^2 + z^3 / (1 - z)
    % (2 - z - z^2) f = 2z + 2z^2 + z^3 / (1 - z)
    % f = z(2 - z) / (1 - z)^2 (2 + z)
    % f = A/(1 - z) + 1/3(1 - z)^2 + -8/9(2 + z)
\end{example}

\begin{example}
    The Haar basis for $L^1[0,1]$ is conditional. Let $\{ g_n \}$ be the subsequence of the Haar basis $\{ h_n \}$ consisting solely of members such that $h_n(0) \neq 0$. Then
    %
    \[ \sum_{j = 1}^n g_j(t) = 2^{n-1} \]
    %
    for $t \in [0,1/2^{n-1}]$, and is zero elsewhere. Thus $\| \sum_{j = 1}^n g_j \|_{L^1[0,1]} = 1$ uniformly in $n$. Now consider a further subsequence of the $\{ g_j \}$. But we can produce a subsequence of the $\{ g_j \}$ such that $\| \sum_{j = 1}^k g_j \|_{L^1[0,1]} \geq k/2$, which would be impossible if $\{ h_n \}$ were unconditional.

    On the other hand, the Haar basis on $L^p[0,1]$ \emph{is} an unconditional basis for any $1 < p < 2$, a result due to Paley. TODO
\end{example}

Suppose that a Banach space $X$ and a Banach space $Y$ have two Schauder bases $\{ e_n \}$ and $\{ f_n \}$, such that $\sum a_n e_n$ converges in $X$ if and only if $\sum b_n f_n$ converges in $Y$. Then it is easy to see that there is an isomorphism $T: X \to Y$ such that $Te_n = f_n$. A useful consequence is that sequences not too far from a Schauder basis must also be a Schauder basis.

\begin{theorem}
    Suppose $\{ x_n \}$ is a basic sequence in a Banach space $X$ with basis constant $K$. If $\{ x_n' \}$ is another sequence in $X$ such that $\sum_n \| x_n \|^{-1} \| x_n - x_n' \| \leq 1/2K$, then $\{ x_n' \}$ is another basic sequence. If $\{ x_n \}$ is a Schauder basis, so is $\{ x_n' \}$.
\end{theorem}
\begin{proof}
    We note that if $\sum_n a_n x_n$ converges, then
    %
    \[ \sum_n a_n x_n' = \sum_n a_n x_n + \sum_n a_n (x_n' - x_n). \]
    %
    Now
    %
    \[ \sum_n \| a_n (x_n' - x_n) \| \leq \sum_n |a_n| \| x_n' - x_n \| \]
\end{proof}

\begin{corollary}
    If $X$ is a Banach space with a Schauder basis, and $X_0 \subset X$ is a dense subset, we can find a Schauder basis with elements in $X_0$.
\end{corollary}

TODO: Rest of Megginson on Schauder bases.









\chapter{Hilbert Spaces}

\section{Real Inner Product Spaces}

Hilbert spaces are those infinite dimensional vector spaces which behave analytically as close to $\RR^n$ as is possible. They are therefore the spaces where we have the most powerful. Just like in a Banach space, in a Hilbert space we can measure the distances between two points via a norm. But unlike in Banach spaces, in Hilbert spaces, we can measure the angle between two vectors with an \emph{inner product} connected to this norm.

Recall that in $\RR^2$, the angle $\theta \in [0,\pi]$ between two vectors $v$ and $w$ in $\RR^n$ satisfies the equation
%
\[ v_1w_1 + \dots + v_n w_n = |v||w| \cos \theta, \]
%
Where $|v|^2 = v_1^2 + \dots + v_n^2$ and $|w|^2 = w_1^2 + \dots + w_n^2$. The left hand quantity is often easier to work with than the angle $\theta$ itself, because it is \emph{bilinear} in the two vectors $v$ and $w$. It is called the \emph{scalar product} of the two vectors, and we denote it by $v \cdot w$. The length of a vector $v$ can then be described via the scalar product via the equation $|v|^2 = v \cdot v$.

Inner product spaces generalize the scalar product on $\RR^n$ to more general settings. An inner product on a vector space $V$ is a map associating to any two vectors $v$ and $w$ a scalar value $\langle v, w \rangle$, which is \emph{bilinear}, \emph{symmetric}, and \emph{positive definite}, in the sense that $\langle v, v \rangle > 0$ for $v \neq 0$. A n inner product space is then a vector space equipped with a fixed inner product.

\begin{example}
    For each $n \geq 0$, the vector space $\RR^n$ is an inner product space equipped with the scalar product $v \cdot w$.
\end{example}

\begin{example}
    For any measure space $X$, the space $L^2(X)$ of square-integrable real-valued functions is an inner product space, where the inner product of $f,g \in L^2(X)$ is
    %
    \[ \langle f, g \rangle = \int_{-\infty}^\infty f(x) g(x)\ dx. \]
    %
    The Cauchy-Schwartz inequality guarantees that this quantity is finite for any two functions. A particular example is $L^2(\RR)$, the square-integrable functions on the real line. Often more tractable for feeling out the basic theory is the space $l^2(\ZZ)$ of functions $f: \ZZ \to \RR$, such that
    %
    \[ \sum_{n \in \mathbf{Z}} f(n)^2 < \infty, \]
    %
    i.e. the space of square-integrable sequences.
\end{example}

Any inner product space $V$ is naturally a norm space if we define the length of a vector $v \in V$ by the equation $\| v \|^2 = \langle v, v \rangle$. One can see immediately that $\| \lambda v \| = |\lambda| \| v \|$, and that $\| v \| = 0$ if and only if $v = 0$. The triangle inequality will follow shortly, once we prove the most fundamental inequality in analysis, the Cauchy-Schwartz inequality, in the abstract setting of inner product spaces. One way to view the Cauchy-Schwartz inequality here is in terms of a generalization of the Pythagorean theorem in this setting. We say two vectors $v_1,v_2$ in an inner product space $V$ are \emph{orthogonal}, or \emph{perpendicular}, if $\langle v_1, v_2 \rangle = 0$. This means precisely that the two vectors form a right angle with one another.

\begin{theorem}[Pythagorean Theorem]
    If $v_1,\dots,v_n \in V$ are orthogonal,
    %
    \[ \| v_1 + \dots + v_n \|^2 = \| v_1 \|^2 + \dots + \| v_n \|^2 \]
\end{theorem}
\begin{proof}
    We just calculate that
    %
    \begin{align*}
        \| v_1 + \dots + v_n \|^2 &= \langle v_1 + \dots + v_n, v_1 + \dots + v_n \rangle\\
        &= \sum_{i,j = 1}^n \langle v_i, v_j \rangle\\
        &= \sum_{i = 1}^n \langle v_i, v_i \rangle\\
        &= \sum \| v_1 \|^2 + \dots + \| v_n \|^2. \qedhere
    \end{align*}
\end{proof}

\begin{corollary}
    Let $V$ be an inner product space. Fix $v_0 \in V$. Then for any $v \in V$, the projection
    %
    \[ P_{v_0}(v) = \frac{\langle v, v_0 \rangle}{|v_0|^2} v \]
    %
    is the closest vector on the line spanned by $v_0$ to the vector $v$.
\end{corollary}
\begin{proof}
    The vector $v - P_{v_0}(v)$ is orthogonal to any scalar multiple of $v_0$, and so the Pythagorean theorem guarantees that for any $\lambda \in \RR$,
    %
    \begin{align*}
        \| v - \lambda v_0 \|^2 &= \| v - P_{v_0}(v) + P_{v_0}(v) - \lambda v_0 \|^2\\
        &= \| v - P_{v_0}(v) \|^2 + \| P_{v_0}(v) - \lambda v_0 \|^2\\
        &\geq \| v - P_{v_0}(v) \|^2,
    \end{align*}
    %
    with equality if and only if $\lambda v_0 = P_{v_0}(v)$.
\end{proof}

\begin{theorem}[Cauchy Schwartz Inequality]
    Let $V$ be an inner product space. Then for any $v_1,v_2 \in V$,
    %
    \[ |\langle v_1, v_2 \rangle| \leq |v_1||v_2|. \]
\end{theorem}
\begin{proof}
    The idea of this proof, in two dimensions, is intuitively simple, a trivial consequence of the Pythagorean theorem: the hypotenuse of a triangle is it's longest side. We just rephrase this argument in terms of an inner product space. The two vectors $v_2 - P_{v_1}(v_2)$ and $P_{v_1}(v_2)$ are orthogonal to one another, and so the Pythagorean theorem guarantees that
    %
    \[ |v_2|^2 = \| P_{v_1}(v_2) \|^2 + \| v_2 - P_{v_1}(v_2) \|^2 \geq \| P_{v_1}(v_2) \|^2 = \frac{\langle v_1, v_2 \rangle^2}{|v_1|^2} \]
    %
    Rearranging the equation and taking square roots gives the inequality.
\end{proof}

\begin{remark}
    The inequality in this proof shows the Cauchy-Schwartz inequality is only tight when $v_1$ and $v_2$ are close to being scalar multiplies to one another, i.e. when $\| v_2 - P_{v_1}(v_2) \|$ is small. In particular, the Cauchy-Schwartz inequality is an \emph{equality} if and only if $v_1$ and $v_2$ are scalar multiples of one another.
\end{remark}

This theorem justifies the fact that we can define the angle $\theta \in [0,\pi]$ between two vectors by the equation
%
\[ \langle v, w \rangle = \|v\| \|w\| \cos \theta \]
%
The result also implies that the norm we have defined satisfies the triangle inequality.

\begin{theorem}[Triangle Inequality]
    Let $V$ be an inner product space. Then for any $v_1,v_2 \in V$,
    %
    \[ \| v_1 + v_2 \| \leq \| v_1 \| + \| v_2 \| \]
\end{theorem}
\begin{proof}
    We just calculate
    %
    \[ \| v_1 + v_2 \|^2 = \| v_1 \|^2 + 2 \langle v_1, v_2 \rangle + \| v_2 \|^2 \leq \| v_1 \|^2 + 2 \|v_1\| \|v_2\| + \|v_2\|^2 = \left(\|v_1 \| + \|v_2\| \right)^2 \]
    %
    and then we take square roots.
\end{proof}

We can now think of an inner product space as a kind of `infinite dimensional' Euclidean space, with a topology given by the length function, and induced by the triangle inequality. If the metric structure is {\it complete}, then we say that the inner product space is a \emph{Hilbert space}.

\section{Complex Inner Product Spaces}

Generalizing inner product spaces to vector spaces over the complex numbers presents several difficulties, because unless $V = 0$, there is no bilinear, symmetric, positive-definite form $\langle \cdot, \cdot \rangle$ on $V$, since we would then have for any $v \in V$,
%
\[ \langle iv, iv \rangle = i^2 \langle v, v \rangle = - \langle v, v \rangle. \]
%
To obtain a definition that works in this setting, we turn to our most basic example, the real vector space $\RR^2$, which we can identify with $\CC$. Here, complex conjugation plays a role in defining the distance function, removing the rotation properties of a complex number. The length of a particular complex number $z$ is given by the equation $|z|^2 = z \overline{z}$. Over $\mathbf{C}^n$, we then introduce the scalar {\it Hermitian product}
%
\[ (v,w) = v_1 \overline{w_1} + \dots + v_n \overline{w_n} \]
%
this equation agrees with the standard scalar product when $v$ and $w$ have real coordinates, and also gives a notion of length, since $(v,v) > 0$ for any $v \neq 0$. Unlike the scalar product, the hermitian product is not bilinear, but instead \emph{sesquilinear} (`one and a half linear' in Greek), i.e. linear in the first argument, but {\it antilinear} in the second, i.e. that
%
\[ (v, w + u) = (v,w) + (v,u) \quad\text{and}\quad (v, \lambda w) = \overline{\lambda} (v,w). \]
%
In general, we define a \emph{Hermitian inner product} $(\cdot, \cdot)$ over a complex vector space to be a sesquilinear symmetric map which is positive definite. A Hermitian inner product space is just a space with a Hermitian inner product. A complex Hilbert space is just a Hermitian inner product space whose norm gives a complete metric space structure. These spaces have much the same basic theory as their real counterpart, and we will treat them identically in the sequel until we get to more advanced contexts.

\begin{example}
    For any measure space $X$, the space $L^2(X)$ of square integrable functions is a  Hermitian inner product spaces under the inner product
    %
    \[ (f,g) = \int_X f(x) \overline{g(x)}\ dx. \]
    %
    As a special case, so is the space $l^2(\ZZ)$ of square integrable sequences, equipped with the inner product
    %
    \[ (f,g) = \sum_{n \in \mathbf{Z}} f(n) \overline{g(n)} \]
    %
    The conjugation generalize these spaces to complex functions.
\end{example}

\begin{example}
    We define the \emph{Hardy space} $H^2(\mathbf{D})$ to be the space of all functions $f(z)$, defined on the interior of the unit disk, and holomorphic there, such that the quantity
    %
    \[ \| f \|_{H^2(\mathbf{D})} = \sup_{0 \leq r < 1} \left( \frac{1}{2\pi} \int_{-\pi}^\pi |f(re^{it})|^2 \right)^{1/2} \ dt \]
    %
    is finite. This norm is induced by the inner product
    %
    \[ (f,g) = \sup_{0 \leq r < 1} \frac{1}{2\pi} \int_{-\pi}^\pi f(re^{it}) \overline{g(re^{it})} \]
    %
    which is finite by the Cauchy Schwartz inequality on $L^2[-\pi,\pi]$. Thus $H^2(\mathbf{D})$ is a Hermitian inner product space.

    We claim $H^2(\mathbf{D})$ is complete, so that it is a complex Hilbert space. To do this, we will show that, as an inner product space, it is isomorphic to $l^2(\NN)$. Let us begin by noting that any $f \in H^2(\mathbf{D})$ must have a power series expansion of the form
    %
    \[ f(z) = \sum_{n = 0}^\infty a_n z^n, \]
    %
    which converges on the interior of the unit disk. Moreover, for any $0 < r < 1$, Cauchy's integral formula shows that
    %
    \[ a_n = \frac{r^{-n}}{2 \pi} \int_{-\pi}^\pi f(re^{it}) e^{-nit}\; dt. \]
    %
    Thus Parseval's inequality, and the orthogonality of the exponentials on any disk centred at the origin, shows that
    %
    \[ \frac{1}{2\pi} \int_{-\pi}^\pi |f(re^{it})|^2 = \sum |a_n|^2 r^{2n}. \]
    %
    One immediate consequence of this is that the quantities
    %
    \[ \frac{1}{2\pi} \int_{-\pi}^\pi |f(re^{it})|^2 \]
    %
    are monotonically increase in $r$, so that
    %
    \[ \| f \|_{H^2(\mathbf{D})} = \lim_{r \to 1} \left( \frac{1}{2\pi} \int_{-\pi}^\pi f(re^{it})^2 \right)^{1/2} \ dt. \]
    %
    Moreover, we have $\| f \|_{H^2(\mathbf{D})} = \| a \|_{l^2(\NN)}$. Thus we have found an isometry $H^2(\mathbf{D}) \to l^2(\NN)$. For any $a \in l^2(\NN)$, $\limsup_{n \to \infty} |a_n|^{1/n} \leq 1$, and so the power series
    %
    \[ f(z) = \sum_{n = 0}^\infty a_n z^n \]
    %
    is holomorphic on the interior of the unit disk, and Cauchy Schwartz shows this is clearly the inverse map to the one above. Thus $H^2(\mathbf{D})$ is a complete Hermitian inner product space.

    It is a standard result of basic Fourier analysis that if $a = \{ a_n \}$ lies in $l^2(\NN)$, and we consider the square integrable function
    %
    \[ u(z) = \sum_{n = 0}^\infty a_n z^n, \]
    %
    in $L^2(\TT)$, then the Poisson kernels
    %
    \[ (P_r * u)(z) = \sum a_n r^n z^n \]
    %
    converge pointwise almost everywhere to $u$. Thus we obtain from our discussion \emph{Fatou's theorem}, that for any $f \in H^2(\mathbf{D})$, and for almost every $|z| = 1$, the \emph{radial limit}
    %
    \[ \lim_{r \to 1} f(rz) \]
    %
    is well defined, and on the boundary defines a square-integrable function in $L^2(\TT)$. The family of all functions obtained in this way is denoted $H^2(\TT)$. It is precisely the family of functions $f \in L^2(\TT)$ such that
    %
    \[ \widehat{f}(n) = 0 \]
    %
    for any $n < 0$.

    The function $f(z) = (1 - z)^{-i}$ lies in $H^2(\TT)$, but does not have a radial limit on the arc to one. If $\delta > 0$ is suitably small, it follows that for any countable set $\{ w_k \}$ on $\TT$
    %
    \[ g(z) = \sum_{i = 1}^\infty \delta^i f( z \overline{w_k} ) \]
    %
    lies in $H^2(\DD)$, but does not have radial limits on the arc towards any of the points $w_k$.

    For any $0 < s < 1$ and $w \in \TT$, the set $\Gamma_s(w)$ is defined to be the smallest convex set containined $w$ and the ball of radius $s$ at the origin. The \emph{tangential limit} of a function $f$ defined on the open unit disk at a point $w$ is
    %
    \[ \lim_{\substack{z \to w\\z \in \Gamma_s(w)}} f(z). \]
    %
    For any $0 < s < 1$, there exists $\varepsilon > 0$ such that for $z \in \Gamma_s(w)$,
    %
    \[ |(z - w) \cdot w| \geq \varepsilon \cdot (z - w). \]
    %
    Thus Cauchy Schwartz implies that if $w = e^{it_0}$, and $z \in \Gamma_s(w)$ can be written as $r e^{it}$, then there is $C > 0$ such that
    %
    \[ |t - t_0| \leq C (1 - r). \]
    %
    Thus if $f \in L^1(\TT)$, and $t_0$ lies in the Lebesgue set of $f$, then
    %
    \[ |(P_r * f)(t) - f(t_0)| \leq \int |f(s + t_0) - f(t_0)| P_r(s - (t - t_0))\; ds = (P_{r,t-t_0} * f)(t_0) - f(t_0), \]
    %
    where $P_{r,s_0}(s) = P_r(s - s_0)$. Now we know $\{ P_r \}$ satisfies bounds of the form
    %
    \[ |P_r(s)| \leq C_1 / (1 - r) \quad\text{and}\quad |P_r(s)| \leq C_1 (1 - r) / |s|^2, \]
    %
    and these bounds are sufficient to prove that $(P_r * f)(t_0)$ converges to $f(t)$ for any $t_0$ in the Lebesgue set of $f$. Similarily, where $C$ is as above, let $P_{r,s_0}(s) = P_r(s - s_0)$ for any $|s_0| \leq C (1 - r)$. We claim that we have uniform bounds of the form
    %
    \[ |P_{r,s_0}(s)| \leq C_2 / (1 - r) \quad\text{and}\quad |P_{r,s_0}(s)| \leq C_2 (1 - r) / |s|^2. \]
    %
    It would then follow that as $r \to 1$, $(P_{r,s_0} * f)(t)$ converges to $f(t_0)$ for any $t_0$ in the Lebesgue set of $f$, which would complete our argument. The first bound here follows immediately from the corresponding bound for $P_r$. On the other hand, if $|s| \geq 2C (1 - r)$, then
    %
    \[ |P_{r,s_0}(s)| = |P_r(s - s_0)| \leq C_1 (1 - r) / |s - s_0|^2 \leq 2 C_1 (1 - r) / |s|^2. \]
    %
    If $|s| \leq 2C (1 - r)$, then
    %
    \[ |P_{r,s_0}(s)| = |P_r(s - s_0)| \leq C_1 / (1 - r) = C_1 (1 - r) / (1 - r)^2 \leq 4C^2 C_1 (1 - r) / |s|^2. \]
    %
    Thus setting $C_2 = \max(2C_1,4C^2 C_1)$ implies we have shown what was required.
\end{example}

If $W$ is a closed subspace of a Hilbert space $H$, in the sense that it is a subspace, and it is a closed set under the topology induced by the metric structure of the hermitian product, then $W$ is also a Hilbert space, because a closed subset of a complete metric space is also complete. Conversely, a complete subspace is also closed. In the example above, the space $H^2(\TT)$ is a closed subspace of $L^2(\TT)$, and thus a Hilbert space.

\begin{example}
    If $\Omega$ is an open subset of the complex plane, the space $L^2_a(\Omega)$ of holomorphic square-integrable functions is a Hilbert space. If $B_r(x)$ is a ball of radius $r$ centered at $x$ whose closure is contained in $\Omega$, and $f$ is analytic in $\Omega$, then the mean value property guarantees that
    %
    \[ f(x) = \fint_{B_r(x)} f(y)\ dy \]
    %
    It follows from Cauchy Schwarz that if $0 < r < \text{dist}(x,\partial \Omega)$, then
    %
    \[ |f(x)| \leq \frac{1}{r\sqrt{\pi}} \|f \|_2 \]
    %
    In particular, this implies that if a sequence of analytic functions in $L^2_a(\Omega)$ converges in $L^2$ norm to some function, then they must also converge locally uniformly, so the resultant function is analytic. It follows that $L^2_a(\Omega)$ is closed in $L^2(\Omega)$, and so $L^2_a(\Omega)$ is a Hilbert space. In particular, since
    %
    \[ \| f \|_{L^2_a(\mathbf{D})}^2 \lesssim \| f \|_{H^2(\mathbf{D})}, \]
    %
    the Hardy space $H^2(\mathbf{D})$ is a closed subspace of $L^2_a(\mathbf{D})$.
\end{example}

There is some way to recover the intuition of the inner product measuring angles between vectors in the Hermitian setting. As we have seen in real spaces, the normalized inner product
%
\[ \frac{\langle v, w \rangle}{|w|^2} \]
%
is the scalar needed to projection $v$ onto the line spanned by the vector $w$. In complex vector spaces, the space spanned by a single vector is a {\it complex line}, or a one dimensional plane. If we set
%
\[ \frac{\langle v, w \rangle}{|w|^2} \]
%
to be the value we must scale $w$ by to obtain the projection of $v$ onto the plane spanned by $w$, then we might have to rotate $w$, hence $\langle v, w \rangle$ might have a rotational part. Indeed, this is exactly the correct geometrical definition of the inner product, if we already have a well defined projection operation. The Pythagorean theorem, Cauchy Schwartz inequality,and triangle inequality goes through for Hermitian product spaces unperturbed, and the proofs are essentially the same, as the reader can verify. Ultimately, however, the reason that everything for real inner product spaces `seems' to work for Hermitian products, is one of the main reasons we push through the initial lack of intuition.

Another approach to understanding Hermitian inner products is to use the fact that we can construct a Hermitian inner product from a real inner product. Recall that a complex vector space $V$ is just a real vector space with a fixed twisting linear map $J: V \to V$ with $J^2 = -1$. If we already have a {\it real} inner product $\langle \cdot, \cdot \rangle$ on $V$, then our hope is to extend the real inner product to a complex inner product $(\cdot,\cdot)$, where the real part of the Hermitian inner product is our original real product, so that
%
\[ \Re((v,w)) = \langle v, w \rangle \]
%
By geometric intuition, it is reasonable to assume that $J$ is a twist by a right angle, so that $\langle v, Jv \rangle = 0$ for all vectors $v$, and that $J$ is an isometry with respect to the inner product. It then follows that
%
\[ \langle v, Jw \rangle = \langle Jv, J^2w \rangle = \langle -Jv, w \rangle \]
%
so we cannot hope for the extension to be complex linear, only sesquilinear. If $(\cdot, \cdot)$ is a sesquilinear form, then we calculate
%
\[ \Im (v,w) = \Re(-i (v,w)) = \Re (v,iw) = \langle v, iw \rangle \]
%
so the {\it only} way to define $(\cdot, \cdot)$ properly is as
%
\[ (v,w) = \langle v, w \rangle + i \langle v, iw \rangle \]
%
The fact that $(v,w)$ measures the projection of $v$ onto the complex line containing $w$ appears again, because over the real numbers, the complex line is spanned over real scalars by $w$ and $Jw$, and the value above, once normalized, gives the real projection onto this two dimensional plane.

If $V$ is any real inner product space, then we can always complexify it by embedding $V$ in $V \oplus V$ by the trivial map $v \mapsto (v,0)$, and defining a real inner product $\langle (v_1,v_2), (w_1,w_2) \rangle = \langle v_1,w_1 \rangle + \langle v_2,w_2 \rangle$. We can then add an artifical twisting map $J(v,w) = (-w,v)$ gives $V \oplus V$ a complex structure, and since it satisfies the properties of the discussion above, the real inner product extends to give a complex inner product. The space with all these gadgets attached is called the \emph{complexification} of $V$, denoted $V^\mathbf{C}$. If $V$ is a Hilbert space, then so $V^\mathbf{C}$ will also be a Hilbert space. In particular, because of this fact, most results for inner product spaces can be proved first for hermitian inner products, and then reduced to inner products by this embedding.

\section{Orthogonality}

Recall the notion of orthogonal, that two vectors $x,y$ in an inner product space are orthogonal if $(x,y) = 0$. We sometimes write this statement in shorthand as $x \perp y$. Orthogonality is a symmetric relation, since if $(v,w) = 0$,
%
\[ (w,v) = \overline{(v,w)} = \overline{0} = 0 \]
%
The theory of right triangles essentially manifests in the theory of equations involving two to three vectors, as we have already seen in the case of the Pythagorean identity.

\begin{theorem}[Parallelogram Law]
    Let $H$ be a Hermitian inner product space. If $x,y \in H$, then
    %
    \[ \| x + y \|^2 + \| x - y \|^2 = 2 \| x \|^2 + 2 \| y \|^2 \]
    %
    which is geometrically intuitive, viewing $0, x, y$, and $x + y$ as vertices of a parallelogram.
\end{theorem}
\begin{proof}
    We simply calculate
    %
    \begin{align*}
        \| x + y \|^2 + \| x - y \|^2 &= \langle x + y, x + y \rangle + \langle x - y, x - y \rangle\\
        &= 2 \| x \|^2 + 2 \| y \|^2 + \langle x, y \rangle + \langle y, x \rangle - \langle x, y \rangle - \langle y, x \rangle\\
        &= 2 \| x \|^2 + 2 \| y \|^2
    \end{align*}
    %
    and this shows the theorem is true.
\end{proof}

\section{Optimizing over Convex Sets}

One of the biggest advantages of Hilbert spaces is that we have powerful theorems about the structure of convex sets in the space. A \emph{convex set} in a vector space is a set $C$ such that the line between any two points is contained in $C$. That is, for any $x, y \in C$, and $\lambda \in [0,1]$, $\lambda x + (1 - \lambda) y \in C$.

\begin{theorem}
    If $C$ is any closed convex subset of a Hilbert space, and $x \in H$ is arbitrary, then there is a unique point $y \in C$ which minimizes $\| x - y \|$, over all choices of $y$ in the convex set.
\end{theorem}
\begin{proof}
    Since $C - x$ is a convex set, we may assume without loss of generality that $x = 0$, and so we are trying to minimize the norm over $C$. If $x,y \in C$ minimize this norm, with $\| x \| = \| y \| = d$, then
    %
    \[ d \leq \left\| \frac{x + y}{2} \right\| \leq \frac{1}{2}(d + d) = \frac{d}{2} \]
    %
    Hence the parallogram law implies
    %
    \[ d^2 = \left\| \frac{x + y}{2} \right\|^2 = d^2 - \left\| \frac{x - y}{2} \right\|^2 \]
    %
    which implies $x = y$. We know that there must a sequence $x_i \in C$ with $\| x_i \| \to d$ monotonically, and then the parallelogram law implies that if $\| x_n \|, \| x_m \| \leq d + \varepsilon$, then since $(x_n + x_m)/2 \in C$, then
    %
    \[ \left\| \frac{x_n - x_m}{2} \right\|^2 = \frac{1}{2} \| x_n \|^2 + \frac{1}{2} \| x_m \|^2 - \left\| \frac{x_n + x_m}{2} \right\|^2 \leq 2 d \varepsilon + \varepsilon^2 \to 0 \]
    %
    hence $\{ x_n \}$ is a Cauchy sequence, and therefore converges to a point $x$, which satisfies $\| x \| = d$.
\end{proof}

Linear subspaces of a vector spaces are trivially verified to be convex sets, and we find that

\begin{theorem}
    If $V$ is a closed linear subspace of a Hilbert space $H$, and $v \in V$ is the closest point to a fixed point $x$, then $v - x \perp V$, in the sense that $v - x \perp w$ for any $w \in V$. Conversely, if $v - x \perp V$, then $v$ is the closest point to $x$.
\end{theorem}
\begin{proof}
    If $\langle v - x, w \rangle \neq 0$, then by multiplying $w$ by a scalar value, we may assume that $\langle v - x, w \rangle < 0$, and $\| w \| = 1$, and then for $\alpha > 0$,
    %
    \[ \| \alpha w + v - x \|^2 = \| v - x \|^2 + 2 \alpha \Re \langle w, v - x \rangle + |\alpha|^2 \| w \|^2  \]
    %
    and so as $\alpha \to 0$, we find that eventually $\| \alpha w + v - x \|^2 < \| v - x \|^2$. Conversely, if $\langle v - x, w \rangle = 0$ for all $w$, then for any $w$,
    %
    \[ \| w - x \|^2 = \| (w - v) + (v - x) \|^2 = \| w - v \|^2 + \| v - x \|^2 \geq \| v - x \|^2 \]
    %
    so $v$ minimizes the distance to $x$ among all vectors in $V$.
\end{proof}

For any subset $X$ of an inner product space $V$, define $X^\perp$ to be the set of vectors $v$ such that $\langle x, v \rangle = 0$ for all $x \in X$. It is clearly a closed linear subspace of $V$. If $V$ is a closed linear subspaces of a Hilbert space $H$, then for any $x \in H$, there is a unique $y \in V$ with $y - x \perp V$, and we may define a map $P: H \to V$ such that $Px = y$, which we call the \emph{orthogonal projection} of $H$ onto $V$.

\begin{theorem}
    If $P: H \to V$ is defined as above, then
    %
    \begin{enumerate}
        \item $P$ is a linear map.
        \item $\| Px \| \leq \| x \|$.
        \item $P^2 = P$.
        \item $\ker P = V^\perp$, and $\im P = V$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    If $x_0 - y_0 \perp V$ and $x_1 - y_1 \perp V$, then $(\alpha x_0 - \alpha y_0) + (\beta x_1 - \beta y_1) \perp V$, hence $P(\alpha x_0 + \beta x_1) = \alpha y_0 + \beta y_1$, and so $P$ is a linear map. It is obvious that $P$ is a contraction, because $\| x \|^2 = \| Px \|^2 + \| x - Px \|^2$, and $\| x - Px \|^2 \geq 0$. For any $y \in V$, $y - y \perp V$, hence $Py = y$, and so $P^2 = P$. Finally, $Py = 0$ if and only if $y \perp V$, in which case we find $y \in V^\perp$.
\end{proof}

\begin{example}
    Given $f \in L^2[-\pi,\pi]$, we can consider an $L^2$ Fourier expansion
    %
    \[ f(x) = \sum_{n = -\infty}^\infty \widehat{f}(n) e^{nit}. \]
    %
    The partial sum operators are
    %
    \[ S_N f(x) = \sum_{n = -N}^N \widehat{f}(n) e^{nit} \]
    %
    and these operators are \emph{precisely} the projection operators onto the finite dimensional subspace of $L^2[-\pi,\pi]$ spanned by $\{ e_{-N}, \dots, e_N \}$. We also have an orthogonal projection
    %
    \[ Pf(x) = \sum_{n = 0}^\infty \widehat{F}(n) e^{nit} = \frac{1}{2 \pi i} \int_{\partial \mathbf{D}} \frac{f(w)}{w - z}\; dw. \]
    %
    whose image we can identify with $H^2(\TT)$, e.g. the space of all functions which form the boundary values of holomorphic functions in $H^2(\mathbf{D})$.
\end{example}

\begin{corollary}
    For any closed subspace $V$ of a Hilbert space $H$, $(V^\perp)^\perp = V$. If $X$ is an arbitrary subset of a Hilbert space $H$, then $(X^\perp)^\perp = \overline{\text{span}}(X)$ is the closure of the linear span of the elements of $X$.
\end{corollary}
\begin{proof}
    If $P$ is the orthogonal projection of $H$ onto $V$, and $Q$ is the orthogonal projection of $H$ onto $V^\perp$, then $Px + Qx = x$ for all $x \in H$, and so the kernel of $Q$ is exactly the set of vectors $x$ for which $Px = x$, from which we conclude that $x \in V$. If $X$ is arbitrary, then $\overline{\text{span}}(X)$ is contained in $(X^\perp)^\perp$, because $(X^\perp)^\perp$ is a closed subspace of $H$ containing all elements in $X$. Conversely, we conclude that $X^\perp = \overline{\text{span}}(X)^\perp = X^\perp$, because if $\langle y, x \rangle = 0$ for all $x \in X$, then surely $\langle y, \sum \alpha_i x_i \rangle = \sum \alpha_i \langle y, x_i \rangle = 0$, so $y \perp \text{span}(X)$, and if $v_i \to v$, with $v_i \in \text{span}(X)$, then by continuity of the inner product we find that $0 = \langle y, v_i \rangle \to \langle y, v \rangle$, hence $\langle y, v \rangle = 0$, and so $y \perp \overline{\text{span}}(X)$.
\end{proof}

\begin{corollary}
    A subspace $V$ is dense in $H$ if and only if $V^\perp = 0$.
\end{corollary}
\begin{proof}
    For then the closure of $V$ is $(V^\perp)^\perp = 0^\perp = H$.
\end{proof}

Given two Hilbert spaces $H_0$ and $H_1$, we can define a canonical Hilbert space structure on $H_0 \oplus H_1$ by letting $\langle x_0 + x_1, y_0 + y_1 \rangle = \langle x_0, y_0 \rangle + \langle x_1, y_1 \rangle$. The induced topology is precisely the product topology, because $x_i + y_i \to x + y$ if and only if $x_i \to x$, and $y_i \to y$. The orthogonal projection of $H$ onto $V$ is incredibly important to the theory of Hilbert spaces, because it gives us a direct sum decomposition of $H$ into $V \oplus V^\perp$. The Pythagorean theorem tells us that for any $v \in V$ and $w \in V^\perp$,
%
\[ \| v + w \|^2 = \| v \|^2 + \| w \|^2 \]
%
and so the map $x \mapsto Px + (1 - P)x$ is actually an \emph{isometry} between vector spaces (it preserves the inner product). This is the basic notion of equivalence between various Hilbert spaces, and so we really can think of $H$ as being composed of the two closed subspaces $V$ and $V^\perp$.

\section{The Riesz Representation Theorem}

Many problems in functional analysis can be reduced to the analysis of certain linear functions between vector spaces. The easiest linear functions to analyze are the \emph{linear functionals}, which are linear maps $f: V \to K$, where $K$ is the base field of the vector space $V$. We will say a linear functions $f: V \to K$ is \emph{bounded} if there exists $M > 0$ such that $|f(x)| \leq M \| x \|$ for all $x \in V$. The bounded linear maps are exactly the continuous linear maps (and to verify continuity, we need only verify continuity at zero, so bounded linear maps are essentially `uniformly continuous'). If $f$ and $g$ are continous, then $\lambda f + \gamma g$ is continuous, and so the set $V^*$ of bounded linear functionals on $V$ is a vector space, called the \emph{dual space} of $V$. On the dual space of a norm space, we may define a dual norm by letting $\| f \|$ be the smallest number $M$ such that $|f(x)| \leq M \| x \|$ holds for all $x$. Since $|f(x) + g(x)| \leq |f(x)| + |g(x)|$, we find $\| f + g \| \leq \| f \| + \| g \|$, and it is easy to verify that $\| \lambda f \| = |\lambda| \| f \|$.

Given a Hilbert space $H$, we can use the inner product structure on $H$ to transport vectors in $H$ to functionals in $H^*$. That is, for a given $x \in H$, we define $x^* \in H^*$ to be the functional $x^*(y) = \langle x, y \rangle$. The Cauchy-Schwarz inequality tells us that $\| x^* \| = \| x \|$, so the dual map is an isometry. The Riesz representation theorem tells us that this isometry is actually invertible.

\begin{theorem}[Riesz]
    For any bounded linear function $f \in H^*$, there exists a unique vector $x \in H$ such that $f(y) = \langle x, y \rangle$ for all $y \in H$.
\end{theorem}
\begin{proof}
    Assume without loss of generality that $f \neq 0$. Let $V$ be the kernel of $f$. Then $V$ is a closed linear subspace of $H$, and $V^\perp$ is one dimensional by the first isomorphism theorem. Consider the orthogonal projection $P$ onto $V^\perp$. If $x$ is a nonzero vector in $V^\perp$, with $\| x \| = 1$ and $f(x) = \gamma$, then for each $y$ there is $\lambda(y)$ for which $f(y) = f(Py) = \lambda(y)$, and what's more, $\langle y, x \rangle = \langle Py, x \rangle = \lambda(y)$, and so we have found a unique $y$ as specified, and $y$ is unique because the dual map is an isometry.
\end{proof}

\begin{example}
    For any measure space $X$, $L^2(X)$ is a Hilbert space, and so for any bounded linear functional $\alpha: L^2(X)^*$, there is a unique $g \in X$ such
    %
    \[ \alpha(f) = \int_X f(x) \overline{g(x)}\; dx. \]
    %
    Thus studying linear functionals on $L^2(X)$ reduces to integration theory.
\end{example}

\section{Constructing an Orthonormal Basis}

If $\{ e_1, \dots, e_n \}$ are an \emph{orthonormal} family of vectors in an inner product space $V$, in the sense that they are pairwise orthogonal, and $\| e_n \| = 1$ for all $i$, then the Pythagorean identity implies
%
\[ \| \sum_n a_ne_n \| = \left( \sum |a_n|^2 \right)^{1/2}. \]
%
Using slightly more analysis, and using the fact that we are working in a Hilbert space $H$, this result remains true for an infinite family of orthonormal vectors $\{ e_n \}$, provided that $\sum |a_n|^2 < \infty$, because then the partial sums of $\sum a_n e_n$ are Cauchy, and because a Hilbert space is \emph{complete}, the partial sums must converge to an infinite sum. For any vector $v$, the sum $\sum_n (v,e_n) e_n$ also converges to some vector in $H$, and by continuity of the inner product, the vector $v - \sum (v,e_n) e_n$ is orthogonal to all of the vectors $\{ e_n \}$. Thus we find that
%
\[ \| v \|^2 = \| v - \sum (v,e_n) e_n \|^2 + \sum_n |(v,e_n)|^2 \geq \sum_n |(v,e_n)|^2. \]
%
This inequality is \emph{Bessel's inequality}. If $v = \sum (v,e_n) e_n$, then we find that
%
\[ \| v \|^2 = \sum_{i = 1}^\infty |(v,e_n)|^2. \]
%
This equation is known as {\it Parsevel's identity}. We now give a series of conditions on $\{ e_n \}$ guaranteeing that for any $v \in H$, $v = \sum_n (v,e_n) e_n$.

\begin{theorem}
    The following properties of an orthonormal family $\{ e_n \}$ are equivalent:
    %
    \begin{itemize}
        \item[(a)] For any vector $v \in H$, $v = \sum_n (v,e_n) e_n$.
        \item[(b)] Finite linear combinations of the $e_i$ are dense in $H$.
        \item[(c)] If $(v,e_n) = 0$ for all $n$, then $v = 0$.
        \item[(d)] Parseval's identity holds.
    \end{itemize}
    %
    We then say the sequence is an \emph{orthonormal basis} for the space.
\end{theorem}
\begin{proof}
    (a) clearly implies (b). Assuming (b), note that if $(v,e_i) = 0$ for all $i$, then in particular, $(v,w) = 0$ for all $w$ which can be expressed as finite linear combinations of the $e_i$. But we may choose $w_n$ with $w_n \to v$ in $L^2$, and then since $|(v,w_n - v)| \leq \|v\|\|w_n - v\| \to 0$, we conclude that $0 = (v,w_n) = (v,v) + (v,w_n-v)$ converges to $(v,v)$, so $(v,v) = 0$, and hence $v = 0$. If $v$ is given, then Bessel's inequality ensures that
    %
    \[ \sum_{i = 1}^\infty |(v,e_i)|^2 \leq \| v \| < \infty \]
    %
    and so the partial sums of the sequence $\sum_{i = 1}^\infty |(v,e_i)|^2$ are Cauchy, and because we are working in a Hilbert space, converge to some vector $w$. Now $v - w$ is orthogonal to $e_i$ for each $i$, since
    %
    \[ (w,e_i) = \lim \sum_{j = 1}^n (v,e_j)(e_j,e_i) = (v,e_i) \]
    %
    and therefore $v = w$, proving (a). (a) implies (d) is trivial, and if (d) holds, then Pythagoras' identity implies that
    %
    \[ \| v - \sum_{i = 1}^n (v,e_i) e_i \|^2 = \sum_{i = n+1}^\infty |(v,e_i)|^2 \to 0 \]
    %
    so (a) holds.
\end{proof}

A topological space is {\it separable} if it has a countable dense subset. If $H$ is a separable Hilbert space, then by performing the Gram Schmidt process to a countable dense subset, one finds an orthogonal sequence, which is easily verified to be an orthonormal basis for the Hilbert space. Thus \emph{every separable Hilbert space has a orthonormal basis}. With some more work on the geometry of Hilbert spaces, and the axiom of choice, we can show that an arbitrary Hilbert space has an orthonormal basis, if we allow bases ranging over an uncountable index set. Before we do this, however, we consider some applications.

\begin{example}
    Fourier analysis was the first area of mathematics where the ideas of orthogonality were employed in infinite dimensions. Since $[-\pi,\pi]$ is a finite measure space, the space $L^2[-\pi,\pi]$ is contained within $L^1[-\pi,\pi]$, and so if $f \in L^2[-\pi,\pi]$, we can consider it's Fourier series
    %
    \[ \widehat{f}(n) = \frac{1}{2\pi} \int_{-\pi}^\pi f(x) e^{-nix}\ dx. \]
    %
    If we define $e_n(x) = e^{nix}$, then $\widehat{f}(n) = (f,(2\pi)^{-1} e_n)$, and
    %
    \[ (e_n,e_m) = \int_{-\pi}^\pi e^{(n-m)ix} = \begin{cases} 2 \pi & n = m \\ 0 & n \neq m \end{cases} \]
    %
    It follows that $\{ (2 \pi)^{-1} e_n \}$ is an orthonormal sequence in $L^2[-\pi,\pi]$. Using the basic fact from Fourier analysis that for any $f \in C[-\pi,\pi]$ with $f(-\pi) = f(\pi)$, the partial sums of
    %
    \[ \sum \widehat{f}(n) e^{nix} \]
    %
    converge uniformly to $f$, and the fact that continuous functions are dense in $L^2[-\pi,\pi]$, we conclude that the exponentials $\{ (2\pi)^{-1} e_n \}$ are an orthonormal basis for $L^2[-\pi,\pi]$. As a result, we obtain the classic Parseval's inequality
    %
    \[ \fint_{-\pi}^\pi |f(x)|^2\ dx = \sum_{n = -\infty}^\infty |\widehat{f}(n)|^2, \]
    %
    and that for any $f \in L^2[-\pi,\pi]$, the partial sums of
    %
    \[ \sum \widehat{f}(n) e^{nix} \]
    %
    converge in $L^2[-\pi,\pi]$ to $f$.
\end{example}

\begin{example}
    Viewing $\TT$ as the boundary of the unit disk in $\CC$, equipped with the normalized Lebesgue measure. Then for appropriately smooth $f,g \in L^2(\TT)$, we can interpret $(f,g)$ as a contour integral, i.e.
    %
    \[ (f,g) = \int_{\TT} \frac{f(z) \overline{g(z)}}{2 \pi iz}\; dz, \]
    %
    where the torus is given the standard orientation. Now consider the M\"{o}bius transformation
    %
    \[ z = \frac{i - x}{i + x}, \]
    %
    which is an oriented mapping from $\RR$ to $\TT - \{ -1 \}$. Now
    %
    \[ dz = -2i \frac{1}{(x + i)^2}. \]
    %
    Thus
    %
    \[ (f,g) = \frac{1}{\pi} \int_{-\infty}^\infty f \left( \frac{i - x}{i + x} \right) \overline{g \left( \frac{i - x}{i + x} \right)} \frac{1}{1 + x^2}\; dx. \]
    %
    Thus if we consider the map $T: L^2(\TT) \to L^2(\RR)$ given by
    %
    \[ Tf(x) = f \left( \frac{i - x}{i + x} \right) \frac{\pi^{-1/2}}{x + i}, \]
    %
    then $T$ is an \emph{isometry}, i.e. $(Tf, Tg) = (f,g)$ for each $f,g \in L^2(\TT)$. In particular, since $L^2(\TT)$ has an orthonormal basis of the form $\{ z^n : n \in \ZZ \}$, we conclude that $L^2(\RR)$ has an orthonormal basis of the form
    %
    \[ \pi^{-1/2} \left( \frac{i - x}{i + x} \right)^n \frac{1}{i + x}. \]
\end{example}

We can generalize to non separable spaces by an application of Zorn's lemma. If $\{ S_\alpha \}$ is a chain of orthonormal sets, then $\bigcup S_\alpha$ is an orthonormal set, so we may apply Zorn's lemma to conclude that a maximal orthonormal set $S$ exists. We claim this is an orthonormal basis, which is equivalent to showing that $S^\perp = \{ 0 \}$. And indeed, if $S^\perp$ was nontrivial, we could find $x \in S^\perp$ with $\| x \| = 1$, and then $S \cup \{ x \}$ would be an orthonormal set, contradicting maximality. Thus orthonormal bases exist on any Hilbert space.

\begin{theorem}
    Any two orthonormal bases of a Hilbert space $H$ have equal cardinality.
\end{theorem}
\begin{proof}
    If $S$ is an orthonormal basis for a Hilbert space $H$, then one of two possibilities hold:
    %
    \begin{itemize}
        \item Some orthonormal basis of $H$ is finite: Then $H$ is finite dimensional, and the result follows by standard linear algebra.
        \item There does not exist a finite orthonormal basis for $H$: Let $T$ be another orthonormal basis. For each $x \in H$, the set $T_x = \{ e \in T: (e,x) \neq 0 \}$ is at most countable, since the sum $\sum |(e,x)|^2$ converges. But $T = \bigcup_{s \in S} T_s$ since $S$ is an orthonormal basis, which means that
        %
        \[ \#(T) \leq \#(S) \cdot \#(\NN) = \#(S). \qedhere \]
    \end{itemize}
\end{proof}

We may therefore define the \emph{dimension} of a Hilbert space to be the cardinality of any orthonormal basis. This actually uniquely defines the Hilbert space, for if $H_0$ and $H_1$ are Hilbert spaces with orthonormal bases $\{ e_i \}$ and $\{ f_i \}$, then we have an isometry $T: H_0 \to H_1$ given by
%
\[ T(\sum a_i e_i) = \sum a_i f_i. \]
%
This is well defined, for the sum $\sum a_x x$ converges if and only if $\sum |a_x|^2 < \infty$, and the same criterion shows $\sum a_x f(x)$ converges.

\begin{example}
    For any set $X$ equipped with the counting measure, we have an orthonormal basis on $l^2(X)$, given by the family of functions $\{ e_x : x \in X \}$, where $e_x(y) = \mathbf{I}(x = y)$. The family is certainly orthonormal. If $f \in l^2(X)$ satisfies $f(x) = \langle f, e_x \rangle = 0$ for all $x \in X$, then $f = 0$, so the set is a basis. Thus the dimension of the Hilbert space $l^2(X)$ is $\#(X)$. If $X$ is not countable, then $l^2(X)$ is not separable.
\end{example}

\begin{example}
    Define an inner product on the space $\text{AP}_0(\RR)$ of all finite linear combinations of exponentials on $\RR$, i.e. an inner product on the span of $\{ e^{2 \pi i \lambda x} : \lambda \in \RR \}$, by declaring the set to be orthonormal. For any two $f,g \in \text{AP}_0(\RR)$, one can verify that
    %
    \[ \langle f,g \rangle = \lim_{T \to \infty} \frac{1}{2T} \int_{-T}^T f(t) \overline{g(t)}\; dt, \]
    %
    i.e. this is an inner product space in which each of the exponentials in $\{ e^{2 \pi i \lambda x} \}$ are separable. Write the resulting norm as $\| f \|_{\text{AP}(\RR)}$. The \emph{completion} of $\text{AP}_0(\RR)$ is denoted $\text{AP}(\RR)$. Note that since $\text{AP}(\RR)$ has an uncountable basis, it is \emph{not separable}.

    A continuous function $f \in C_b(\RR)$ is called \emph{uniformly almost periodic} if it is the uniform limit of a family of functions in $\text{AP}_0(\RR)$. By virtue of the fact that $\| f \|_{\text{AP}(\RR)} \leq \| f \|_{L^\infty(\RR)}$, almost periodic functions can be identified with a certain subfamily of elements of $\text{AP}(\RR)$. If $f_0$ is uniformly almost periodic, and $0 < \varepsilon \leq 1$, then we can find $f \in \text{AP}_0(\RR)$ such that $\| f - f_0 \|_{L^\infty(\RR)} \leq \varepsilon$. The function $f$ is the linear combination of finitely many exponentials oscillating at a certain family of frequencies $\{ \lambda_1,\dots,\lambda_n \}$, i.e. we can write $f = \sum a_i e_{\lambda_i}$. If $|\lambda_1|, \dots, |\lambda_n| \leq \alpha$, then
    %
    \begin{align*}
        |f(t + s) - f(t)| &\leq \sum a_i |e_{\lambda_i}(t+s) - e_{\lambda_i}(t)|\\
        &\leq \left( \sum |a_i|^2 \right)^{1/2} \left( \sum |\lambda_i s + \ZZ|^2 \right)^{1/2}\\
        &= \| f \|_{\text{AP}(\RR)} \left( \sum |\lambda_i s + \ZZ|^2 \right)^{1/2}\\
        &\leq 2 \| f_0 \|_{L^\infty(\RR)} \left( \sum |\lambda_i s + \ZZ|^2 \right)^{1/2}.
    \end{align*}
    %
    Consider the sequence $X(n)_i = \lambda_i n + \ZZ$ in $\TT^n$. Then
    %
    \[ |X(n)| = \left( \sum |\lambda_i n + \ZZ|^2 \right)^{1/2}. \]
    %
    By Weyl equidistribution, this sequence is either periodic, or equidistributed on some finite union of cosets of a subtorus of $\TT^n$. Fix some $R > 0$ large enough such that $X(1), \dots, X(R)$ is a $\varepsilon$ cover of this finite union of cosets. Then, given any interval $I \subset \RR$ of length $R+1$, we can find an integer $n_0 \in I$ such that $[n_0 - R, n_0] \subset I$. Find $n \in [1,R]$ such that $|X(n) - X(n_0)| \leq \varepsilon$, then $n - n_0 \in I$, and $|X(n - n_0)| = |X(n) - X(n_0)| \leq \varepsilon$. Thus
    %
    \[ |f_(t + n) - f(t)| \leq |f(t + n) - f(t)| + \varepsilon \leq (1 + 2\| f_0 \|_{L^\infty(\RR)}) \cdot \varepsilon. \]
    %
    Adjusting parameters, we conclude that for any almost periodic function $f_0$, and any $\varepsilon > 0$, we can find $L > 0$ such that any interval $I$ of length $L$ contains an `almost period' $x_0$, such that $|f_0(x + x_0) - f_0(x)| \leq \varepsilon$ for all $x \in \RR$. Conversely, suppose that for each $\varepsilon > 0$, we can find $L > 0$ such that any interval of length $L$ contains a $\varepsilon$ almost period for $f$. Consider a trigonometric polynomial $g$, periodic on $[-10L,10L]$, such that $\| f - g \|_{L^\infty[-10L,10L]} \leq \varepsilon$. Then for any $x_1 \in \RR$, we can find $x_0 \in \RR$ with $|x_1 - x_0| \leq L$ such that $|f(x + x_0) - f(x)| \leq \varepsilon$ for all $x \in \RR$. Then
    %
    \[ f(x_1) = f(x_1 - x_0) + O(\varepsilon) = g(x_1 - x_0) + O(\varepsilon) = g(x_1 - x_0 + 10L) \]
    %
    \[ |f(x + x_0) - f(x)| \leq \varepsilon. \]
    %
    We can then find a trigonometric polynomial $g = \sum a_n e_{n/x_0}$ such that $\| f - g \|_{L^\infty[0,x_0]} \leq \varepsilon$. TODO FINISH THIS PROOF PLUS LOOK UP BESICOVITCH, ALMOST PERIODIC FUNCTIONS.
\end{example}



\section{Adjoints on Hilbert Spaces}

If $H$ is a Hilbert space, then the characterization of $H^*$ is very easy - it is naturally isomorphic to $H$ itself. And Hilbert spaces form very neat isomorphism classes, so, roughly speaking, the theory of problems about the elements of Hilbert spaces is roughly speaking, a solved problem. Modern research on Hilbert spaces is really about the bounded operators on a Hilbert space, which form a rich and nontrivial class for which many open problems remain unsolved. Let us begin a deeper study of these operators.

The first powerful tool in the study of operators on Hilbert spaces is the existence of \emph{adjoints}. Given a linear map $T: X \to Y$ between Banach spaces, we obtain a natural adjoint map $T^*: Y^* \to X^*$. If $X$ and $Y$ are Hilbert spaces, then we have antilinear isomorphisms $X \cong X^*$ and $Y \cong Y^*$, and so we obtain an equivalent definition of the adjoint as a linear map $T^*: Y \to X$, which satisfies, for $x \in X$ and $y \in Y$, $\langle Tx, y \rangle = \langle x, T^* y \rangle$. Let us introduce this a little more concretely.

\begin{lemma}
    If $( \cdot, \cdot )$ is a continuous sesquilinear form on a Hilbert space $H$, then there is a unique $T \in B(H)$ such that $(x,y) = \langle x, Ty \rangle$ for all $x,y \in H$.
\end{lemma}
\begin{proof}
    Since $( \cdot, \cdot)$ is continuous, the uniform boundedness principle implies that there exists a quantity $L > 0$ such that for any $x,y \in H$,
    %
    \[ |(x,y)| \leq L \| x \| \| y \|. \]
    %
    For each $x$, $(\cdot, x)$ is a bounded linear functional, so there is a unique $Tx \in H$ such that $(y,x) = \langle y, Tx \rangle$. Now
    %
    \begin{align*}
        \langle y, T(ax_1 + bx_2) \rangle &= (y, ax_1 + bx_2)\\
        &= \overline{a} (y,x_1) + \overline{b} (y,x_2)\\
        &= \overline{a} \langle y, Tx_1 \rangle + \overline{b} \langle y, Tx_2 \rangle\\
        &= \langle y, a Tx_1 + b Tx_2 \rangle,
    \end{align*}
    %
    it follows that $T$ is a linear map, i.e. $T(ax_1 + bx_2) = aTx_1 + b T x_2$. Moreover, since
    %
    \[ |\langle x, Ty \rangle| = |(x,y)| \leq L \| x \| \| y \|, \]
    %
    it follows that $\| Ty \| \leq L \| y \|$.

    \[ \langle x, My \rangle = (x,y) \leq \|(\cdot, \cdot)\| \| x \| \| y \| \]
    %
    So $\| M \| \leq \|(\cdot,\cdot)\|$, but also
    %
    \[ (x,y) = \langle x, My \rangle \leq \| M \| \| x \| \| y \| \]
    %
    so $\|(\cdot, \cdot)\| \leq \| M \|$, and we have equality of norms.
\end{proof}

\begin{theorem}
    Let $H_1$ and $H_2$ be Hilbert spaces. If $T: H_1 \to H_2$ is a bounded linear map, then there is a unique bounded linear map $T^*: H_2 \to H_1$ such that
    %
    \[ \langle Tx, y \rangle = \langle x, T^*y \rangle \]
    %
    The map $T^*$ is the \emph{adjoint} of $T$. We have $T^{**} = T$, and $\| T^* \| = \| T \|$.
\end{theorem}
\begin{proof}
    The map $(x,y) = \langle Tx, y \rangle$ is a sesquilinear form, so there is a unique $T^*$ such that $(x,y) = \langle x, T^*y \rangle$. By taking suprema over $x \in H$ with $\| x \| = 1$ on the right hand side of the equation defining $T^*$, we have
    %
    \[ \| T^* y \| = \sup_{\| x \| = 1} \langle Tx, y \rangle \leq \| T \| \| y \|. \]
    %
    Thus $\| T^* \| \leq \| T \|$. It is simple to see that $T^{**} = T$, because for any $x \in H_1$ and $y \in H_2$,
    %
    \[ \langle T^{**} x, y \rangle = \langle x, T^* y \rangle = \langle Tx, y \rangle. \]
    %
    But this means by symmetry that $\| T \| = \| T^{**} \| \leq \| T^* \|$, which implies $\| T \| = \| T^* \|$.
\end{proof}

\begin{example}
    If we are working over the field $\RR$, then every operator $B(\RR^n, \RR^m)$ can be identified with a unique matrix in $M_{n,m}(\RR)$, and the adjoint corresponds to taking the transpose of a matrix. Working over the field $\CC$, every operator $B(\CC^n, \CC^m)$ can be identified with a unique matrix in $M_{n,m}(\CC)$, and the adjoint here is the conjugate tranpose.
\end{example}

\begin{example}
    Given $\phi \in L^\infty(X)$, where $X$ is a $\sigma$-finite measure space, we have an operator $M_\phi \in B(L^2(X))$ defined by $M_\phi(f) = \phi f$. Then
    %
    \[ \langle M_\phi f, g \rangle = \int \phi f \overline{g} = \int f \overline{g \overline{\phi}} = \langle f, \overline{\phi} g \rangle \]
    %
    Thus $M_\phi^* = M_{\overline{\phi}}$.
\end{example}

The adjoint operation $*$ is an antilinear operator from $B(H_1,H_2)$ to $B(H_2,H_1)$, such that $(ST)^* = T^*S^*$. This is verified by calculation, since
%
\[ \langle ST x, y \rangle = \langle Tx, S^* y \rangle = \langle x, T^*S^* y \rangle \]
%
We also have $\| T^*T \| = \| T \|^2$, since
%
\[ \langle T^*T x, x \rangle = \langle Tx, Tx \rangle = \| T \|^2 \| x \| \]
%
so $\| T^*T \| \leq \| T \|^2$, and this is inequality is attained in the suprema, since if $x_i \in B_H$ is such that $\| T x_i \| \to \| T \|$, then
%
\[ \langle T^*T x_i, x_i \rangle = \| T x_i \|^2 \to \| T \|^2 \]
%
Thus $*$ is what is known as an \emph{involution}, and makes $B(H)$ into a \emph{$C^*$ algebra}.

Next, we prove a result which generals the rank-nullity theory theorem from finite dimensional linear algebra.

\begin{theorem}
    Fix a linear operator $T: H \to H$ on a Hilbert space $H$. Then
    %
    \[  H = \text{Im}(T) \oplus \text{Ker}(T^*) = \text{Ker}(T) \oplus \text{Im}(T^*). \]
\end{theorem}
\begin{proof}
    By symmetry, it suffices to prove the first equality. The kernel of $T^*$ is closed, and so it suffices to show that $\text{Ker}(T^*) = \text{Im}(T)^\perp$. If $x \in \text{Im}(T)$ and $y \in \text{Ker}(T^*)$, then we can write $x = Tx'$, and then
    %
    \[ \langle x, y \rangle = \langle Tx', y \rangle = \langle x', T^* y \rangle = \langle x', 0 \rangle = 0. \]
    %
    Thus $\text{Ker}(T^*) \subset \text{Im}(T)^\perp$. Conversely, if $y \in \text{Im}(T)^*$, then for any $x \in H$,
    %
    \[ \langle T^* y, x \rangle = \langle y, Tx \rangle = 0. \]
    %
    This means $T^* y = 0$.
\end{proof}

In the special case where $T$ is \emph{self adjoint}, i.e. $T^* = T$, we conclude that
%
\[ H = \text{Ker}(T) \oplus \text{Im}(T). \]
%
We will later see this remains true for the class of normal operators, and we can continue refining this orthogonal decompositions into approximate eigenspaces, which, when $T$ is compact, can be completely refined to give a complete eigenfunction expansion.

Any operator $M: H \to H$ can be expressed as $M = T + iS$, where $T$ and $S$ are self adjoint operators, by defining
%
\[ T = \frac{1}{2}(M + M^*)\ \ \ \ \ S = \frac{1}{2i}(M - M^*) \]
%
That $T$ and $S$ are unique follows because if $T_1$ and $T_2$ are self-adjoint, and $T_1 = iT_2$, then we also have
%
\[ T_1 = T_1^* = (iT_2)^* = -i T_2^* = -iT_2 \]
%
Thus $iT_2 = -iT_2$, which means $T_1 = T_2 = 0$. We denote $T$ by $\text{Re}(M)$ and $S = \text{Im}(M)$. An operator $M: H \to H$ is \emph{normal} if $M^* M = MM^*$, or equivalently, if $\text{Re}(M) \text{Im}(M) = \text{Im}(M) \text{Re}(M)$.

\begin{theorem}
    If $T$ is a self-adjoint operator, then
    %
    \[ \| T \| = \sup \{ \langle Tx, x \rangle: \| x \| \leq 1 \} \]
\end{theorem}
\begin{proof}
    Let $M$ be the supremum. Clearly $M \leq \| T \|$. If $\| x \|, \| y \| \leq 1$,
    %
    \begin{align*}
        \langle T(x \pm y), x \pm y \rangle &= \langle Tx, x \rangle \pm \langle Ty, x \rangle \pm \langle Tx, y \rangle + \langle Ty, y \rangle\\
        &= \langle Tx, x \rangle \pm \overline{\langle Tx, y \rangle} \pm \langle Tx, y \rangle + \langle Ty, y \rangle\\
        &= \langle Tx, x \rangle \pm 2 \Re \langle Tx, y \rangle + \langle Ty, y \rangle
    \end{align*}
    %
    Then, subtracting, we find
    %
    \[ 4\Re \langle Tx, y \rangle = \langle T(x+y),x+y \rangle - \langle T(x - y), x - y \rangle \]
    %
    Thus
    %
    \[ 4 \Re \langle Tx, y \rangle \leq M(\|x + y\|^2 + \|x - y\|^2) = 2M(\|x\|^2 + \|y\|^2) \leq 4M \]
    %
    Let $\lambda$ be such that $\langle Tx, y \rangle = \lambda |\langle Tx, y \rangle|$. If we replace $x$ by $\lambda x$, we find
    %
    \[ |\langle Tx, y \rangle| = \overline{\lambda} \langle Tx, y \rangle = \langle T(\overline{\lambda}x), y \rangle \leq M \]
    %
    Hence
    %
    \[ \| Tx \| = \sup \{ |\langle Tx, y \rangle| : y \in B_H \} \leq M \]
    %
    and therefore $\| T \| \leq M$.
\end{proof}

\begin{remark}
    We will later see that this result remains true for all \emph{normal operators} $T$, using the spectral theory of such operators. The main idea is that the spectral theory implies the existence of an \emph{approximate eigenvalue} $\lambda \in \CC$ for $T$ with $|\lambda| = \| T \|$, i.e. a scalar such that for any $\varepsilon > 0$, there is $x \in H$ with $\| x \| = 1$ such that $\| Tx - \lambda x \| \leq \varepsilon$. This implies that if $M$ is as in the proof above, then
    %
    \[ M \geq \left| \langle Tx, x \rangle \right| \geq |\lambda| - \varepsilon = \| T \| - \varepsilon, \]
    %
    and we can take $\varepsilon \to 0$.
\end{remark}

\begin{corollary}
    Let $T: H \to H$ be self adjoint. Then
    %
    \[ \langle Tx, x \rangle = 0 \]
    %
    for all $x \in H$ if and only if $T = 0$.
\end{corollary}

\begin{lemma}
    An operator $T \in B(H)$ in a complex Hilbert space is self adjoint if and only if $\langle Tx, x \rangle \in \mathbf{R}$ for each $x \in H$.
\end{lemma}
\begin{proof}
    If $T$ is self adjoint, then
    %
    \[ \langle Tx, x \rangle = \langle x, Tx \rangle = \overline{\langle Tx, x \rangle} \]
    %
    Conversely, if $\langle Tx, x \rangle \in \RR$ for each $x \in H$, then
    %
    \[ \langle Tx, x \rangle = \overline{\langle x, Tx \rangle} = \langle x, Tx \rangle = \langle T^*x, x \rangle \]
    %
    for each $x$. This means that if $S = i(T - T^*)$, then $S$ is self-adjoint, and $\langle Sx, x \rangle = 0$ for all $x \in H$. This implies $S = 0$, which can only be true if $T = T^*$. Thus $T$ is self-adjoint.
\end{proof}

\begin{corollary}
    If $\langle Tx, x \rangle = \langle Sx, x \rangle$ for each $x \in H$, then $T = S$.
\end{corollary}

\begin{corollary}
    An operator $T: H \to H$ is normal if and only if for any $x \in H$,
    %
    \[ \| Tx \| = \| T^*x \|. \]
\end{corollary}
\begin{proof}
    If $T$ is normal, then for any $x \in H$, we calculate that
    %
    \[ \| Tx \|^2 = \langle Tx, Tx \rangle = \langle T^*Tx, x \rangle = \langle TT^*x, x \rangle = \langle T^*x, T^*x \rangle = \| T^* x \|^2. \]
    %
    Conversely if $\| Tx \| = \| T^* x \|$ for all $x \in H$, then we find for any $x \in H$,
    %
    \[ \langle T^*Tx, x \rangle = \langle TT^*x, x \rangle. \]
    %
    Since $T^*T$ and $TT^*$ are both self-adjoint, this means that $T^*T = TT^*$, i.e. $T$ is normal.
\end{proof}

The next few lemmas show that these are the unique self-adjoint operators with this property. This is essentially like beginning with $\mathbf{C}$, and identifying $\mathbf{R}$ as those elements invariant under complex conjugation.

\begin{theorem}
    $T$ is normal if and only if $\| Tx \| = \| T^* x \|$ for each $x$.
\end{theorem}
\begin{proof}
    For then, for each $x$,
    %
    \[ \| Tx \|^2 = \langle Tx, Tx \rangle = \langle T^*Tx, x \rangle \]
    \[ \| T^*x \|^2 = \langle T^*x, T^*x \rangle = \langle TT^*x, x \rangle \]
    %
    which implies $T^*T = TT^*$. The converse repeats backwards through the proof, since the norm of $\| T \|$ is specified by its action through the inner product.
\end{proof}

\begin{theorem}
    Any normal operator $T$ has the following properties.
    %
    \begin{enumerate}
        \item[(a)] $\text{ker}(T) = \text{ker}(T^*)$.
        \item[(b)] $T(H)$ is dense in $H$ if and only if $T$ is injective.
        \item[(c)] $T$ is a surjective isomorphism if and only if $\| Tx \| \geq C \| x \|$ for some $C$.
        \item[(d)] If $Tx = \lambda x$, then $T^*x = \overline{\lambda} x$.
        \item[(e)] If $\lambda$ and $\gamma$ are distinct eigenvalues, then the eigenspaces are orthogonal to one another.
    \end{enumerate}
\end{theorem}
\begin{proof}
    To prove $(a)$ notice that $\| Tx \| = 0$ if and only if $\| T^*x \| = 0$. $(b)$ follows since
    %
    \[ T(H)^\perp = \text{ker}(T^*) = \text{ker}(T) \]
    %
    If $\| Tx \| \geq C \| x \|$, then $T(H)$ is closed, and dense, therefore $T(H) = H$. The converse follows by definition. If $T$ is normal, then $\lambda - T$ is also normal, since $(\lambda - T)^* = \overline{\lambda} - T^*$, so $\text{ker}(\lambda - T) = \text{ker}(\lambda - \overline{\lambda})$. Finally, if $Tx = \lambda x$, and $Ty = \gamma y$, then
    %
    \[ \lambda \langle x, y \rangle = \langle Tx, y \rangle = \langle x, T^*y \rangle = \langle x, \overline{\gamma} y \rangle = \gamma \langle x, y \rangle \]
    %
    so if $\lambda \neq \gamma$, then $\langle x,y \rangle = 0$.
\end{proof}

The natural automorphisms in the category of Hilbert spaces are the \emph{unitary operators}, which we now introduce.

\begin{theorem}
    If $U \in B(H)$, then the following are equivalent.
    %
    \begin{enumerate}
        \item[(a)] $U$ is unitary.
        \item[(b)] $GL(H) = H$ and $\langle Ux, Uy \rangle = \langle x, y \rangle$.
        \item[(c)] $GL(H) = H$ and $\| Ux \| = \| x \|$, so $U$ is an isometry from $H$ to itself.
    \end{enumerate}
    %
    If any of these results are true, we say $U$ is a \emph{unitary operator}.
\end{theorem}
\begin{proof}
    If $U$ is unitary, then $U$ is invertible, so $GL(H) = H$, and
    %
    \[ \langle Ux, Uy \rangle = \langle U^*Ux, y \rangle = \langle x, y \rangle \]
    %
    If $(b)$ holds, then
    %
    \[ \| Ux \|^2 = \langle Ux, Ux \rangle = \langle x, x \rangle = \| x \|^2 \]
    %
    If $(c)$ holds, then
    %
    \[ \langle U^*Ux, x \rangle = \langle Ux, Ux \rangle = \|Ux\|^2 = \|x\|^2 = \langle x, x \rangle \]
    %
    Thus $U^*U$ is the identity. Since $U$ is injective and surjective, the open mapping theorem tells us $U$ is invertible, so $U^* = U^{-1}$.
\end{proof}

We have already seen the importance of orthogonal projections in the basic theory of Hilbert spaces, and now we offer an operator theoretic characterization of these orthogonal projections.

\begin{theorem}
    If $P: H \to H$ whose image is a subspace $W$ of $H$, then the following are equivalent:
    %
    \begin{enumerate}
        \item[(a)] $P$ is self-adjoint.
        \item[(b)] $P$ is normal.
        \item[(c)] $W = \text{ker}(P)^\perp$.
        \item[(d)] $\langle Px, x \rangle = \| Px \|^2$.
    \end{enumerate}
    %
    In any of the three cases, we say $P$ is the \emph{orthogonal projection} onto $W$, and $W$ uniquely defines $P$.
\end{theorem}
\begin{proof}
    If $P$ is self-adjoint, then $P$ is trivially normal. If $P$ is normal, then
    %
    \[ P(H) = \text{ker}(P^*)^\perp = \text{ker}(P)^\perp \]
    %
    If (c) holds, then every $x = y + Px$, where $Py = 0$. Then
    %
    \[ \langle P(y + Px), y + Px \rangle = \langle Px, y + Px \rangle = \langle Px, Px \rangle = \| Px \|^2 \]
    %
    If (d) holds, then $\langle Px, x \rangle \in \mathbf{R}$ for each $x$, so $P$ is self-adjoint. If (c) holds, then $\langle Px, x \rangle$
\end{proof}

If $H$ is a Hilbert space, then two orthogonal projections $P_1$ and $P_2$ might not necessarily commute. A simple computation verifies that if $P_1$ and $P_2$ are one-dimensional projections, then $P_1 P_2 = P_2 P_1$ if and only if these lines are orthogonal to one another, or if $P_1 = P_2$. We have a more general characterization of commuting projections.
% As a basic example, let us consider only projections onto a line. Fix $\| v_1 \| = \| v_2 \| = 1$, and consider $P_i x = \langle x, v_i \rangle v_i$. Then
%
%\[ P_1 P_2 x = \langle x, v_2 \rangle \langle v_2, v_1 \rangle v_1 \]
%
%and
%
%\[ P_2 P_1 x = \langle x, v_1 \rangle \langle v_1, v_2 \rangle v_2. \]
%
%If the lines are orthogonal, i.e. $\langle v_1, v_2 \rangle = 0$, then $P_1 P_2 = P_2 P_1 = 0$ trivially commute. On the other hand, if the lines are not orthogonal, then we find that $v_1$ and $v_2$ are scalar multiplies of one another, in which case $P_1 = P_2$. Thus one dimensional projections only commute if they are equal to one another, or projections onto orthogonal subspaces.

\begin{theorem}
    Let $H$ be a Hilbert space, and let $P_1$ and $P_2$ be orthogonal projections onto subspaces $W_1$ and $W_2$ of $H$. Let $U_1 = (W_1 \cap W_2)^\perp \cap W_1$ and $U_2 = (W_1 \cap W_2)^\perp \cap W_2$. Then the following are equivalent:
    %
    \begin{itemize}
        \item $P_1$ and $P_2$ commute.
        \item $P_1 P_2$ is an orthogonal projection.
        \item $U_1$ is perpendicular to $U_2$.
    \end{itemize}
\end{theorem}
\begin{proof}
    Suppose $P_1$ and $P_2$ commute, and let $P = P_1 P_2 = P_2 P_1$. Then
    %
    \[ P^2 = P_1 P_2 P_1 P_2 = P_1^2 P_2^2 = P_1 P_2 = P. \]
    %
    Thus $P$ is a projection. Moreover,
    %
    \[ P^* = (P_1 P_2)^* = P_2 P_1 = P_1 P_2 = P. \]
    %
    Thus $P$ is an orthogonal projection. Since $P = P_1 P_2$, the image of $P$ is contained in $W_1$. Since $P = P_2 P_1$, the image of $P$ is contained in $W_2$. Thus the image of $P$ is contained in $W_1 \cap W_2$. Conversely, if $x \in W_1 \cap W_2$, then $Px = x$. Thus $P$ is projection onto $W_1 \cap W_2$. On the other hand, suppose $P_1 P_2$ is an orthogonal projection. Then $P_1 P_2$ is self adjoint, so $P_1 P_2 = (P_1 P_2)^* = P_2 P_1$, so $P_1$ and $P_2$ commute.

    Now suppse $P_1$ and $P_2$ commute. Write $P_i = P_0 + P_{U_i}$, where $P_0$ is orthogonal projection onto $W_1 \cap W_2$, and $\tilde{U_i}$ is orthogonal projection onto $U_i$. Then
    %
    \[ P_1 P_2 = (P_0 + P_{U_1})(P_0 + P_{U_2}) = P_0 + P_{U_1} P_{U_2} \]
    %
    and
    %
    \[ P_2 P_1 = (P_0 + P_{U_2}) (P_0 + P_{U_1}) = P_0 + P_{U_2} P_{U_1}. \]
    %
    If $P_1$ and $P_2$ commute, $P_1 P_2 = P_2 P_1 = P_0$, and so $P_{U_1} P_{U_2} = P_{U_2} P_{U_1} = 0$. Thus $U_1$ is orthogonal to $U_2$. Conversely, if $U_1$ is orthogonal to $U_2$, then $P_{U_1} P_{U_2} = P_{U_2} P_{U_1} = 0$, and from the equations above we see $P_1$ and $P_2$ commute.
\end{proof}






\chapter{Basic Spectral Theory}

TODO: If $T: X \to X$ is an operator on a Banach space, then $\sigma(T^*) = \sigma(T)$.

Recall that an \emph{eigenvector} of a linear operator $T: X \to X$ with eigenvalue $\lambda$ is a vector $x \in X$ such that $Tx = \lambda x$. The spectral theory encountered in linear algebra shows that any self-adjoint linear operator on a finite dimensional vector space can be diagonalized. Here we try and generalize this result to infinite dimensional spaces. In general, this is not so simple to generalize. The most basic reformulation (any self-adjoint operator on a Hilbert space has a basis of eigenvectors) is \emph{not correct}, because of the existence of \emph{almost eigenvectors}.

\begin{example}
    Let $T: L^2[0,1] \to L^2[0,1]$ be the bounded linear operator given by
    %
    \[ Tf(X) = x f(x). \]
    %
    Then $T$ is self-adjoint, but has no eigenvalues, for if $Tf = \lambda f$, for any $\lambda$, then $x f(x) = \lambda f(x)$, which implies $f$ vanishes except at $\lambda$, which means $f = 0$ almost everywhere. On the other hand, any $\lambda \in [0,1]$ is \emph{almost} an eigenvalue, in the following sense. If $f(x) = \mathbf{I}(x \in I)$, where $I$ is an interval of length $\delta$ containing $\lambda$, then $|Tf(x) - \lambda f(x)| \leq \delta \cdot \mathbf{I}(x \in I)$, and so
    %
    \[ \| Tf - \lambda f \|_{L^2[0,1]} \leq \delta |I|^{1/2} \leq \delta \| f \|_{L^2[0,1]}. \]
    %
    Thus we have $Tf \approx \lambda f$, and we can find functions $f$ such that this approximation holds to any degree of accuracy.
\end{example}

In the finite dimensional setting, the existence of an eigenvalue $\lambda$ for a linear map $T: X \to X$ is equivalent to the operator $T - \lambda: X \to X$ being \emph{non-invertible}. But in infinite dimensions an operator can fail to be invertible in various qualitatively different ways:
%
\begin{itemize}
    \item $T - \lambda$ can fail to be injective: This happens if and only if there is a non-zero point $x \in X$ such that $Tx = \lambda x$, i.e. $\lambda$ is a true eigenvalue for $T$.
    \item $T - \lambda$ is injective, but does not have closed range: Then for any $\varepsilon > 0$ we can find $x \in X$ with $\| x \| = 1$ such that $\| Tx - \lambda x \| \leq \varepsilon$. Thus $\lambda$ is an \emph{almost} eigenvalue of $T$.
    \item $T - \lambda$ is injective with closed range, but fails to be surjective: Then $T - \lambda$ is an isomorphism of $X$ onto a proper, closed subspace of $X$.
\end{itemize}
%
We call the set of values $\lambda$ for which the first property holds the \emph{point spectrum} of $T$, denoted $\sigma_p(T)$, the second set the \emph{continuous spectrum} $\sigma_c(T)$, and the third set the \emph{residual spectrum} $\sigma_r(T)$. These form three disjoint sets, whose union is the \emph{spectrum} of $T$, denoted $\sigma(T)$, which consists precisely of the set of $\lambda$ such that $T - \lambda$ is \emph{not invertible}. The residual spectrum is difficult to detect, but we will find that for a large class of operators that occur in practice, most importantly, the normal operators on a Hilbert space, $\sigma_r(T)$ is empty, so we do not have to analyze this spectrum very often.

\begin{example}
    Let $\{ \lambda_n \}$ be an enumeration of all rational numbers in $[0,1]$. Then consider the bounded operator $T: l^2(\NN) \to l^2(\NN)$ given by
    %
    \[ (Ta)(n) = \lambda_n \cdot a(n) \]
    %
    We see that $\langle Ta, a \rangle \geq 0$ for all $a \in l^2(\NN)$, and $\| Ta \|_{l^2(\NN)} \leq \| a \|_{l^2(\NN)}$ for all $a \in H$, from which it follows that $\sigma_p(T) \subset [0,1]$. In fact, we have $\sigma_p(T) = [0,1] \cap \QQ$, since
    %
    \[ Te_n = \lambda_n e_n \]
    %
    for each $n$, so $[0,1] \cap \QQ \subset \sigma_p(T)$. And conversely, if $\lambda$ and $a$ exist such that $Ta = \lambda a$, then $\lambda a(n) = \lambda_n a(n)$ for all $n$, which implies that $\lambda = \lambda_n$ for some $n$, or $a(n) = 0$ for all $n$. Thus we have fully calculated the point spectrum. On the other hand, for any $\lambda \in [0,1] - \QQ$, we can find a sequence $\{ \lambda_{n_i} \}$ in $\QQ \cap [0,1]$ converging to $\lambda$, and then we have
    %
    \[ \| T e_{n_i} - \lambda e_{n_i} \| \to 0, \]
    %
    so $\lambda \in \sigma_c(T)$. Thus we see that $\sigma(T) = [0,1]$, and $\sigma_r(T) = \emptyset$. That the residual spectrum of $T$ is empty will follow from the general theory because $T$ is a self-adjoint operator.
\end{example}

\section{Spectral Theory for Compact Operator}

We have seen that in infinite dimensions, linear operators have the phenomenon of \emph{almost eigenvalues}. One main reason to specialize to the study of compact operators is that this phenomenon disappears almost entirely: every almost eigenvalue is actually a proper eigenvalue.

\begin{lemma}
    If $T: X \to X$ is a compact operator on a Banach space, then $\sigma_c(T) = \emptyset$.
\end{lemma}
\begin{proof}
    Suppose that there exists $\lambda$, and a sequence $\{ x_n \}$ in $X$ such that $\| x_n \| = 1$ for all $n > 0$, and $\| Tx_n - \lambda x_n \| \to 0$. Since $T$ is a compact operator, and the sequence is bounded, we may assume without loss of generality that $Tx_n$ converges to some $x \in X$ with $\| x \| = 1$. Now
    %
    \[ \| Tx - \lambda x \| = \lim_{n \to \infty} \| T^2 x_n - \lambda T x_n \| \leq \| T \| \lim_{n \to \infty} \| Tx_n - \lambda x_n \| = 0. \]
    %
    Thus $Tx = \lambda x$, so every almost eigenvalue for $T$ is actually a proper eigenvalue.
\end{proof}

Much more can be said in the case of compact operators (and even more can be said when we consider \emph{normal} compact operators on a Hilbert space). We begin with a very useful lemma, due to Riesz.

\begin{lemma}
    Suppose that $X_0$ is a closed, proper subspace of a norm space $X$, and that $0 < \theta < 1$. Then there is $x \in S_X$ such that $d(x,X_0) \geq \theta$.
\end{lemma}
\begin{proof}
    Find $x^*$ in $S_{X^*}$ vanishing on $X_0$. Since $\| x^* \| = 1$, we can find $x \in X$ such that $x^*(x) = \theta$. But then for any $x_0 \in X_0$,
    %
    \[ \theta = |x^*(x)| = |x^*(x) - x^*(x_0)| \leq \| x - x_0 \|. \]
    %
    Thus $d(x,X_0) \geq \theta$.
\end{proof}

\begin{theorem}
    Suppose that $T: X \to X$ is a compact operator on a Banach space. If $\lambda \neq 0$, then $(\lambda - T)(X) = X$ if and only if $\lambda - T$ is injective. In particular, $\sigma_r(T) \subset \{ 0 \}$.
\end{theorem}
\begin{proof}
    Suppose $(\lambda - T)(X) = X$, but $\lambda - T$ is not injective. Then $(\lambda - T)^n$ is also surjective for each $n > 0$, which implies that the spaces $X_n = \text{Ker}((\lambda - T)^n)$ are a strictly increasing family of closed subspaces of $X$. Applying Riesz's lemma, we can find $x_n \in X_{n+1}$ such that $d(x_n,X_n) \geq 1/2$. Since $T$ commutes with $(\lambda - T)^n$ for all $n$, we find $T(X_n) \subset X_n$. But for any pair of integers $n > m$, $(\lambda - T)(x_n) + Tx_m \in X_n$ since
    %
    \[ (\lambda - T)^n \left( (\lambda - T)(x_n) + Tx_m \right) = (\lambda - T)^{n+1}(x_n) + T(\lambda - T)^n (x_m) = 0. \]
    %
    Thus
    %
    \[ \| Tx_n - Tx_m \| = \| \lambda x_n - [(\lambda - T)(x_n) + Tx_m] \| \geq 1/2 |\lambda|. \]
    %
    Thus $\{ Tx_n \}$ does not have a convergent subsequence, contradicting the fact that $T$ is compact. Thus $\lambda - T$ is injective. The converse will follow from the following theorem.
\end{proof}

\begin{theorem}
    Suppose $X$ is a Banach space, $T: X \to X$ is a compact operator, and $\lambda$ is a non-zero scalar. Then $\lambda - T$ has finite dimensional kernel and cokernel and closed range, and the dimension of the kernel is the same as the dimension of the cokernel.
\end{theorem}
\begin{proof}
    The kernel of $\lambda - T$ cannot be infinite dimensional, for otherwise we would have an infinite family of $\{ x_n \}$ in $X$ with $\| x_n - x_m \| \geq 1$ for $n \neq m$, and with $Tx_n = \lambda x_n$ for all $\lambda$, which would contradict the compactness of $T$.

    Now since the kernel of $\lambda - T$ is finite dimensional, it has a complemented subspace $X_0$ in $X$, and the restriction of $\lambda - T$ to $X_0$ is injective. If $\lambda - T$ was not an isomorphism from $X_0$ to the range of $\lambda - T$, then there would be a sequence $\{ x_n \}$ in $B_{X_0}$ such that $(\lambda - T)(x_n)$ converged to zero. Since $\{ x_n \}$ is bounded and $T$ is compact, there is a subsequence $\{ x_{n_i} \}$ such that $\{ Tx_{n_i} \}$ converges to some $x \in X$. But then $\lambda x_{n_i}$ converges to $x$, so that $x \in X_0 - \{ 0 \}$. But
    %
    \[ (\lambda - T)(x) = \lim_{n_i} (\lambda - T)(x_{n_i}) = 0, \]
    %
    contradicting the fact that $\lambda - T$ is injective on $X_0$. Thus $\lambda - T$ is an isomorphism when restricted to $X_0$, and so $\lambda - T$ has closed range.

    It remains to show that the cokernel of $\lambda - T$ is finite dimensional, and has the same dimension as the kernel. We will repeatedly rely on the fact that for any linear operator $T$, the kernel of $T$ is equal to ${}^\perp (T^*(Y^*))$, and the kernel of $T^*$ is equal to $T(X)^\perp$.

    Because $T$ is compact, $T^*$ is compact, and thus the kernel of $\lambda - T^*$ is finite dimensional. Now the kernel of $\lambda - T^*$ is equal to $(\lambda - T)(X)^\perp$, and thus isomorphic to the dual space of $\text{Coker}(\lambda - T) = X / (\lambda - T)(X)$. This implies that $X / (\lambda - T)(X)$ is finite dimensional, and thus that $\lambda - T$ has finite codimension, with
    %
    \[ \dim(\text{Coker}(\lambda - T)) = \text{dim}(\text{Ker}(\lambda - T^*)). \]
    %
    Now we work the other way. We have $\text{Ker}(\lambda - T) = {}^\perp ((\lambda - T^*)(X^*))$. Since $T^*$ is compact, $\lambda - T^*$ also has closed range, and thus weak $*$ closed range. But this means that $\text{Ker}(\lambda - T)^\perp = (\lambda - T^*)(X^*)$, and so $X^* / (\lambda - T^*)(X^*)$ is isomorphic to $X^* / \text{Ker}(\lambda - T)^\perp$, which is isomorphic to $\text{Ker}(\lambda - T)^*$. We know $\text{Coker}(\lambda - T^*)$ is finite dimensional because $T^*$ is compact, which means that
    %
    \[ \dim(\text{Ker}(\lambda - T)) = \dim(\text{Coker}(\lambda - T^*)). \]
    %
    We claim our proof would be complete if we could prove that
    %
    \[ \dim(\text{Ker}(\lambda - T)) \leq \dim(\text{Coker}(\lambda - T)). \]
    %
    Indeed, if we could prove this, then it would also follow by duality that
    %
    \[ \dim(\text{Ker}(\lambda - T^*)) \leq \dim(\text{Coker}(\lambda - T^*)) \]
    %
    which implies, in virtue of the above identities, that
    %
    \[ \dim(\text{Ker}(\lambda - T)) \geq \dim(\text{Coker}(\lambda - T^*)). \]
    %
    Write $X_0 = \text{Ker}(\lambda - T)$, and $X_1 = (\lambda - T)(X)$. Find complementary subspaces $X_0'$ and $X_1'$ to $X_0$ and $X_1$ respectively. Our goal is to prove that $\dim(X_0) \leq \dim(X_1')$. Suppose that $\dim(X_0) > \dim(X_1')$. Then we can find a non-injective operator $S: X_0 \to X$ with $S(X_0) = X_1'$. Then $S$ is finite rank, and thus compact. If $P: X \to X_0$ is a continuous projection of $X$ onto $X_0$, then $T + SP$ is a compact, non-injective operator. But $T + SP$ is surjective, since
    %
    \begin{align*}
        (\lambda - (T + SP)) &= ((\lambda - T) - SP)(X_0) + ((\lambda - T) - SP)(X_0')\\
        &= SP(X_0) + (\lambda - T)(X_0')\\
        &= X_1' + X_1 = X,
    \end{align*}
    %
    which gives a contradiction. Thus $\dim(X_0) \leq \dim(X_1')$.
\end{proof}

\begin{remark}
    The proof of this theorem also shows that if $T: X \to X$ is a compact operator on a Banach space, then $\text{Ker}(\lambda - T) = \text{Ker}(\lambda - T^*)$.
\end{remark}

Now we move on to showing that a compact operator can only have countably many eigenvalues.

\begin{lemma}
    Let $T: X \to X$ be a compact operator, and let $\{ \lambda_n \}$ be a sequence of distinct elements of $\sigma_p(T)$. Then $\lambda_n \to 0$.
\end{lemma}
\begin{proof}
    Without loss of generality, assume that $\lambda_n \neq 0$ for all $n$. Recursively construct a sequence $\{ x_n \}$ in $X$ with the following properties:
    %
    \begin{itemize}
        \item $\| x_n \| = 1$.
        \item $Tx_n = \lambda_n x_n$.
        \item If $X_n$ is the span of $\{ x_1, \dots, x_n \}$, then $d(x_{n+1},X_n) \geq 1/2$.
    \end{itemize}
    %
    This is possible by Riesz's lemma. Thinning this sequence, since $T$ is compact, we may assume $\{ Tx_n \} = \{ \lambda_n x_n \}$ converges. But for $n > m$,
    %
    \[ |\lambda_{n+1} x_{n+1} - \lambda_n x_n | \geq |\lambda_{n+1}| d(x_{n+1},X_n) \geq |\lambda_{n+1}|/2. \]
    %
    Thus
    %
    \[ \lim_{n \to \infty} |\lambda_n| = \lim_{n \to \infty} |\lambda_{n+1}| \leq 2 \lim_{n \to \infty} |\lambda_{n+1} x_{n+1} - \lambda_n x_n| = 0. \qedhere \]
\end{proof}

Recall that a \emph{generalized eigenvector} for a linear operator $T: X \to X$ with eigenvalue $\lambda$ is a point $x$ such that $(\lambda - T)^n x = 0$ for some $n > 0$. We can also consider generalized eigenvectors for compact operators on Banach spaces.

\begin{theorem}
    Let $T: X \to X$ be a compact operator, and consider $\lambda \neq 0$. Then the space of all generalized eigenvectors for $T$ with eigenvalue $\lambda$ is finite dimensional.
\end{theorem}
\begin{proof}
    Let $X_n = \text{Ker}((\lambda - T)^n)$. Then $\{ X_n \}$ is an increasing family of closed subspaces of $X$, that we claim stabilizes after finitely many iterations. We note that $(\lambda - T)^n = \lambda^n - S_n$ for some compact operator $S_n$. Thus the kernel of $(\lambda - T)^n$ is finite dimensional, with the same dimension as it's cokernel. If $X_{n_0} = X_{n_0 + 1}$ for some $n_0$, then we claim $X_n = X_m$ for all $n,m \geq n_0$. Indeed, if $m > n_0$, and $(\lambda - T)^m x = 0$, then certainly $(\lambda - T)^{n_0+1} (\lambda - T)^{m - n_0 - 1} x = 0$., which implies
    %
    \[ (\lambda - T)^{n_0+m-1} x = (\lambda - T)^{n_0} (\lambda - T)^{m - n_0 - 1} x = 0. \]
    %
    Thus $X_m = X_{n_0}$. Thus if the sequence does not stabilize, it is strictly increasing for all $n$. Thus by Riesz's lemma, we can find a sequence $\{ x_n \}$ such that $x_n \in X_{n+1}$ with $\| x_n \| = 1$, and $d(x_n,X_n) \geq 1/2$. But this means that, as in the proof that surjective compact operators are injective, that
    %
    \[ \| Tx_n - Tx_m \| \geq 1/2|\lambda|, \]
    %
    which is impossible if $T$ is compact. Thus by contradiction, the sequence $\{ X_n \}$ must stabilize, and so the sequence of generalized eigenvectors is finite dimensional.
\end{proof}

Thus the \emph{algebraic multiplicity} of each nonzero eigenvalue is well defined for a compact operator, as the dimension of the space of generalized eigenvectors for that eigenvalue.

We refer to the introductory section on the spectral theory of Banach algebras that for any bounded linear operator $T: X \to X$, the spectrum $\sigma(T)$ is a compact set. We have now built the tools to prove the main result of this section.

\begin{theorem}
    Let $T: X \to X$ be a compact operator on a Banach space $X$. Then
    %
    \begin{itemize}
        \item $\sigma(T) = \{ 0 \} \cup \sigma_p(T)$.
        \item $\sigma(T)$ is a countable, compact set whose only limit point is $0$.
        \item for any $\lambda \neq 0$, the eigenspace of $T$ and $T^*$ for $\lambda$ have the same dimension, which is finite.
    \end{itemize}
\end{theorem}
\begin{proof}
    Since $T$ is compact, we know that $\sigma_c(T) = \emptyset$, and $\sigma_r(T) \subset \{ 0 \}$, from which it follows that $\sigma(T) \subset \{ 0 \} \cup \sigma_p(T)$. But since compact operators cannot have closed ranges, it follows that $T$ is not invertible, and so $0 \in \sigma(T)$, so the first point follows. That $\sigma(T)$ only has limit point zero follows from the last result. And we have proved that the eigenspaces of $T$ and $T^*$ have the same dimension in our analysis of the dimension of kernels and cokernels of compact operators.
\end{proof}

We can also summarize some of our results in an equation form known as the \emph{Fredholm alternative}.

\begin{theorem}
    Suppose $X$ is a Banach space, $T: X \to X$ is a compact operator, and $\lambda \neq 0$, then the following are equivalent:
    %
    \begin{itemize}
        \item For every $x_0 \in X$, the equation $(\lambda - T)(x) = x_0$ has a solution in $X$.
        \item For every $x^*_0 \in X$, the equation $(\lambda - T^*)(x^*) = x_0^*$ has a solution in $X^*$.
        \item The equation $(\lambda - T)(x) = 0$ has only the trivial solution $x = 0$.
        \item The equation $(\lambda - T^*)(x) = 0$ has only the trivial solution $x = 0$.
    \end{itemize}
\end{theorem}




\section{Spectra of Compact Normal Operators}

The full power of spectral theory comes into play for the analysis of a compact operator $T: H \to H$ on a Hilbert space $H$ which is \emph{normal}, in the sense that $T$ and $T^*$ commute. We will find that if we define, for each $\lambda \in \sigma(T)$, the closed subspace
%
\[ H_\lambda = \{ x \in H: Tx = \lambda x \}, \]
%
then $\{ H_\lambda \}$ is an orthogonal family of subspaces of $H$, and $H$ is the direct sum of these spaces. It follows that $T$ is determined by it's eigenvectors.

\begin{lemma}
    If $T: H \to H$ is a bounded normal operator on a Hilbert space $H$, and $\lambda_1 \neq \lambda_2$, then $H_{\lambda_1}$ and $H_{\lambda_2}$ are orthogonal.
\end{lemma}
\begin{proof}
    Let us begin by assuming $T$ is self adjoint. Then $\lambda_1$ and $\lambda_2$ are real, since a self-adjoint operator can only have real eigenvalues. If $x_1 \in H_{\lambda_1}$ and $x_2 \in H_{\lambda_2}$, then
    %
    \[ \lambda_1 \langle x_1, x_2 \rangle = \langle Tx_1, x_2 \rangle = \langle x_1, Tx_2 \rangle = \lambda_2 \langle x_1, x_2 \rangle, \]
    %
    so since $\lambda_1 \neq \lambda_2$, $\langle x_1, x_2 \rangle = 0$. Now assume $T$ is bounded and normal. Since it then follows that $\| Tx \| = \| T^* x \|$ for all $x \in H$, it follows that $Tx = \lambda x$ for some $x \in H$ and some scalar $\lambda$ if and only if $T^*x = \overline{\lambda} x$. Thus if $T = T_1 + i T_2$, where $T_1$ and $T_2$ are self-adjoint, then $T_1 x = \text{Re}(\lambda) x$ and $T_2 x = \text{Im}(\lambda) x$. Thus, for a linear operator $S: H \to H$, if we temporarily adopt the notation $H_\lambda(S)$ for the eigenspace of $H$ for $S$ with eigenvalue $\lambda$, then $H_\lambda(T) = H_{\text{Re}(\lambda)}(T_1) \cap H_{\text{Im}(\lambda)}(T_2)$. But this is sufficient to justify that the family $\{ H_\lambda(T) \}$ is orthogonal, since the spaces $\{ H_{\lambda_1}(T_1) \}$ and $\{ H_{\lambda_2}(T_2) \}$ are both orthogonal.
\end{proof}

Now we restrict to the study of compact self-adjoint operators on a Hilbert space. It follows from the following theorem that if $T$ is a compact self-adjoint operator on a separable Hilbert space, then $\sigma(T)$ is precisely the set of eigenvalues for $T$.

\begin{theorem}
    If $T: H \to H$ is a compact self-adjoint operator on a separable Hilbert space, then $H$ has an orthonormal basis $\{ e_i \}$ such that $Te_i = \lambda_i e_i$, where $\{ \lambda_i \}$ are a family of eigenvalues having no limit point but zero.
\end{theorem}
\begin{proof}
    Assume without loss of generality that $T \neq 0$. Since
    %
    \[ \| T \| = \sup \langle Tx, x \rangle \]
    %
    we can find a sequence $x_i$ with $\| x_i \| = 1$, such that
    %
    \[ \langle Tx_i, x_i \rangle \to \lambda, \]
    %
    where $\lambda = \pm \| T \|$. Since $T$ is compact, we may assume $Tx_i \to y$. Then
    %
    \[ \langle y, x_i \rangle \to \| T \|, \]
    %
    so $y \neq 0$. But
    %
    \[ \| Tx_i - \lambda x_i \|^2 = \| Tx_i \|^2 + \lambda^2 - 2 \lambda \text{Re}( \langle Tx_i, x_i \rangle ) \to 0. \]
    %
    Thus $\lambda x_i \to y$, which means $T y = \lim T(\lambda x_i) = \lambda y$, so an eigenvector exists. A Zorn's lemma argument then allows us to find a orthogonal basis of eigenvectors for $T$, since, to extend this argument given a subfamily of indepedent eigenvectors $S$ whose closed span forms a subspace $H_0$ of $H$, we just apply the argument above to $H_1 = H_0^\perp$, which is an invariant subspace of $T$ since $T$ is self adjoint.
\end{proof}

Let $\{ T_\alpha \}$ be a family of \emph{commuting} self-adjoint compact operators on a Hilbert space $H$. Then we claim that $H$ has an orthogonal basis simultaneously diagonalizing the operators $T_\alpha$. We can do this by an application of the well-ordering principle. Suppose we have a basis $\{ e_i \}$ which simultaneously diagonalizes $\{ T_\alpha \}$ for $\alpha < \alpha_0$. Then the eigenspaces $\bigcap_{\alpha < \alpha_0} H_{\alpha,\lambda_\alpha}$ are $T_{\alpha_0}$-invariant for all choices of scalars $\lambda_\alpha$, since if $x \in \bigcap_{\alpha < \alpha_0} H_{\alpha,\lambda_\alpha}$,
%
\[ T_\alpha (T_{\alpha_0} x) = T_{\alpha_0} (T_\alpha x) = \lambda_\alpha T_{\alpha_0} x. \]
%
Because each of these spaces is $T_{\alpha_0}$-invariant, we can diagonalize each of these spaces, finding an orthonormal basis on each one, and then put them back together, and we will have found a basis which simultaneously diagonalizes $\{ T_\alpha : \alpha \leq \alpha_0 \}$. This means there is no smallest index $\alpha_0$ such that we cannot simultaneously diagonalize $\{ T_\alpha : \alpha \leq \alpha_0 \}$, which means we can simultaneously diagonalize all of the operators.

\begin{corollary}
    If $T: H \to H$ is a normal compact operator, then $H$ has an orthonormal basis of eigenvalues for $T$.
\end{corollary}
\begin{proof}
    We can write $T = T_0 + i T_1$, where $T_0$ and $T_1$ are self-adjoint compact operators that commute. If we find a basis $\{ e_n \}$ for $H$ which simultaneously diagonalizes $T_0$ and $T_1$, i.e. such that $T_0e_n = \lambda_n e_n$ and $T_1 e_n= \gamma_i e_n$ for some sequences $\{ \lambda_n \}$ and $\{ \gamma_n \}$, then $Te_n = (\lambda_n + i \gamma_n) e_n$.
\end{proof}

\begin{remark}
    One \emph{can} obtain a spectral theory for general normal operators, but one then has to deal with the fact that $\sigma(T)$ can be uncountable, and one must replace discrete sums as in the compact case with integrals over subspaces. The techniques are best left to more advanced contexts.
\end{remark}









\section{Strong and Weak Convergence}

It is often a useful tool to weaken the notion of convergence of operators. Given two norm spaces $X$ and $Y$, we say a net of operators $\{ T_\alpha : X \to Y \}$ \emph{converges in norm} to an operator $T: X \to Y$ if $\| T - T_\alpha \| \to 0$. We say a net $\{ T_\alpha \}$ \emph{strongly converges} to $T$ if $T_\alpha x$ converges in norm of $Tx$ for every $x \in X$ (note that this is, somewhat confusingly, \emph{weaker} than norm convergence). Finally, $\{ T_n \}$ \emph{weakly converges} to $T$ if $T_n x$ converges to $Tx$ in the weak topology for every $x \in X$. These give rise to the weak and strong operator topologies on the space $B(X,Y)$ of bounded operators between two norm spaces, which are both locally convex since they are induced by the family of seminorms $T \mapsto \| Tx \|$ for each $x \in X$, and $x \mapsto |y^*(Tx)|$ for each $x \in X$ and $y^* \in Y^*$ respectively.

If $X$ is a separable Banach space, then the unit ball of $B(X,Y)$ is separable in the strong operator topology. If in addition $Y^*$ is a separable Banach space, the $B(X,Y)$ is also separable in the weak operator topology.

It is easy to see that the adjoint operation is continuous in the weak operator topology. On the other hand, it is \emph{not} continuous in the strong operator topology. To see this, let $S: l^2 \to l^2$ be the shift operator given by $Se_n = e_{n+1}$. Then $S^*e_1 = 0$, and $S^*e_n = e_{n-1}$ for $n > 1$. Then the sequence $\{ (S^*)^n \}$ converges in the strong operator topology to zero, but $\{ S^n \}$ is a family of isometries in $l^2$, and thus does not converge to zero.

Left and right multiplication by a fixed operator is continuous in both the weak and strong operator topology. However, multiplication is not jointly continuous in either topology. Taking $S$ to be the shift operator again, we see that $(S^*)^n$ and $S^n$ both converge in the weak operator topology to zero, but $(S^*)^n S^n = 1$ does not converge to zero. TODO: Prove not jointly continuous in SOT. Nonetheless, restricted to the unit ball, multiplication is continuous in the strong operator topology. Indeed, if $\{ T_\alpha \}$ and $\{ S_\alpha \}$ are nets converging to $T$ and $S$ in the strong operator topology, then for any $x \in H$,
%
\[ \| (ST - S_\alpha T_\alpha) x \| \leq \| (S - S_\alpha) T x \| + \| S_\alpha \| \| (T - T_\alpha) x \| \leq \| (S - S_\alpha) Tx \| + \| (T - T_\alpha) x \|, \]
%
which converges to zero as $\alpha \to \infty$.

The uniform boundedness theorem implies any \emph{sequence} of operators converging in the weak operator topology or the strong operator topology must be uniformly bounded. The proof of the Banach-Alaoglu theorem can also be adapted to show that the weak operator topology has the Heine-Borel property (and this is really a generalization of the Banach Alaoglu theorem since if $Y$ is one dimensional, then $B(X,Y)$ is isomorphic to $X^*$, and then the strong and weak operator topologies coincide and are both the weak $*$ topology on $X^*$). The strong operator topology, however does \emph{not} satisfy the Heine-Borel property, since $\{ S^n \}$ has no convergent subsequence in the strong operator topology.

\begin{theorem}
    Let $X$ and $Y$ be Banach spaces. Then the linear functionals on $B(X,Y)$ continuous in the weak operator topology or the strong operator topology coincide, and any such linear functional can be written as
    %
    \[ T \mapsto \sum_{i = 1}^N y_i^*(Tx_i) \]
    %
    for some $x_1,\dots,x_n \in X$, and $y_1^*, \dots, y_n^* \in Y^*$. In particular, a convex subset of $B(X,Y)$ is closed in the weak operator topology if and only if it is closed in the strong operator topology.
\end{theorem}
\begin{proof}
    It is simple to see all the linear functionals above are continuous in the weak operator topology, and thus the strong operator topology. Conversely, suppose that $f$ is a strong operator topology continuous linear functional. Then there are $x_1,\dots,x_n \in X$ such that if $T \in B(X,Y)$ and $\| Tx_1 \|, \dots, \| Tx_n \| < 1$, then $f(T) < 1$. but this means that for any $T \in B(X,Y)$,
    %
    \[ |f(T)| \leq \max_{1 \leq i \leq n} \| Tx_i \|, \]
    %
    so $f(T)$ is determind by the values $\{ Tx_1, \dots, Tx_n \}$. But this means that there exists $(y_1^* \oplus \dots \oplus y_n^*) \in (Y^*)^{\oplus n} \cong (Y^{\oplus n})^*$ such that for any $T \in B(X,Y)$,
    %
    \[ f(T) = y_1^*(Tx_1) + \dots + y_n^*(Tx_n), \]
    %
    which completes the proof.
\end{proof}























\chapter{Topological Vector Spaces}

Basic functional analysis deals with norm spaces. But in many problems, more qualitative notions of convergence are encountered that cannot be induced by a norm structure. Here are some examples:
%
\begin{itemize}
    \item We say a sequence of functions $\{ f_i: X \to Y \}$ converges \emph{pointwise} to a function $f$ if $f_i(x) \to f(x)$ for each $x \in X$.

    \item If $X$ is a locally compact, but non-compact topological space, then we say a sequence of functions $\{ f_i: X \to \CC \}$ converges \emph{locally uniformly} to $f: X \to \CC$ if for any compact set $K \subset X$, $f_i$ converges to $f$ uniformly on $K$.

    \item Given a norm space $X$, we say a sequence $\{ x_i \}$ in $X$ converges \emph{weakly} to some $x$ if for each linear functional $\phi \in X^*$, $\phi(x_i) \to \phi(x)$. Similarily, we say $\{ \phi_i \}$ in $X^*$ converges in the \emph{weak $*$ topology} to some $\phi$ if for any $x \in X$, $\phi_i(x) \to \phi(x)$.
\end{itemize}
%
The notions of convergence considered in these examples cannot be induced by a norm, but they can be induced by a more qualitative `topological structure'. Our goal in this part of notes is to study vector spaces equipped with this more general topological structure.

We begin with the most general family of objects.
%In order to be completely general, we begin by making a framework to study function spaces with a topology under the weakest assumptions.
A \emph{topological vector space} $X$ is a vector space equipped with a topology making both addition and scalar multiplication continuous.
%In terms of nets, this continuity means exactly that if $x_\alpha \to x$, $y_\beta \to y$, and $\lambda_\gamma \to \lambda$, then $\lambda_\gamma (x_\alpha + y_\beta) \to \lambda (x + y)$. In terms of neighbourhoods, the continuity says that if $V$ is an open subset containing $\lambda(x + y)$, then there exists $\varepsilon > 0$, and neighbourhoods $W$ of $x$ and $U$ of $y$ such that if $|\lambda - \gamma| < \varepsilon$, then $\gamma(W + U) \subset V$.
In particular, this means that translation and dilation are homeomorphisms of the vector space.
%For each $x' \in X$, and $\lambda \neq 0$, the maps $T_{x'}(x) = x + x'$ and $M_\lambda(x) = \lambda x$ are homeomorphisms of $X$, as a consequence of the continuity of addition and multiplication.
As a consequence, every neighborhood of a point $x \in X$ can be written as $x + N$, where $N$ is a neighborhood of the origin. Thus most of the topological structure of a topological vector space can be understood by looking at the structure of neighborhoods of the origin. Given a topological space $X$, we define the \emph{neighborhood basis} $\mathcal{F}_0$ to be the filter of neighborhoods in the origin in $X$.

\begin{remark}
    A topological vector space is, of course, an abelian topological group. However, we shall see that, unlike in the case of topological groups, it is only the finite dimensional topological vector spaces which are locally compact. This is why these two theories proceed down very different paths.
\end{remark}

%Algebraically, a vector space arised from a representation of a field $k$ over the automorphisms of an abelian group. The topological vector spaces can also be characterized by such a structure. If $G$ is an abelian topological group, and $\rho: k \to \text{Aut}(G)$ is a map into the continuous automorphisms of $G$, then the conditions which guarantee that this representation gives a topological vector space structure to $G$ is that if $g = \rho(\lambda) h$, then for any neighbourhood of the origin $W$ there is $\varepsilon > 0$ and $U$ such that if $|\lambda - \gamma| < \varepsilon$ then $\rho(\gamma) U \subset gW$. This is sufficiently satisfied if the representation is continuous with respect to the topology on $\text{Aut}(G)$ generated by neighbourhoods of the origin of the form
%
%\[ W_{UV} = \{ \varphi \in \text{Aut}(G) : \varphi(U) \subset V  \} \]
%
%where $U$ and $V$ are neighbourhoods of the origin, which can be seen as a topology analogous to the topology of functions converging locally uniformly.

\begin{example}
    The main example of topological vector spaces we deal with in practice are of the following form: we have a vector space $X$, and a subset of linear functionals $S$ on $X$, which \emph{separates} $X$, in the sense that for any two distinct $x_1,x_2 \in X$, there is $x^* \in S$ such that $x^*(x_1) \neq x^*(x_2)$. We then give $X$ the structure of a topological vector space by declaring that a net $\{ x_\alpha \}$ converges to some $x$ if $x^*(x_\alpha)$ converges to $x^*(x)$ for all $x^* \in S$. We call this topology the $\sigma(X,S)$ topology. One neighborhood base in this topology consists of sets of the form
    %
    \[ \varepsilon \cdot B_{x_1^*, \dots, x_n^*} = \{ x \in X: |x_1^*(x)|, \dots, |x_n^*(x)| < \varepsilon \}. \]
    %
    Let us consider some particular examples:
    %
    \begin{itemize}
        \item If $A$ is a set, then the product topology on the spaces $\RR^A$ and $\CC^A$ is precisely the $\sigma(X,S)$ topology, where $S$ is the set of linear functionals on $X$ given by evaluation at a point, i.e. the linear functionals $x^*$ given as $x^*(f) = f(a_0)$ for some fixed $a_0 \in A$. A natural neighborhood basis about the origin are sets of the form
        %
        \[ \varepsilon B_{x_1,\dots,x_n} = \{ f : |f(x_1)|, \dots, |f(x_n)| < \varepsilon \}. \]
        %
        The spaces are only first countable if $X$ is countable.

        \item Given a topological vector space $X$, we let $X^*$ denote the family of all continuous linear functionals on $X$. We can then equip $X$ with a coarser topology to the one it already has, called the \emph{weak topology}, which is the $\sigma(X,X^*)$ topology. Similarily, we can define the \emph{weak $*$ topology} on the dual $X^*$ by giving it the $\sigma(X^*,X)$ topology, viewing $X$ as a family of linear functionals on $X^{**}$. If $X$ is a norm space, the weak $*$ topology on $X^*$ is coarser than the weak topology on $X^*$ viewing $X^*$ as a norm space, which is coarser than the standard topology on $X^*$ induced by the norm.
    \end{itemize}
\end{example}

\begin{example}
    If $\Omega$ is an open subset of $\RR^d$, define $C^\infty(\Omega)$ to be the space of all smooth functions $f$ on $\Omega$ such that $D^\alpha f \in L^\infty(\Omega)$ for all multi-indices $\alpha$, under the topology that a net of functions $\{ f_\beta \}$ converges to some function $f$ if, for each multi-index $\alpha$, the net $\{ D^\alpha f_\beta \}$ converges uniformly to $D^\alpha f$. Then $C^\infty(\Omega)$ is a topological vector space. A neighborhood basis about the origin for this space is given by sets of the form
    %
    \[ \varepsilon \cdot B_k = \{ f \in C^\infty(\Omega): |D^\beta f(x)| < \varepsilon\ \text{for all $|\beta| \leq k$ and $x \in \Omega$} \}. \]
\end{example}

\begin{example}
    Let $X$ be a locally compact topological space, and let $F$ be a sheaf of topological vector spaces on $X$, which is a subsheaf of some sheaf $G$ of vector spaces. We define $F_{\text{loc}}(X)$ to be the subspace of $G(X)$ consisting of all $g \in G(X)$ such that for any precompact open set $U \subset X$, $g|_U \in F(U)$. We give $F_{\text{loc}}(X)$ the structure of a topological vector space by defining a net $\{ g_\alpha \}$ to converge to some $g$ if for any precompact set $U \subset X$, the nset $\{ g_\alpha|_U \}$ converges to $g|_U$ in $F(U)$. Here are some particular examples:
    %
    \begin{itemize}
        \item If $X$ is a locally compact topological space equipped with a Borel measure, we can consider the space $L^p_{\text{loc}}(X)$.
        \item If $\Omega$ is an open subset of $\RR^d$, we can consider the spaces $C^k_{\text{loc}}(\Omega)$ and $C^\infty_{\text{loc}}(\Omega)$.
        \item If $X$ is a locally compact metric space, we can consider the space $\Lambda_{\text{loc}}(X)$ of locally Lipschitz functions.
    \end{itemize}
\end{example}

\begin{example}
    Consider the space $L_0(X)$ of {\it all} measurable functions on a measure space $X$ with the topology induced by convergence of measure. That is, a net $\{ f_\alpha \}$ converges to $f$ if, for any $\varepsilon > 0$,
    %
    \[ \mu ( |f_\alpha - f| \geq \varepsilon ) \to 0. \]
    %
    This example often occurs in probability theory, where, once we fix some probability space, $L_0$ consists of all random variables, where a net of random variables $\{ X_\alpha \}$ converges \emph{in probability} to some random variable $X$ if $\PP(|X_\alpha - X| \geq \varepsilon)$ converges to zero for all $\varepsilon > 0$. The neighbourhoods of the origin with respect to this topology are of the form
    %
    \[ \varepsilon U_\delta = \{ f \in L_0(X): \mu(|f| < \varepsilon) \leq \delta \} \]
    %
    This space is therefore first countable.
\end{example}

\begin{example}
    One might expect the space $L_0(X)$ to have an additional topological structure corresponding to \emph{pointwise convergence almost everywhere}, i.e. a net of measurable functions $\{ f_\alpha \}$ converges to a measurable function $f$, if there exists a set $X_0 \subset X$ such that $X_0^c$ has measure zero, and $\{ f_\alpha \}$ converges pointwise to $f$ on $X_0$. But this does \emph{not} induce a topology on $L_0(X)$. To see why, consider the space $L_0[0,1]$, induced by the Lebesgue measure on $[0,1]$. Consider the typewrite sequence of characteristic functions $\chi_n$, defined in terms of the sets $[H_n, H_{n+1}]$ modulo 1, where $H_n = \sum_{k = 1}^n 1/k$ is the $n$th Harmonic number. Since $H_n \to \infty$, every point intersects infinitely many of the intervals, so that $\chi_n$ does not converge anywhere pointwise to zero. But $\chi_n$ converges to zero in measure. This means that there exists a subsequence $\{ \chi_{n_i} \}$ of the typewriter sequence which does converge pointwise almost everywhere to zero. But this contradicts the topological fact that for any topological space $X$, a net $\{ x_\alpha \}$ converges to a point $x \in X$ if and only if every subnet $\{ x_{\alpha_\beta} \}$ has a further subnet which converges to $x$. Thus there can be no topology whose convergence agrees with almost everywhere convergence.
\end{example}

\begin{example}
    Consider the space $k[[X]]$ of formal power series in a single variable. Define a topology on $k[[X]]$ by letting a net $\{ f_\alpha \}$ converge to some $f$ if, for any $n \geq 0$, there is $\alpha_0$ such that $f_\alpha - f$ is divisible by $X^n$ for $\alpha \geq \alpha_0$. Addition and multiplication in this ring is continuous, and a neighborhood basis consists of the family $\{ \mathfrak{m}^n: n \geq 1 \}$, where $\mathfrak{m}$ is the unique maximal ideal in $k[[X]]$, i.e. the ideal generated by $X$. However, scalar multiplication is \emph{not} continuous. Thus $k[[X]]$ is a topological ring (over any field), but not a topological vector space. It is a useful object of study in algebra, which favors the algebraic properties of it's topological structure, but it is not so useful from the perspective of topological vector spaces. However, we can weaken the topology on $k[[X]]$ by instead taking the topology given by pointwise convergence of the coefficients, and this gives $k[[X]]$ a vector space structure, but it no longer reflects the algebraic structure quite as strongly.
\end{example}

\section{Basic Theorems}

We now introduce some terminology from analytical linear algebra, which will become even more prevalent in the theory of topological vector spaces. Let $X$ be a vector space:
%
\begin{itemize}
    \item A set $C \subset X$ is \emph{convex} if for any $x,y \in C$, and any $t \in [0,1]$,
    %
    \[ tx + (1 - t)y \in C. \]
    %
    Thus $C$ contains all line segments between points in itself. In set notation, we can write, this condition, for any $t \in [0,1]$, as saying that $tC + (1 - t)C \subset C$. More generally, this condition implies that for any $t,u > 0$,
    %
    \[ tC + uC \subset (t + u) C, \]
    %
    which reduces to the ordinary convex equation by dividing both sides by $t + u$.

    \item A set $B \subset X$ is \emph{balanced} if $\lambda B \subset B$ for all $|\lambda| \leq 1$. If $B$ is balanced, then $\lambda B$ is balanced for any scalar $\lambda$, and if $|\lambda| = 1$, then $\lambda B = B$.

    \item A set $A \subset X$ is \emph{absorbing} if for any $x \in X$, there is $M > 0$ such that for $|\lambda| > M$, $x \in \lambda A$. If $A$ is balanced, it suffices only to show that for each $x \in X$, there exists some $t_0 > 0$ such that $x \in t_0 A$.

    \item A set $B \subset X$ is \emph{(Von Neumann) bounded} if, for every neighborhood $W$ of the origin, there is $M > 0$ such that for $t > M$, $B \subset tW$. If $B$ is balanced, it suffices to show that for any neighborhood $W$ of the origin, there is $t_0 > 0$ such that $B \subset t_0 W$.

    % As an example, compact sets are bounded. If $x \neq 0$, then the set $\{ x, 2x, \dots \}$ is {\it never} boundedness, because otherwise if $W$ is a balanced neighbourhood, with $nx \in tW$ for all $n$, then $x \in (t/n)W$ for all $n$, which for $t < n$ implies $x \in W$. But this means $x$ is in every neighbourhood of the origin
\end{itemize}
%
One of the cornerstones of the basic theory of topological vector spaces is that analytic properties of a space can be established from the existence of open sets which possess some of the properties above.

If $X$ is a topological vector space, the fact that $0 + 0 = 0$, and the continuity of addition implies that for any neighborhood $W$ of the origin, there is a neighborhood $V$ of the origin such that $V + V \subset W$. We can of course iterate this process to find a neighborhood $V$ such that $V + \dots + V \subset W$. Similarily, the continuity of multiplication implies that for any $\lambda \neq 0$, and any neighborhood $W$ of the origin, $\lambda W$ is a neighborhood of the origin.

\begin{theorem}
    Let $X$ be a topological vector space. Then every neighborhood of the origin in $X$ is absorbing.
\end{theorem}
\begin{proof}
    Let $W$ be a neighborhood of the origin, and fix $x \in X$. By continuity of multiplication, $\lim_{\lambda \to 0} \lambda x = 0$. Thus there is $r > 0$ such that for $|\lambda| \leq r$, $\lambda x \in W$. But this means that $x \in t W$ for all $t > 1/r$.
\end{proof}

\begin{corollary}
    Every topological vector space has a basis of balanced sets.
\end{corollary}
\begin{proof}
    It suffices to show every neighborhood $W$ of the origin contains an open balanced subneighborhood. By continuity of multiplication, there is $\varepsilon > 0$ and an open subset $U$ of the origin such that $\lambda U \subset W$ for $|\lambda| < \varepsilon$. But then
    %
    \[ \bigcup_{|\lambda| < \varepsilon} \lambda V \]
    %
    is an open balanced neighborhood of the origin contained in $W$.
\end{proof}

Let $\mathcal{F}_0$ be a filter forming a neighborhood basis at the origin for a topological vector space $X$. Then we have shown the following properties hold:
%
\begin{itemize}
    \item For any $U \in \mathcal{F}_0$, there is $V \in \mathcal{F}_0$ such that $V + V \subset U$.
    \item For any $U \in \mathcal{F}_0$ contains some balanced set $V \in \mathcal{F}_0$.
    \item For any $U \in \mathcal{F}_0$ and $\lambda \neq 0$, $\lambda U \in \mathcal{F}_0$.
    \item Every $U \in \mathcal{F}_0$ is absorbing.
\end{itemize}
%
In fact, given any vector space $X$, and any filter $\mathcal{F}_0$ of subsets of $X$ containing the origin with these four properties induce a topological structure on $X$ which makes $\mathcal{F}_0$ into a neighborhood basis. Indeed, we can then given $X$ a topology by defining a set $U \subset X$ to be open if, for any $x \in U$, $U - x \in \mathcal{F}$. To show this gives the structure of a topological space, it suffices to show that the map $T: k \times X \times X \to X$ given by $T(\lambda,x_1,x_2) = \lambda(x_1 + x_2)$ is continuous. It thus siffices to show that if $x_0 \in X$, and there is $\lambda_0 \in k$ and $x_1,x_2 \in X$ such that $\lambda_0(x_1 + x_2) = x_0$, then for any $U \in \mathcal{F}_0$, there is $V \subset \mathcal{F}_0$ and $\varepsilon > 0$ such that for $|\gamma| < \varepsilon$,
%
\[ T(\lambda + \gamma, x_1 + V, x_2 + V) \subset x_0 + U. \]
%
Expanding this out shows that this is equivalent to
%
\[ T(\gamma,x_1,x_2) + T(\lambda,V,V) + T(\gamma,V,V) \subset U. \]
%
Applying Property 1 and Property 2, there is a balanced set $V_1 \subset \mathcal{F}_0$ such that $V_1 + V_1 + V_1 \subset U$. Our proof would be complete if we could find $\varepsilon > 0$ and $V$ such that $T(\gamma,x_1,x_2)$, $T(\lambda,V,V)$, and $T(\gamma,V,V)$ are all contained in $V_1$. Thus our proof breaks into choosing parameters so these three claims hold.
%
\begin{itemize}
    \item Applying Property 4 to $V_1$, there is $\varepsilon > 0$ such that for $|\gamma| < \varepsilon$, $x_1 + x_2 \in (1/\gamma) V_1$, which means that $T(\gamma,x_1,x_2) \in V_1$.

    \item If $\lambda = 0$, then $T(\lambda,V,V) \subset \{ 0 \}$, and thus $T(\lambda,V,V) \subset V_1$ trivially. If $\lambda \neq 0$, then Property 3 implies that $(1/\lambda) V_1 \in \mathcal{F}_0$, and applying Property 1 allows us to find $V$ such that $V + V \subset (1/\lambda) V_1$, which means that $T(\lambda,V,V) \subset V_1$.

    \item Property 1 allows us to find $V$ such that $V + V \subset V_1$. If $\varepsilon < 1$, then for any $|\gamma| < 1$, the fact that $V_1$ is balanced means that
    %
    \[ T(\gamma,V,V) = \gamma(V + V) \subset \gamma V_1 \subset V_1. \]
\end{itemize}
%
Thus we have verified that $X$ is a topological vector space.

\begin{theorem}
    Let $X$ be a topological vector space. If $X$ has an open subset $U$ which is bounded, then $X$ is first countable.
\end{theorem}
\begin{proof}
    Assume without loss of generality that $W$ is a balanced neighborhood of the origin. Because $U$ is bounded, this means there is some $n > 0$ such that $U \subset n W$. But this means $(1/n) U \subset W$, which goes to show that the family of sets
    %
    \[ \{ (1/n) \cdot U : n > 0 \} \]
    %
    is a neighborhood basis about the origin.
\end{proof}

It is of course important to assume separation properties in order to get useful information in a topological vector space. Fortunately, the weakest separation property implies virtually the strongest separation property, so we do not need to separately study a menagerie of different topological vector spaces.

\begin{theorem}
    Let $X$ be a topological vector space, and let $x_0,x_1 \in X$. If $U$ is a neighborhood of the origin such that $x_0 + U$ does not contain some $x_1$, then there is a neighborhood $V$ of the origin such that $x_0 + V$ is disjoint from $x_1 + V$.
\end{theorem}
\begin{proof}
    We can find an open, balanced neighborhood $V$ of the origin such that $V + V \subset U$. Then $x_0 + V$ is disjoint from $x_1 + V$.
\end{proof}

\begin{corollary}
    $T_1$ topological vector spaces are Hausdorff.
\end{corollary}

In the sequel, we will assume all the topological vector spaces we deal with are Hausdorff, which amounts to showing that $\{ 0 \}$ is a closed set. In most cases, the study of non Hausdorff topological spaces can be studied by taking the quotient space by $\overline{\{ 0 \}}$, e.g. in the study of the $L^p$ spaces in the Banach regime.

\begin{corollary}
    Let $X$ be a Hausdorff topological space containing a compact set $K \subset X$ and a closed set $C \subset X$ which are disjoint from one another. Then there is a neighbourhood $W$ of the origin such that $K + W$ is disjoint from $C + W$.
\end{corollary}
\begin{proof}
    For each $x \in K$, we may find a balanced set $U_x$ such that $x + U_x + U_x$ is disjoint from $C + U_x$. By compactness, finitely many $U_{x_1}, \dots, U_{x_n}$ cover $K$, and then if $W = U_{x_1} \cap \dots \cap U_{x_n}$, we find $K + W$ is disjoint from $C + W$.
\end{proof}

\begin{corollary}
    Hausdorff topological vector spaces are regular.
\end{corollary}

\begin{corollary}
    Let $X$ be a topological vector space. Every neighborhood of the origin contains a closed neighborhood of the origin.
\end{corollary}

\begin{theorem}
    Let $X$ be a topological vector space.
    %
    \begin{enumerate}
        \item[(i)] If $V$ is a neighbourhood of the origin, and $A$ is any set, then $\overline{A} \subset A + V$, and in fact $\overline{A} = \bigcap_V A + V$.
        \item[(ii)] If $A,B \subset X$, then $\overline{A} + \overline{B} \subset \overline{A + B}$.
        \item[(iii)] The closure of a subspace is a subspace.
        \item[(iv)] If $C$ is a convex, then $C^\circ$ and $\overline{C}$ are convex subsets.
        \item[(v)] If $B$ is balanced, then so is $\overline{B}$, and if $0 \in B^\circ$, then $B^\circ$ is balanced.
        \item[(vi)] If $E$ is a bounded subset of $X$, then so is $\overline{E}$.
        \item[(vii)] A set $E \subset X$ is bounded if and only if any sequence $\{ x_n \}$ with elements in $E$ is bounded, i.e. if $E$ is sequentially bounded.
        \item[(viii)] A sequence $\{ x_n \}$ is bounded if and only if $\lambda_n x_n \to 0$ for any sequence $\{ \lambda_n \}$ converging to zero.
        \item[(ix)] If $A$ and $B$ are bounded, then so is $A + B$.
        \item[(x)] If $A$ and $B$ are compact, so is $A + B$.
        \item[(xi)] If $A$ is compact, and $B$ is closed, $A + B$ is closed.
    \end{enumerate}
\end{theorem}
\begin{proof}
    $x \in \overline{A}$ if and only if $x + V$ intersects $A$ for every neighbourhood $V$ of the origin, hence $x \in A - V$, and $-V$ is a neighbourhood of the origin if and only if $V$ is. This proves (i). The continuity of addition essentially proves (ii), for if $x \in \overline{A}$, and $y \in \overline{B}$, then we find $x_\alpha \to x$, $y_\alpha \to y$ with $x_\alpha, y_\alpha \in X$, then $x_\alpha + y_\alpha \to x + y$, and $x_\alpha + y_\alpha \in A + B$. If $Y$ is a subspace, then (ii) implies $\overline{Y} + \overline{Y} \subset \overline{Y + Y} = \overline{Y}$, and $\lambda \overline{Y} = \overline{\lambda Y} = \overline{Y}$, so $\overline{Y}$ is closed under addition and multiplication, and is therefore a subspace. Similarily, if $C$ is convex, then
    %
    \[ \lambda \overline{C} + (1 - \lambda) \overline{C} = \overline{\lambda C + (1 - \lambda) C} \subset \overline{C} \]
    %
    and if $x,y \in C^\circ$, we can find a neighbourhood $U$ such that $x + U, y + U \subset C$, and then for any $\lambda$, by convexity
    %
    \[ (\lambda x + (1 - \lambda) y) + (\lambda U + (1 - \lambda) U) = \lambda(x + U) + (1 - \lambda)(y + U) \subset C \]
    %
    which implies $\lambda x + (1 - \lambda) y \in C^\circ$, hence (iv) is shown. If $\lambda B \subset B$ for $|\lambda| < 1$, then $\lambda \overline{B} = \overline{\lambda B} \subset \overline{B}$, so $\overline{B}$ is balanced. Conversely, $\lambda B^\circ$ is open for each $\lambda \neq 0$, so for nonzero $|\lambda| < 1$, $\lambda B^\circ \subset B^\circ$, and provided $0 \in B^\circ$, we find that $\lambda B^\circ \subset B^\circ$ for all $|\lambda| < 1$. If $E$ is a bounded subset, and $V$ is some neighbourhood, then there is some $W$ such that $\overline{W} \subset V$, and if we find $t$ such that $E \subset uW$ for all $u > t$, then $\overline{E} \subset u\overline{W} \subset uV$, hence $\overline{E}$ is bounded. If $E$ is bounded, then certainly any sequence $\{ x_n \}$ with values in $E$ is bounded. Conversely, if $E$ is unbounded, then there exists a balanced neighborhood $V$ of the origin such that $E$ is not a subset of $tV$ for any $t > 0$. Thus we may pick $x_n \in E - nV$, and then the sequence $\{ x_n \}$ is also unbounded. Now if a sequence $\{ x_n \}$ is bounded, and $\{ \lambda_n \}$ are scalars converging to zero, then for any balanced open set $U$, $\{ x_n \}$ is contained in $tU$ for some $t > 0$, and then if $n_0$ is chosen such that $|\lambda_n| < 1/t$ for $n \geq n_0$, then $\lambda_n x_n \in U$. Thus $\lambda_n x_n \to 0$. Conversely, if $\{ x_n \}$ is unbounded, then for any $n$, there is $k_n$ such that $x_{k_n} \not \in n U$. Thus if $\{ \lambda_n \}$ is a sequence converging to zero such that $\lambda_{k_n} = 1/n$, then $\{ \lambda_n x_n \}$ does not converge to zero. If $A$ and $B$ are bounded, then $A + B$ is bounded, because if $W$ is fixed, we can find $U$ with $U + U \subset W$, and if $A,B \subset \lambda U$, then
    %
    \[ A + B \subset \lambda(U + U) \subset \lambda W \]
    %
    If $A$ and $B$ are compact, then $A + B$ is compact, as it is the image of $A \times B$ under the continuous addition map. If $A$ is compact, and $B$ is closed, then $A + B$ is closed, because if $x_\alpha + y_\alpha \to z$ in $A + B$, then $x_\alpha$ has a subnet converging to some $x \in A$, and then $y_\alpha \to z - x$, hence $z - x \in B$, and therefore $z = x + (z - x) \in A + B$.
\end{proof}




\section{Convexity and Metrizability}

We say a topological vector space $X$ is \emph{locally convex} if it has a neighborhood basis consisting of convex sets.

\begin{theorem}
    Let $X$ be a topological vector space, and let $U \subset X$ be a bounded neighborhood of the origin. Then $U$ contains a convex, balanced subneighborhood.
\end{theorem}
\begin{proof}
    If $U$ is a convex neighbourhood, consider the set $A = \bigcap_{|\lambda| = 1} \lambda U$, which is, convex, and balanced because if $|\gamma| < 1$,
    %
    \[ \gamma A = \gamma \bigcap_{|\lambda| = 1} \lambda U = \bigcap_{|\lambda| = 1} \gamma \lambda U = \bigcap_{|\lambda| < |\gamma|} \lambda U \subset A \]
    %
    If $V \subset U$ is balanced, then $V \subset A$, so $0 \in A^\circ$, and in particular, $A^\circ$ is a balanced, convex neighbourhood of the origin.
\end{proof}

Given a set $A$, we let $\text{Conv}(A)$ denote the \emph{convex hull} of $A$, the smallest convex set containing $A$. It can be written as
%
\[ \text{Conv}(A) = \bigcup_{n = 1}^\infty \bigcup_{t_1 + \dots + t_n = 1} (t_1A + \dots + t_nA) \]
%
It follows that the convex hull of an open set is open.

\begin{theorem}
    Let $X$ be a locally convex space. Then the convex hull of any bounded set is also bounded. The convex hull of a precompact set is precompact.
\end{theorem}
\begin{proof}
    If $W$ is a convex neighbourhood of the origin, and $A$ is bounded, then $A \subset tW$ for some $t$, and so $\text{Conv}(A) \subset tW$. If $W$ is a precompact set, then for any convex neighborhood $V$ of the origin, there exists finitely many points $x_1,\dots,x_n \in X$ such that $W$ is covered by $x_1 + V, \dots, x_n + V$. The convex hull of $x_1,\dots,x_n$ is compact, so there exists finitely many points $y_1,\dots,y_m$ such that he convex hull of $\{ x_1, \dots, x_n \}$ is covered by $y_1 + V, \dots, y_m + V$. But these sets then also cover the convex hull of $W$. Because $V$ was an arbitrary convex neighborhood, this completes the proof.
\end{proof}

It is natural to study \emph{metrizable} topological vector spaces. The most useful metrics on a vector space are the \emph{translation invariant} ones, which satisfy $d(a + x, a + y) = d(x,y)$ for all points $a,x,y$ in the vector space. It turns out that every first countable vector space has such a metrization, in particular, all metrizable topological vector spaces have a translation invariant metric which generates the same topology.

\begin{theorem}
    If $X$ is a first countable topological vector space. Then $X$ is metrizable by an invariant metric $d$ whose open balls are balanced. If $X$ is locally convex, the metric can be chosen so the open balls are convex.
\end{theorem}
\begin{proof}
    We consider a balanced neighbourhood base of the origin $V_1, V_2, \dots$ with $V_{n+1} + V_{n+1} + V_{n+1} \subset V_n$. If $X$ is locally convex, choose the $V_n$ to be convex as well. With each Dyadic rational number $r = \sum c_n 2^{-n}$ in $[0,1]$, we can associate the interval $V_r = \sum c_n V_n$, with $V_1 = X$. These neighbourhoods are monotonic because of the inclusion properties of these neighbourhoods, and satisfy $V_r + V_s \subset V_{r+s}$ (this is most easily proved when $r = s$, in which case the general case is proved). Thus we can define
    %
    \[ f(x) = \inf \{ r \in [0,1] : x \in V_r \} \]
    %
    and $d(x,y) = f(x-y)$. It is clear $d$ is invariant, and satisfies the triangle inequality, because if $x - y \in V_r$ and $y - z \in V_s$, then $x - z \in V_r + V_s \subset V_{r + s}$. Since the $V_r$ is balanced if the $V_n$ are, the open balls of $d$ are balanced, and if the $V_r$ are convex, the open balls of $d$ are convex.
\end{proof}

\begin{remark}
    The metric $d$ obtained in the proof above is always bounded, i.e. $0 \leq d \leq 1$, and so we cannot use the same techniques to show that certain topological vector spaces are normable. We will describe a technique to obtain norms which define the topology of a topological vector space shortly.
\end{remark}

An \emph{$F$-space} is a topological vector space $X$ equipped with a complete translation invariant metric $d$. If the space $X$ is locally convex, then $X$ is called a \emph{Fr\'{e}chet Space}.

\begin{example}
    A quasinorm space is a vector space $X$ equipped with a non-negative real-valued function $\| \cdot \|$ such that $\| x \| = 0$ if and only if $x = 0$, $\| \lambda x \| = |\lambda| \| x \|$, and $\| x + y \| \lesssim \|x\| + \|y\|$, rather than the stronger inequality $\| x + y \| \leq \| x \| + \| y \|$. Then $X$ is a metrizable topological vector space, but need not be locally convex. The main example of such a space is the spaces $L^p(X)$, for $0 < p < 1$, defined to be the family of all measurable functions such that the quasinorm
    %
    \[ \| f \|_{L^p(X)} = \left( \int |f(x)|^p\; dx \right)^{1/p} \]
    %
    is finite. Then $L^p(X)$ is an $F$-space, with the proof of completeness following much the same proof strategy as the spaces $L^p(X)$ for $1 \leq p < \infty$.
\end{example}

\begin{remark}
    It is true that any metrizable topological vector space $X$ which is complete with respect to one metric $d$ has a translation invariant metric $d_0$ such that the space is complete with respect to $d_0$. But this is a nontrivial theorem, due to Victor Klee.
\end{remark}

The notion of Cauchy sequences in a topological vector space $X$ can still be discussed, even in the absense of a metric. We say $\{ x_\alpha \}$ is a \emph{Cauchy net} in a topological vector space if, for every neighbourhood $U$ of the origin, there is $\alpha_0$ such that for $\alpha_1,\alpha_2 \geq \alpha_0$, $x_{\alpha_1} - x_{\alpha_2} \in U$. If the topology on $X$ is induced by a translation invariant metric $d$, then a net is Cauchy precisely when it is Cauchy with respect to this metric. This gives the suprising result that if any translation invariant metric on a metrizable topological vector space is complete, then \emph{any} metric for the vector space is complete.

\begin{theorem}
    Cauchy sequences are bounded.
\end{theorem}
\begin{proof}
    Let $\{ x_n \}$ be a Cauchy sequence. If $U$ is a balanced neighbourhood of the origin, then there exists $n_0$ such that for $n \geq n_0$, $x_n - x_{n_0} \in U$. Because $U$ is absorbing, we know that for some scalar $\lambda \geq 1$, the points $x_1, \dots, x_{n_0}$ are in $\lambda U$, hence all of the $x_n$ are in $\lambda U$. Since $U$ was arbitrary, this shows the sequence is bounded.
\end{proof}

Note that Cauchy nets are \emph{not} necessarily bounded, which causes problems later on in the theory (e.g. in the study of bounded linear maps).

With a theory of Cauchy nets, we can now study \emph{complete} topological vector spaces, even in absense of a translation invariant metric, i.e. as spaces in which every Cauchy net. We can also form \emph{completions}. Given any topological vector space $X_0$, there is a unique topological vector space $X$, up to isomorphism, containing $X_0$, called the completion of $X_0$, such that $X_0$ is dense in $X$, and any continuous linear map $X_0 \to Y$ extends to a continuous linear map $X \to Y$. This space will be homeomorphic to the usual completion of $X_0$ has a translation invariant metric.

\begin{theorem}
    Let $X$ be a topological vector space. Then every complete subspace of $X$ is closed.
\end{theorem}
\begin{proof}
    Suppose $X_0 \subset X$ is complete. To show it is closed, suppose $\{ x_\alpha \}$ is a net in $X_0$ converging to a point $x \in X$. Then $\{ x_\alpha \}$ is a Cauchy net in $X_0$, and thus converges in $X_0$ to some point, which, because $X$ is Hausdorff, must be $x$. Thus limits of points in $X_0$ remain in $X_0$, and so $X_0$ is closed.
\end{proof}

Recall the definition of a seminorm. In the theory of Banach spaces, we mainly concentrated on a space with a single \emph{norm}. There is an alternate method to giving a vector space a Hausdorff topological structure, by considering a \emph{family} of seminorms on $X$. If $X$ is a vector space, then a family of seminorms $\mathcal{S}$ is called \emph{separating} if for every $x \neq 0$, there is $\rho \in \mathcal{S}$ such that $\rho(x) \neq 0$. We then define a Hausdorff topology on $X$ by declaring a net $\{ x_\alpha \}$ to converge to some $x \in X$ if $\rho(x_\alpha - x) \to 0$ for all $\rho \in \mathcal{S}$. The space $X$ is also locally convex, because a neighbourhood basis is given by the family of convex sets
%
\[ \varepsilon B_{\rho_1 \dots \rho_n} = \{ x: \rho_1(x), \dots, \rho_n(x) < \varepsilon \} = \varepsilon(B_{\rho_1} \cap \dots \cap B_{\rho_n}) \]
%
and each of these neighbourhoods is convex, since they are the intersections of convex balls with respect to an individual seminorm. In other words, the family of convex balls $\varepsilon B_\rho = \{ x : \rho(x) < \varepsilon \}$ form a subbase for a topology. If $\mathcal{S}$ is a directed set, then these balls actually form a neighborhood base.

As we have seen in our discussion of the Hahn-Banach theorem, there is a duality between seminorms and convex sets. If $X$ is a locally convex space, then every convex neighborhood $U$ of the origin corresponds to a Minkowski seminorms $\rho_U$, and if $\mathcal{B}$ is a neighborhood base of $X$ consisting of convex sets, then the topology of $X$ is induced by the family of Minkowski seminorms $\{ \rho_U : U \in \mathcal{B} \}$. Thus \emph{every} locally convex space is specified by a family of seminorms.

If the topology of a space $X$ is specified by a countable family of seminorms, then the space is first countable, and therefore metrizable. However, in this situation we can come up with a natural metric to use, rather than appealing to the theorem we just came up with. If $\rho_1, \rho_2, \dots$ is an ordering of the seminorms, then the functions
%
\[ d_n(x,y) = \frac{\rho_n(x - y)}{1 + \rho_n(x-y)} \]
%
are bounded and satisfy the triangle inequality, and the metric for the space can be given by
%
\[ d(x,y) = \sum_{n = 1}^\infty \frac{d_n(x,y)}{2^n} = \sum_{n = 1}^\infty \frac{\rho_n(x-y)}{2^n (1 + \rho_n(x-y))} \]
%
Since {\it any} invariant metric on a complete, metrizable space is complete, there is normally not a need to switch to a different metric when doing general calculations on metric spaces, unless it is possible to switch to a single norm for the entire space.

\begin{theorem}
    Let $X$ be a locally convex topological vector space with a topology generated by a family of seminorms $\{ \rho_\alpha \}$. Then a set $E$ is bounded if and only if $\rho_\alpha(E)$ is bounded for each $\alpha$.
\end{theorem}
\begin{proof}
    Suppose $E$ is bounded. If $U_\alpha$ is the unit ball with respect to $\rho_\alpha$, then for some $t > 0$, $E \in t U_\alpha$, so $\rho_\alpha(E) \leq t$. Conversely, suppose that $\rho_\alpha(E) \leq t_\alpha$ for each $\alpha$. Then we consider a neighbourhood basis of the elements $\varepsilon U_{\rho_1 \dots \rho_n}$, as introduced above, and for each such element we find
    %
    \[ E \in \max(t_1, \dots, t_n) \varepsilon \cdot U_{\rho_1 \dots \rho_n} \]
    %
    so $E$ is bounded.
\end{proof}

To finish our basic understanding of locally convex spaces, we ask when the topology of a locally convex space can be given by a single norm. If $\| \cdot \|$ is a norm for the space, then the unit ball with respect to this norm is bounded by the theorem above, and convex. It turns out this is sufficient for a locally convex space to be normable.

\begin{theorem}
    A topological vector space $X$ is normable if and only if the origin has a bounded, convex neighbourhood.
\end{theorem}
\begin{proof}
    If $U$ is a bounded, convex neighbourhood about the origin, then the $\varepsilon U$ form a neighbourhood basis around the origin. It follows that the topology of the space is specified by the single Minkowski functional $\rho_U$. The fact that $\rho_U$ is actually a norm, rather than a seminorm, follows from the fact that the space is Hausdorff.
\end{proof}

\begin{example}
    Consider the class $C_{\text{loc}}(X)$ of continuous functions on a locally compact space $X$, under the topology of locally uniform convergence. Then $C(X)$ is a topological vector space. For each compact set $K$, we have the seminorms $\| \cdot \|_{L^\infty(K)}$, which induce the topology of $C(X)$. Neighborhood basis consist precisely of the balls
    %
    \[ B_{K,\varepsilon} = \{ f \in C(X): \| f \|_{L^\infty(K)} < \varepsilon \}. \]
    %
    Thus $C(X)$ is a locally convex $F$-space. If $X$ is also $\sigma$ compact, then we can write $X = \lim_{n \to \infty} K_n$, and so we see that in this case $C(X)$ is a Fr\'{e}chet space. The space $C(X)$ is never normable if $X$ is not a compact set, since none of the balls $B_{K,\varepsilon}$ is bounded.
\end{example}

\begin{example}
    If $\Omega$ is an open subset of $\CC$, then the space $H(\Omega)$ of holomorphic functions on $\Omega$ is also a topological vector space under locally uniform convergence. Since the local uniform limit of a holomorphic function is a holomorphic function, $H(\Omega)$ is a closed subspace of $C(\Omega)$, and is therefore a Fr\'{e}chet space. Montel's theorem in the theory of complex variables says that $H(\Omega)$ has the Heine-Borel property: Every closed, bounded subset of $H(\Omega)$ is compact. But since $H(\Omega)$ is never finite dimensional, $H(\Omega)$ cannot be locally compact, hence $H(\Omega)$ cannot be locally bounded. It follows that $H(\Omega)$ is also not normable.
\end{example}

\begin{example}
    For $\Omega \subset \RR^d$, the spaces $C^\infty_{\text{loc}}(\Omega)$ are Fr\'{e}chet spaces. The Arzela-Ascoli theorem essentially says that $C^\infty_{\text{loc}}(\Omega)$ has the Heine-Borel property. Since $C^\infty_{\text{loc}}(\Omega)$ is never finite dimensional if $\Omega$ is nonempty, this space cannot be locally bounded, hence $C^\infty_{\text{loc}}(X)$ isn't normable.
\end{example}

\begin{example}
    The space $\mathcal{S}(\RR^d)$ of rapidly decreasing \emph{Schwartz functions} on $\RR^d$ is a Fr\'{e}chet space if equipped with the seminorms
    %
    \[ \rho_{N,M}(f) = \sup_{|\alpha| \leq M} \sup_{x \in \RR^d} \langle x \rangle^N |D^\alpha f(x)|. \]
    %
    This space also has the Heine-Borel property, essentially because of the Arzela-Ascoli theorem again.
\end{example}




\section{Linear Maps}

As in the functional analysis of Banach spaces, linear maps play an important role in the theory. In particular, we are interested in those linear maps between topological vector spaces which are continuous. Continuous maps are exactly those which are uniformly continuous, because if $T: X \to Y$ is continuous, then for every neighbourhood $V$ of the origin in $Y$, there is a neighbourhood $U$ of the origin in $X$ such that if $x - y \in U$, then $Tx - TY \in V$. Given two topological vector spaces $X$ and $Y$, we let $B(X,Y)$ be the space of bounded maps from $X$ to $Y$ (linear maps that map bounded sets to bounded sets), and $L(X,Y)$ the space of continuous linear maps from $X$ to $Y$.

\begin{lemma}
    Let $X$ and $Y$ be topological vector spaces, and $T: X \to Y$ a linear map between $X$ and $Y$.  If $T$ is continuous at $0$, then $T$ is continuous everywhere.
\end{lemma}
\begin{proof}
    Suppose $T$ is continuous at the origin. If $\{ x_\alpha \}$ is a net converging to $x \in X$, then $\{ x_\alpha - x \}$ is a net converging to zero, hence $\{ T(x_\alpha - x) \} = \{ T(x_\alpha) - T(x) \}$ converges to zero, so the net $\{ T(x_\alpha) \}$ converges to $T(x)$. Thus $T$ is continuous everywhere.
\end{proof}

The theory of linear functions is difficult to classify in the general case, but linear {\it functionals} are easier to understand. Given a vector space $X$, we let $X^\sharp$ denote the class of linear functionals, and $X^*$ the class of {\it continuous} linear functionals.

\begin{example}
    Consider the space $L^p[0,1]$ for $0 < p < 1$. We claim that the only convex non-empty open subset of $L^p[0,1]$ is $L^p[0,1]$ itself. To see this, let $W$ be such an open set. Without loss of generality, suppose $0 \in W$. Then there is $\varepsilon > 0$ such that $W$ contains all $f \in L^p[0,1]$ with $\| f \|_{L^p[0,1]} < \varepsilon$. Given any $f \in L^p[0,1]$, and any positive integer $n$, divide $[0,1]$ into $n$ disjoint intervals $I_1, \dots, I_n$ such that on each such interval $\| f \|_{L^p(I_j)} = n^{-1/p} \| f \|_{L^p[0,1]}$. Since $1 - 1/p < 0$, it follows that for sufficiently large $n$, and any $1 \leq j \leq n$, if we set $f_j = n f 1_{I_j}$, then
    %
    \[ \| f_j \|_{L^p[0,1]} \| n f 1_{I_j} \|_{L^p[0,1]} = n^{1 - 1/p} \| f \|_{L^p[0,1]} < \varepsilon. \]
    %
    Thus $f_1,\dots,f_n \in W$, so by convexity, $f = (f_1 + \dots + f_n) / n$ lies in $W$. Thus $W = L^p[0,1]$.

    One consequence of this is that for any locally convex topological space $X$, the only continuous linear map $T: L^p[0,1] \to X$ is the zero map, since $T^{-1}(U) = L^p[0,1]$ for any neighborhood $U$ of the origin in $X$. We will later see that all bounded maps with $L^p[0,1]$ as a domain are continuous, so there are no non-zero bounded linear maps $T: L^p[0,1] \to X$ either.
\end{example}

\begin{example}
    Let $X$ be a locally convex space given by the $\sigma(X,S)$ topology, where $S$ is a subset of linear functionals on $X$. If $A \subset X$, and $g: B \to A$ is a function between topological spaces, then $f$ is continuous if and only if $\lambda \circ g$ is continuous for any $\lambda \in S$. A set $B \subset X$ is bounded if and only if $\lambda(B)$ is bounded for each $\lambda \in S$. Most importantly, \emph{any} continuous linear functional on $X$ is an element of the linear span of $S$, i.e. $X^* = \langle S \rangle$. To see this, if $f \in X^*$ is a continuous linear functional on $X$< then there exists $f_1,\dots,f_n \in S$ such that if $|f_1(x)|, \dots, |f_n(x)| < 1$, then $|f(x)| < 1$. But this means that $\text{Ker}(f_1) \cap \dots \text{Ker}(f_n) \subset \text{Ker}(f)$, so $f$ is a linear combination of the functions $\{ f_1, \dots, f_n \}$.
\end{example}

\begin{theorem}
    If $f$ is a non-zero linear functional on a topological vector space $X$, then the following are equivalent:
    %
    \begin{enumerate}
        \item $f$ is continuous.
        \item The nullspace of $f$ is closed.
        \item The nullspace of $f$ is not dense in $X$.
        \item There exists a neighborhood $U$ of the origin such that $f(U)$ is bounded.
    \end{enumerate}
\end{theorem}
\begin{proof}
    (1) implies (2) implies (3) is obvious. If (3) holds, so the nullspace of $f$ is not dense in $X$, we can find $x \in X$ and a balanced open set $U \subset X$ such that $0 \not \in f(x + U) = f(x) + f(U)$. Since $U$ is balanced, this means that $f(u) \neq \lambda f(x)$ for any $u \in U$ and any scalar $\lambda$ with $|\lambda| \geq 1$, so $|f(u)| < |f(x)|$ for all $u \in U$. Thus $f(U)$ is bounded, implying (4). Assuming (4), rescaling $U$ if necessary, we have $f(U) \subset (-1,1)$. Then for any $\varepsilon > 0$, $\varepsilon U \subset f^{-1}((-\varepsilon,\varepsilon))$, which implies that $f$ is continuous at the origin. But this means $f$ is continuous.
\end{proof}

\begin{example}
    Suppose $X$ is a locally convex space, with the topology specified by a family of seminorms $\{ \rho_\alpha \}$ closed under the maximum operation. Then the balls $\varepsilon U_{\rho_\alpha}$ form a basis for the space. It follows that if $f \in X^*$ is a continuous linear functional, then $f$ is bounded on a neighbourhood on the origin, so there exists a seminorm $\rho_\alpha$, $\varepsilon > 0$, and $M > 0$ such that if $\rho(x) \leq \varepsilon$, then $|f(x)| \leq M$. It then follows that for any $x \in X$,
    %
    \[ |f(x)| = \frac{\rho(x)}{\varepsilon} \left| f \left( \frac{\varepsilon x}{\rho(x)} \right) \right| \leq (M/\varepsilon) \rho(x) \]
    %
    Thus a bounded linear functional $f \in X^*$ is precisely a linear functional for which there exists a

    so the functional is directly bounded above by a seminorm.
\end{example}

Refer to the section of these notes on the Hahn-Banach theorem, where it is proved that for any locally convex space $X$, continuous linear functionals on subspaces of $X$ extend to continuous linear functionals on $X$, and the dual space $X^*$ separates $X$. Moreover, in this case $X^*$ determines the \emph{closed convex} subsets of $X$, in the sense that for any two locally convex topologies $\tau_1$ and $\tau_2$ on $X$, if $(X,\tau_1)^* = (X,\tau_2)^*$, then a convex set $C$ is closed in $(X,\tau_1)$ if and only if it is closed in $(X,\tau_2)$.

Continuous linear maps with finite dimensional domain are particularly easy to consider.

\begin{lemma}
    Every linear map $T: k^n \to X$ is continuous.
\end{lemma}
\begin{proof}
    If we write $x_i = Te_i$, where $e_i$ is the $i$'th element of canonical basis of $k^n$, then $T(a) = \sum a_i x_i$, and the continuity follows because addition and scalar multiplication is continuous.
\end{proof}

\begin{corollary}
    Every finite dimensional vector space has a unique topology making it into a topological vector space.
\end{corollary}
\begin{proof}
    if $\tau_0$ and $\tau_1$ are two topologies on $k^n$ turning it into a topological vector space, then the identity map on $k^n$ is a continuous map from $(k^n, \tau_0)$ to $(k^n, \tau_1)$, and from $(k^n, \tau_1)$ to $(k^n, \tau_0)$, hence we conclude the identity map is a homeomorphism.
\end{proof}

\begin{corollary}
    Every finite dimensional subspace of a topological vector space is closed.
\end{corollary}
\begin{proof}
    All finite dimensional topological vector spaces are complete.
\end{proof}

\begin{theorem}
    Every locally compact vector space $X$ has finite dimension.
\end{theorem}
\begin{proof}
    If a space $X$ has a precompact open set $V$, then that neighborhood is bounded, so $\{ 2^{-n} V \}$ is a local base for $X$. There are $x_1, \dots, x_n \in V$ such that
    %
    \[ \overline{V} \subset (x_1 + V/2) \cup \dots \cup (x_n + V/2) \]
    %
    The span of the points $x_i$ is a finite dimensional, closed subspace $Y$ of $X$, and $\overline{V} \subset Y + V/2$. This means of course that $V \subset Y + V/2$. Iterating this inequality gives that $V \subset Y + 2^{-n} V$ for all $n > 0$. But since $\{ 2^{-n} V \}$ forms a local base, and $Y$ is closed, this means that $V \subset Y$. But this means that $Y = X$ since it contains a neighborhood of the origin.
\end{proof}

Since we have a definition of boundedness in any topological vector space $X$, we can ask if a version of the Heine Borel theorem holds in $X$. That is, are closed, bounded subsets of $X$ compact.

\begin{corollary}
    No infinite dimensional space can be locally bounded and satisfy the Heine-Borel theorem.
\end{corollary}
\begin{proof}
    Every bounded neighborhood of a space satisfying the Heine-Borel theorem is a precompact neighborhood. Thus a locally bounded space of this form, it is is locally compact, and thus finite dimensional.
\end{proof}

In the context of norm spaces, we found a linear map was continuous if and only if it was bounded. We can generalize the notion of boundedness to topological vector spaces. A linear map $T: X \to Y$ is \emph{bounded} if for any bounded set $B \subset X$, $T(B)$ is bounded. It is easy to prove that a continuous map is bounded, but the converse is not always true. However, in a metrizable space, it is true.

\begin{lemma}
    if $X$ is a first countable topological vector space, and $\{ x_n \}$ is a sequence in $X$ converging to zero, then there are constants $\{ \lambda_n \}$ converging to infinity such that $\lambda_n x_n \to 0$.
\end{lemma}
\begin{proof}
    If $d$ is a translation invariant metric inducing the topology on $X$, it is easy to prove that for any $x$, $d(nx,0) \leq n d(x,0)$. If $d(x_n,0) \to 0$, there are an increasing sequence of positive integers $n_m$ with $d(x_n,0) \leq 1/m^2$ for $n \geq n_m$. Then $d(mx_{n_m},0) \leq 1/m$, so if $\lambda_n$ is the largest $m$ with $n_m \leq n$, then $\lambda_n x_{n_m} \to 0$.
\end{proof}

Any \emph{sequentially continuous} map is bounded, since if $B$ is bounded, any sequence $\{ T(x_n) \}$ in $T(B)$ with $\{ x_n \}$ in $B$, and any sequence $\{ \lambda_n \}$ converging to zero, $\lambda_n x_n \to 0$, and thus $\lambda_n Tx_n \to 0$ by sequential continuity, so that $\{ Tx_n \}$ is bounded, and thus $TB$ is bounded. Of course, there are sequentially continuous maps that are not necessarily continuous.

\begin{example}
    If $X$ is not first countable, it is not necessarily true that this lemma holds. For instance, consider the space $\CC^{[0,1]}$ of all complex functions on $[0,1]$, under the topology of pointwise convergence. Since $[0,1]$ has the same cardinality as the space $\mathbf{C}^{\mathbf{N}}$ of all complex-valued sequences converging to zero pointwise, we can define a bijective map $F: [0,1] \to \mathbf{C}^{\mathbf{N}}$, and then define a sequence of functions $f_n: [0,1] \to \CC$ by setting $f_n(x) = F(x)_n$. For any sequence of constants $\{ \lambda_i \}$ converging to $\infty$, then there is $x \in [0,1]$ such that $F(x) = \{ 1/\lambda_n \}$, and then $\lambda_n f_n(x) = \lambda_n/\lambda_n = 1$, so the sequence of functions $\{ \lambda_n f_n \}$ does not converge pointwise to zero.
\end{example}

\begin{theorem}
    Let $X$ be a first countable topological vector space, and $Y$ another topological vector space. Then every bounded operator $T: X \to Y$ is continuous.
\end{theorem}
\begin{proof}
    Since $X$ is first countable, it suffices to prove that if $\{ x_n \}$ is a sequence converging to zero, then the sequence $\{ Tx_n \}$ converges to zero in $Y$. Since $T$ is bounded, the sequence $\{ Tx_n \}$ is bounded. If we pick $\{ \lambda_n \}$ as in the last lemma, then $\{ \lambda_n x_n \}$, so $\{ \lambda_n Tx_n \}$ is bounded, and since $\lambda_n^{-1} \to 0$, this means that
    %
    \[ Tx_n = \lambda_n^{-1} (\lambda_n Tx_n) \to 0. \qedhere \]
\end{proof}

\begin{example}
    If $X$ is a barelled space, then $X^*_b$ and $X^*_{\sigma^*}$ have the same bounded sets. Indeed, this is precisely the statement of the uniform boundedness theorem for barelled spaces. But this means that the identity map $X^*_\sigma \to X^*_b$ is bounded, \emph{but not continuous}, since there are open sets in $X^*_b$ which are not open in $X^*_\sigma$.
\end{example}

We say a space $X$ is \emph{Bornological} if all bounded linear maps with $X$ as a domain and with a locally convex codomain $Y$ are continuous. The last theorem shows that all first countable spaces are Bornological. All LF spaces are Bornological, as are the more general category of spaces obtained by locally convex limits of an increasing family of first countable spaces.

An arbitrary locally convex space is not necessarily Bornological. For instance, the weak topology on the dual of an infinite dimensional norm space is not bornological. Nonetheless, we have a useful criterion for continuity, which is somewhat analogous to proving an operator is boundedness.

\begin{theorem}
    Let $T: X \to Y$ be a map between locally convex spaces. Then $T$ is continuous if and only if for any continuous seminorm $\| \cdot \|_Y$ on $Y$, there is a continuous seminorm $\| \cdot \|_X$ on $X$ such that $\| Tx \|_Y \leq \| x \|_X$.
\end{theorem}
\begin{proof}
    Suppose $T$ is continuous. If we consider the open set
    %
    \[ B = \{ y \in Y: \| y \| < 1 \}, \]
    %
    then $T^{-1}(B)$ is an open neighborhood of the origin in $X$, and therefore contains a convex, balanced subset about the origin which is the unit ball with respect to some continuous seminorm $\| \cdot \|_X$. Thus $\| Tx \|_Y \leq 1$ if $\| x \|_X \leq 1$, which implies that
    %
    \[ \| Tx \|_Y = \| x \|_X \left\| T \left( x/\|x \|_X \right) \right\|_Y \leq \| x \|_X \]
    %
    Conversely, if the other condition is satisfied and $U$ is an open convex set about the origin in $Y$, then it is the unit ball with respect to some norm $\| \cdot \|_Y$, and so there is $\| \cdot \|_X$ such that $\| Tx \|_Y \leq \| x \|_X$, hence the unit ball with respect to the $X$ norm is an open neighbourhood which maps into $U$, so $T$ is continuous at the origin, hence continuous everywhere.
\end{proof}

If $X$ and $Y$ are generated by an upper directed set of seminorms $\{ \rho_\alpha \}$ and $\{ \psi_\beta \}$, we can restrict the conditions of the theorem to only the seminorms contained in this group, if we weaken the condition $\| Tx \|_Y \leq \| x \|_Y$ to an equation $\| Tx \|_Y \lesssim \| x \|_X$ with a universal constant.

\section{Quotient Spaces}

If $X_0$ is a closed subspace of a topological vector space $X$, then $X/X_0$, equipped with the quotient topology, is automatically a topological vector space.

\begin{theorem}
    Let $X$ be a topological vector space, with a closed subspace $X_0$. Then the quotient map $\pi: X \to X/X_0$ is open, and a neighbourhood base for $X/X_0$ is given by the family $\{ \pi(U) \}$, where $U$ ranges over a neighborhood basis of $X$.
\end{theorem}
\begin{proof}
    The fact that $\pi(U)$ is open when $U$ is open follows because
    %
    \[ \pi^{-1}(\pi(U)) = U + Y \]
    %
    is obviously a union of open neighbourhoods. If $V$ is an open neighbourhood of the origin in $X/Y$, then $\pi^{-1}(V)$ is a neighbourhood of the origin, hence contains some $U$ in a neighbourhood base, and then $\pi(U) \subset V$.
\end{proof}

\begin{theorem}
    The quotient of a locally convex / locally bounded / metrizable / normable / complete metrizable space also has these properties.
\end{theorem}
\begin{proof}
    The image of a convex / balanced / absorbing / bounded set under a quotient map is also convex / balanced / absorbing / bounded. This shows that the quotient of a locally convex / bounded space is locally convex / bounded. Since the quotient of a first countable space is first countable, this shows that the quotient of a metrizable space is metrizable. The fact that the quotient of a normable space is normable follows because a normable is precisely a locally convex and locally bounded space. The only remaining fact is to show that the quotient of a complete metrizable space is complete. If $d$ is a complete invariant metric for the space, with $f(x) = d(0,x)$, then the metric for the quotient is given by
    %
    \[ d(x_0 + Y, x_1 + Y) = \inf \{ f((x_1 - x_0) + y) : y \in Y \} \]
    %
    The triangle inequality follows because if $f((x_1 - x_0) + y_0) \leq \alpha$ and $f((x_2 - x_1) + y_1) \leq \beta$, then $d(x_1 + y_0, x_0) \leq \alpha$, and $d(x_2 + y_1 + y_0, x_1 + y_0) \leq \beta$, and so the triangle inequality on $X$ gives
    %
    \[ d(x_0, x_2 + y_1 + y_0) \leq \alpha + \beta \]
    %
    so $d(x_0 + Y, x_2 + Y) \leq \alpha + \beta$, and taking infima gives the triangle inequality. If $x_\alpha + Y \to 0$ in $X/Y$, then we can pick $y_\alpha$ with $x_\alpha + y_\alpha \to 0$ in $X$, hence $d(0, x_\alpha + y_\alpha) \to 0$, so $d(0,x_\alpha + Y) \to 0$. Conversely, if $d(0,x_\alpha + Y) \to 0$, then there are $y_\alpha$ with $d(0, x_\alpha + y_\alpha) \to 0$, so $x_\alpha + y_\alpha \to 0$, hence $x_\alpha + Y \to 0$. Thus $d$ is a metric for the topological structure of $X/Y$. Now if $d$ is complete on $X$, it remains to show $d$ is complete on $X/Y$. Suppose $x_n + Y$ is Cauchy. Then we can inductively, for each $n$, pick $k_n$ and $y_n$ such that $d(x_{k_{n+1}} + y_{n+1}, x_{k_n} + y_n) \leq 1/2^n$. This means $x_{k_n} + y_n$ is Cauchy, hence converges, hence $x_{k_n} + Y$ converges, and because of the Cauchy property it follows that $x_n + Y$ converges to the same value.
\end{proof}

\begin{corollary}
    If $Y$ is a closed subspace of $X$, and $Z$ is a finite dimensional subspace, then $Y + Z$ is closed.
\end{corollary}
\begin{proof}
    $Z/Y$ is finite dimension in $X/Y$, hence is closed. But $Y + Z$ is the inverse image of $Z/Y$ under the quotient map, which is continuous, so $Y + Z$ is closed.
\end{proof}

Just like with norm spaces, we can consider \emph{complementary subspaces} $X_1$ to a given closed subspace $X_0$ of a topological vector space $X$, which are precisely those subspaces such that the induced map $X_0 \oplus X_1 \to X$ is an isomorphism. The existence of a complementary subspace in this scenario is also equivalent to the existence of a continuous projection map $\pi: X \to X_0$.








\section{Limits}

We have a theory of topological vector spaces, and maps between them, so we have a category. Let us study  limits in this space. \emph{Projective limits are the simplest}. Given a family of topological vector spaces $\{ X_\alpha \}$, together with maps $\{ f_{\alpha \beta}: X_\alpha \to X_\beta \}$ for each $\alpha \leq \beta$, we can form the topological projective limit $X$, which has maps $f_\alpha: X \to X_\alpha$ for each index $\alpha$ such that $f_{\alpha \beta} \circ f_\alpha = f_\beta$, such that $X$ is equipped with the coarsest topology such that each of the maps $\{ f_\alpha \}$ is continuous. It is siple to give $X$ a topological vector space structure, and that $X$ is then the projective limit in the category of topological vector spaces, since a linear map $T: Y \to X$ will then be continuous if and only if each of the maps $T_\alpha = f_\alpha \circ T$ is continuous for all $\alpha$. If each of the spaces $\{ X_\alpha \}$ is complete, then the projective limit will also be complete, and if each of the spaces $\{ X_\alpha \}$ is locally convex, then the projective limit will also be locally convex, and this projective limit will also be the projective limit in the subcategory of locally convex topological vector spaces.

\begin{example}
    Let $X$ be a complete, locally convex space. Then there exists a family of seminorm defining the topology of $X$. For each continuous seminorm $\rho$ on $X$, if we consider the closed subspace $X_\rho = \rho^{-1}(0)$ of $X$, then the quotient space $X(\rho)$ defined to be $X / X_\rho$ is a Banachable space, with norm given by $\rho$. If $\rho_1 \leq \rho_2$, then we have a natural continuous map from $X(\rho_2) \to X(\rho_1)$ by generla properties of the quotient map. We claim that $X$ is the projective limit of the spaces $\{ X(\rho) \}$. Indeed, a linear map $T: Y \to X$ from a locally space $Y$ is continuous if and only if for each continuous seminorm $\rho$ on $X$, $\eta = \rho \circ T$ is a continuous seminorm on $Y$, which holds if and only if $T_\rho: Y \to X(\rho)$ is continuous. In particular, if $X$ is a \emph{Fr\'{e}chet space}, then we can select a subsequence from the family $\{ X(\rho) \}$, and we conclude that every Fr\'{e}chet space is the projective limit of a sequence of Banach spaces.
\end{example}

Direct limits are more technical. Given a system $\{ f_{\alpha \beta}: X_\alpha \to X_\beta \}$ of linear maps, we can form the topological inductive limit $X$, together with the family of maps $f_\alpha: X_\alpha \to X$ such that $U \subset X$ is open if and only if $f_\alpha^{-1}(U)$ is open in $X_\alpha$ for each $\alpha$. This is precisely the topology with the most open sets, such that each of the maps $f_\alpha$ is continuous. It is simple to give $X$ a topological vector space structure such that $X$ is then the inductive limit in the family of topological vector spaces. Nonetheless, the deceptive simplicity of this construction hides some complexities with this generality, however. In particular, for instance, the inductive limit of a family of Hausdorff topological vector spaces need not be Hausdorff. TODO: WHAT ARE SOME OTHER PATHOLOGIES OF THIS CONSTRUCTION.

Our construction is more nicer if we instead work in the subcategory of \emph{locally convex topological spaces}. Working in this subcategory, we require less functions to be continuous, and so the direct limit can be defined with fewer open sets. In particular, if each of the spaces $\{ X_\alpha \}$ is locally convex, then to form an direct limit in this subcategory, then we obtain an direct limit in the category of locally convex topological vector spaces by declaring a \emph{convex} set $U \subset X$ to be an open neighborhood of the origin if and only if $f_\alpha^{-1}(U)$ is open for each $\alpha$, and then using these sets as a basis for the overall topology. It is easy to see this is a basis, because the intersection of convex sets is convex, and thus we get a topology. When we refer to a direct limit of locally convex spaces, we will always mean the definition given here, rather than the more general direct limit defined above.

\begin{example}
    Here is the most important application of LF spaces in analysis. Consider the space $C^\infty_{\text{loc}}(\RR^d)$ of all smooth functions on $\RR^d$. We can make $C^\infty_{\text{loc}}(\RR^d)$ into a Fr\'{e}chet space if we equip it with the topology of locally uniform convergence of all derivatives of a function. For each $\phi \in C^\infty_{\text{loc}}(\RR^d)$, we define it's \emph{support} to be the closure of the set $\{ x \in \RR^d: \phi(x) \neq 0 \}$. We wish to define a complete locally convex topology on the space $C_c^\infty(\RR^d)$ of all smooth functions with compact support.

    The space $C_c^\infty(\RR^d)$ is not a closed subspace of $C^\infty(\RR^d)$, since mass can `escape at infinity'. For instance, the closure $C_0^\infty(\RR^d)$ of $C_c^\infty(\RR^d)$ contains functions like the Gaussian. Thus we cannot use the relative topology to obtain a complete topology. Instead, we apply the theory of inductive limits. For each compact set $K \subset \RR^d$, we let $C_c^\infty(K)$ denote the family of all functions in $C_c^\infty(\RR^d)$ supported on $K$. This \emph{is} a closed subspace of $C^\infty(\RR^d)$. We then give $C_c^\infty(\RR^d)$ the topology given as the inductive limit of the spaces $\{ C_c^\infty(K) \}$. We will later see that an inductive limit of this form is a complete locally convex space, so we have obtained what was desired.

    We can perform this construction more generally. Let $X$ be a $\sigma$-compact, locally compact space, and consider a sheaf $F$ of complete, locally convex topological vector spaces on $X$. Define $F_c(X)$ to be the subspace of all $f \in F(X)$ with compact support. For any compact set $K$, define $F_c(K)$ to be the subspace of $F_c(X)$ containing all $f \in F(X)$ with $\text{supp}(f) \subset K$. If we make the a priori assumption that each of the spaces $F_c(K)$ is closed in $F(X)$, then we can give $F_c(X)$ the direct limit topology of the spaces $F_c(K)$. Examples of this more general construction include the spaces $L^p_c(X)$, when $X$ is equipped with a Borel measure, and the spaces $C^k_c(\RR^d)$.
\end{example}

\begin{remark}
    We see from this result that the inductive limit of locally convex spaces has open sets highly localized around any particular set. One can see this quite explicitly if we look at seminorms. Let us restrict ourselves to the LF space $C_c^\infty(\Omega)$. If $\{ U_i \}$ are a strictly increasing family of precompact open sets in $\Omega$ such that $\Omega = \bigcup U_i$, then for any increasing sequence $\{ \alpha_i \}$ and any increasing sequence $\{ k_i \}$ of positive integers, the norm
    %
    \[ \| f \|_{\alpha,k} = \sup_i \alpha_i \cdot \| f \|_{C^{k_i}(U_{i+1} - U_i)} \]
    %
    is well defined and continuous on $C_c^\infty(\RR^d)$ since the supremum is finite when restricted to any particular $K_i$. In fact, these seminorms completely induce the topology on $C_c^\infty(\RR^d)$. Thus if $B \subset C_c^\infty(\RR^d)$ is a bounded set, then for any choice of $\alpha$ and $k$, there is a universal constant such that for any $f \in B$, and any $i$, $\| f \|_{C^{k_i}(U_{i+1} - U_i)} \lesssim_{\alpha,k} 1/\alpha_i$. This means that as we approach the boundary of $\Omega$, functions in $B$ must converge incredibly rapidly to zero. The last result shows essentially shows that this implies the union of the domains of the functions $f_i$ must actually be precompact. It is this `uniform compactness' that gives us completeness of $C_c^\infty(\RR^d)$, when compared to viewing $C_c^\infty(\RR^d)$ as a subspace of $C^\infty(\RR^d)$, in which case $C_c^\infty(\RR^d)$ is not compact.
\end{remark}

Even in the subcategory of locally convex spaces, the direct limit of Hausdorff locally convex spaces need not be Hausdorff, unless we impose several assumptions which are satisfied by many practical examples of inductive limits.

\begin{lemma}
    Consider a countable direct system
    %
    \[ X_1 \to X_2 \to \dots, \]
    %
    where each $X_i$ is a locally convex $T1$ space, and such that each map $f_i: X_i \to X_{i+1}$ is injective. Then the direct limit $X$ is $T1$ (and thus Hausdorff / Tychonoff).
\end{lemma}
\begin{proof}
    Consider $z,w \in X$. We define an increasing family of open sets $\{ U_i \}$ in the sets $\{ X_i \}$ inductively. Suppose $i_0$ is the smallest index such that $z,w \in X_i$. Define $U_i = \emptyset$ for $i < i_0$. Starting with $i = i_0$, define a convex set $U_i$, which is open in $X_i$, such that $U_{i-1} \subset U_i$, $z \in U_i$, but $w$ is not an element of $\overline{U_i}$. To do this, we start by finding a convex, balanced open neighborhood of the origin $V$ in $X_i$ such that $\overline{w + V} = w + \overline{V}$ is disjoint from $\overline{U_{i-1}}$. Then $U_i = U_{i-1} + V$ is convex, open, contains $z$, and it's closure is disjoint from $w$. Then the set $\bigcup_i U_i$ is a convex, open neighborhood of $z$ in $X$, not containing $w$, so $X$ is $T1$.
\end{proof}

Suppose $X = \lim_i X_i$ is a locally convex space defined as the inductive limit of a family of locally convex spaces $\{ X_i \}$, such that the maps $X_i \to X_{i+1}$ are embeddings. This does not directly imply that the maps $X_i \to X$ will be embeddings, but this is the case. It is clear that the induced topology on $X_i$ as a subset of $X$ is weaker than that of it's original topology. The converse follows from the following lemma.

\begin{lemma}
    Let $X$ be a locally convex space containing a subspace $X_0$. Suppose $U_0$ is an a neighborhood of the origin in $X_0$. Then there exists a convex neighborhood of the origin $U$ in $X$, also not containing $x$, such that $U \cap X_0 = U_0$. If $X_0$ is a \emph{closed} subspace of $X$, and $x \not \in U_0$, we can also pick $U$ so that it also does not contain $x$.
\end{lemma}
\begin{proof}
    There certainly exists a neighborhood $V$ of the origin in $X$ such that $V \cap X_0 = U_0$, the trouble is finding one that is both convex, and does not contain $U$. Since $X$ is locally convex, $V$ does contain a convex open neighborhood $V_0$ of the origin. Let $V_1$ be the convex hull of $V_0 \cup U_0$. Then $V_1$ is also a neighborhood of the origin in $X$, and we claim that $V_1 \cap X_0 = U_0$. Indeed, if $x_1 \in V_0$ and $x_2 \in U_0$, and $tx_1 + (1 - t)x_2 \in X_0$ for some $0 < t < 1$, then $x_1 \in X_0$, and $X_0 \cap V_0 \subset U_0$. The only problem is that $V_1$ might contain $x$. Without loss of generality we may assume $x \in X - X_0$, since this is the only case where $V_1$ might contain $x$. But working with the quotient space $X / X_0$, which is Hausdorff and locally convex space, we can find a convex open set $W$ containing $X_0$, but not containing $\{ x_0 \}$, and we can let $U = V_1 \cap W$.
\end{proof}

Thus under the assumptions considered before this lemma, if $U_i$ is a convex neighborhood of the origin in $X_i$, then there exists an increasing sequence of sets $\{ U_i, U_{i+1}, \dots \}$, where $U_j$ is a convex neighborhood of the origin in $X_j$ for all $j \geq i$, and $X_i \cap U_{i+1} = U_i$. It follows that $U = \bigcup_{j \geq i} U_j$ is a convex neighborhood of the origin in $X$, and $U \cap X_i = U_i$. Similarily, if each of the spaces $X_i$ are closed in $X_{i+1}$ (e.g. if the sequence is a sequence of complete locally convex spaces), then $X_i$ will be embedded as a closed subspace of $X$.

\begin{remark}
    If $X$ is an inductive limit of an increasing sequence of spaces $\{ X_i \}$, it is \emph{not} true that the relative topology of a closed subspace $M$ of $X$ is the same as the inductive limit of the sequence of spaces $\{ M \cap X_i \}$.
\end{remark}

A \emph{LF space} is a locally convex space $X$, which is the direct limit of an increasing sequence of Fr\'{e}chet spaces $\{ X_i \}$, such that $X_i$ is a closed subspace of $X_j$ for $i < j$. This is the kind of direct limit that comes up in most applications of this theory in analysis, and it is a family of spaces where we can still obtain useful results. The next result will imply any LF space is complete.

\begin{lemma}
    Let $E$ be a bounded subset of an inductive limit $X$ of an increasing sequence of locally convex spaces $\{ X_i \}$. Then $E$ is contained in $X_i$ for some $i$.
\end{lemma}
\begin{proof}
    Suppose $E$ is not contained in $X_{i_0}$ for some $i_0$. Then for each $i$, by Hahn-Banach, we can find $\phi_i \in X^*$ vanishing on $X_i$, but such that there is $x_i \in E$ such that $\phi(x_i) = 1$. For each $i$, consider the open set
    %
    \[ W_i = \{ x \in X : |\phi_i(x)| < 1/i \}. \]
    %
    We claim that $W = \bigcap_i W_i$ is open. Certainly, this set is convex and balanced. Moreover, for each $i$,
    %
    \[ W \cap X_i = (W_1 \cap X_i) \cap \dots \cap (W_i \cap X_i) \]
    %
    is an open set. Thus $W$ is open. But this gives a contradiction, since $E$ is not a subset of $tW$ for any $t > 0$.
\end{proof}

The last result is sufficient to show that every inductive limit of complete locally convex spaces is \emph{quasicomplete}, every bounded Cauchy net converges. An LF space is actually complete, though this requires some more work.

\begin{theorem}
    Any LF Space is complete.
\end{theorem}
\begin{proof}
    TODO: SEE Treves.
\end{proof}

Since a map $T: X \to Y$ is continuous from an LF space if and only if $T_i: X_i \to Y$ is continuous, and the domain of these maps are Fr\'{e}chet spaces, it follows that a linear map is continuous on an LF space if and only if it is bounded. Thus LF spaces are Bornological.

\begin{theorem}
    Let $Y$ be an inductive limit of an increasing family of locally convex spaces $\{ Y_i \}$. Then if $X$ is a first countable topological vector space, and $T: X \to Y$ is a bounded map, then $T(X) \subset Y_{i_0}$ for some $i_0$.
\end{theorem}
\begin{proof}
    Suppose we can find a sequence $\{ x_i \}$ in $X$ such that $Tx_i \in Y_i - Y_{i-1}$ for each $i$. Then for any family of non-zero scalars $\{ \alpha_i \}$, the sequence $\{ T(\alpha_i x_i) \} = \{ \alpha_i Tx_i \}$ is unbounded, since the only bounded subsets of $Y$ are contained in a particular space $Y_{i_0}$. Since $T$ is bounded, this means that the sequences $\{ \alpha_i x_i \}$ are unbounded for any choice of non-zero scalars $\alpha_i$. But this is certainly not the case, since if $\{ U_i \}$ is a decreasing family of open sets forming a neighborhood base about the origin for $X$, then we can pick $\{ \alpha_i \}$ such that $\alpha_i x_i \in U_i$ for each $i$, so that $\{ \alpha_i x_i \}$ converges to zero.
\end{proof}

These notes are probably worth looking into if I ever want to learn more about limits of convex spaces: Bierstedt, Klaus-Dieter (1988). An Introduction to Locally Convex Inductive Limits. Functional Analysis and Applications. Singapore-New Jersey-Hong Kong: Universittsbibliothek. pp. 35133. MR 0046004. Retrieved 20 September 2020. Or Treves' book on distribution theory, Chapter 13.

\section{Topologies on the Dual}

Let $X$ be a topological vector space. Then the space $X^*$ of all continuous linear functionals on $X$ is a vector space. We have seen that if $X$ is a norm space, then there is a canonical way to make $X^*$ into a Banach space by equpping it with the dual norm. There is a generalization of this topology to general topological vector spaces, but in the general topological scheme we see there are several other topologies one can put on $X^*$, which are useful even in the case where $X$ is a norm space. We can put most of the useful topologies in a general scheme in terms of a tool known as the polar.

For a set $A \subset X$, we define the \emph{polar} of $A$, denoted $A^\circ$, to be all $x^* \in X$ such that for all $x \in A$, $|x^*(x)| \leq 1$.
%
\begin{itemize}
    \item If $A \subset B$, then $B^\circ \subset A^\circ$.
    \item $A^\circ \cap B^\circ = (A \cup B)^\circ$, and $A^\circ \cup B^\circ \subset (A \cap B)^\circ$.
    \item $A^\circ$ is a convex, balanced subset of $X^*$ containing the origin.
    \item For any scalar $\lambda \neq 0$, $(\lambda A)^\circ = \lambda^{-1} A^\circ = |\lambda|^{-1} A^\circ$.
    \item If $A$ is a cone in $X$, then $A^\circ$ is a subspace of $X^*$, and is equal to the set
    %
    \[ A^\perp = \{ x^* \in X^*: x^*(x) = 0\ \text{for all $x \in A$} \}. \]
    \item If $B$ is a bounded subset of $X$, then $B^\circ$ is an absorbing subset of $X^*$.
\end{itemize}
%
Now let $\Sigma$ be a family of \emph{bounded} subsets of $X$ such that:
%
\begin{itemize}
    \item For any $A,B \in \Sigma$, there is $C \in \Sigma$ such that $A \cup B \subset C$.
    \item For an $A \in \Sigma$ and any scalar $\lambda \neq 0$, there is $B \in \Sigma$ with $\lambda A \subset B$.
    \item $X = \bigcup_{A \in \Sigma} A$.
\end{itemize}
%
The family of sets $\{ A^\circ : A \in \Sigma \}$ is a family of convex, balanced, absorbing subsets of the origin, and we can take these as a neighborhood basis generating a locally convex topology on $X^*$. The only nontrivial thing to verify here is that for any $A \in \Sigma$, there is $B \in \Sigma$ with $B^\circ + B^\circ \subset A^\circ$. But if we choose $B$ such that $2 A \subset B$, then $B^\circ \subset 2^{-1} A^\circ$, and since $A^\circ$ is convex, we find that
%
\[ B^\circ + B^\circ \subset 2^{-1} A^\circ + 2^{-1} A^\circ = A^\circ. \]
%
Thus we have a locally convex topology on $X^*$, which we call the $\Sigma$ topology on $X^*$, and denote the topological vector space equipped with this topology $X^*_\Sigma$. The final property is needed to ensure that the topology is Hausdorff (technically we need only assume the union is dense in $X$, but it will make the later part of the theory nicer, and is true in all the practical topologies on these spaces.

It is more useful to see the topology here in terms of nets. A net $\{ x_\alpha^* \}$ in $X^*$ will converge in the $\Sigma$ topology to some $x^* \in X$ if and only if $\{ x_\alpha^*|_A \}$ converges uniformly to $x^*|_A$ for any $A \in \Sigma$. Thus we might call the $\Sigma$ topology on $X^*$, the topology of uniform convergence over $\Sigma$.

Here are some examples of useful choices of $\Sigma$:
%
\begin{itemize}
    \item If $\Sigma$ is the family of all finite subsets of $X$, then the $\Sigma$ topology on $X^*$ is called the \emph{weak $*$ topology} on $X^*$, also denoted by $\sigma(X^*,X)$, and with this topology we denote $X^*$ by $X^*_{\sigma^*}$. This is the topology of pointwise convergence on $X^*$.

    \item If $\Sigma$ is the family of all convex, compact subsets of $X$, then the resulting $\Sigma$ topology on $X^*$ is sometimes denoted $\gamma(X^*,X)$, and called the topology of convex compact convergence. We denote $X^*$ with this topology as $X^*_\gamma$.

    \item If $\Sigma$ is the family of all compact subsets of $X$, then we get the topology of compact convergence on $X^*$. This does not necessarily equal the $\gamma(X^*,X)$ topology, though it does if $X$ is a Fr\'{e}chet space (since the closed convex hull of a compact set is compact). We denote $X^*$ with this topology as $X^*_c$.

    \item The \emph{strong topology} on $X$ is given by letting $\Sigma$ be all bounded subsets of $X$. If $X$ is a norm space, then the strong topology on $X^*$ is \emph{precisely} the topology induced by the natural operator norm on $X^*$. We denote this by $X^*_b$. In particular, this means the strong dual of a norm space is a Banach space.
\end{itemize}
%
The most important topologies on $X^*$ are the weak-$*$ and strong topologies, though the topology of compact, convex convergence also plays an important role.

We note that $X^*_{\sigma^*}$ is rarely complete. For instance, if $X$ is locally convex, then the completion of $X^*_{\sigma^*}$ can be identified with the space of all (not necessarily continuous) linear maps from $X$ to $k$, equipped with the topology of pointwise convergence. To see this we note that if $\{ X_\alpha \}$ is a net giving all finite dimensional subspaces of $X$, and if $f: X \to k$ is an arbitrarily linear functional, then $f|_{X_\alpha}$ is trivially continuous for each $\alpha$, so Hahn-Banach allows us to find a family of linear functionals $\{ f_\alpha \}$ in $X^*$ such that $f_\alpha|_{X_\alpha} = f|_{X_\alpha}$. Then $\{ f_\alpha \}$ is Cauchy in $X^*_{\sigma^*}$, and converges pointwise to $f$. Conversely, it is simple to see that the space of all linear functionals is complete in the pointwise topology, since the pointwise limit of a family of linear functionals is linear.

On the other hand, if $X$ is a barelled space, then $X^*_{\sigma^*}$ is at least \emph{quasicomplete}. This is because if $\{ x_\alpha^* \}$ is a bounded Cauchy net in $X^*_{\sigma^*}$, then it converges pointwise to some (not necessarily continous) linear functional $x^*: X \to Y$. But since $\{ x_\alpha^* \}$ is bounded, the uniform boundedness principle will imply the net is equicontinuous, which implies that $x^*$ is continuous also. Thus $X^*_{\sigma^*}$ is quasicomplete.

On the other hand, if $X$ is bornological, then $X^*_b$ will be complete, and if $X$ is a Fr\'{e}chet space, or an LF space, then $X^*_c$ and $X^*_\gamma$ will be complete, as the next theorem shows.

\begin{theorem}
    The space $X^*_\Sigma$ is complete if and only if for any linear function $f: X \to k$, if $f|_A: A \to k$ is continuous for any $A \in \Sigma$, then $f$ is continuous.
\end{theorem}
\begin{proof}
    Suppose $\{ f_\alpha \}$ is a Cauchy net in $X^*_\Sigma$. Then $\{ f_\alpha \}$ is a Cauchy net in $X^*_{\sigma^*}$, and thus converges pointwise to some linear functional $f: X \to k$. In fact, one can actually see that $\{ f_\alpha \}$ will converge pointwise uniformly to $f$ on each set $A \in \Sigma$. This implies $f|_A$ is continuous for each $A \in \Sigma$, which by assumption means that $f$ is continuous, and thus $X^*_\Sigma$ is complete.
\end{proof}

If $X$ is bornological, and $f: X \to k$ is a linear functional such that $f|_B$ is continuous for any bounded set $B \subset X$, then $f$ itself is bounded, and thus continuous, which implies $X^*_b$ is complete. If $f$ is a function such that $f|_A$ is continuous for any compact, convex, subset $A$ of $X$, then we claim that $f$ is sequentially continuous, which implies that $f$ is continuous when $X$ is a Fr\'{e}chet or LF space. Indeed, for any sequence $\{ x_n \}$ in $X$ converging to $x$, the set $\{ x_n \} \cup \{ x \}$ is compact, and so it's closed convex hull $A$ is compact, and so $\{ f(x_n) \}$ converges to $f(x)$ since $f|_A$ is continuous.

If $T: X \to Y$ is a continuous linear map, then we can of course introduce the linear adjoint map $T^*: Y^* \to X^*$ by setting $T^*(y^*) = y^* \circ T$. The next lemma characterizes when the map $T^*$ will be continuous in the various topologies we can put on $Y^*$ and $X^*$ using polars. In particular, $T^*$ will be continuous in the weak $*$ topologies, the strong topologies, the topologies of compact convergence, and the topologies of compact convex convergence.

\begin{theorem}
    Let $X$ and $Y$ be topological vector spaces, and $\Sigma$ and $\Pi$ families of subsets of $X$ and $Y$ respectively, of the form above. If $T: X \to Y$ is a continuous linear map, then $T^*: Y^* \to X^*$ is continuous from the $\Pi$ topology on $Y$ to the $\Sigma$ topology on $X$ provided that for each $A \in \Sigma$, there is $B \in \Pi$ such that $T(A) \subset B$.
\end{theorem}

If $\bigcup_{A \in \Sigma} A = X$, then for each $x \in X$, the map $x^* \mapsto x^*(x)$ will be a continous linear functional on $X^*$ in the $\Sigma$ topology, a fact we will always assume in the sequel for any $\Sigma$ topology we consider. This is equivalent to assuming that the $\Sigma$ topology is finer than the weak-$*$ topology. Thus under these assumptions, we get a continuous map $X \to (X^*_\Sigma)^*$. If $\Sigma$ gives the weak $*$ topology, then we have seen that the map $X \to (X^*_{\sigma^*})^*$ is onto. The map $X \to (X^*_\gamma)^*$ is also onto, as we will see later. In general, the map will not be injective, but it will always be injective if $X$ is locally convex, as a consequence of the Hahn-Banach theorem.

\begin{lemma}
    TODO: When does $X$ have a locally convex topology with the same dual as it usually does.
\end{lemma}

Now for any topological vector space $X$, we define $X^{**}$, the \emph{bidual} of $X$, to be the locally convex space $(X^*_b)^*_b$. The map $X \to X^{**}$ is then continuous. If $X$ is a locally convex barelled space, then $X \to X^{**}$ is actually an embedding, since $X \cap B^\circ$ is a barrel in $X$ for any bounded set $B \subset X^*$. If $X$ is locally convex, and the map $X \to X^{**}$ is a bijection, we say $X$ is a \emph{semireflexive space}. If $X \to X^{**}$ is an isomorphism, we say $X$ is a \emph{reflexive space}. This agrees with the definition given in the case that $X$ is a norm space.

\begin{theorem}
    If $X$ is semireflexive, then $X^*_b$ is barelled.
\end{theorem}
\begin{proof}
    We apply Mackey's theorem (to be proved later), which says that, since $X^*_b$ and $X^*_{\sigma^*}$ have the same linear functionals, they have the same bounded sets. Thus if $T$ is a barrel in $X^*_b$, then $T$ is weak $*$ closed, and so $T = ({}^\circ T)^\circ$. To show $T$ is a neighborhood of the origin in $X^*_b$, it now only suffices to show that ${}^\circ T$ is bounded, which is equivalent to showing it is weakly bounded. But this follows because $T$ is an absorbing set.
\end{proof}

\begin{theorem}
    A locally convex space $X$ is semireflexive if and only if it has the Heine-Borel property in the weak topology.
\end{theorem}
\begin{proof}
    If $X$ is semireflexive, it has the Heine-Borel theorem because $X^*_b$ is a barelled space, and so we can apply the uniform boundedness principle to $X$ to show that if $B$ is a closed and bounded subset of $X_\sigma = X_{\sigma^*}$, then $B$ is equicontinuous, and thus compact in $X_\sigma = X_{\sigma^*}$. Conversely, if $X$ has the Heine Borel property with respect to it's weak topology, we note that $X^*_b$ has the topology given by uniform convergence on weakly closed convex, balanced and bounded subsets of $X$, and these sets are all weakly compact.
\end{proof}

We note that the topology on any locally convex set $X$ is equivalent to the topology of uniform convergence on equicontinuous subsets of $X^*$.

\begin{theorem}
    A locally convex space $X$ is reflexive if and only if it is barelled, and has the Heine-Borel property in the weak topology.
\end{theorem}
\begin{proof}
    If $X$ is reflexive, it is barelled, because it is the strong dual of the semireflexive space $X^*_b$. Conversely, if $X$ is barelled, and has the Heine-Borel property in the weak topology, then the topology on $X$ is equivalent to the topology of uniform convergence on equicontinuous subsets of $X^*$, where $(X^*_b)^*$ is the topology of uniform convergence on bounded subsets of $X^*$, and these are identified if $X$ is barelled.
\end{proof}

We can use similar topologies to define topologies on the spaces $L(X,Y)$ of continuous linear maps between two topological vector spaces. Given a bounded subset $B$ of $X$, and a neighborhood of zero $W$ in $Y$, we define
%
\[ U(B,W) = \{ T \in L(X,Y): T(B) \subset W \}. \]
%
Then $U(B,W)$ is absorbing, is convex if $W$ is convex, and is balanced if $W$ is balanced. Given a family $\Sigma$ of \emph{bounded} subsets of $X$ satisfying analogous conditions to those used to define topologies on $X^*$, we consider the topology generated by the sets $\{ U(B,W) \}$, where $B$ ranges over elements of $\Sigma$, and $W$ over neighborhoods of zero. We call this the $\Sigma$ topology on $L(X,Y)$, and denote by $L_\Sigma(X,Y)$ the topological vector space of such functions. This is precisely the topology of uniform convergence on subsets of $\Sigma$. Here are some examples:
%
\begin{itemize}
    \item The \emph{strong operator topology} on $X$, denoted $L_{\sigma^*}(X,Y)$, is given by letting $\Sigma$ be all finite subsets of $X$. Thus a net $\{ T_\alpha \}$ converges to some $T \in L(X,Y)$ if $\{ T_\alpha(x) \}$ converges to $T(x)$ for all $x \in X$.

    \item The topology $L_\gamma(X,Y)$ is given by uniform convergence on convex, compact subsets of $X$.

    \item The topology $L_c(X,Y)$ of uniform convergence on compact sets of $X$.

    \item The topology $L_b(X,Y)$ given by uniform convergence on bounded sets of $X$.
\end{itemize}
%
Again, the topologies $L_{\sigma^*}$ and $L_b$ topologies will be most important.

As with linear functionals, the completion of $L_{\sigma^*}(X,Y)$ can be identified, if $X$ and $Y$ are locally convex, with the family of all linear maps from $X$ to $Y$ under the topology of pointwise convergence. As with functionals, the space $L_\Sigma(X,Y)$ will in general only be complete if, any linear map $T: X \to Y$ is complete when the restrictions $T|_A$ are complete for each $A \in \Sigma$. In particular, this implies that for any bornological locally convex space $X$, and any complete locally convex space $Y$, $L_b(X,Y)$ is complete. And for any Fr\'{e}chet or LF space $X$, and any complete locally convex space $Y$, $L_c(X,Y)$ and $L_\gamma(X,Y)$ will be complete.

Now we move onto the bipolar theorem. Given any $A \subset X^*$, we can consider the prepolar
%
\[ {}^\circ A = \{ x \in X : |x^*(x)| \leq 1\ \text{for all $x^* \in A$} \}, \]
%
which is a closed, convex, balanced set in the weak topology. Similarily, for $A \subset X$, we can consider the annihilator
%
\[ A^\perp = \{ x^* \in X^* : x^*(a) = 0\ \text{for all $a \in A$} \}. \]
%
Conversely, given $A \subset X^*$, we consider
%
\[ {}^\perp A = \{ x \in X : a^*(x) = 0\ \text{for all $a \in A$} \}. \]
%
For any $A \subset X$, we have $A \subset {}^\circ A^\circ \subset {}^\perp A^\perp$, and for any $B \subset X^*$, we have $B \subset ({}^\circ B)^{\circ} \subset ({}^\perp B)^{\perp}$.

we can fully characterize these sets.

\begin{theorem}
    For any $A \subset X$:
    %
    \begin{itemize}
        \item ${}^\circ A^\circ$ is the weak closure of the balanced convex hull of $A$.
        \item ${}^\perp A^\perp$ is the weak closure of the linear span of $A$.
    \end{itemize}
    %
    For any $B \subset X^*$:
    %
    \begin{itemize}
        \item $({}^\circ B)^{\circ}$ is the weak $*$ closure of the convex, balanced hull of $B$.
        \item $({}^\perp B)^{\perp}$ is the weak $*$ closure of the linear span of $B$.
    \end{itemize}
\end{theorem}
\begin{proof}
    It is clear that the weak closure of the convex balanced hull of $A$ lies in the bipolar of $A$ since the bipolar is weakly closed, balanced, convex, and contains $A$. Conversely, if $x_0$ does not lie in the weak closed balanced convex hull of $A$, then by the Hahn Banach theorem, there exists $\phi \in A^\circ$ such that $\phi(x_0) > \sup_{a \in A} \phi(a)$. Thus $x_0$ is not in the bioplar of $A$. The remaining cases are similar.
\end{proof}

\section{Weak $*$ Topologies on Norm Spaces}

TODO: I am not aware how many of these results generalize to the locally convex setting

If $X$ is a norm space, then the norm on $X^*$ is not weak $*$ continuous. However, it \emph{is} lower semicontinuous, in the sense that for any net $\{ x_\alpha^* \}$ in $X^*$ converging to some $x \in X$ in the weak $*$ topology,
%
\[ \| x \| \leq \liminf_{\alpha \to \infty} \| x_\alpha^* \|. \]
%
A similar statement remains true for the weak topology on $X$. However, we now detail some fundamental differences between the two topologies which can occur if $X$ is not reflexive.

\begin{example}
    A convex set is norm closed if and only if it is weakly closed. This is no longer true for the weak $*$ topology on a non semireflexive space $X$, since this means precisely that there is a linear functional on $X^*$ which is continuous on $X^*_b$, but not on $X^*_{\sigma^*}$. The kernel of this functional is a convex set, closed in $X^*_b$, but not closed in $X^*_{\sigma^*}$.
\end{example}

\begin{example}
    A linear operator $T: X \to Y$ is norm continuous if and only if it is weakly continuous. On the other hand, a linear map $T: X^* \to Y^*$ can be continuous on the strong topologies of $X^*$ and $Y^*$, but not necessarily continuous on the weak $*$ topologies of $X^*$ and $Y^*$. As an example, equip $l^1$ with the weak $*$ topology by identifying it with the dual of $c_0$. Define $T: l^1 \to l^1$ by setting
    %
    \[ Ta = \left( \sum_{n = 1}^\infty a_n, a_2, a_3, \dots \right). \]
    %
    Then $\| Ta \|_{l^1} \leq 2 \| a \|_{l^1}$, so $T$ is a norm continuous isomorphism of $l^1$, since it has a continuous inverse
    %
    \[ T^{-1}b = \left( b_1 - \sum_{n = 2}^\infty b_n, b_2, b_3, \dots \right). \]
    %
    Now the sequence $\{ e_n \}$ converges to the weak $*$ topology to zero. But $Ta = e_1 + e_n$ does \emph{not} converge in the weak $*$ topology to zero. So $T$ is not weak $*$ continuous.
\end{example}

On the other hand, it \emph{is} true that if $X$ and $Y$ are norm spaces, and $T: X^* \to Y^*$ is continuous on the weak $*$ topologies of $X^*$ and $Y^*$, then it is continuous on the norm topologies of $X^*$ and $Y^*$. This is because of the following result, which says that if $S: Y^* \to X^*$ is weak $*$ continuous, then there exists a weakly continuous linear operator $T: X \to Y$ such that $S = T^*$, and on norm spaces, weakly continuous operators are continuous.

\begin{theorem}
    Let $X$ be a Bornological vector space, and let $Y$ be locally convex. Then a linear map $S: Y^* \to X^*$ is continuous with respect to the weak $*$ topology if and only if $S = T^*$ for some continuous linear operator $T: X \to Y$.
\end{theorem}
\begin{proof}
    If $T: X \to Y$ is continuous, then $T^*: Y^* \to X^*$ is weak $*$ continuous. Conversely, if $S: Y^*_\sigma \to X^*_\sigma$ is continuous, then we get a map $S^*: (X^*_{\sigma^*})^* \to (Y^*_{\sigma^*})^*$, and if we identify $X$ and $Y$ with these double duals, then we get a map $T: X \to Y$, and it is easy to see that $T^* = S$. Since $T^* = S$ maps $Y^*$ to $X^*$, we will later see this implies $T$ is continuous.
\end{proof}

\begin{corollary}
    Let $X$ be a norm space with subspace $X_0$. Then the canonical identification of $(X/X_0)^*$ with $X_0^\perp$ is a homeomorphism from the weak $*$ topology induced from $X/X_0$, to the relative weak $*$ topology induced from $X^*$.
\end{corollary}
\begin{proof}
    The canonical identification is given by the adjoint of $\pi: X \to X/X_0$, i.e. $\pi^*: (X/X_0)^* \to X_0^\perp$. Since $\pi$ is a bounded linear map, $\pi^*$ is weak $*$ continuous. Thus it suffices to show that $(\pi^*)^{-1}$ is weak $*$ continuous. Suppose $\{ x_\alpha^* \}$ is a net in $X_0^\perp$ converging in the weak $*$ topology on $X^*$ to some $x^* \in X_0^\perp$. Then for any $x \in X$,
    %
    \[ (\pi^*)^{-1} x_\alpha^*(x + X_0) = x_\alpha^*(x) \to x^*(x) = (\pi^*)^{-1} x^* (x + X_0). \]
    %
    Thus $(\pi^*)^{-1}$ is also continuous.
\end{proof}

\begin{comment}
\begin{theorem}[Banach-Alaoglu]
    Let $X$ be a norm space. Then every bounded subset of $X^*$ is weak $*$ compact.
\end{theorem}
\begin{proof}
    It suffices to prove that $B_X$ is weak $*$ compact. Recall Tychonoff's theorem, which says that an arbitrary product of compact topological spaces is compact. If
    %
    \[ B = \{ \alpha : |\alpha| \leq 1 \} \]
    %
    is the unit ball of scalars, then $B$ is compact, and so
    %
    \[ A = \prod_{x \in B_X} B \]
    %
    is compact. The map $f: B_{X^*} \to A$ given by
    %
    \[ f(x^*) = \prod_{x \in B_X} x^*(x) \]
    %
    is an embedding where $B_{X^*}$ has the weak $*$ topology. It thus suffices to show that $f(B_X^*)$ is closed in $A$. But if there exists a function $f: B_X \to B$ and a net $\{ x_\alpha^* \}$ in $B_{X^*}$ such that $x_\alpha^*(x) \to f(x)$ for each $x \in B_X$, then it is simple to see that $f$ extends to a linear function $f: X \to B$, $x_\alpha^*(x) \to x$ for all $x \in X$, and
    %
    \[ |f(x)| = |\lim_{\alpha} x_\alpha^*(x)| \leq \limsup_{\alpha \to \infty} \| x_\alpha^* \| \| x \| \leq \| x \|. \]
    %
    Thus $f \in B_{X^*}$, completing the proof.
\end{proof}
\end{comment}

\begin{corollary}
    A norm space $X$ is separable if and only if every bounded subset of $X^*$ is weak $*$ metrizable.
\end{corollary}
\begin{proof}
    Fix a bounded set $A \subset X^*$ and suppose $X$ is separable. Without loss of generality, assume $A$ is weak $*$ closed in $X^*$, and thus weak $*$ compact. If $\{ x_1, x_2, \dots \} \subset B_X$ is a countable, dense subset, then we can define, for $a_1,a_2 \in A$,
    %
    \[ d(a_1, a_2) = \sum_{n = 1}^\infty 2^{-n} |a_1(x_n) - a_2(x_n)|. \]
    %
    It is simple to see that this gives a metric for the weak $*$ topology on $X^*$.

    Conversely, suppose that $B_{X^*}$ is weak $*$ metrizable, with some metric $d$. For each $n$, the weak $*$ metric ball $B_n$ of radius $1/n$ about the origin is relatively open in $B_{X^*}$, so there exists a finite set $A_n \subset X$ such that
    %
    \[ \{ x^* \in B_{X^*}: |x^*(x)| < 1\ \text{for all $x \in A_n$} \} \subset B_n. \]
    %
    It follows that if $A = \bigcup_n A_n$, then $A^\perp = \{ 0 \}$, and so $A$ is dense in $X$, proving $X$ is separable.
\end{proof}

\begin{corollary}
    Let $X$ be a Banach space. Then every weak $*$ Cauchy sequence is weak $*$ convergent.
\end{corollary}
\begin{proof}
    If $\{ x_n^* \}$ is weak $*$ Cauchy, then it is bounded, and thus has a convergent subsequence, from which one can find $x$ such that $\{ x_n^* \}$ converges to $x$ in the weak $*$ topology.
\end{proof}

It follows that if $X$ is a Banach space, then $X^*$ is always weak $*$ sequentially complete. This is not necessarily true if $X$ is not complete.

\begin{example}
    Let $X$ be the subspace of $l^1$ consisting of sequences with finite support, and consider the induced weak $*$ topology on $X^*$, which we identify with $l^\infty$. If we consider the sequence
    %
    \[ x_n^* = \sum_{m = 1}^n 2^n e_n, \]
    %
    then $\{ x_n^* \}$ is weak $*$ Cauchy since for any $x \in X$, $x_n^*(x)$ is eventually constant for large $n$. But it does not converge to an element of $X^*$. The sequence is no longer Cauchy in the weak $*$ topology on $l^\infty$ induces by viewing the space as the dual of $l^1$, since then
    %
    \[ x_n^* \left( \sum_{m = 1}^\infty e_m / m^2 \right) = \sum_{m = 1}^n 2^m / m^2 \]
    %
    does not converge as $n \to \infty$.
\end{example} 

Since $B_{X^*}$ is a compact Hausdorff space in the weak $*$ topology, and any $x \in X$ acts as a continuous function on $B_{X^*}$, it follows that any norm space $X$ is isometric to a subspace of $C(K)$ for some compact space $K$.

The space $X^{**}$ also has a weak $*$ topology induced from it being the dual of $X^*$. The inclusion $X \to X^{**}$ is then an embedding of the \emph{weak} topology of $X$ into the \emph{weak $*$} topology on $X^{**}$. Now $X$ is a closed subset of $X^{**}$, and so if $X$ is not reflexive, $X$ is not dense, nor weakly dense in $X^{**}$. However, it \emph{is always} weak $*$ dense. This is \emph{Goldstine's Theorem}.

\begin{theorem}
    If $X$ is a norm space, then $B_{X^{**}}$ is the weak $*$ closure of $B_X$.
\end{theorem}
\begin{proof}
    To start with, we show that $B_X$ is weak $*$ dense in $B_{X^{**}}$. It suffices to show $B_{X^{**}}$ is closed in the weak $*$ closure of $B_X$ in $X^{**}$, which we will denote by $A$. For simplicity, assume $X$ is a real norm space (the complex case is analogous). If $f \in X^{**}$ is not in the weak $*$ closure of $B_X$, then we can apply a Hahn-Banach separation theorem between $A$ and $f$ to find $x^* \in X^*$ such that 
    %
    \[ |f(x^*)| > \sup_{x \in A} x^*(x) \geq \sup_{x \in B_X} x^*(x) = \| x^* \|. \]
    %
    But this means that $\| f \| > 1$, and thus is not in $B_{X^{**}}$.

    If $\{ x_\alpha \}$ is a net in $B_X$ converging in the weak $*$ topology to some $f \in X^{**}$, then for $x^* \in X^*$,
    %
    \[ |f(x^*)| \leq \liminf_{\alpha \to \infty} |x^*(x_\alpha)| \leq \liminf_{\alpha \to \infty} \| x_\alpha \| \| x^* \| \leq \| x^* \|. \]
    %
    Thus $f \in B_{X^{**}}$.
\end{proof}

\begin{corollary}
    For any norm space $X$, $X$ is weak $*$ dense in $X^{**}$.
\end{corollary}

Using a weaker topology than the weak $*$ topology, called the \emph{bounded weak $*$ topology}, which has the same linear functionals as the weak $*$ topology, one can prove the following result, called the Krein-Smulian theorem.

\begin{theorem}
    Let $X$ be a Banach space, and $C \subset X^*$ a convex set. Then $C$ is weak $*$ closed if and only if $C \cap t B_{X^*}$ is weak $*$ closed for each $t > 0$.
\end{theorem}

\begin{corollary}
    Let $X$ be a norm space. If $M$ is a subspace of $X^*$, then $M$ is weak $*$ closed if and only if $M \cap B_{X^*}$ is weak $*$ closed.
\end{corollary}

\begin{corollary}
    Let $X$ be a separable Banach space. Then a convex subset of $X^*$ is weak $*$ closed if and only if it is weak $*$ sequentially closed.
\end{corollary}
\begin{proof}
    Suppose $C$ is a weak $*$ sequentially closed subset of $X^*$. It suffices to show that $C \cap t B_{X^*}$ is weak $*$ closed for all $t > 0$. But since $X$ is separable, the weak $*$ topology on $t B_{X^*}$ is metrizable. Thus the result follows immediately.
\end{proof}

Because $X$ with the weak topology is embedded in $X^{**}$, equipped with the weak $*$ topology, a subset $A$ of $X$ is weakly compact if and only if $A$ is bounded, and weak $*$ closed in $X^{**}$. We thus find an interesting characterization of reflexivity.

\begin{theorem}
    A norm space $X$ is reflexive if and only if $B_X$ is weakly compact.
\end{theorem}
\begin{proof}
    Suppose $X$ is reflexive. To show $B_X$ is weakly compact, it suffices to show it is bounded and weakly closed. But this follows because of the lower semicontinuous of the norm on $X$. Conversely, suppose $B_X$ is weakly compact. Then it is bounded and weak $*$ closed in $X^{**}$. But $B_X$ is weak $*$ dense in $B_{X^{**}}$, from which it follows that $X = X^{**}$.
\end{proof}

TODO: REST OF SECTION 2.7, 2.8, and 2.10 of MEGGINSON.

\section{Equicontinuity}

A family of functions $\mathcal{S} \subset L(X,Y)$ will be called \emph{equicontinuous} if, for any open neighborhood $W$ of the origin in $Y$, there is a neighborhood $V$ of the origin in $X$ such that $T(V) \subset W$ for all $T \in S$. If $\mathcal{S}$ is equicontinuous, then it is certainly bounded in $L_{\sigma^*}(X,Y)$, $L_\gamma(X,Y)$, $L_c(X,Y)$, and $L_b(X,Y)$.

\begin{theorem}
    If $\mathcal{S} \subset L(X,Y)$ is equicontinuous, then it's closure in $L_{\sigma^*}(X,Y)$ is also equicontinuous.
\end{theorem}
\begin{proof}
    Let $\mathcal{H}$ be the closure of $\mathcal{S}$ in $L_{\sigma^*}(X,Y)$. For each open neighborhood $V$ of the origin in $Y$, find a balanced neighborhood $W$ such that $W + W \subset V$, then find $U$ such that $T(U) \subset W$ for each $T \in \mathcal{S}$. Now if $T \in \mathcal{H}$, then for every $x \in U$, there is $S \in \mathcal{S}$ such that $T(x) - S(x) \in W$. Then $T(x) \in S(x) + W \subset W + W \subset V$. Thus $T(U) \subset V$. Thus $\mathcal{H}$ is equicontinuous.
\end{proof}

In fact, the topologies $L_{\sigma^*}(X,Y)$, $L_\gamma(X,Y)$, and $L_c(X,Y)$ all coincide on an equicontinuous family, so that the closure is the same in each of these families.

\begin{theorem}
    If $\mathcal{S}$ is an equicontinuous family of maps between two topological vector spaces $X$ and $Y$, then the relative topologies on $\mathcal{S}$ induced by $L_{\sigma^*}(X,Y)$ and $L_c(X,Y)$ coincide on $\mathcal{S}$.
\end{theorem}
\begin{proof}
    Suppose $\{ T_\alpha: X \to Y \}$ is a net of equicontinuous operators, converging pointwise to a continuous map $T: X \to Y$. For any open neighborhood $V$ of the origin in $Y$, find a neighborhood $W$ such that $W + W + W \subset V$. Then we can find a neighborhood $U$ of the origin in $X$ such that $T_\alpha(U) \subset W$ for all indices $\alpha$, and $T(U) \subset W$. If $K$ is a compact subset of $X$, then there are finitely many points $x_1,\dots,x_n \in X$ such that $K$ is covered by the family of sets $x_1 + U, \dots, x_n + U$. There is $\alpha_0$ such that for $\alpha \geq \alpha_0$ and any $1 \leq i \leq n$, $T_\alpha(x_i) - T(x_i) \in W$. But for any $x \in K$, there is $i$ such that $x \in x_i + U$, which means that for $\alpha \geq \alpha_0$,
    %
    \begin{align*}
        T_\alpha(x) &\in T_\alpha(x_i) + T_\alpha(U)\\
        &\subset T(x_i) + W + W\\
        &\subset T(x) + T(U) + W + W\\
        &\subset T(x) + W + W + W \subset T(x) + V.
    \end{align*}
    %
    Thus for $\alpha \geq \alpha_0$, $(T - T_\alpha)(K) \subset V$. Thus $\{ T_\alpha \}$ converges uniformly on compact sets to $T$.
\end{proof}

\begin{remark}
    If $\{ T_\alpha \}$ is an equicontinuous net of continuous linear maps in $L(X,Y)$, and there is another continuous linear map $T$ in $L(X,Y)$, for any two topological vector spaces $X$ and $Y$, and there exists a dense subset $X_0$ of $X$ such that $\{ T_\alpha(x_0) \}$ converges to $T(x_0)$ for all $x_0 \in X_0$, then $\{ T_\alpha(x) \}$ converges to $T(x)$ for all $x \in X$. Indeed, if $V$ is an arbitrary neighborhood of the origin in $Y$, and if $W$ is another balanced neighborhood such that $W + W + W \subset V$, then by equicontinuity, we can find a balanced neighborhood $U$ of the origin such that $T_\alpha(U) \subset W$ for all $\alpha$, and $T(U) \subset W$. Then for any $x \in X$, we can find $x_0 \in X_0$ such that $x - x_0 \in U$. There is $\alpha_0$ such that if $\alpha \geq \alpha_0$, then $T_\alpha(x_0) - T(x_0) \in W$, and so for that same range of $\alpha$,
    %
    \begin{align*}
        T_\alpha(x) - T(x) &= [T_\alpha(x) - T_\alpha(x_0)] + [T_\alpha(x_0) - T(x_0)] + [T(x_0) - T(x)]\\
        &\in W + W + W \subset V.
    \end{align*}
    %
    Thus if $\mathcal{S}$ is an equicontinuous set, then the topology of pointwise convergence is equivalent on $\mathcal{S}$ to the topology of pointwise convergence on a dense subset.
\end{remark}

\begin{theorem}
    If $\mathcal{S}$ is an equicontinuous family of maps between two topological vector spaces $X$ and $Y$, then it is bounded in $L_b(X,Y)$.
\end{theorem}
\begin{proof}
    It suffices to show that for any bounded set $B$ in $X$, and any balanced open neighborhood of the origin $W$ in $Y$, there is $\lambda > 0$ such that $\mathcal{S} \subset \lambda U(B,W)$. Since $\mathcal{S}$ is equicontinuous, there is a balanced neighborhood $V$ in $X$ such that $T(V) \subset W$ for all $T \in \mathcal{S}$. Since $B$ is bounded, $B \subset \lambda V$ for some $\lambda > 0$, so that $\mathcal{S} \subset \lambda U(B,W)$.
\end{proof}

One of the huge advantages of working in the weak $*$ topology is that because the weak $*$ open sets are bigger, there are more compact sets. The standard version of the Banach-Alaoglu theorem states that for any norm space $X$, the unit ball in $X^*$ is weak $*$ compact. The next theorem is a generalization of the Banach-Alaoglu theorem, in light of the uniform boundedness principle.

\begin{theorem}
    If a set $\mathcal{S}$ is an equicontinuous family of linear maps between two topological vector spaces $X$ and $Y$, then $\mathcal{S}$ is precompact, viewed as a subset of $L_{\sigma^*}(X,Y_\sigma)$. In particular, the weak $*$ closure of any equicontinuous family of linear functionals on a topological vector space $X$ is compact in the weak $*$ topology.
\end{theorem}
\begin{proof}
    A net $\{ T_\alpha \}$ converges to some $T$ in $L_{\sigma^*}(X,Y_\sigma)$ if and only if for any $x \in X$, $\{ T_\alpha x \}$ converges weakly to $Tx$, which holds if and only if $y^*(T_\alpha x)$ converges to $y^*(Tx)$ for all $y^* \in Y^*$. Thus the topology of $L_{\sigma^*}(X,Y_\sigma)$ can be obtained by considering it as a subset of $\CC^{X \times Y^*}$, and we can consider an inclusion map $i: L_{\sigma^*}(X,Y_\sigma) \to \CC^{X \times Y^*}$.

    The closure of $L_{\sigma^*}(X,Y_\sigma)$ consists of (not necessarily continuous) linear maps $T: X \to Y$. To see this, if $\{ T_\alpha \}$ are a family of functions in $L_{\sigma^*}(X,Y_\sigma)$ converging to some function $f: X \times Y^* \to \CC$, then it is immediate that $f$ is bilinear, and this means there exists a unique (not necessarily continuous) linear map $T: X \to Y$ such that $f(x,y^*) = y^*(Tx)$.

    Now by equicontinuity, for each $x \in X$ and $y \in Y^*$, $i(\mathcal{S})(x,y^*)$ is a bounded subset of $\CC$, and thus precompact. It follows by Tychonoff's theorem that $i(\mathcal{S})$ is precompact subset of $\CC^{X \times Y^*}$. If we could show that the closure of $i(\mathcal{S})$ contains only continuous linear maps, our proof would be complete. But this is true, and equivalent to showing the standard result that for any equicontinuous family of linear maps $\{ T_\alpha \}$ which converge pointwise to a linear map $T$, the map $T$ is continuous, which follows because if $T_\alpha(W) \subset V$ for all $\alpha$, then $T(W) \subset \overline{V}$.
\end{proof}

Let us now focus on equicontinuity of linear functionals. A set $\mathcal{S}$ of linear functionals on $X$ will be equicontinuous if and only if for any $\varepsilon > 0$, there is a neighborhood $W$ of the origin in $X$ such that for each $x \in W$ and $x^* \in \mathcal{S}$, $|x^*(x)| < \varepsilon$.

\begin{theorem}
    A set $\mathcal{S}$ in $X^*$ is equicontinuous if and only if it is a subset of $W^\circ$, where $W$ is a neighborhood of zero in $X$.
\end{theorem}
\begin{proof}
    If $W$ is a neighborhood of zero, then $W^\circ$ is equicontinuous, since if $x^* \in W^\circ$, then for any $\varepsilon > 0$, if $x \in (\varepsilon/2) W$, $|x^*(x)| \leq \varepsilon/2 < \varepsilon$. Conversely, if $\mathcal{S}$ is equicontinuous, then there is a neighborhood $W$ of the origin such that for each $x \in W$, and $x^* \in \mathcal{S}$, $|x^*(x)| < 1/2$. But this means that $\mathcal{S} \subset W^\circ$.
\end{proof}

Refer to the section of these notes on the generalization of the three fundamental theorems, in which it is shown that if $X$ is a barelled space, and $Y$ is a locally convex space, then a subset of $L(X,Y)$ is equicontinuous if and only if it is bounded in $L_{\sigma^*}(X,Y)$, which holds if and only if it is bounded in $L_b(X,Y)$. This condition is essentially equivalent to being barelled, as the next lemma shows.

\begin{lemma}
    If $X$ is locally convex, and every bounded subset of $X^*_{\sigma^*}$ is equicontinuous, then $X$ is barelled.
\end{lemma}
\begin{proof}
    If $B \subset X$ is a barrel, then $B^\circ$ is bounded in $X^*_{\sigma^*}$ since it is absorbing, hence equicontinuous, and thus there is an open neighborhood $U$ of the origin such that $B^\circ \subset U^\circ$. But this means that $U \subset B$, for if $x_0 \in B - U$, then $B$ and $\{ x_0 \}$ are both closed, convex subsets of $X$, and $\{ x_0 \}$ is compact, which means by Hahn-Banach, there is $x^* \in X^*$ such that
    %
    \[ \text{Re} \{ x^*(x_0) \} < \inf_{x \in B} \text{Re} \{ x^*(x) \} = - \sup_{x \in B} |x^*(x)|. \]
    %
    Write the left hand side as $-A_1$ and the right hand side as $-A_2$ for $0 < A_2 < A_1$. Then the function $f(x) = x^*(x) / A_2$ lies in $B^\circ$, but not $U^\circ$, which gives a contradiction. Thus $U \subset B$.
\end{proof}

If $B \subset X$ is absorbing, then $B^\circ$ is bounded in $X^*_{\sigma^*}$, and thus equicontinuous. In particular, if $B \subset X$ is a barrel, then $B^\circ$ is equicontinuous, and so there is a family of open sets $\{ U_\varepsilon : \varepsilon > 0 \}$ such that if $f \in B^\circ$, then $f(U_\varepsilon) \subset$

\section{Generalizing the Fundamental Theorems}

To some extent of generality, each of the three fundamental theorems of Banach space theory generalize to the study of topological vector spaces, though the assumptions of each of these results differs from one another. We begin with the open mapping theorem.

\begin{theorem}
    Let $X$ and $Y$ be $F$-spaces. Then every surjective continuous linear map $T: X \to Y$ is an open map. In particular, every bijective linear map $T: X \to Y$ is an isomorphism.
\end{theorem}
\begin{proof}
    Very similar to Banach space proof, i.e. using Baire category techniques.
\end{proof}

\begin{corollary}
    Any two distinct $F$ space topologies on a vector space $X$ are incomparable.
\end{corollary}
\begin{proof}
    If $X$ is a vector space equipped with two topologies making it into two distinct $F$ spaces $X_1$ and $X_2$, then $X_2$ has a coarser topological structure than $X_1$ if and only if the identity map $i: X_1 \to X_2$ is continuous. But then by the open mapping theorem, $i$ is an isomorphism, which means $X_1$ and $X_2$ have precisely the same topology.
\end{proof}

The closed graph theorem is a corollary of the open mapping theorem, and thus follows from the same assumptions.

\begin{theorem}
    Let $X$ and $Y$ be $F$ spaces. Then a linear map $T: X \to Y$ is continuous if and only if it's graph is closed.
\end{theorem}

\begin{remark}
    Martineau proved another version of this result, stating that the closed graph theorem holds for any linear map $T:X \to Y$, where $X$ is the inductive limit of \emph{Banach spaces}. Schwartz proved that if we assume that $Y$ is a Souslin space (a space homeomorphic to a Polish space), then any linear map $T: X \to Y$ whose graph is Borel measurable is continuous (TODO: Treves Appendix).
\end{remark}

Recall the uniform boundedness theorem. It states that if $X$ is a Banach space, and $\{ T_\alpha: X \to Y \}$ is a family of bounded linear operators such that $\{ T_\alpha(x) \}$ is bounded for each $x \in X$, then $\sup_\alpha \| T_\alpha \|$ is continuous. Since one can only define the operator norm if $X$ or $Y$ are norm spaces, the consequence of a generalization of this result must look slightly different. This is obtained by replacing the operator norm condition with \emph{equicontinuity}, since the operator norm condition is precisely equivalent to the operator norms being uniformly bounded.

Let $X$ be a topological vector space. A set $B$ is a \emph{barrel} if it is absorbing, balanced, closed, and convex. If $U$ is any balanced neighborhood of the origin, then the closed convex hull $\overline{\text{co}}(U)$ is a barrel. Thus every neighborhood of the origin is contained in a barrel. But it is not true that we always have a neighborhood basis of barrels, it is precisely the locally convex spaces that have this property.

The domain of the map under which we can apply the uniform boundedness condition is allowed to be any barelled space. We say a space $X$ is \emph{barelled} if every barrel is a neighborhood of the origin. A barelled space need not be locally convex. A quotient of a barelled space is baralled, and the product of barelled spaces is barelled, but it is \emph{not} true that a closed subspace of a barelled space is barelled. The next result shows that all Fr\'{e}chet and LF spaces are barelled.

\begin{theorem}
    Any Baire topological vector space is barelled.
\end{theorem}
\begin{proof}
    Let $B$ be a barrel in a Baire topological vector space $X$. Then $X = \bigcup_{k = 1}^\infty kB$. By the Baire property of the space, $B$ thus cannot have empty interior. The interior of $B$ is a convex, balanced set, and thus the interior of $B$ contains the origin.
\end{proof}

A natural generalization of the uniform boundedness theorem then follows from the following result.

\begin{theorem}
    Let $X$ be a barelled space, and $Y$ a locally convex space. Then a set $\mathcal{S}$ of continuous linear maps is equicontinuous if and only if it is bounded in $L_{\sigma^*}(X,Y)$, if and only if it is bounded in $L_b(X,Y)$.
\end{theorem}
\begin{proof}
    The only nontrivial part of this result is that if $\mathcal{S}$ is bounded in $L_{\sigma^*}(X,Y)$, then it is equicontinuous. Assuming boundedness, if $W$ is an arbitrary neighborhood of zero in $Y$, then we may assume without loss of generality, since $Y$ is locally convex, that $W$ is a closed barrel. The set
    %
    \[ \bigcap_{T \in \mathcal{S}} T^{-1}(W) \]
    %
    is then convex, balanced, and closed, and thus a closed neighborhood of the origin in $X$ provided we can show it is absorbing. But this follows immediately from the fact that $\mathcal{S}$ is bounded in $L_{\sigma^*}(X,Y)$.
\end{proof}

Here is an important special case of the uniform boundedness theorem.

\begin{theorem}
    Let $X$ be a barelled topological vector space. Then $A \subset X^*$ is bounded in the weak $*$ topology if and only if it is bounded in the strong topology, which happens if and only if $A$ is an equicontinuous family of linear functionals, i.e. precompact in the weak $*$ topology.
\end{theorem}

This is often a useful result to verify a space is not barelled.

\begin{example}
    It follows that the space $C_c(\RR)$ of compactly supported continuous functions is \emph{not} barelled when equipped with the supremum norm. The sequence of linear functionals $\lambda_k(f) = k f(k)$ converges to zero in the weak $*$ topology. Thus $\{ \lambda_k \}$ is bounded in the weak $*$ topology. But it is certainly not bounded in the strong topology, since $\| \lambda_k \| = k$ is not bounded independantly of $k$. Thus $C_c(\RR)$ cannot be barelled.
\end{example}

\section{Montel Spaces}

A space $X$ is \emph{Montel} if it is locally convex, barelled, and satisfies the Heine-Borel property. The spaces $C^\infty(\Omega)$, $C_c^\infty(\Omega)$, $\mathcal{S}(\RR^d)$ are all Montel spaces, as can be verified using the Azela-Ascoli theorem.

\begin{theorem}
    If $X$ is Montel, and $Y$ is any other topological vector space, then the topology of compact convergence on $L(X,Y)$ agrees with the strong topology, i.e. of bounded convergence.
\end{theorem}

If $X$ is Montel, then $X^*_b$ satisfies the Heine-Borel property. Indeed, if $B \subset X^*_b$ is closed and bounded, then $B$ is equicontinuous, which means that the weak $*$ topology and the topology of compact convergence agree on $B$. But the compact convergence topology on $B$ is precisely the strong topology since $X$ is Montel, so that the weak $*$ topology is the same as the strong topology on $B$. Thus $B$ is compact in the strong topology.

It follows that every weak $*$ convergent sequence in $X^*$ is strongly convergent. Thus weak convergence of sequences in the distribution spaces $\mathcal{D}(\Omega)^*$, $\mathcal{E}(\Omega)^*$ and $\mathcal{S}(\Omega)^*$ is equivalent to strong convergence of sequences, which differs drastically from the study of weak convergence in norm spaces. Of course, the topologies differ from one another, so convergence of \emph{nets} can differ from one another.

Now every Montel space is barelled, and has the Heine-Borel property with respect to the weak topology. Thus any Montel space is reflexive. Moreover, if $X$ is a Montel space, then the reflexivity of $X$ implies that $X^*_b$ is barelled, and also satisfies the Heine-Borel space. Thus $X^*_b$ is also a Montel space.



\section{The Weak Topology}

The study of general locally convex spaces is still integral to more advanced studies of Banach and Hilbert space theory, because of the notions of \emph{weak convergence}, which equips any Banach space $X$ with another locally convex topology. Give a topological vector space $X$, and a net $\{ x_\alpha \}$, we say that net converges to $x \in X$ \emph{weakly} if, for any $x^* \in X^*$, the net $\{ x^*(x_\alpha) \}$ converges to $x^*(x)$. This gives $X$ a locally convex topology, called the \emph{weak topology}, or $\sigma(X,X^*)$ topology, and the space equipped with this topology will be denoted $X_\sigma$. It is clear that if $\{ x_\alpha \}$ converges in $X$ to some $x \in X$, it will converge weakly to $x$. That is, $X_\sigma$ has fewer open sets than $X$. But if $X$ is locally convex, the converse only holds when $X$ is finite dimensional. This gives $X$ an additional topology, which is useful to analyze the structure of $X$ in relation to it's dual.

One can view this topology through the lens of the dual topologies we have previously introduced. If $X$ is a topological vector space, then $X$ is naturally isomorphic as a vector space to the family of all continuous linear functionals on $(X^*_{\sigma^*})$. It follows that we can view $X$ as a dual of a locally convex vector space, and thus equip it with the associated topologies of pointwise / uniform convergence. In particular, the weak topology on $X$ is precisely the weak $*$ topology, viewing $X$ as the dual of $X^*_{\sigma^*}$.

\begin{theorem}[Mackey's Theorem]
    Let $X$ be a locally convex space. Then $A \subset X$ is bounded if and only if it is weakly bounded.
\end{theorem}

More generally, Mackey's theorem proves that if $X$ is equipped with an alternate locally convex topology $\mathcal{T}$ such that the dual of $X_{\mathcal{T}}$ is equal to the dual of $X$, then $A \subset X$ is bounded if and only if it is bounded in the topology $\mathcal{T}$. In particular, since $X^*_{\sigma^*}$ and $X^*_\gamma$ have the same duals, it follows that they also have the same bounded sets.

It follows that any weakly compact subset of a locally convex space is bounded. Moreover, if $X$ is a Bornological locally convex space, and $Y$ is locally convex then $T: X \to Y$ is continuous if and only if $T$ is weakly continuous.

\begin{corollary}
    Let $X$ be a Bornological locally convex space, and let $Y$ be locally convex. If $T: X \to Y$ is a linear map then $T$ is continuous if and only if $T^*$ maps $Y^*$ into $X^*$, i.e. $y^* \circ T \in X^*$ for each $y^* \in Y^*$.
\end{corollary}
\begin{proof}
    If $T$ is bounded, then it is clear that $T^*$ maps $Y^*$ into $X^*$. Conversely, if $y^* \circ T \in X^*$ for each $y^*$ in $Y^*$, then it follows that $T$ is continuous from the weak topology on $X$ to the weak topology on $Y$, and thus $T$ is continuous in the standard topology.
\end{proof}

We have seen that a linear operator $T: X \to Y$ between norm spaces is bounded if and only if $T^* y^* \in X^*$ for each $y^* \in Y^*$. But this condition is equivalent to $T$ being bounded from the weak topology on $X$ to the weak topology on $Y$. Thus a linear operator is norm bounded if and only if it is bounded between the weak topologies. In particular two norm spaces are isomorphic if and only if they are weakly isomorphic.

If $X$ is a norm space, then any weakly continuous linear functional on $X$ is continuous in the usual topology on $X$. Thus $X$ and $X_\sigma$ have the same dual space, which, by the Hahn Banach theorem, means that if $X$ is locally convex, then a convex set in $X$ is closed if and only if it weakly closed. 

\begin{theorem}
    If $\{ x_\alpha : \alpha \in I \}$ is a net in a locally convex space converging weakly to some $x \in X$, then some net in the convex combinations of the elements $\{ x_\alpha \}$ converges to $x$ in the normal topology of $X$.
\end{theorem}
\begin{proof}
    The closed convex hull of $\{ x_\alpha : \alpha \in I \}$ in $X$ is the same as the weakly closed convex hull in $X$, and the weakly closed convex hull of this set contains $x$.
\end{proof}

On the other hand, there are certainly open convex sets which are not weakly open if $X$ is infinite dimensional, since any weakly open convex set is a finite intersection of half planes, and is thus unbounded.

The weak topology on an infinite dimensional norm space is \emph{not metrizable}. Indeed, if the weak topology on $X$ \emph{was} metrizable, and if $B$ is the open unit ball with respect to this metric, then because all weakly open subsets of $X$ are unbounded, for each $n$, we can find $x_n \in n^{-1} \cdot B$ such that $\| x_n \| \geq n$. But then $\{ x_n \}$ converges to zero weakly, hence $\{ x_n \}$ is weakly bounded, hence $\{ x_n \}$ is bounded, which is impossible.

The weak topology on an infinite dimensional norm space is also \emph{not complete}. Let $X$ be an infinite dimensional norm space, and pick some unbounded linear function $f$ on $X^*$. For any finite set $S = \{ x_1^*, \dots, x_n^* \} \subset X^*$, the function $f$ is bounded on $\langle S \rangle$, so for any scalars $\alpha_1,\dots,\alpha_n$,
%
\[ |\alpha_1 f(x_1^*) + \dots + \alpha_n f(x_n^*)| \lesssim_S \| \alpha_1 x_1^* + \dots + \alpha_n x_n^* \|. \]
%
It follows from Helly's theorem that there is $x_S \in X$ such that $x_i^*(x_S) = f(x_i^*)$ for each $i \in \{ 1, \dots, n \}$. Now the net $\{ x_S \}$, ordered by inclusion of finite sets, is weakly Cauchy. If $X$ was complete under the weak topology, then the net $\{ x_S \}$ would converge weakly to some $x \in X$. But then $x^{**}$ would equal $f$, which would contradict the fact that $f$ was bounded.

The norm on an infinite dimensional norm space is also \emph{not} a weakly continuous function. This means that if a net $\{ x_\alpha \}$ in $X$ converges weakly to some point $x \in X$, it is \emph{not} true that $\| x_\alpha \|$ converges to $x$. However, we do have
%
\[ \| x \| \leq \liminf_{\alpha \to \infty} \| x_\alpha \|. \]
%
This follows because we can find $x^* \in X^*$ with $\| x^* \| = 1$ such that $x^*(x) = \| x \|$, and then
%
\[ \| x \| = x^*(x) = \lim_{\alpha \to \infty} x^*(x_\alpha) \leq \| x_\alpha \|. \]
%
Thus the norm is weakly lower semicontinuous.

The relation between the dual spaces of subspaces of a norm spaces and quotients of the dual shows that the weak topology on any subspace $X_0$ of a space $X$ is the same as the relative weak topology induced from taking the weak topology on $X$. We note that $X_0$ is weakly closed in $X$ if and only if it is norm closed in $X$, since any subspace is a convex set.

The weak topology is not complete. But it might still be \emph{sequentially complete}, i.e. any weakly Cauchy \emph{sequence} may still converge weakly. In fact, sequences can often behave quite differently to nets.

\begin{example}
    Let $\{ a_n \}$ be a weakly Cauchy sequence in $l^1$. Since point evaluation is a continuous linear functional on $l^1$, it follows that the sequences $\{ a_n \}$ converge pointwise to some sequence $a$. Since $\{ a_n \}$ is weakly bounded in $l^1$, it is also bounded in $l^1$, so there exists $M > 0$ such that for all $n$,
    %
    \[ \sum_{m = 1}^\infty |a_n(m)| \leq M. \]
    %
    But then we conclude that
    %
    \[ \sum_{m = 1}^\infty |a(m)| \leq M, \]
    %
    i.e. $a \in l^1$. Moreover, one can verify that this uniform boundedness actually implies that the sequence $\{ a_n \}$ converges to $a$ in norm as well. Thus, \emph{for sequences}, norm convergence and weak convergence are equivalent in $l^1$.
\end{example}

A space is said to have \emph{Schur's property} if weakly convergent sequences are norm convergent. Thus $l^1$ has Schur's property.

\begin{example}
    It is \emph{not} true that $l^2$ has Schur's property, since the sequence $\{ e_n \}$ converges weakly to zero, yet does not converge in norm to zero. In general, if $H$ is a Hilbert space, and $\{ x_n \}$ in $H$ converges weakly to $x$ in $H$, then we can guarantee that $\{ x_n \}$ converges in norm to $x$ in $H$, under the additional assumption that $\| x_n \|_H \to \| x \|_H$, because we have
    %
    \[ \| x_n - x \|_H^2 = \| x_n \|^2_H + \| x \|^2_H - 2 \text{Re}( (x_n \cdot x) ). \]
    %
    Since $x_n \cdot x$ converges to $\| x \|^2$ as $n \to \infty$, we find that $\| x_n - x \|_H \to 0$.
\end{example}

We thus see that any Hilbert space $H$ has the \emph{Radon-Riesz property}, if $\{ x_n \}$ converges weakly to $x$, and $\| x_n \| \to \| x \|$, then $\{ x_n \}$ converges to $x$.

Despite the fact that the weak topology on a metric space is not metrizable, compactness of the weak topology is still equivalent to sequential compactness, though we will not prove this. This is the Eberlein-Smulian theorem.

\begin{theorem}
    Let $X$ be a norm space, and $A \subset X$ a subset. Then $A$ is weakly compact if and only if $A$ is weakly sequentially compact.
\end{theorem}

One consequence is that if $x \in X$ is in the weak closure of a precompact set $A \subset X$, then there is a sequence in $A$ which converges to $x$, and that a set $A \subset X$ is weakly compact if and only if $A \cap X_0$ is weakly compact for any closed separable subspace $X_0$ of $X$.

\begin{corollary}
    A norm space $X$ is reflexive if and only if every bounded sequence in $X$ has a weakly convergent subsequence.
\end{corollary}
\begin{proof}
    $X$ is reflexive if and only if bounded subsets of $X$ are weakly compact, and so the theorem follows from the fact that bounded subsets are weakly compact if and only if they are weakly sequentially compact.
\end{proof}

\begin{corollary}
    A norm space $X$ is reflexive if and only if $X_0$ is reflexive for each closed separable subspace $X_0$ of $X$.
\end{corollary}

\begin{corollary}
    Every reflexive norm space is weakly sequentially complete.
\end{corollary}

\begin{corollary}
    No infinite dimensional norm space satisfies Schur's property.
\end{corollary}
\begin{proof}
    If $X$ is an infinite dimensional norm space satisfying Schur's property, then all bounded subsequences of $X$ have convergent subsequences, so $X$ satisfies the Heine-Borel property, but this is impossible for a norm space.
\end{proof}


\begin{comment}
\begin{example}
    The weak topology on $X$ is rarely equal to the original topology on $X$, except in degenerate circumstances. For instance, if we consider $e_n \in l_p$, for $1 < p < \infty$, then $\| e_n - e_m \| = 1$, so $e_n$ does not converge, yet if we consider any element $f \in l_q$ of the dual space, then $\langle f, e_n \rangle = f(n)$, which converges to zero as $n \to \infty$. Thus $e_n$ converges to zero in the weak topology. In $l_1$, $e_n$ does not converge at all, because the function $f(n) = (-1)^n$ is in $l_\infty$, and $\langle f, e_n \rangle = (-1)^n$ does not converge. It is important to note that the weak $*$ topology depends on the predual we are using. If we consider $l_1 = c_0^*$, then the $e_n$ converges to zero in the weak $*$ topology, yet if we consider $l_1$ as $c^*$, then the $e_n$ do not converge in the induced weak $*$ topology.

    The weak topology on a space is rarely first countable. If $A = \{ \sqrt{n} e_n : n \in \mathbf{N} \}$ is viewed as a subset of $l_2$, then 0 is contained in the weak closure of $A$, because if $\varepsilon > 0$ and we are given a finite set of sequences $\{ a_n^k \}_{n \in \mathbf{N}}$, for $1 \leq k \leq K$ and, for any $n$, there is $k_n$ such that $|a_n^{k_n}| \sqrt{n} > \varepsilon$, and if we consider the sum $\sum_k |a_n^k| \in l^2$, then
    %
    \[ \sum_n \left|\sum_k |a_n^k| \right|^2 \geq \sum_n |a_n^{k_n}|^2 \geq \varepsilon \sum \frac{1}{n} = \infty \]
    %
    so there must be an element of $A$ in each $U_{\varepsilon, \{ a_n^1 \}, \dots, \{ a_n^K \}}$, and hence $0$ is in the weak closure of $A$. However, 0 is not the weak limit of any sequence of elements in $A$, for
    %
    \[ \sum_{n = 0}^\infty \frac{e_n}{n} \in l_2 \]
    %
    and if we consider any sequence $\sqrt{n_i} e_{n_i}$ that converges weakly to zero, then we find that $(n_i)^{-1/2}$ converges to zero, hence $n_i \to \infty$. But then the sequence is not norm bounded, and hence cannot converge!
\end{example}
\end{comment}

\section{Continuous Maps With Closed Image}

Let $X$ and $Y$ be locally convex spaces. It follows immediately from the Hahn-Banach theorem that if $T: X \to Y$ is continuous, and $T^*: Y^* \to X^*$ is injective, then the image of $T$ is dense in $Y$. We now determine a condition on $T^*$ that will guarantee that the image of $T$ is closed. It turns out one such condition is that $T^*$ has closed image in the weak $*$ topology of $X^*$. It thus becomes of interest to determine a characterization of the weak $*$ closed subsets of $X^*$.

\begin{theorem}
    Let $X$ be a Fr\'{e}chet space. Then the following are equivalent:
    %
    \begin{itemize}
        \item A subspace $X_0^*$ of $X_0$ is closed in the weak $*$ topology.
        \item For any neighborhood $U$ of the origin in $X$, $X_0^* \cap U^\circ$ is weak $*$ closed.
        \item For any equicontinuous set $H \subset X^*$, $X_0^* \cap H$ is weak $*$ closed.
    \end{itemize}
\end{theorem}
\begin{proof}
    The second point is equivalent to the third point because $U^\circ$ is equicontinuous for any neighborhood $U$ of the origin in $X$. Conversely, any equicontinuous set $H$ is contained in $U^\circ$ for some neighborhood $U$ of the origin. Since $U^\circ$ is weak $*$ compact, and thus weak $*$ closed, if $X_0^*$ is closed in the weak $*$ topology, then $X_0^* \cap U^\circ$ is weak $*$ closed for any neighborhood $U$ of the origin. Conversely, suppose that $X_0^* \cap H$ is weak $*$ closed for any equicontinuous set $H \subset X^*$. Since the weak $*$ topologies and uniformly compact topologies agree on an equicontinuous set, it suffices to show that $H^c$ is open in $X^*_c$. TODO: This is Lemma 37.3 of Treves.
\end{proof}

Given a locally convex space $X$ equipped with a continuous seminorm $\rho$, we let $X^*_\rho$ be all linear functionals continuous with the topology defined by $\rho$. Then $X^*_\rho$ is canonically a Banach space, equipped with the norm
%
\[ \| x^* \|_{\rho^*} = \sup_{\rho(x) \leq 1} |x^*(x)|. \]
%
This notation will be useful in the sequel.

\begin{theorem}
    Let $X$ and $Y$ be Fr\'{e}chet spaces, and let $T: X \to Y$ be continuous. Then the following are equivalent:
    %
    \begin{itemize}
        \item $T$ is onto.
        \item $T^*$ is injective, and it's image is weak $*$ closed in $X^*$.
        \item For any continuous seminorm $\rho$ on $X$, there is a continuous seminorm $\psi$ on $Y$ such that for any $(T^*)^{-1}(X^*_\rho) \subset Y^*_\psi$, and for any $y \in Y$, there is $x \in X$ with $\psi(Tx - y) = 0$.
        \item For any continuous seminorm $\rho$ on $X$, there is a linear subspace $Y_0$ of $Y$ such that $(T^*)^{-1}(X^*_\rho) \subset Y_0$, and for each $y \in Y$, there is $x \in X$ with $Tx - y \in Y_0$.
        \item There is a nonincreasing family of subspaces $\{ Y_k \}$ of $Y$ with intersection $\{ 0 \}$, such that for any $y \in Y$ and any $k$, there is $x \in X$ with $Tx - y \in Y_k$, and for any continuous seminorm $\rho$ on $X$, there is $k_0$ such that if $Tx \in Y_{k_0}$, then there is a sequence $\{ x_n \}$ in $\text{Ker}(T)$ such that $\rho(x - x_n) \to 0$.
    \end{itemize}
\end{theorem}
\begin{proof}
    TODO
\end{proof}

Let us consider some applications of this result.

\begin{theorem}
    Any formal power series in $n$ variables is the Taylor series at the origin for some function $f \in C^\infty_{\text{loc}}(\RR^n)$.
\end{theorem}
\begin{proof}
    Let $T: C^\infty_{\text{loc}}(\RR^n) \to \CC[[z_1,\dots,z_n]]$ be the linear map that assigns each smooth function with it's Taylor expansion at the origin, i.e.
    %
    \[ Tf(z) = \sum_\alpha \frac{1}{\alpha!} D^\alpha f(0) z^\alpha. \]
    %
    We give $\CC[[z_1,\dots,z_n]]$ the locally convex topology given by pointwise convergence of coefficients. Then $T$ is continuous. The dual of $C^\infty_{\text{loc}}(\RR^n)$ is the space $\mathcal{E}(\RR^n)^*$ of compactly supported distributions. The dual of $\CC[[z_1,\dots,z_n]]$ can be naturally identified with the space $\CC[z_1,\dots,z_n]$ of polynomials in $n$-variables, where if $f = \sum a_\alpha$ is a formal power series, and $g = \sum b_\alpha z^\alpha$ is such a polynomial, then
    %
    \[ \langle f, g \rangle = \sum a_\alpha b_\alpha. \]
    %
    Then we calculate $T^*: \CC[z_1,\dots,z_n] \to \mathcal{E}(\RR^n)^*$ to be
    %
    \[ \langle f, T^* g \rangle = \langle Tf, g \rangle = \sum_\alpha b_\alpha \frac{1}{\alpha!} D^\alpha f(0). \]
    %
    Thus $T^*g = g(-\partial / \partial x) \delta_0$. Thus $T^*$ is injective, and it's image consists precisely of all distributions supported at the origin. This space is weak $*$ closed in $\mathcal{E}(\RR^n)^*$, and so we conclude that $T$ is a surjective map.
\end{proof}

Here is another application, to differential equations. Given a linear differential operator $L$ on $\RR^n$ of the form
%
\[ Lf(x) = \sum c_\alpha(x) D^\alpha_x f(x), \]
%
with coefficients $\{ c_\alpha \}$ in $C^\infty_{\text{loc}}(\RR^n)$, then $L$ is a continuous operator from $C_c^\infty(\RR^n)$ to itself, and so we can consider the adjoint with respect to this operator, which we will denote by $L^t: \mathcal{D}(\RR^n) \to \mathcal{D}(\RR^n)$. Then we can write
%
\[ L^t u = \sum (-1)^{|\alpha|} D_x \{ c_\alpha u \}. \]
%
We will determine necessary and sufficient conditions for the equation $Lf = g$ to be solvable on a domain $\Omega$, for $f,g \in C^\infty_{\text{loc}}(\RR^n)$.

We say an open set $\Omega \subset \RR^n$ is \emph{$L$-convex} if for every compact set $K \subset \Omega$, and for any integer $m \geq 0$, there is a compact set $\tilde{K}_m$ such that if $\mu$ is a distribution of order $\leq m$, and $D^t \mu$ is supported on $K$, then $\mu$ is supported on $\tilde{K}_m$. We say an operator $L$ is \emph{semiglobally solvable} on $\Omega$ if for any precompact open subset $\Omega_0$ of $\Omega$, such that for any $g \in C^\infty_{\text{loc}}(\Omega)$, there is $f \in C^\infty_{\text{loc}}(\Omega)$ such that $Lf = g$ on $\Omega_0$.

\begin{theorem}
    Let $L$ be a linear differential operator with coefficients in $\loc{C^\infty}(\Omega)$. Then $Lf = g$ is solvable for $f \in \loc{C^\infty}$ given any $g \in \loc{C^\infty}$ if and only if $\Omega$ is $L$-convex and $L$ is semiglobally sovable in $\Omega$.
\end{theorem}
\begin{proof}
    View $L$ as a continuous map from $\loc{C^\infty}(\Omega)$ to itself. Then use the conditions in the theorem above to show $L$ is surjective. TODO
\end{proof}






















\chapter{Tensor Products}

In this chapter, we develop the analytic theory of the tensor product. Recall that in the algebraic theory, given two vector spaces $V$ and $W$, we can consider their algebraic tensor product $V \otimes W$, which is the universal vector space such that any bilinear map $V \times W \to U$ factors through the inclusion map $V \times W \to V \otimes W$. Elements of $V \otimes W$ can be written as finite sums of the form
%
\[ \sum_{i=1}^N v_i \otimes w_i \]
%
for $v_1,\dots,v_N \in V$ and $w_1,\dots,w_N \in W$. If $V$ and $W$ are finite dimensional, so is $V \otimes W$, and $\dim(V \otimes W) = \dim(V) \dim(W)$.

There are several properties we wish to have of an analytical completion of tensor products. Let us list them here:
%
\begin{itemize}
    \item Given two sets $X$ and $Y$, and two functions $f: X \to \CC$ and $g: Y \to \CC$, we can define the tensor product $f \otimes g: X \times Y \to \CC$ by setting $(f \otimes g)(x,y) = f(x) g(y)$. This gives a bilinear map $\CC^X \otimes \CC^Y \to \CC^{X \times Y}$, which is an isomorphism if $X$ and $Y$ are finite. We wish to come up with an analytic replacement of the algebraic tensor product which generalizes this property to infinite dimensional settings, so that, for instance, the tensor product of $L^p(X)$ and $L^p(Y)$ is isomorphic to $L^p(X \times Y)$, and the tensor product of $C_c^\infty(\Omega_1)$ with $C_c^\infty(\Omega_2)$ is isomorphic to $C_c^\infty(\Omega_1 \times \Omega_2)$. This method is often used as a strategy for understanding a function $f(x,y)$ via the method of \emph{separation of variables}, i.e. writing $f$ as an infinite sum
    %
    \[ f(x,y) = \sum_{n = 1}^\infty g_n(x) h_n(y) \]
    %
    for suitable functions $\{ g_n \}$ and $\{ h_n \}$.

    \item Let $U$ be an open subset of $k^d$, then any $n$-times differentiable function $f: U \to k^d$ can be written as $f_1 \otimes e_1 + \dots + f_n \otimes e_n$, for $f_1,\dots,f_n \in C^n(U)$, and where for $v \in k^d$ and $f \in C^n(U)$,
    %
    \[ (f \otimes v)(x) = f(x) v. \]
    %
    Thus $C^n(U,k^d)$ can be identified with the tensor product $C^n(U) \otimes k^d$. We wish to come up with a more general notion of tensor product so that $C^n(U,Y)$ is isomorphic to the tensor product of $C^n(U)$ and $Y$, for more general topological vector spaces $Y$.
\end{itemize}
%
More abstractly, we might want the following properties to extend to infinite dimensional completions of the tensor product:
%
\begin{itemize}
    \item If $V$ and $W$ are finite dimensional, then $V \otimes W$ is isomorphic to $B(V^*,W)$ via a map $i: V \otimes W \to B(V^*,W)$ such that for $v \in V$, $w \in W$, and $v^* \in V^*$,
    %
    \[ i(v \otimes w)(v^*) = v^*(v) w. \]

    \item If $V$ and $W$ are finite dimensional, then $V \otimes W$ can be naturally identified with the space $\text{Bil}(V^*,W^*)$ of all scalar-valued bilinear maps on $V^* \times W^*$, where for $v \in V$ and $w \in W$, we have a bilinear map $B_{v,w}$ given by
    %
    \[ B_{v,w}(v^*,w^*) = v^*(v) w^*(w). \]
    %
    Similarily, $(V \otimes W)^*$ can be naturally identified with the vector space $\text{Bil}(V,W)$.
\end{itemize}
%
We will come up with several definitions of the tensor product, which, when applied to the correct situation, give versions of these properties in the infinite dimensional setting, interpreted appropriately.

\section{Bilinear Maps}

Let $X$, $Y$, and $Z$ be topological vector space. We say a bilinear map $B: X \times Y \to Z$ is \emph{separately continuous} if for each fixed $x_0 \in X$ and $y_0 \in Y$, the two linear functionals $y \mapsto B(x_0,y)$ and $x \mapsto B(x,y_0)$ are continuous maps from from $Y$ to $Z$ and from $X$ to $Z$ respectively. The map is \emph{continuous} if it is continuous in the product topology, which if $X$ and $Y$ are locally convex, is equivalent to the existence of two continuous seminorms $\rho$ and $\sigma$ on each space such that for all $x \in X$ and $y \in Y$,
%
\[ |B(x,y)| \lesssim \rho(x) \sigma(y). \]
%
For first countable topological vector spaces, boundedness is equivalent to continuity.

\begin{theorem}
    For any topological vector spaces $X$, $Y$, and $Z$, a bounded bilinear map $B: X \times Y \to Z$ is sequentially continuous.
\end{theorem}
\begin{proof}
    Suppose $B$ is bounded. Then it follows that $B$ is separately continuous in $X$ and $Y$ (by our theory for linear maps), and so if $\{ x_n \}$ and $\{ y_n \}$ are two convergent sequences in $X$ and $Y$, converging to $x \in X$ and $y \in Y$, then it suffices to show that
    %
    \[ B(x_n,y_n) - B(x,y) = B(x_n - x, y_n) + B(x,y_n - y) \]
    %
    converges to zero. The decomposition above shows that without loss of generality that either $x = 0$ or $y = 0$. Suppose for notational convenience that $x = 0$, and we must therefore show that $B(x_n,y_n) \to 0$. We may find $\{ \lambda_n \}$ converging to infinity such that $\{ \lambda_n x_n \}$ converges to zero. This means that $\{ \lambda_n B(x_n,y_n) \}$ is a bounded sequence, and since $1/\lambda_n$ converges to zero, this means that $B(x_n,y_n)$ converges to zero.
\end{proof}

Almost all bilinear maps studied in analysis are separately continuous. We will begin by showing that if $X$, $Y$, and $Z$ are of a particular form, then all separately continuous bilinear maps are continuous (in particular, this is true if $X$, $Y$, and $Z$ are Banach spaces. But it is not always true that a separately continuous bilinear map is continuous.

\begin{theorem}
    Let $X$ be a Frech\'{e}t space, $Y$ a first countable topological vector space, and $Z$ a locally convex space. Then every separately continuous bilinear map $B: X \times Y \to Z$ is continuous.
\end{theorem}
\begin{proof}
    It suffices to show that $B$ is bounded. So fix two bounded subsets $A_1$ and $A_2$ of $X$ and $Y$. For each $y \in Y$, let $B_y: X \to Z$ be the continuous linear map induced by the bilinear form $B$, and consider the family of maps $\{ B_y: y \in A_2 \}$, which map a barelled space to a locally convex space. For each fixed $x \in X$, the map $B^x: Y \to Z$ induced by the bilinear form is continuous in the $x$-variable, so that $\{ B_y(x): y \in A \} = B^x(A)$ is bounded in $Z$. But the uniform boundedness principle thus implies that the family of maps $\{ B_y : y \in A \}$ is equicontinuous. Thus for any balanced open subset $W$ of $Z$, there exists a balanced open subset $V$ of $X$ such that $B_y(V) \subset W$ for all $y \in A$, i.e. $B(V,A_2) \subset W$. But there exists $\lambda > 0$ such that $A_1 \subset \lambda V$, which eans that $B(A_1,A_2) \subset \lambda W$. Thus $B(A_1,A_2)$ is bounded, which completes the proof.
\end{proof}

\begin{remark}
    
\end{remark}

Here is a similar result.

\begin{theorem}
    Let $Y$ and $Z$ be strong duals of reflexive Fr\'{e}chet spaces, and let $X$ be a norm space, or a strong dual of a reflexive Frech\'{e}t space. Then every separately continuous bilinear map $B: X \times Y \to Z$ is continuous.
\end{theorem}
\begin{proof}
    Suppose first that $X$ is a norm space. For each $x \in X$, consider the induced map $B': X \times Z^* \to Y^*$ obtained by taking the adjoint of the map $B$ viewed as a parameterized family of continuous linear maps from $Y$ to $Z$.

    We claim that $B'$, viewed as a map from $X \times Z^*_b \to Y^*_{\sigma^*}$, is continuous. Since $X$ is a norm space, and $Z^*_b$ is a Fr\'{e}chet spaces, it suffices to verify the map is separately continuous. For any sequence $\{ x_n \}$ in $X$ converging to some $x \in X$, then for any $z^* \in Z^*$, and $y \in Y$, we have
    %
    \[ \langle B'(x_n,z^*), y \rangle = \langle B(x_n,y), z^* \rangle, \]
    %
    and these values converge to $\langle B(x,y), z^* \rangle = \langle B'(x,z^*), y \rangle$ because $B$ is separately continuous. Thus $\{ B'(x_n, z^*) \}$ converges to $B'(x,z^*)$ in the weak $*$ topology. Next, if $\{ z_n^* \}$ is a sequence in $Z^*_b$ converging to some $z^*$, then for any $x \in X$ and $y \in Y$,
    %
    \[ \langle B'(x,z_n^*), y \rangle = \langle B(x,y), z_n^* \rangle \]
    %
    and this converges to $\langle B(x,y), z^* \rangle$ since $\{ z_n^* \}$ also converges to $z^*$ in $Z^*_{\sigma^*}$. This completes the proof of continuity.

    Now let $W$ be an arbitrary neighborhood of zero in $Z$. By definition of the topology on $Z$, there exists a bounded set $A_1 \subset Z^*_b$ such that $A_1^\circ \subset W$. Then if $D$ is the closed unit ball in $X$, then $B'(D,A_1)$ is a bounded subset of $Y^*_{\sigma^*}$. But by Mackey's theorem, $B'(D,A_1)$ is then also a bounded subset of $Y^*_b$. Denote this set by $A_2$. Then $A_2^\circ$ is a neighborhood of the origin in $Y$, and $B(D,A_2^\circ) \subset A_1^\circ \subset W$, which proves continuity.

    Next, assume that $X$ is the strong dual of a reflexive Fr\'{e}chet space. Consider a decreasing neighborhood basis of closed, convex, balanced subsets $\{ W_n \}$ for $X^*_b$. Let $X_n$ be the span of $W_n^\circ$, together with the Banach space given by declaring the closed, convex, balanced set $W_n^\circ$ to be the closed unit ball in $X_n$. Indeed, this is a norm topology, which is finer than the relative topology on $X_n$ since $W_n^\circ$ is a bounded set, $X_n$ is sequentially closed in $X$ since $W_n^\circ$ is closed, and since $X$ is complete, $X_n$ is sequentially complete, hence a Banach space. The maps $B_n: X_n \times Y \to Z$ remains separately continuous, and thus continuous by the last result we proved. It follows that if $V$ is a convex balanced, closed subset of $Z$, then for each $n$ there is a bounded subset $A_n$ of $Y^*$ such that $B(W_n^\circ, A_n^\circ) \subset V$. But then since $X^*$ is a Fr\'{e}chet space, hence is first countable, there exists a sequence $\{ \varepsilon_n \}$ such that $A = \bigcup \varepsilon_n A_n$ is a bounded subset of $Y^*$. Now $A^\circ \subset (\varepsilon_n A_n)^\circ$, and so
    %
    \[ B(\varepsilon_n W_n^\circ, A^\circ) \subset B(\varepsilon_n W_n^\circ, (\varepsilon_n A_n^\circ)) \subset V. \]
    %
    Since $V$ is closed, convex, and balanced, if $U$ is the closed, convex, balanced hull of $\bigcup \varepsilon_n W_n^\circ$, then $B(U,A^\circ) \subset V$. The proof would be complete if we could show that $U$ is a neighborhood of the origin. By reflexivity, $U = U^{\circ \circ}$. Thus that $U$ is a neighborhood would follow if $U^\circ$ was bounded, which follows because $U^\circ \subset (\varepsilon_n A_n^\circ)^\circ$.
\end{proof}

\begin{example}
    We can define the convolution of two compactly supported distributions, and we therefore get a bilinear map
    %
    \[ \mathcal{E}(\RR^d)^* \times \mathcal{E}(\RR^d)^* \to \mathcal{E}(\RR^d)^*. \]
    %
    This map is clearly seen to be separately continuous when all sets are given the strong dual topology of $\mathcal{E}(\RR^d)^*$, and thus the map is jointly continuous. The same is true for the tensor product map
    %
    \[ \mathcal{E}(\RR^n)^* \times \mathcal{E}(\RR^m)^* \to \mathcal{E}(\RR^n \times \RR^m)^*. \]
    %
    On the other hand, the multiplication maps
    %
    \[ \loc{C^\infty}(\RR^n) \times C_c^\infty(\RR^n) \to C_c^\infty(\RR^n) \]
    %
    and 
    %
    \[ \loc{C^\infty}(\RR^n) \times \mathcal{D}(\RR^n)^* \to \mathcal{D}(\RR^n)^* \]
    %
    are separately continuous, but \emph{not} jointly continuous. The convolution maps
    %
    \[ C_c^\infty(\RR^n) \times \mathcal{D}(\RR^n)^* \to \loc{C^\infty}(\RR^n) \]
    %
    or
    %
    \[ \mathcal{E}(\RR^n)^* \times \mathcal{D}(\RR^n)^* \to \mathcal{D}(\RR^n)^* \]
    %
    are also separately continuous, but not jointly continuous.
\end{example}

Despite some of the bilinear maps $B$ above not being jointly continuous, they still satisfy the property of being \emph{hypocontinuous} maps, which means that for any bounded set $A \subset X$, the set $\{ B^x: x \in A \}$ is equicontinuous, and for any bounded set $A \subset Y$, the set $\{ B_y : y \in A \}$ is equicontinuous. This is equivalent to the fact that if $\{ x_\alpha \}$ is a net in $X$ converging to zero, and $\{ y_\alpha \}$ is a bounded net, then the net $\{ B(x_\alpha,y_\alpha) \}$ converges to zero.

\begin{theorem}
    If $X$ and $Y$ are barelled spaces, and $Z$ is locally convex, then every separately continuous bilinear map $B: X \times Y \to Z$ is hypocontinuous.
\end{theorem}
\begin{proof}
    TODO: Apply Banach Steinhaus.
\end{proof}

\section{Equicontinuous Tensor Product}

The first topology we study is obtained via the natural correspondence with identifies the algebraic tensor product $X \otimes Y$ of two topological vector spaces with a certain family of linear maps from $X^*$ to $Y$. Indeed, given $x \otimes y$, we consider the map $i(x \otimes y)(x^*) = x^*(x) y$. The image of $i$ consists solely of finite rank linear operators, and in fact, is easily seen to be equal to the family of \emph{all} finite rank linear operators which are continuous when $X^*$ is given the weak $*$ topology (the topology on $Y$ is arbitrary, since they will all agree on finite dimensional subspaces). We can thus give $X \otimes Y$ a natural topology by identifying it as a subspace of $L(X^*_{\sigma^*}, Y_\sigma)$, where the space of linear maps is given the $\varepsilon$ topology, i.e. induced by uniform convergence on equicontinuous subsets of $X^*$. The completion of which will be denoted $X \widehat{\otimes}_\varepsilon Y$. We now flesh out this notion of convergence, and identify it with an appopriate convergence on an analogous space of bilinear maps.

Let us begin by considering a version of this that works over bilinear maps. Given two locally convex topological vector spaces $X$, $Y$, and $Z$, let $\mathcal{B}il(X,Y;Z)$ be the family of all \emph{separately continuous} scalar valued bilinear maps, and let $\text{Bil}(X,Y;Z)$ be the family of \emph{jointly continuous} scalar valued bilinear maps. If $\Sigma$ and $\Pi$ are a family of bounded subsets of $X$ and $Y$ with the appopriate properties analogoue to that of the introduction of the topologies on the duals, and the space of continuous linear functionals, then we can introduce a topological vector space structure on $\text{Bil}_{\Sigma,\Pi}(X,Y;Z)$ given by uniform convergence on sets of the form $A_1 \times A_2$ with $A_1 \in \Sigma$ and $A_2 \in \Pi$. However, this does not necessarily introduce a topological vector space structure on $\mathcal{B}il_{\Sigma,\Pi}(X,Y;Z)$, since the sets
%
\[ U(A_1,A_2;W) = \{ B \in \mathcal{B}il(X,Y): B(A_1, A_2) \subset W \} \]
%
need not be absorbing. It \emph{is} absorbing if $B(A_1,A_2)$ is bounded for any $B \in \mathcal{B}il(X,Y)$, and any $A_1 \in \Sigma$ and $A_2 \in \Pi$, $B(A_1,A_2)$ is bounded. Most important for our purposes, this is true if $B \in \mathcal{B}il(X^*_b, Y^*_b)$, and $A_1$ and $A_2$ are families of equicontinuous functions (thus if $X$ and $Y$ are barelled, there is no problem with the definition).

\begin{lemma}
    If $A_1$ and $A_2$ are equicontinuous subsets of $X^*$ and $Y^*$, and $B \in \mathcal{B}il(X^*_b, Y^*_b; Z)$, then $B(A_1,A_2)$ is a bounded subset of $Z$.
\end{lemma}
\begin{proof}
    We may assume that $A_1$ and $A_2$ are closed, convex, and balanced. Since these sets are also bounded, they are weak $*$ compact. The family $\{ B_x: x \in A_1 \}$ is bounded in $L_{\sigma^*}(Y^*_b,Z)$ But then $\{ B_x : x \in A_1 \}$ is bounded in the topology of uniform convergence on convex, balanced, bounded subsets of $Y^*_b$. Then $B(A_1,A_2) = \bigcup_{x \in A_1} B_x(A_2)$ is bounded in $Z$.
\end{proof}

Thus we can define the $\varepsilon$ topology, or \emph{equicontinuous} topology on $\mathcal{B}il(X^*_b, Y^*_b)$ given by uniform convergence over products of \emph{equicontinuous} subsets of $X^*_b$ and $Y^*_b$. The space of such maps will be denoted $\mathcal{B}il_\varepsilon(X^*_b, Y^*_b)$.

\begin{lemma}
    Let $X$ and $Y$ be locally convex spaces. Then there is a natural isomorphism
    %
    \[ \mathcal{B}_\varepsilon(X^*_{\sigma^*}, Y^*_{\sigma^*}) \cong L_\varepsilon(X^*_{\sigma^*}, Y_\sigma), \]
    %
    where $L_\varepsilon$ is the space of continuous linear functions equipped with the topology of uniform convergence on equicontinuous subsets of the domain.
\end{lemma}
\begin{proof}
    Given a continuous linear map $T: X^*_{\sigma^*} \to Y_\sigma$, we define
    %
    \[ B(x^*,y^*) = y^*(Tx^*). \]
    %
    Then $B$ is separately continuous in $X^*_{\sigma^*}$ and $Y^*_{\sigma^*}$. If $B = 0$, then Hahn-Banach implies that $T = 0$, so the correspondence is injective. Conversely, if $B$ is a separately continuous bilinear map on $X^*_{\sigma^*}$ and $Y^*_{\sigma^*}$, for each $x^* \in X^*$, we get a continuous linear functional on $Y^*{\sigma^*}$, which must correspond to some unique element $y \in Y$, and we define $Tx^* = Y$. Thus the correspondence is bijective. That the map is a homeomorphism is left as an exercise.
\end{proof}

\begin{remark}
    A linear map $T: X^* \to Y$ is weak-$*$ to weak continuous if and only if it is continuous from the \emph{Mackey topology} $X_\tau$, and the standard topology on $Y$, where the Mackey topology $\tau$ is given by uniform convergence on convex, balanced, weakly compact subsets of $X$. See Proposition 42.2 of Treves. In particular, $L_\varepsilon(X^*_{\sigma^*}, Y_\sigma)$ is the same space topologically as $L_\varepsilon(X^*_\tau, Y)$.
\end{remark}

\begin{theorem}
    Let $X$ and $Y$ be locally convex spaces. Then $\mathcal{B}il_\varepsilon(X^*_\sigma, Y^*_\sigma)$ is complete if and only if $X$ and $Y$ are complete.
\end{theorem}
\begin{proof}
    Proposition 42.3 of Treves.
\end{proof}

Let us consider the case of norm spaces. If $X$ is locally convex, then by Mackey's theorem, every convex, balanced, weakly compact subset of $X$ is bounded in the standard topology on $X$. Thus $X^*_\tau$ is equipped with a coarser topology than the topology of $X^*_b$, which means that $L(X^*_\tau, Y)$ is a subset of $L(X^*_b,Y)$. If $X$ is a norm space, then the equicontinuous subsets of $X^*_b$ are precisely those sets contained in a ball of some finite radius, which means the $\varepsilon$ topology is the same as the strong topology. Thus $L_\varepsilon(X^*_{\sigma^*}, Y_\sigma)$ is embedded as a subspace of $L_b(X^*, Y)$, which we will also denote by $L_\varepsilon(X^*,Y)$. Thus, in particular, $L_\varepsilon(X^*,Y)$ is a norm space when we consider the operator norm. It follows from the results above that $L_\varepsilon(X^*,Y)$ is a Banach space if and only if $X$ and $Y$ are Banach spaces.

For norm spaces $X$ and $Y$, the norm $\varepsilon$ induced on the algebraic tensor product $X \otimes Y$ equipped with the equicontinuous topology will be the operator norm obtained by considering the inclusion $X \otimes Y \to B(X^*,Y)$, and thus $\varepsilon(a)$ is the smallest quantity such that for any expression $a = \sum_{i = 1}^N x_i \otimes y_i$, and any $x^* \in X^*$ and $y^* \in Y^*$,
%
\[ \left\| \sum_{i = 1}^N x^*(x_i) y^*(y_i) \right\| \leq \varepsilon(a) \| x^* \| \| y^* \|.  \]
%
Thus we have a concrete description of the topology in this case.

\section{Equicontinuous Tensor Products in Practice}

Let $\Omega$ be an open subset of $k^d$, and let $Y$ be a topological vector space. We define the partial derivatives of a function $f: \Omega \to Y$, for $x_0 \in \Omega$, to be
%
\[ D^i f(x_0) = \lim_{t \to 0} \frac{f(x_0 + t e_i) - f(x_0)}{t}. \]
%
We say $f$ is \emph{$n$ times continuously differentiable} if all iterated partial derivatives $D^\alpha f$ exist for all multi-indices $|\alpha| \leq n$, and for such $\alpha$ the functions $D^\alpha f: \Omega \to Y$ are all continuous. As in the finite dimensional case, if $f$ is $n$ times continuously differentiable, then order does not matter in iterated partial derivatives up to order $n$. The space of all $n$-times continuously differentiable functions is denoted $C^n(\Omega,Y)$, and equipped with the topology such that a net $\{ f_\alpha \}$ converges to $f$ if $\{ D^\beta f_\alpha \}$ converges uniformly to $D^\beta f$ on $\Omega$. We also consider the same space $\loc{C^n}(\Omega,Y)$, equipped with the topology of locally uniform convergence.
%
\begin{itemize}
    \item If $Y$ is locally convex, then $C^n(\Omega,Y)$ and $\loc{C^n}(\Omega,Y)$ are locally convex spaces.
    %since if $\Omega$ is the limit of an increasing family of compact subsets $\{ K_n \}$ of $\Omega$, and if $\{ \rho_\alpha \}$ are a family of continuous seminorms on $Y$ giving $Y$ it's topology, then $C^n_{\text{loc}}(\Omega,Y)$ has the topology given by the seminorms
    %
    %\[ f \mapsto \sup_{x \in K_n} \rho_\alpha(f(x)). \]

    \item If $Y$ is metrizable, then so is $C^n(\Omega,Y)$ and $\loc{C^n}(\Omega,Y)$, with metric given by
    %
    \[ d(f,g) = \sum_{n = 1}^\infty 2^{-n} \max \left( \sup_{x \in K_n} d(f(x),g(x)), 1 \right). \]

    \item If $Y$ is complete, then so is $C^n(\Omega,Y)$ and $\loc{C^n}(\Omega,Y)$.
    %, since if $\{ f_\alpha \}$ is a Cauchy net in this space, then for each $x$, $\{ f_\alpha(x) \}$ is Cauchy in $Y$, thus converging to a value $f(x) \in Y$. One can verify that $\{ f_\alpha \}$ then converges to $f$ in $C^n_{\text{loc}}(\Omega,Y)$.
\end{itemize}

\begin{remark}
    We note that if $Y$ is a Banach space, the notation $C^n(\Omega,Y)$ is usually reserved for the Banach space (Fr\'{e}chet for $n = \infty$) consisting of $C^n$ functions $f: \Omega \to Y$ such that $\| D^\alpha f \|$ is uniformly bounded on $\Omega$ for each $|\alpha| \leq n$, equipped with the norm
    %
    \[ \sup_{|\alpha| \leq n} \| D^\alpha f \|. \]
    %
    The larger space $C^n(\Omega,Y)$ defined above then has this space as a closed subspace. To avoid confusion, sometimes the Banach space defined here is denoted $C^n_b(\Omega,Y)$.
\end{remark}

Since $\Omega$ is locally compact, we can also consider the space $C^n_c(\Omega,Y)$ of compactly supported $C^n$ functions. If $Y$ is a Fr\'{e}chet space, then $C^n_c(\Omega,Y)$ will be an LF space.

Now let us see how the tensor product enters the picture. We can identify the tensor product $C^n_{\text{loc}}(\Omega) \otimes Y$ with a subspace of $C^n_{\text{loc}}(\Omega,Y)$ by identifying a sum $f_1 \otimes y_1 + \dots + f_m \otimes y_m$ with the function $f(x) = f_1(x) y_1 + \dots + f_m(x) y_m$. This identification gives as a special case an identification of $C^n_c(\Omega) \otimes Y$ with a subspace of $C^n_c(\Omega,Y)$.

\begin{lemma}
    Let $\Omega$ be an open subset of $\RR^n$. For any complete locally convex space $Y$, $C_c^\infty(\Omega) \otimes Y$ is dense in $C^n_{\text{loc}}(\Omega,Y)$.
\end{lemma}
\begin{proof}
    We first note that $C_c^n(\Omega,Y)$ is dense in $C^n_{\text{loc}}(\Omega,Y)$, so it suffices to show that any element of $C_c^n(\Omega,Y)$ can be approximated uniformly to an arbitrary degree of precision by an element of $C_c^\infty(\Omega) \otimes Y$.

    For each $\varepsilon > 0$, let $\Omega_\varepsilon$ be the open subset of $\Omega$ lying at least $\varepsilon$ from $\partial \Omega$. If $f: \Omega \to Y$ lies in $C_c^n(\Omega,Y)$, then there is $\varepsilon > 0$ such that $f$ is supported on $\Omega_{10 \varepsilon}$. Thus if we consider a mollifier $\rho \in C_c^\infty(\RR^d)$ with $\text{supp}(\rho)$ contained in $|x| \leq 1$, with $\rho \geq 0$, and with $\int \rho(x)\; dx = 1$, and we define $\rho_\varepsilon(x) = \varepsilon^{-n} \rho(x / \varepsilon)$, then we can define a function $\rho_\varepsilon * f$ in $C_c^\infty(\Omega,Y)$ given formally by the integral
    %
    \[ (\rho_\varepsilon * f)(x) = \int \rho_\varepsilon(x - y) f(y)\; dy. \]
    %
    Indeed, integration by parts implies that $D^\alpha (\rho_\varepsilon * f) = \rho_\varepsilon * D^\alpha f$, and it is a standard result about mollifiers that as $\varepsilon \to 0$, if $D^\alpha f$ is continuous and compactly supported, then $\rho_\varepsilon * D^\alpha f$ converges uniformly to $D^\alpha f$. But this implies density.
\end{proof}

In fact, $C^n_{\text{loc}}(\Omega,Y)$ is \emph{precisely} the equicontinuous completion of $C_c^\infty(\Omega) \otimes Y$.

\begin{theorem}
    Suppose $\Omega$ is an open subset of $\RR^n$, and $Y$ is a complete locally convex space, then $C^n_{\text{loc}}(\Omega,Y)$ is naturally isomorphic to $C^n_{\text{loc}}(\Omega) \widehat{\otimes}_\varepsilon Y$.
\end{theorem}
\begin{proof}
    It suffices to associate with each element of $C^n_{\text{loc}}(\Omega,Y)$ a continuous linear operator from $Y^*_\tau$ to $C^n_{\text{loc}}(\Omega)$ which extends the natural map from $C^n_{\text{loc}}(\Omega) \otimes Y$ to this space, and agrees with the required topologies. It is simple to see that if $f \in C^n_{\text{loc}}(\Omega,Y)$, then we should consider the linear operator $T[f]: Y^* \to C^n_{\text{loc}}(\Omega)$ given by setting
    %
    \[ T[f](y^*) = y^* \circ f. \]
    %
    To verify continuity, if $\{ y^*_\alpha \}$ converges in the Mackey topology to $y^* \in Y^*$, then for any compact set $K \subset \Omega$, and any multi-index $\beta$ with $|\beta| \leq n$, $D^\beta f(K)$ is compact in $Y$. Since $Y$ is complete, the closed, balanced convex hull $H_\beta$ of $D^\beta f(K)$ is compact, and thus weakly compact. Since $\{ y_\alpha^* \}$ converges uniformly on $H_\beta$, it follows that for each $\alpha$,
    %
    \[ D^\alpha T[f](y^*_\alpha) = y^*_\alpha \circ D^\alpha f \]
    %
    converges uniformly on $K$ to $D^\alpha T[f](y^*) = y^* \circ D^\alpha f$. Thus $T[f](y^*_\alpha)$ converges in $C^n_{\text{loc}}(\Omega,Y)$ to $T[f](y^*)$, verifying continuity. Thus we have a map $T; C^n_{\text{loc}}(\Omega,Y) \to L_\varepsilon(Y^*_\tau, C^n_{\text{loc}}(\Omega))$. It suffices to show this is a continuous embedding, and this is left as an exercise.
\end{proof}

Similarily, $C_c^n(\Omega,X)$ is naturally isomorphic to $C_c^n(\Omega) \widehat{\otimes}_\varepsilon X$.

As another example, if $\Omega$ is a locally compact topological space, and $X$ is a complete locally convex space, then $C(\Omega,X)$ is naturally isomorphic to $C(\Omega) \widehat{\otimes}_\varepsilon X$.


TODO: Move to distribution theory notes (also discuss 51 of Treves and relation to Schwartz kernel theorem, 52)



Similarily, $C_c^\infty(\Omega_1 \times \Omega_2)$ is isomorphic to $C_c^\infty(\Omega_2, C_c^\infty(\Omega_1))$.

We have natural pairings
%
\[ \mathcal{D}(\Omega_1)^* \times C_c^\infty(\Omega_1 \times \Omega_2) \to C_c^\infty(\Omega_2) \]
%
and
%
\[ \mathcal{D}(\Omega_2)^* \times C_c^\infty(\Omega_1 \times \Omega_2) \to C_c^\infty(\Omega_1) \]
%
given by
%
\[ \langle u, \phi \rangle(y) = \int \phi(x,y) u(x)\; dx. \]
%
and
%
\[ \langle v, \phi \rangle(x) = \int \phi(x,y) v(y)\; dy. \]
%
That the image lies in $C_c^\infty(\Omega_2)$ follows because any $\phi$ is equivalent both to an element of $C_c^\infty(\Omega_2, C_c^\infty(\Omega_1))$, and an element of $C_c^\infty(\Omega_1, C_c^\infty(\Omega_2))$.

Given a distribution $u \in \mathcal{D}(\Omega_1)^*$ and $v \in \mathcal{D}(\Omega_2)^*$, we can therefore define the tensor product distribution $u \otimes v \in \mathcal{D}(\Omega_1 \times \Omega_2)^*$ such that for $\phi \in C_c^\infty(\Omega_1 \times \Omega_2)$,
%
\[ \langle u \otimes v, \phi \rangle = \langle u, \langle v, \phi \rangle \rangle = \langle v, \langle u, \phi \rangle \rangle. \]
%
The equality of the latter two quantities is trivial if $\phi \in C_c^\infty(\Omega_1) \otimes C_c^\infty(\Omega_2)$. But this space is dense in $C_c^\infty(\Omega_1 \times \Omega_2)$. That one can apply these distributions in either order is essentially a version of Fubini's theorem for distributions. The space $\mathcal{D}(\Omega_1)^* \otimes \mathcal{D}(\Omega_2)^*$ is dense in $\mathcal{D}(\Omega_1 \times \Omega_2)^*$ (in which topology).

Here is another example. TODO: $l^1 \widehat{\otimes}_\varepsilon X$ is the space of convergent sequences of $X$, Chapter 44 of Treves.

\section{The Projective Topology}

Given two locally convex spaces, the $\pi$ topology, or \emph{projective topology} on $X \otimes Y$ is the finest locally convex topology such that the inclusion map $i: X \times Y \to X \otimes Y$ is continuous. Thus a convex set $U \subset X \otimes Y$ is open if and only if $i^{-1}(U)$ is open. In particular, this means that if $\{ \rho_\alpha \}$ is a family of seminorms on $X$ which specify the topology of $X$, and $\{ \psi_\beta \}$ are seminorms on $Y$ giving it it's topology, then $\{ \rho_\alpha \otimes \psi_\beta \}$ specifies the $\pi$ topology on $X \otimes Y$, where $(\rho_\alpha \otimes \psi_\beta)(a)$ is the infinum of quantities of the form
%
\[ \sum_{i = 1}^N \rho_\alpha(x_i) \psi_\beta(y_i) \]
%
such that $a = \sum_{i = 1}^N x_i \otimes y_i$. The completion of $X \otimes Y$ in this topology will be denoted by $X \widehat{\otimes}_\pi Y$.

If $B \in \text{Bil}(X,Y)$, then $B$ acts on $X \otimes Y$ by setting
%
\[ \langle B, a \rangle = \sum_{i = 1}^N B(x_i,y_i). \]
%
Since $B$ is continuous, there exists continuous seminorms $\rho$ on $X$ and $\psi$ on $Y$ such that $|B(x,y)| \leq \rho(x) \psi(y)$ for all $x \in X$ and $y \in Y$, and so
%
\[ |\langle B, a \rangle| \leq (\rho \otimes \psi)(a). \]
%
Thus $B$ acts as a continuous linear functional on $X \otimes Y$ in the $\pi$ topology.

To show that $X \otimes Y$ is a \emph{Hausdorff} locally convex space, we fix a nonzero $a \in X \otimes Y$. Then we can write
%
\[ a = \sum_{i = 1}^N \sum_{j = 1}^M a_{ij} x_i \otimes y_j, \]
%
where $\{ x_i \}$ and $\{ y_j \}$ are linearly independent, and $a_{11} = 1$. By Hahn-Banach, we can find $x^* \in X^*$ and $y^* \in Y^*$ such that $x^*(x_1) = y^*(y_1) = 1$, and $x^*(x_i) = y^*(y_j) = 0$ otherwise. But then $B(x,y) = x^*(x) y^*(y)$ is a continuous, bilinear functional, and
%
\[ \langle B, a \rangle = 1. \]
%
Thus there exists an open neighborhood of the origin in the $\pi$ topology not containing $a$, which means the $\pi$ topology is Hausdorff.

If $X$ and $Y$ are norm spaces, then next lemma shows the projective topology gives $X \otimes Y$ the structure of a norm space.

\begin{lemma}
    Let $\rho$ be a seminorm on $X$, and $\psi$ a seminorm on $Y$. The seminorm $\rho \otimes \psi$ on $X \otimes Y$ is a norm if and only if $\rho$ and $\psi$ are norms.
\end{lemma}
\begin{proof}
    Suppose $\rho \otimes \psi$ is a norm. If $x \in X$ and $\rho(x) = 0$, then $(\rho \otimes \psi)(x \otimes y) = 0$ for any $y \in Y$. Thus $x \otimes y = 0$ for all $y \in Y$. If we fix some nonzero $y$, pick $y^* \in Y^*$ such that $y^*(y) = 1$, then for any $x^* \in X$, we conclude that
    %
    \[ x^*(x) = x^*(x) y^*(y) = (x^* \otimes y^*)(x \otimes y) = 0. \]
    %
    Thus by Hahn-Banach, $x = 0$, and so $\rho$ is a norm. Similarily, $\psi$ is a norm. The converse follows because if $\rho$ is a norm, and $\psi$ is a norm, then the $\pi$ topology on $X \otimes Y$ is induced by the seminorm $\rho \otimes \psi$, and since $X \otimes Y$ is Hausdorff, $\rho \otimes \psi$ must therefore be a norm.
\end{proof}

The space $X \otimes_\pi Y$ has the universal property that it is the only complete locally convex space such that for any locally convex space $Z$, the space of continuous linear mappings from $X \otimes_\pi Y$ to $Z$ are naturally isomorphic to the space $\text{Bil}(X,Y;Z)$ of continuous bilinear maps from $X \times Y$ to $Z$ in the canonical way. In particular, $(X \otimes_\pi Y)^*$ is naturally isomorphic to $\text{Bil}(X,Y)$.

\begin{remark}
    We see from this that the $\varepsilon$ topology on $X \otimes Y$ is finer than the $\pi$ topology on$ X \otimes Y$, since the bilinear map $X \times Y \to X \otimes_\varepsilon Y$ is continuous, and thus the induced inclusion $X \otimes_\pi Y \to X \otimes_\varepsilon Y$ is continuous.
\end{remark}

If $T: X_1 \to X_2$ and $S: Y_1 \to Y_2$ are linear maps, then we can simple define $(T \otimes S): X_1 \otimes Y_1 \to X_2 \otimes Y_2$. If $T$ and $S$ are continuous linear maps, then $T \otimes S$ will be continuous both in the $\pi$ topologies, and the $\varepsilon$ topologies. TODO: 43.6 of Treves. Thus these maps extend to two continuous linear maps $T \widehat{\otimes}_\varepsilon S$ and $T \widehat{\otimes}_\pi S$ on the completions of these spaces.

If $T$ and $S$ are isomorphisms, then $T \widehat{\otimes}_\varepsilon S$ will be an isomorphism. Thus it follows that if $X_0$ is a linear subspace of $X$, and $Y_0$ a linear subspace of $Y$, then $X_0 \otimes_\varepsilon Y_0$ is embedded continuously in $X \otimes_\varepsilon Y$. Thus $X_0 \widehat{\otimes}_\varepsilon Y_0$ is a closed subspace of $X \widehat{\otimes}_\varepsilon Y$.

\begin{remark}
    One can find surjective maps $T: X_1 \to X_2$ and $S: Y_1 \to Y_2$ such that $T \widehat{\otimes}_\pi S$ is \emph{not} surjective. Similarily, if $T: X_1 \to X_2$ and $S: Y_1 \to Y_2$ are isomorphisms, it is not necessarily true that $T \widehat{\otimes}_\pi S$ is an isomorphism.
\end{remark}

On the other hand, it is true that if $T: X_1 \to X_2$ and $S: Y_1 \to Y_2$ have dense image, then $T \widehat{\otimes}_\pi S$ will also have dense image, and will actually be surjective if $X_1$ and $Y_1$ are first countable. TODO: 43.9 of Treves.

If $X$ and $Y$ are norm spaces, then we have specified $X \otimes_\pi Y$ is a norm space with norm
%
\[ \pi(a) = \inf \sum_{i = 1}^N \| x_i \| \| y_i \|. \]
%
where the infinum is taken over expressions of $a = \sum_{i = 1}^N x_i \otimes y_i$. But there is an alternate description which is often useful in this case. The space $\text{Bil}_b(X,Y)$ is a norm space, with a natural norm being the quantity $\| B \|$ such that for all $x \in X$ and $y \in Y$,
%
\[ |B(x,y)| \leq \| B \| \| x \| \| y \|. \]
%
It turns out that $\pi(a)$ is precisely the smallest quantity such that $|\langle B, a \rangle| \leq \pi(a) \| B \|$ for all $B \in \text{Bil}(X,Y)$. We have an expression of $\pi$ that then directly implies that $\text{Bil}(X,Y)$ is isometric to $(X \otimes_\pi Y)^*$.

\begin{lemma}
    For any $a \in X \otimes Y$,
    %
    \[ \pi(a) = \sup_{\| B \| \leq 1} |\langle B, a \rangle|. \]
\end{lemma}
\begin{proof}
    Let $\lambda(a)$ denote the right hand side of the equation above. Given any bilinear map $B$, let $\| B \|_\pi$ and $\| B \|_\lambda$ denote the operator norms with respect to the two norms $\pi$ and $\lambda$, viewing $B$ as a functional on $X \otimes Y$. We claim that for any bilinear map $B$, we have $\| B \|_\pi = \| B \|_\lambda$. Given a continuous bilinear map $B$, and any $a \in X \otimes Y$, if $a = \sum x_i \otimes y_i$, then
    %
    \[ |\langle B, a \rangle| \leq \sum_i |B(x_i,y_i)| \leq \| B \|_\lambda \sum_i \| x_i \| \| y_i \|. \]
    %
    Taking infima over the right hand side, we find that
    %
    \[ |\langle B, a \rangle| \leq \pi(a) \| B \|_\lambda. \]
    %
    Taking suprema over all $a \in X \otimes Y$, we conclude that $\| B \|_\pi \leq \| B \|_\lambda$. This is equivalent to proving that $\lambda \leq \pi$, since by Hahn-Banach, for any $a \in X \otimes Y$, we can find a continuous bilinear map $B$ with $\| B \|_\lambda = 1$ such that $\langle B, a \rangle = \pi(a)$, and then
    %
    \[ \lambda(a) = \langle B, a \rangle \leq \| B \|_\pi \pi(a) \leq \| B \|_\lambda \pi(a) \leq \pi(a). \]
    %
    For any $x \in X$ and $y \in Y$, it follows that
    %
    \[ \| x \| \| y \| = \lambda(x \otimes y) \leq \pi(x \otimes y) \leq \| x \| \| y \|. \]
    %
    Thus we have $\pi(x \otimes y) = \| x \| \| y \|$ for any $x \in X$ and $y \in Y$. But this means that for any bilinear map $B$,
    %
    \[ \| B \|_\lambda = \sup_{\| x \| \| y \| \leq 1} |B(x,y)| \leq \sup_{\| x \| \| y \| \leq 1} \| B \|_\pi \pi(x \otimes y) = \sup_{\| x \| \| y \| \leq 1} \| B \|_\pi \| x \| \| y \| = \| B \|_\pi. \]
    %
    A similar Hahn-Banach argument thus shows that $\pi \leq \lambda$, which gives equality.
\end{proof}

\begin{corollary}
    If $X$ and $Y$ are norm spaces, then for any $x \in X$ and $y \in Y$,
    %
    \[ \pi(x \otimes y) = \varepsilon(x \otimes y) = \| x \| \| y \|. \]
    %
    This gives an alternate proof that $\pi = (\rho \otimes \psi)$ is a norm if $\rho$ and $\psi$ are norms.
\end{corollary}

If $X$ and $Y$ are norm spaces, then because $X \otimes_\pi Y$ is the completion of $X \otimes Y$, every element $a \in X \otimes_\pi Y$ can be written as an absolutely convergent infinite series
%
\[ \sum_{i = 1}^\infty x_i \otimes y_i, \]
%
i.e. a series such that
%
\[ \sum_{i = 1}^\infty \| x_i \| \| y_i \| < \infty. \]
%
It is simple to see from this that
%
\[ \pi(a) \inf \left\{ \sum_{i = 1}^\infty \| x_i \| \| y_i \| : a = \sum_{i = 1}^\infty x_i \otimes y_i, \sum_{i = 1}^\infty \| x_i \| \| y_i \| < \infty \right\}. \]
%
For any bilinear map $B$, we have
%
\[ \langle B, a \rangle = \sum_{i = 1}^\infty B(x_i,y_i). \]
%
and similarily, for any bounded linear map $T: X \to Y^*$,
%
\[ \langle T, a \rangle = \sum_{i = 1}^\infty \langle T(x_i), y_i \rangle. \]
%
This should be compared to $X \otimes_\varepsilon Y$, whose elements can certainly be written as an absolutely convergent series
%
\[ \sum_{i = 1}^\infty a_i \]
%
with $a_i \in X \otimes Y$ and with $\sum \varepsilon(a_i) < \infty$, but these infinite sums \emph{cannot} be reduced to an absolutely convergent sum of simple tensor products like with $X \otimes_\pi Y$.

\begin{remark}
    We can actually strengthen the absolute summability result in the $\pi$ topology to obtain the following results:
    %
    \begin{itemize}
        \item If $X$ and $Y$ are Banach spaces, then every element $a$ of the open unit ball in $X \widehat{\otimes_\pi} Y$ can be written as
        %
        \[ a = \sum_{i = 1}^\infty \lambda_i (x_i \otimes y_i), \]
        %
        where $\{ x_i \}$ and $\{ y_i \}$ are sequences in the open unit ball of $X$ and $Y$ converging to zero, and $\sum_i |\lambda_i| < 1$.

        \item More generally, if $X$ and $Y$ are Fr\'{e}chet spaces, and $U$ and $V$ are convex balanced open neighborhoods of the origin, then for any compact subset $K_0$ of the convex balanced hull of $U \otimes V$, there is a compact subset $K_1$ of $l^1$, a sequence $\{ x_n \}$ contains in $U$ converging to zero in $X$, and a sequence $\{ y_n \}$ converging to zero in $Y$ such that for each $a \in U \otimes V$, there is a sequence $\{ \lambda_n \}$ in $K_1$ such that
        %
        \[ a = \sum_{n = 0}^\infty \lambda_n x_n \otimes y_n. \]
        %
        Proofs of these results are found in Treves, Chapter 45.

        \item Thus if $X$ and $Y$ are Fr\'{e}chet spaces, then every compact subset of $X \widehat{\otimes}_\pi Y$ is contained in the closed convex balanced hull of $K_1 \otimes K_2$ for compact subsets $K_1$ and $K_2$ of $X$ and $Y$. In particular, the linear bijection from $\text{Bil}(X,Y;Z)$ to $L(X \widehat{\otimes}_\pi Y, Z)$ becomes an isomorphism if we equip $\text{Bil}(X,Y;Z)$ with the topology of uniform convergence on compact sets, and the latter space with the topology of compact convergence.
    \end{itemize}
 \end{remark}

 TODO: Chapter 44, 46 of Treves.

 \section{The Projective Topology in Practice}

 TODO: If $X$ is a Banach space, then $l^1 \widehat{\otimes}_\pi X$ is naturally equivalent to the space of absolutely summable sequences in $X$.

 TODO: More generally, for any measure space $\Omega$, $L^1(\Omega) \widehat{\otimes}_\pi X$ is naturally equivalent to the Banach space $L^1(\Omega,X)$ of absolutely integrable $X$-valued functions on $\Omega$. The space $L^p(\Omega,X)$ for $1 < p < \infty$, on the other hand, is bigger than the space $L^p(\Omega) \widehat{\otimes}_\pi X$. TODO: Chapter 46 of Treves.

If $\Omega$ is a complete measure space, and $X$ is a separable Banach space, then we can define $L^\infty(\Omega,X)$ as the natural quotient of the space of measurable functions $f: \Omega \to X$ bounded almost everywhere in the norm of $X$, modulo the space of measurable functions equal to zero almost everywhere. The theorem of Dunford-Pettis states that if $T: X \to L^\infty(\Omega)$ is a continuous linear map, then there is $g \in L^\infty(\Omega,X^*)$ such that for any $x \in X$ and $\omega \in \Omega$,
 %
 \[ Tx(\omega) = \langle g(\omega), x \rangle. \]
 %
 This is equivalent to the existence, for any continuous linear map $S: L^1(\Omega) \to X^*$, of a map $g \in L^\infty(\Omega,X^*)$ such that
 %
 \[ Sf(x) = \int f(\omega) \langle g(\omega), x \rangle\; d\omega. \]
 %
 This is also equivalent to the existence, for any continuous bilinear map $B \in \text{Bil}(L^1(\Omega), X)$, of a function $g \in L^\infty(\Omega,X^*)$ such that
 %
 \[ B(f,x) = \int f(\omega) \langle g(\omega), x \rangle\; d\omega. \]
 %
 This is what proves the equivalence above. TODO: Chapter 46 of Treves.





\section{Nuclear Operators}

TODO: Chapter 47, 48, 49, 50, 51 of Treves.

An operator $T: X \to Y$ is \emph{nuclear} if it can be written in the form
%
\[ Tx = \sum_{i = 1}^\infty \langle x_i^*, x \rangle y_i \]
%
for some $\{ x_i^* \}$ in $X^*$ and $\{ y_i \}$ in $Y$ with $\sum \| x_i^* \| \| y_i \| < \infty$, so that the series above is absolutely convergent. The nuclear norm $\| T \|_N$ of $T$ is defined to be the infinum of $\sum \| x_i^* \| \| y_i \|$ over all possible choices of representations. The space $N(X,Y)$ of nuclear operators between spaces then becomes a Banach space. Since
%
\[ \| Tx \| \leq \left( \sum_{i = 1}^\infty \| x_i^* \| \| y_i \| \right) \| x \| \]
%
it follows that all nuclear operators are bounded, with $\| T \| \leq \| T \|_N$. Since it is also clear that finite rank operators are dense in $N(X,Y)$, equipped with the nuclear norm, it follows that all operators in $N(X,Y)$.

Similar to the class of compact operators, the property of being nuclear is closed under taking adjoints, and under composition on the left and right by bounded maps. In particular, $N(X)$ is an ideal in $B(X)$, and if $H$ is a Hilbert space, $N(H)$ is a $*$ closed ideal in $B(H)$. In Hilbert space theory, the family of nuclear operators is important because it is the space of operators for which we can extend the notion of a \emph{trace} of a matrix to an infinite dimensional setting.

As should be expected, we have a natural surjective map $J$ from $X^* \otimes_\pi Y$ to $N(X,Y)$. But we cannot necessarily identify these spaces, since this map is not necessarily an isomorphism.

TODO: Section 16.2 and 16.3 of Banach Space Theory, Fabian, Habala, Hajek, MOntesinos, Zizler.

\section{Nuclear Space}

A space $X$ is \emph{nuclear} if it is locally convex, and for any other locally convex space $Y$, the $\pi$ and $\varepsilon$ topologies agree on $X \otimes Y$. TODO.

That $C_c^\infty$ is nuclear explains why the Schwartz kernel is true, but $L^2$, which explains why we do not have a Schwartz kernel theorem for this space. In fact, no infinite dimensional Banach space is nuclear.

\section{Projective Topology in Practice}



\section{The Trace}

Given any Banach space $X$, the space $(X^* \otimes_\pi X)^*$ is naturally isomorphic to $B(X^*,X^*)$. In particular, the identity map $1: X^* \to X^*$ corresponds to a linear functional on $X^* \otimes_\pi X$. We call this linear functional the \emph{trace} and denote it by $\text{Tr}$. The reason is because for any element $a$ of $X^* \otimes X$, if $a = \sum x_i^* \otimes x_i$, we find that
%
\[ \text{Tr}(a) = \sum_i \text{Tr}(x_i^* \otimes x_i) = \sum_i x_i^*(x_i). \]
%
This agrees with the trace in the finite dimensional setting, if we identify $n \times n$ matrices with maps from $\RR^n$ to $(\RR^n)^*$.

Since the finite rank operators from $X$ to itself can naturally be identified with a dense subset of $X^* \otimes_\pi X$, it follows that we can define the trace of any finite rank operator on $X$, i.e. if we can write
%
\[ Tx = \sum x_i^*(x) x_i, \]
%
then $\text{Tr}(T) = \sum x_i^*(x_i)$. The formulas above hint that we might be able to extend $\text{Tr}$ to an operator on the class $N(X)$ of all nuclear operators on a Banach space. But this is \emph{only possible} if $X$ has the approximation property.

\begin{theorem}
    The map $\text{Tr}$ can be defined on $N(X)$ if and only if $X$ has the approximation property.
\end{theorem}
\begin{proof}
    TODO
\end{proof}

\begin{theorem}
    Suppose $T: X \to X$ is a bounded operator, and it either has finite rank, or is nuclear, and $X$ has the approximation property. Then $\text{Tr}(T^*) = \text{Tr}(T)$.
\end{theorem}

\begin{theorem}
    Suppose $T: X \to Y$ and $S: Y \to X$ are bounded operators, and either $T$ has finite rank, or $T$ and $S$ are both nuclear, and have the approximation property. Then $\text{Tr}(T \circ S) = \text{Tr}(S \circ T)$.
\end{theorem}

The trace functional gives us some interesting duality relations between families of operators.

\begin{theorem}
    Let $X$ be a reflexive Banach space with the approximation property. Then $K(X)^* = N(X)$, and $N(X)^* = B(X)$, where the pairing is given for $T \in K(X) \cup B(X)$ and $S \in N(X)$ by setting
    %
    \[ \langle T, S \rangle = \text{Tr}(T \circ S). \]
\end{theorem}
\begin{proof}
    TODO
\end{proof}

Now we move onto the trace for Hilbert spaces. We note that if $T^*$ denotes the Hilbert space adjoint of a nuclear operator $T$, then $\text{Tr}(T^*) = \overline{\text{Tr}(T)}$ rather than $\text{Tr}(T^*) = \text{Tr}(T)$, given the antilinear correspondence between $H$ and $H^*$.

\begin{theorem}
    If $H$ is a separable complex Hilbert space with basis $\{ e_i \}$, and $T \in N(H)$, then
    %
    \[ \text{Tr}(T) = \sum_{i = 1}^\infty \langle Te_i, e_i \rangle. \]
\end{theorem}
\begin{proof}
    Denote the right hand side by $A(T,\{ e_i \})$. If $T$ is nuclear, there are $v_i, w_i \in H$ such that
    %
    \[ Tx = \sum \langle x, v_i \rangle w_i \]
    %
    with $\sum \| v_i \| \| w_i \| < \infty$. It follows that for any basis $\{ e_i \}$
    %
    \[ \sum_{i = 1}^\infty |\langle Te_i, e_i \rangle| = \sum_{i = 1}^\infty |\langle e_i, v_i \rangle| |\langle w_i, e_i \rangle| \leq \sum_{i = 1}^\infty \| v_i \| \| w_i \| < \infty. \]
    %
    Thus $A(T, \{ e_i \})$ is defined by an absolutely convergent series for any basis $\{ e_i \}$, and thus is well defined and finite. Fix an orthonormal basis $\{ e_i \}$. For any nuclear $T$, a polar decomposition allows us to write $T = V_1DV_2$, where $V_1$ and $V_2$ are unitary, and $D$ is a diagonal, positive-semidefinite nuclear operator with respect to the basis $\{ e_i \}$. Now
    %
    \[ A(D, \{ e_i \}) = \sum_i \langle De_i, e_i \rangle = \sum_i \lambda_i.  \]
    %
    It follows from our calculations above that $\sum_i |\lambda_i| < \infty$. Thus we have a nuclear representation of $D$ as
    %
    \[ Dx = \sum_i \lambda_i \langle x, e_i \rangle e_i. \]
    %
    Thus
    %
    \[ \text{Tr}(D) = \sum_i \lambda_i \langle e_i, e_i \rangle = A(D, \{ e_i \}). \]
    %
    Now if $T = V_1D$, i.e. $V_2 = 1$. Then
    %
    \[ T^*x = DV_1^*(x) = \sum_i \langle x, V_1 \lambda_i e_i \rangle e_i = \sum_i \langle x, Te_i \rangle e_i. \]
    %
    This is again a nuclear representation, and so
    %
    \[ \text{Tr}(T) = \overline{\text{Tr}(T^*)} = \overline{\sum_i \lambda_i \langle e_i, Te_i \rangle} = \sum_i \lambda_i \langle Te_i, e_i \rangle = A(T, \{ e_i \}). \]
    %
    TODO GENERAL CASE.
\end{proof}

\begin{theorem}
    Let $T \in N(H)$ have a polar decomposition $T = UD$. Then $N(T) = N(C) = \text{Tr}(C)$.
\end{theorem}
\begin{proof}
    TODO
\end{proof}








\section{Tensor Products}

Like with Banach spaces, we can take tensor products of more general topological spaces. The projection and injective tensor products have their generalization here.

Recall that the projective tensor product of two Banach spaces $X$ and $Y$ is equipped with the norm
%
\[ \pi(a) = \inf \left\{ \sum_i \| x_i \| \| y_i \| : a = \sum_i x_i \otimes y_i \right\}. \]
%
The projective tensor product $X \otimes_\pi Y$ is then a Banach space whose dual space is naturally isomorphic to $B(X,Y^*)$, or equivalently, the family of bilinear maps from $X$ to $Y^*$. The following result gives the proper way to generalize the projective tensor product to general topological vector spaces, i.e. as the finest topology such that the natural inclusion map $i: X \times Y \to X \otimes Y$ is continuous.

\begin{lemma}
    Let $X$ and $Y$ be Banach spaces. Then a convex set $U \subset X \otimes Y$ containing the origin is a neighborhood of zero in the projective topology if and only if $i^{-1}(U) \subset X \times Y$ is a neighborhood of zero in the product topology.
\end{lemma}
\begin{proof}
    That $i$ is continuous in the projective topology follows from the inequality
    %
    \[ \pi(x \otimes y - v \otimes w) \leq \pi((x - v) \otimes y) + \pi(v \otimes (y - w)) \leq \| x - v \| \| y \| + \| v \| \| y - w \|. \]
    %
    Thus if $U \subset X \otimes Y$ is a neighborhood of zero in the projective topology, so too is $i^{-1}(U)$. Conversely, suppose $U \subset X \otimes Y$ is convex, and $i^{-1}(U)$ is a neighborhood of zero. Thus there is $\varepsilon > 0$ such that if $\| x \|, \| y \| \leq \varepsilon$, then $x \otimes y \in U$. If $\pi(a) \leq \varepsilon^2 / 10$, then we can write
    %
    \[ a = \sum_{i = 1}^n c_i (x_i \otimes y_i), \]
    %
    where $\| x_i \|, \| y_i \| \leq \varepsilon$, $c_i > 0$, and $\sum c_i \leq 1/2$. But this means that $a$ is in the convex hull of $\{ 0, x_1 \otimes y_1, \dots, x_n \otimes y_n \}$, so $a \in U$. Thus $U$ contains a ball of radius $\varepsilon^2/2$ about the origin in $X \otimes Y$, so it is a neighborhood of the origin.
\end{proof}

Because of this property, given any topological vector spaces $X$ and $Y$, we define the projective topology on $X \otimes Y$ to be the finest topology such $i: X \times Y \to X \otimes Y$ is continuous. Thus a convex set $U \subset X \otimes Y$ containing the origin is a neighborhood of the origin if and only if $i^{-1}(U)$ is open. We let $X \otimes_\pi Y$ be the completion of $X \times Y$ with respect to this topology.

The most useful case of this construction occurs when $X$ and $Y$ are locally convex spaces. If $\rho$ is a seminorm on $X$, and $\sigma$ is a seminorm on $Y$, we define a seminorm $\rho \otimes \sigma$ on $X \otimes Y$ by declaring
%
\[ (\rho \otimes \sigma)(a) = \inf \left\{ \sum_{i = 1}^n \rho(x_i) \sigma(y_i) : a = \sum_{i = 1}^n x_i \otimes y_i \right\}. \]
%
Then $\rho \otimes \sigma$ is continuous on $X \otimes Y$, essentially because $(\rho \otimes \sigma)(x \otimes y) \leq \rho(x) \sigma(y)$. Conversely, we claim these seminorms completely specify the topology on $X \otimes Y$. Indeed, if $U$ is a convex neighborhood of the origin, then $i^{-1}(U)$ is a neighborhood of the origin in $X \otimes Y$. Thus there are continuous functions $\rho$ on $X$ and $\sigma$ on $Y$ such that if $V_1 = \{ x : \rho(x) < 1 \}$, and $V_2 = \{ y: \sigma(y) < 1 \}$, then $i(V_1 \times V_2) \subset U$. But this means that $U$ contains all $a \in X \otimes Y$ such that $(\rho \otimes \sigma)(a) < 1$, and thus is an open neighborhood of the origin.

We clearly see that if $X$ and $Y$ are Hausdorff, so is $X \otimes Y$, and if $X$ and $Y$ are Fr\'{e}chet spaces, so is $X \otimes_\pi Y$.

Next, we consider the equicontinuous tensor product. Recall that if $X$ and $Y$ are Banach spaces, then $X \otimes Y$ is isomorphic to the space of all weak $*$ to weak TODO

Because of this property, a convex subset $U \subset X \otimes Y$ will be a neighborhood of zero if and only if $i^{-1}(U)$ is an open neighborhood of $(0,0)$, where $i: X \times Y \to X \otimes Y$ is the natural inclusion map. Indeed, 

Give two topological vector spaces $X$ and $Y$, we define the projective tensor topology on $X \otimes Y$ to be the finest topology such that the 

%
If $X$ and $Y$ are \emph{locally convex spaces}, we equip $X \otimes Y$ with the topology such that for any two continuous seminorms $\rho$ and $\sigma$ on $X$ and $Y$, we have a continuous seminorm $\rho \otimes_\pi \sigma$ on $X \otimes Y$ given by
%
\[ (\rho \otimes_\pi \sigma)(a) = \inf \left\{ \sum_i \rho(x_i) \sigma(y_i) : a = \sum_i x_i \otimes y_i \right\}. \]
%











\chapter{Vector Valued Integration}

Ubiquitous to analysis is the integral, a weighted average of the infinitisimal values of the function $f$ across a certain domain. In this context, $f$ is almost always real or complex valued. But there are many spaces in which an average can be taken. Indeed, in order to define the average of a set of values $a_1, \dots, a_N$, given by
%
\[ \frac{a_1 + \dots + a_N}{N} \]
%
we need only that we can add the quantitys $\{ a_n \}$, and multiply them by scalars. Thus the natural place to generalize the integration of functions is to those which take their values in a topological vector space. In this chapter we analyze such a theory.

The weakest definition of the integral exploits the fact that the integral is linear. Because we think of integration as an infinitisimal sum, and linear functions are interchangable with summation, for any continuous linear operator $\Lambda: X \to Y$ between two topological vector spaces, and any function $f: \Omega \to X$, we ought to have
%
\[ \Lambda \left( \int_\Omega f \right) = \int_\Omega (\Lambda \circ f). \]
%
In particular, if $\Lambda$ is a continuous linear functional, the right hand side is just a scalar-valued function, and we know how to integrate those. This leads to the Pettis, or weak integral, which we now introduce.

\section{The Pettis Integral}

Let $\Omega$ be a measurable space, and $X$ a locally convex space. A function $f: \Omega \to X$ is called \emph{weakly measurable} if for each $x^* \in X^*$, $x^* \circ f$ is a measurable scalar-valued function. One can verify the usual measure theoretic facts that the space of weakly measurable functions forms a vector space closed under pointwise limits. The usual proof of Egorov's theorem also carries through if $\Omega$ is a finite measure space.

If $\Omega$ is a measure space, and $f$ is a weakly measurable function with the property that $x^* \circ f$ lies in $L^1(\Omega)$ for each $x^* \in X$, then we say a vector $x \in X$ is a \emph{weak integral} for $f$ if, for each $x^* \in X^*$,
%
\[ x^*(x) = \int_\Omega x^* \circ f. \]
%
If $X^*$ separates $X$, for instance, if $X$ is locally convex, then these equations uniquely determine $x$, and we will therefore denote it by $I(f)$. We will assume this is true in all that follows. The conditions of the next theorem, which show that such an integral exists, are automatically satisfied if $X$ is a quasicomplete locally convex space, in particular, if $X$ is a Fr\'{e}chet space, an LF space, or the weak-$*$ dual of a barelled space.

\begin{theorem}
    Let $\mu$ is a Borel probability measure on a compact Hausdorff space $\Omega$, and $f: \Omega \to X$ a continuous function into a topological vector space $X$ such that the closed convex hull of $f(\Omega)$ is compact, then the weak integral exists.
\end{theorem}
\begin{proof}
    Let $H$ be the closed convex hull of $f(\Omega)$. Given a finite family of linear functionals $L = \{ \Lambda_1, \dots, \Lambda_N \}$, we consider the family $E_L$ of all points $x \in H$ such that
    %
    \[ \Lambda_n(x) = \int_\Omega (\Lambda_n \circ f)(x)\; d\mu \]
    %
    If we can prove that each $E_L$ is non-empty, then by the compactness of $H$, the intersection of all such $E_L$ will be nonempty, and this will be the required weak integral. If we define a vector $y$ by
    %
    \[ y_n = \int_\Omega (\Lambda_n \circ f)\; d\mu \]
    %
    then it suffices to show that the image of $H$ under the map
    %
    \[ L(x) = (\Lambda_1 x, \dots, \Lambda_N x) \]
    %
    contains $y$. Because $L$ is a linear map into $\mathbf{C}^n$, $L(H)$ is compact and convex. Thus if $y$ was not an element of this set, we can separate $y$ from $H$ by a linear functional. In other words, there are constants $c_1, \dots, c_N$ such that for any $x \in H$,
    %
    \[ \sum c_n y_n < \sum c_n \Lambda_n x \]
    %
    In particular,
    %
    \[ \sum c_n y_n < \sum c_n (\Lambda_n \circ f) \]
    %
    Now integrating the right hand side gives
    %
    \[ \sum c_n y_n < \sum c_n y_n \]
    %
    which contradicts the fact that $y$ was not in the convex hull.
\end{proof}

More generally, a similar proof shows that if $\Omega$ is a finite measure space, $f: \Omega \to X$ is weakly measurable, and there exists a compact convex set $C \subset X$ containing the essential support of $f$, then the Pettis integral of $f$ exists.

TODO: ESTABLISH PROPERTIES ALA RUDIN.

\begin{example}
    Convolution of functions can be realized as a vector valued integral. Consider $g \in L^1(\RR^d)$. Then $g\; dy$ is a finite measure on $\RR^d$. If $f \in L^p(\RR^d)$, then the convolution $f * g$ is then precisely the Pettis integral
    %
    \[ \int f_y g(y)\; dy, \]
    %
    in $L^p(\RR^d)$, since if $h \in L^{p^*}(\RR^d)$, then by Fubini's theorem,
    %
    \[ \int h(x) \left( \int f_y(x) g(y)\; dy \right)\; dx = \int \left( \int h(x) f_y(x)\; dx \right) g(y)\; dy. \]
    %
    The existence of the convolution as a Pettis integral can be justified by the theorem above if $g$ has compact support, since if $1 \leq p < \infty$, the map $y \mapsto f_y$ is continuous in $L^p(\RR^d)$, and thus has compact range on the support of $g$, and the closed convex hull of this space is compact since $L^p(\RR^d)$ is locally convex.
\end{example}

If $\Omega$ is a compact subset of $\RR^n$, $X$ is a complete, locally convex space, and $f: \Omega \to X$ is continuous, then we can obtain a weak integral of $f$ with respect to the Lebesgue measure by using Riemann integration. Namely, because $\Omega$ is compact, the function $f$ is uniformly continuous, which implies we can use Riemann integration. Namely, if we consider a finite set $\mathcal{S} = \{ (S,a_S) \}$, where $\{ S \}$ is an almost disjoint family of rectangles covering $\Omega$, the points $a_S$ lie in $S$ for each $S$, then we can associate the Riemann sum
%
\[ R(\mathcal{S}, f) = \sum_S |S| f(a_S). \]
%
If we make the sets $\{ \mathcal{S} \}$ into a net by partial ordering the partitions by refinement, then under these conditions, we can verify that the Riemann sums will converge in $X$ to a vector $x$. If $x^* \in X^*$, then $x^*(x)$ is a limit of the quantities
%
\[ \sum_S |S| \cdot x^*(f(a_S)) \]
%
which are seen as Riemann sums of $\int x^* \circ f$. Thus the Riemann sums converge to the Pettis integral of $f$.

A version of the fundamental theorem of calculus holds in this scheme. Namely, if $\Omega$ is a compact subset of $\RR^n$, $f: [a,b] \to X$ is weakly measurable, and there exists a continuous function $g: [a,b] \to X$ such that for any linear function $x^* \in X^*$, $x^* \circ g$ is differentiable with derivative $x^* \circ f$, then
%
\[ \int_\Omega f(x)\; dx = g(b) - g(a). \]
%
Indeed, the left hand side is well defined as a Pettis integral, and we can apply the scalar-valued fundamental theorem of calculus to conclude that for any $x^* \in X^*$,
%
\[ \int_\Omega (x^* \circ f)(x)\; dx = x^*(g(b)) - x^*(g(a)). \]
%
Of course, this also implies that we have a theory of integration by parts in this domain.

\section{The Bochner Integral}

\begin{remark}
    This Section is adapted from Piotr Mikusinski's paper ``Integrals with Values in Banach Spaces and Locally Convex Spaces''.
\end{remark}

The Bochner integral proceeds along similar lines to the Lebesgue integral, ala the dominated convergence theorem. We start by working with Banach spaces. If $\Omega$ is a measure space, then a \emph{simple} function $f: \Omega \to X$ will be one of the form
%
\[ f = x_1 \mathbf{I}_{E_1} + \dots + x_n \mathbf{I}_{E_n} \]
%
where $E_1,\dots,E_n \subset \Omega$ are finite measure sets, and $x_1,\dots,x_n \in X$. If $X$ is a Banach space, then a function $f: \Omega \to X$ will be called \emph{Bochner measurable} if there exists a sequence of simple functions $\{ f_n \}$ such that $f_n(\omega) \to f(\omega)$ in $X$ for almost every $\omega \in \Omega$. As with weakly measurable functions, the Bochner measurable functions are a vector space closed under almost everywhere pointwise limits. The usual proof of Egorov's theorem also carries through if $\Omega$ is a finite measure space.

\begin{theorem}[Pettis]
    Let $X$ be a Banach space, and $\Omega$ a $\sigma$-finite measure space. A function $f: \Omega \to X$ is Bochner measurable if and only if it is weakly measurable, and the essential range of $f$ is separable, i.e. there exists $E \subset \Omega$ with $|E| = 0$ such that $f(\Omega - E)$ is a separbale subset of $X$.
\end{theorem}
\begin{proof}
    Assume that $\Omega$ is finite, since the general case can be understood by taking limits.

    Suppose $f$ is Bochner measurable. Then there exists a family of simple functions $\{ f_n \}$ converging pointwise almost everywhere to $f$, which means by Egoroff's theorem, we can find a sequence of sets $\{ E_n \}$ in $\Omega$ such that $|\Omega - E_n| < 1/n$, and $\{ f_n \}$ converges uniformly to $f$ on $E_n$. Each of the functions $f_n$ has finite dimensional, bounded range. This means that $f(E_n)$ is totally bounded, and thus separable. But this means that $\bigcup_n f(E_n)$ is separable, and $\Omega - \bigcup E_n$ has measure zero.

    To see $f$ is weakly measurable, we note that since $f_n(\omega) \to f(\omega)$ pointwise almost everywhere, the same is true for $x^* \circ f_n$ and $x^* \circ f$, for any $x^* \in X^*$, which implies that $x^* \circ f$ is measurable since each of the functions $x^* \circ f_n$ is simple.

    Conversely, suppose $f$ is weakly measurable and has separable essential range. Find $E$ with $|\Omega - E| = 0$, and such that $f(E)$ is separable. Let $\{ x_n \}$ be a countable, dense subset of $f(E)$. Find $\{ x_n^* \}$ in $X^*$ such that $x_n^*(x_n) = \| x_n \|$ and $\| x_n^* \| = 1$. Then
    %
    \[ \| f(\omega) \| = \sup_n | x_n^*(f(\omega)) | \]
    %
    for each $\omega \in \Omega$. This shows that $\| f \|$ is a measurable function. Similarily, so too are the functions $g_n(\omega) = \| f(\omega) - x_n \|$. The sets
    %
    \[ E_{n,\varepsilon} = \{ \omega: g_n(\omega) < \varepsilon \} \]
    %
    are all measurable in the completion of $\Omega$, and cover $\Omega$. Thus we can find measurable sets $B_{n,\varepsilon}$ such that $B_{n,\varepsilon} \Delta E_{n,\varepsilon}$ has measure zero in the completion of $\Omega$. If we define a countably valued function $g_\varepsilon$, with $g_\varepsilon(\omega) = x_n$, where $n$ is the smallest integer such that $\omega \in E_{n,\varepsilon}$, then we have $\| g_\varepsilon(\omega) - f(\omega) \| < \varepsilon$ for almost every $\omega$. Thus we can approximate $f$ by a countably valued function. It is now a simple matter to use the family of countably valued functions to find a family of simple functions converging almost everywhere to $f$.
\end{proof}

\begin{remark}
    We have actually shown the stronger result that a function $f$ is Bochner measurable on a $\sigma$ finite measure space if and only if there is a sequence of countably valued functions $\{ f_n \}$ converging almost uniformly to $f$.
\end{remark}

There are, on the other hand, weakly measureable functions that are not Bochner measurable. For instance, if $H$ is a non separable Hilbert space with a basis $\{ e_t : t \in [0,1] \}$, and we define $f: [0,1] \to H$ by setting $f(t) = e_t$, then $x^* \circ f = 0$ almost everywhere for any $x^* \in H^*$, so that $f$ is weakly measurable. On the other hand, $f([0,1] - E)$ is separable if and only if $[0,1] - E$ is countable, and there is no set $E$ of measure zero such that $[0,1] - E$ is countable, so the essential support of $f$ is not separable.

A Bochner measurable function $f: \Omega \to X$ is \emph{integrable} if there are a sequence of simple functions $\{ f_n \}$ such that
%
\[ \lim_n \int_\Omega \| f_n - f \| = 0. \]
%
The family of all such functions is denoted $L^1(\Omega,X)$. In this case, we define the \emph{Bochner integral} of $f$ to be
%
\[ \int_\Omega f = \lim_n \int_\Omega f_n, \]
%
where the integral of a step function is defined in the obvious way. It is simple to verify that in this case, the integral is independent of the choice of step functions used.

\begin{theorem}
    A Bochner measurable function $f$ on a $\sigma$ finite measure space $\Omega$ is Bochner integrable if and only if
    %
    \[ \int \| f \| < \infty. \]
\end{theorem}
\begin{proof}
    If $\{ f_n \}$ exist such that
    %
    \[ \lim_n \int_\Omega \| f_n - f \| = 0, \]
    %
    then the triangle inequality implies that
    %
    \[ \int \| f \| \leq \int \| f - f_n \| + \int \| f_n \|, \]
    %
    and the right hand side is finite for suitably large $n$.

    Conversely, suppose $f$ is Bochner measurable. Assume that $\Omega$ is finite and $\int \| f \| < \infty$, then, because of the remark above, we can find a sequence of countably valued functions $\{ f_n \}$ such that $\| f - f_n \| < 1/n$. This means that $f_n$ is integrable, since
    %
    \[ \int \| f_n \| \leq \int \| f \| + \int \| f - f_n \| < \infty. \]
    %
    We can then cutoff the functions $\{ f_n \}$ so that they are simple, but we still have
    %
    \[ \lim_{n \to \infty} \int \| f_n - f \| = 0. \]
    %
    The $\sigma$ finite case then follows by an approximation argument.
\end{proof}

The Bochner integral possesses a version of the dominated convergence theorem, namely, if $\{ f_n \}$ is a sequence of integrable functions converging in measure to a function $f$, and if there exists a scalar-valued integrable function $g$ such that $|f_n| \leq g$ for all $n$, then $f$ is Bochner integrable, and
%
\[ \int f = \lim_n \int f_n. \]
%
In fact, we have
%
\[ \lim_{n \to \infty} \int \| f - f_n \| = 0 \]
%
by the scalar dominated convergence theorem, and then we use the inequality
%
\[ \left\| \int g \right\| \leq \int \| g \|, \]
%
valid for any $g \in L^1(\Omega,X)$.

If $f$ is integrable, then for any disjoint sets $\{ E_n \}$ with union $E$, we have
%
\[ \int_E f = \sum_{n = 1}^\infty \int_{E_n} f, \]
%
where the right hand side converges absolutely. Thus the function $F(E) = \int_E f$ is a vector valued measure. It has bounded variation, and $|F|(E) = \int_E \| f \|$. TODO Add notes on vector valued measures. This means that if $f,g \in L^1(\Omega,X)$, and $\int_E f = \int_E g$ for all $E$, then $f = g$ almost everywhere, since then if $F(E) = \int_E (f - g)$, then $0 = |F|(\Omega) = \int_\Omega \| f - g \|$.

The Bochner integral of a Bochner integrable function is a weak integral of the function $f$, as the next result shows, due to Hille.

\begin{theorem}
    Let $T: X \to Y$ be a closed operator between two Banach spaces. If $f \in L^1(\Omega,X)$ and $Tf \in L^1(\Omega,Y)$, then
    %
    \[ T \left( \int_E f \right) = \int_E Tf. \]
    %
    In particular, $\int_E f$ is the weak integral of $f$.
\end{theorem}
\begin{proof}
    TODO: Theorem 6 of Diestel, Chapter 2.
\end{proof}

Now we define the Bochner integral for a complete locally convex space $X$. Given a continuous seminorm $\rho$ on $X$ and a simple function $f$, we define
%
\[ \rho(f) = \int_\Omega \rho \circ f. \]
%
A function $f: \Omega \to X$ will be called \emph{integrable} if there exists a sequence of simple functions $\{ f_n \}$ such that for any continuous seminorm $\rho$ on $X$,
%
\[ \sum_{n = 1}^\infty \rho(f_n) < \infty \]
%
and for any $\omega \in \Omega$ with $\sum_{n = 1}^\infty \rho(f_n(\omega)) < \infty$,
%
\[ \lim_{N \to \infty} \rho \left( f(\omega) - \sum_{n = 1}^N f_n(\omega) \right) = 0. \]
%
In particular, the former condition implies that this condition will hold for almost every $\omega \in \Omega$. The space of integrable functions will be denoted $L^1(\Omega,X)$. By the dominated convergence theorem, we clearly want to define
%
\[ \int f = \sum_{n = 1}^\infty \int f_n \]
%
The right hand converging unconditionally in any of the continuous seminorms $\rho$. The show this is well defined, we just reduce to the study of Banach spaces. Since $X$ is complete, the space $X_\rho = X / \text{Ker}(\rho)$ is a Banach space equipped with the norm $\rho$. If $\pi_\rho: X \to X_\rho$ is the quotient map, then it is clear that if $\pi_\rho$ maps $L^1(\Omega,X)$ to $L^1(\Omega,X_\rho)$. It follows that the sum defining $\int f$ is defined independently of the choice of functions $\{ f_n \}$ we choose, because it's projection under the quotient maps $\pi_\rho$, and these quotients separates $X$. Much of the same properties above hold here. In particular, if $f \in L^1(\Omega,X)$, then $\int f$ will be a weak integral for $X$.

TODO: Fix later sections.

\section{The Riesz Calculus}

\subsection{Vector-valued integration}

\begin{theorem}
    If $X$ is a Banach space, $K \subset X$ is compact, and $\overline{H} = \overline{\text{co}}(K)$, then for each $x \in \overline{H}$ there is a regular Borel probability measure $\mu$ for which $x = \int \text{id}_K d\mu$.
\end{theorem}
\begin{proof}
    The Riesz representation theorem identifies $C(K)^*$ with the space $R$ of all Borel probability measures on $K$. Define $\phi: C(K)^* \to X$ by $\phi(\mu) = \int \text{id}_K d\mu$. The theorem says that $\phi(R) = \overline{H}$. Certainly $K \subset \phi(R)$, for if $\delta_x$ is the unit measure at $x \in K$, then $\phi(\delta_x) = x$, for $\Lambda x = \int \Lambda d \delta_x$. Since $R$ is convex, $\phi(R)$ is convex, and we know from the last theorem that it is a subset of $\overline{H}$. Thus all we need verify is that $\phi(R)$ is closed. This is a consequence of the fact that $R$ is weak $*$ compact in $C(Q)^*$, and $\phi$ is weak $*$ continuous into the weak topology on $X$, so $\phi(R)$ is weakly closed, and all weakly closed sets are strongly closed.

    To see that $R$ is weak $*$ compact, notice that each probability measure has unit variation, and thus lies within the unit ball of $C(K)^*$, which is weak $*$ compact. For $f \in C(K)^*$, let $E_f = \{ \mu : \int f d\mu \geq 0 \}$. Then each $E_f$ is weak $*$ closed, since the map $f \mapsto f(x)$ is weak $*$ continuous. Hence $\bigcap E_f$ is weak $*$ closed. Similarily, the set $E = \{ \mu : \int 1 d\mu = 1 \}$ is weak $*$ closed, and $E \cap \bigcap E_f = R$ is therefore weak $*$ closed.

    It is sufficient to show $\phi$ is weak $*$ continuous at the origin. $X$ has a weak subbasis of neighbourhoods at the origin of the form
    %
    \[ W = \{ x \in X : |\Lambda(x)| < r \} \]
    %
    for $r > 0$, $\Lambda \in X^*$. Thus we must verify that the set of $\mu$ for which $\left| \int \Lambda d\mu \right| < r$ is weak $*$ open. But $\Lambda|_K \in C(K)$, so this set is open almost by definition. Thus $\phi$ is weak $*$ continuous.
\end{proof}

\begin{theorem}
    If $\Omega$ is a compact Hausdorff space with a positive Borel measure $\mu$, $X$ is a Banach space, and $f: \Omega \to X$ is continuous, then
    %
    \[ \left\| \int f d\mu \right\| \leq \int \| f \| d\mu \]
\end{theorem}
\begin{proof}
    For each $\Lambda \in X^*$,
    %
    \[ \left| \Lambda\left(\int f d\mu\right) \right| = \left| \int \Lambda f d\mu \right| \leq \int | \Lambda f | d\mu \leq \| \Lambda \| \int | f | d\mu \]
    %
    If $\Lambda$ is chosen (by the Hahn Banach theorem) such that $\Lambda(\int f d\mu) = \| \int f d\mu \|$, and such that $\| \Lambda \| = 1$, then we obtain the needed inequality.
\end{proof}

If $A$ is a Banach algebra, then $\int f d\mu$ has an additional property, namely, for each $M \in A$,
%
\[ M \int f d\mu = \int Mf d\mu\ \ \ \ \ \ \ \ \ \ \left(\int f d\mu\right) M = \int fM d\mu \]
%
This follows since multiplication on the left and right by $x$ is a bounded linear function. If we let $L: N \mapsto MN$ be the left multiplication operator, and fix $\Lambda \in X^*$, then
%
\[ \Lambda \left( M \left( \int f d\mu \right) \right) = (\Lambda \circ L) \left( \int f d\mu \right) = \int (\Lambda \circ L) f d\mu = \int \Lambda(Mf) d\mu \]
%
the right case is similar.


\newpage

 A \emph{weakly analytic function} is $f: \mathbf{C} \to A$ for which $\langle \phi, f \rangle$ is analytic for any choice of $\phi \in A^*$. It is clear that all strongly analytic functions are weakly analytic, which follows because
%
\[ \lim_{w \to 0} \phi \left( \frac{f(z + w) - f(z)}{w} \right) = \phi \left( \lim_{w \to 0} \frac{f(z + w) - f(z)}{w} \right) \]
%
For fun, we shall also prove that all weakly analytic functions are strongly analytic.

\begin{theorem}
    Every weakly analytic function $f: D \to X$ is strongly analytic.
\end{theorem}
\begin{proof}
    Fix $\phi \in X^*$. Consider a particular contour winding counterclockwise around a point $w$ in the domain, which is at least a unit distance away from $w$ at any point on the contour. If $h,k \in \mathbf{C}$ are small enough that $w + h$ and $w + k$ are contained within the contour, then by the Cauchy integral theorem,
    %
    \begin{align*}
        &\phi\left( \frac{1}{h-k} \left[ \frac{f(w + h) - f(w)}{h} - \frac{f(w + k) - f(w)}{k} \right] \right)\\
        &\ \ \ \ \ =  \phi \left( \frac{f(w)}{hk} + \frac{f(w + h)}{(h - k)h} - \frac{f(w + k)]}{(h - k)k} \right)\\
        &\ \ \ \ \ = \frac{1}{2\pi i} \int_C \frac{\phi[f(z)]\ dz}{[z - (w + h)][z - (w + k)][z - w]}
    \end{align*}
    %
    Find $\delta$ such that if $\| h \| < \delta$, the distance between any point on $C$ and $w + h$ is greater than $1/2$. Then, if $M$ is the length of $C$, and $K$ is the supremum of $f$ on $C$, then
    %
    \[ \left| \frac{1}{2\pi i} \int_C \frac{\phi[f(z)]\ dz}{[z - (w + h)][z - (w + k)][z - w]}\right| \leq \frac{4MK}{2 \pi} \| \phi \| = \frac{2MK}{\pi} \| \phi \| \]
    %
    Applying the Banach Steinhaus theorem (on $X^*$, viewing elements of $X$ as elements of $X^{**}$), we conclude that for all $h,k$ sufficiently small, there exists $K$ such that
    %
    \[ \left| \frac{f(w + h) - f(w)}{h} - \frac{f(w + k) - f(k)}{k} \right| \leq K |h - k| \]
    %
    By the completeness of $X$, the quotients of $h$ and $k$ converge to a well defined quantity as $h - k$ converges to zero.
\end{proof}











\part{Advanced Operator Theory}





\chapter{Banach Algebras}

In this chapter, we develop the machinery to study the various classes of bounded operators which occur when classifying transformations on a space. To do this, we employ tools from abstract algebra. Recall that an algebra over a field $k$ is a (not necessarily unital) ring $A$ together with a fixed embedding of $k$ into $A$, which gives $A$ a vector space structure. A consistant norm attached to an algebra is a nice situation to study operators over a Banach space. What's more, the general theory gives light to many other circumstance  which would not have been apparent were we just analyzing concrete operators. We shall use capital letters, like $M$ and $N$, to denote abstract elements of these algebras, and we denote algebras by capital letters near the beginning of the alphabet, such as $A$ or $B$. A \emph{Banach Algebra} is a Banach space $A$ which is also an algebra, and satisfies $\| MN \| \leq \| M \| \| N \|$ for all $M,N \in A$. A \emph{unital Banach Algebra} is a Banach algebra with an identity $1 \neq 0$, which we will assume also satisfies $\| 1 \| = 1$. Thus multiplication is a continuous operation, and the norm is normalized to give the multiplicative identity unit norm. We shall pretty much only consider Banach algebras over the complex numbers, because here we can apply the unique properties of complex analysis.

\begin{example}
    Let $X$ be a locally compact topological space. Then the space $C_b(X)$ of continuous, bounded functions on $X$ forms a unital commutative Banach algebra under the pointwise product, and equipped with the $L^\infty$ norm. The space $C_0(X)$ of continuous functions vanishing at infinity forms a subalgebra of $C_b(X)$, and is unital precisely when $X$ is compact, in which case $C_0(X) = C_b(X) = C(X)$. Regardless, we can add an identity by consider the family of all functions of the form
    %
    \[ C_0(X)^\# = \{ \lambda + f : \lambda \in \CC, f \in C_0(X) \}, \]
    %
    which can be viewed as the space of all continuous functions which converge to a common value at $\infty$, which can thus be naturally identified with $C(X \cup \{ \infty \})$, where $X \cup \{ \infty \}$ is the one point compactification of $X$.

    As a simple example, if $X = \{ x_1, \dots, x_n \}$ is a discrete topological space, then $C(K) \cong \CC^n$, where the right hand side is a Banach algebra if we equip it with the standard norm, and the pointwise product, since the Cauchy-Schwartz inequality then tells us that $\| xy \| \leq \| x \| \| y \|$ for all $x,y \in \CC^n$.
\end{example}

\begin{example}
    The space $L^\infty(X)$ of essentially bounded functions on a measure space $X$ is a Banach algebra, and it is unital except in the trivial case where $|X| = 0$, in which case $L^\infty(X) = 0$.
\end{example}

\begin{example}
    If $\Omega$ is a compact set in $\CC$, then we define $A(\Omega)$ to be the subset of $C(\Omega)$ consisting of all functions which are analytic on the interior of $\Omega$. Since uniform convergence preserves holomorphicity, $A(\Omega)$ is a closed subalgebra of $C(\Omega)$, and is therefore a Banach algebra. $A(\mathbf{D})$ is known as the \emph{disk algebra}.
\end{example}

\begin{example}
    Let $X$ be a Banach space. Then $X$ has the structure of a somewhat trivial, commutative Banach algebra if we define $x_1 x_2 = 0$ for any $x_1,x_2 \in X$. This is often a useful family of spaces to construct counterexamples, or to show why certain assumptions are necessary in Banach algebra theory.
\end{example}

A Banach algebra $A$ is \emph{generated} by a set $S \subset A$ if there is no proper closed subalgebra of $A$ containing $S$. In this language, the Stone-Weirstrass theorem says that, for any locally compact Hausdorff space $X$, a family of functions $S \subset C_0(X)$ generates $C_0(X)$ if and only if $S$ separates all points in $X$, $S$ has no common zero set in $X$, and if we are working with complex-valued functions, $S$ is closed under conjugation.

\begin{example}
    Let $G$ be a locally compact group, with Haar measure $\mu$, and consider the space of functions $L^1(G)$, where multiplication is convolution,
    %
    \[ (f * g)(x) = \int f(y) g(y^{-1}x) d\mu(y) \]
    %
    One can then verify that
    %
    \[ \| f * g \|_{L^1(G)} \leq \| f \|_{L^1(G)} \| g \|_{L^1(G)}. \]
    %
    Thus $L^1(G)$ is a Banach algebra, commutative when $G$ is commutative. It does not always possess a unit, e.g. $L^1(\RR)$ is non-unital, but $L^1(\ZZ)$ is unital. But we can always enlarge the space so it becomes a unital Banach algebra. Let $M(G)$ be the space of complex-valued finite Borel measures on $G$, and define convolution on $M(G)$ by letting
    %
    \[ \int f d (\eta * \nu) = \int \int f(xy) d\eta(x) d\nu(y), \]
    %
    then $M(G)$ forms a unital Banach algebra, and the Dirac delta function $\delta_0$ at the identity element of $G$ is then a unit for $M(G)$. Thus one can consider the set
    %
    \[ L^1(G)^\# = \{ \delta_0 + f : f \in L^1(G) \}, \]
    %
    which is the smallest unital Banach algebra containing $L^1(G)$ in $M(G)$.

    The space $M(\ZZ) = L^1(\ZZ)$ is a fairly concrete example. It can be taken as the space of integer valued sequences $c = \{ c(n) \}$ (where $\sum |c(n)| < \infty$), with convolution
    %
    \[ (a * b)(n) = \sum_{k \in \mathbf{Z}} a(k) b(n-k) \]
    %
    The Dirac delta function here is just the sequence $\delta_0$, where
    %
    \[ \delta_i(k) = \begin{cases} 1 & k = i \\ 0 & \text{elsewise} \end{cases} \]
    %
    Since $\delta_i = (\delta^1)^i$, $L^1(\ZZ)$ is generated by $\delta^1$ and it's inverse.
\end{example}

One of the prime reasons to study Banach algebras is to study spaces of operators on a Banach space, and this is our primary example of a non-commutative Banach algebra.

\begin{example}
    Let $E$ be a Banach space. The space $B(E)$ of all bounded linear operators from $E$ to itself is a Banach algebra with respect to the operator norm. It is a unital algebra, since it possesses the identity operator. The subset $K(E)$ of compact linear operators is a closed (double-sided) ideal of $B(E)$, and so is also a Banach algebra. $K(E)$ is unital if and only if $E$ is finite dimensional.
\end{example}

Really, all that distinguishes a Banach space from a Banach algebra is a continuous multiplication structure, modulo the norm we use to define the topology of the space.

\begin{prop}
    Let $X$ be a Banach space equipped with a continuous multiplication structure. Then there is an equivalent norm on $X$ which makes the space into a Banach algebra.
\end{prop}
\begin{proof}
    Embed $X$ in $B(X)$ by defining $T_M(N) = MN$ (since multiplication on the right is continuous, $T_M$ truly is in $B(X)$ rather than just being a linear map). If $M_i \to M$, and $T_{M_i} \to T$, then we claim $T_M = T$. Indeed, since multiplication is continuous on the left,
    %
    \[ T(N) = \lim_{i \to \infty} T_{M_i}(N) = \lim_{i \to \infty} M_i N = MN = T_M(N). \]
    %
    Thus the closed graph theorem implies that $M \to T_M$ is a continuous embedding of $X$ in $B(X)$. If we define a norm $\| \cdot \|_A$ on $X$ by setting $\| M \|_A = \| T_M \|$, then it follows that $\| M \|_A \sim \| M \|$, and $\| MN \|_A \leq \| M \|_A \| N \|_A$, so $X$ has a Banach algebra structure with the norm $\| \cdot \|_A$.
\end{proof}

The class of non-unital Banach algebras is not often that different from the class of unital Banach algebras. If $A$ is a Banach algebra without unit, then we can always consider it's unitization
%
\[ A^\# = \{ x + \lambda : x \in A, \lambda \in K \} \]
%
which will also be a Banach algebra by setting
%
\[ (x_1 + \lambda_1)(x_2 + \lambda_2) = (x_1x_2 + \lambda_1 x_2 + \lambda_2 x_1) + \lambda_1 \lambda_2 \]
%
and defining
%
\[ \| x + \lambda \| = \| x \| + |\lambda|. \]
%
A statement is often true about $A$ if and only if it is true about $A^\#$, so that it suffices to solely study unital Banach algebras.

As we saw in the examples above, if $A = C_0(X)$, then we can identify $A^\#$ with $C(X \cup \{ \infty \})$, where $X \cup \{ \infty \}$ is the one-point compactification of $X$. There is a general school of Banach algebra theory known as \emph{non commutative topology}, which studies Banach algebras $A$ heuristically as if they were rings of continuous functions on some `non-commutative space'. Thus one can think of the unitization of a non-unital algebra as the `one-point compactification' of this underlying non commutative space.

\section{Involutions}

An \emph{involution} on a Banach algebra $A$ is an antilinear map $M \mapsto M^*$, which satisfies
%
\[ M^{**} = M\ \ \ \ \ (MN)^* = N^*M^*. \]
%
One can think of an involution as the generalization of the conjugation operator $z \mapsto \overline{z}$ on $\CC$, or of taking the adjoint of an operator on a Hilbert space. A \emph{Banach $*$ Algebra} is a Banach algebra with a fixed involution. A \emph{$C^*$ algebra} is a Banach $*$ algebra $A$ which satisfies
%
\[ \| M^* M \| = \| M \|^2 \]
%
for all $M \in A$.

An arbitrary involution on a Banach algebra can be rather discontinuous, which makes the general study of Banach $*$ algebras not too interesting. But most involutions in Banach $*$ algebra theory turn out to be continuous, in particular, often being isometries, i.e. satisfying $\| M^* \| = \| M \|$. This is true of any $C^*$ algebra $A$, since if $M \in A$, then
%
\[ \| M \|^2 = \| M^* M \| \leq \| M^* \| \| M \| \]
%
implies $\| M \| \leq \| M^* \|$, and
%
\[ \| M^* \|^2 = \| M M^* \| \leq \| M^* \| \| M \| \]
%
implies $\| M^* \| \leq \| M \|$. The next result (a curiosity that can be ignored on a first reading) shows that any involution on a commutative, semisimple Banach algebra is continuous (an algebra $A$ is semisimple if the intersection of all maximal ideals is trivial).

\begin{prop}
    Let $A$ be a commutative, semisimple Banach algebra. Then every involution on $A$ is continuous.
\end{prop}
\begin{proof}
    We rely on a fact (to be proved later), that every algebra homomorphism $\phi: A \to K$ is \emph{automatically continuous}. The semisimplicity of $A$ implies that for any $M_1,M_2 \in A$, $M_1 = M_2$ if and only if for every such algebra homomorphism $\phi: A \to K$, $\phi(M_1) = \phi(M_2)$. Now if $T: A \to A$ is an involution on $A$, and $\phi: A \to K$ is a homomorphism, then $\overline{\phi \circ T}: A \to K$ is also an algebra homomorphism, and thus continuous. Thus if $M_i \to M$, and $TM_i \to N$, then the continuity of $\overline{\phi \circ T}$ implies that
    %
    \[ \overline{\phi(TM)} = \overline{\phi(N)}, \]
    %
    i.e. $\phi(TM) = \phi(N)$. Thus $TM - N$ is in the kernel of every character $\phi$. Since $A$ is semisimple, this implies that $TM = N$. Thus by the closed graph theorem, $T$ is continuous.
\end{proof}

\begin{remark}
    A similar proof shows that every homomorphism $T: A \to B$ from a Banach algebra $A$ into a semisimple commutative Banach algebra $B$ is continuous.
\end{remark}

\begin{example}
    Let $H$ be a Hilbert space. Then for any bounded operator $T: H \to H$, we define the adjoint operator $T^*: H \to H$ by the equation
    %
    \[ \langle Tx, y \rangle = \langle x, T^* y \rangle. \]
    %
    Now if $\| x \|, \| y \| \leq 1$, then
    %
    \[ | \langle T^* T x, y \rangle | = | \langle Tx, Ty \rangle | \leq \| T \|^2 \]
    %
    and this is attained, for if there are $\| x_i \| \leq 1$ such that $\| Tx_i \| \to \| T \|$, then
    %
    \[ \langle T^* T x_i, x_i \rangle = \| Tx_i \|^2 \to \| T \|^2. \]
    %
    Thus $\| T^* T \| = \| T \|^2$, and so $B(H)$ is a $C^*$ algebra. If we are working over $\mathbf{C}^n$, then each element of $B(\CC^n)$ can be expressed as a matrix in $M_n(\mathbf{C})$, and the involution in $B(\mathbf{C}^n)$ is then equivalent to taking the complex transpose on sets of matrices, which gives a finite dimensional example. In particular, $M_n(\CC)$ is also a $C^*$ algebra.
\end{example}

\begin{example}
    Other examples of $C^*$ algebras can be obtained by taking closed involutive subalgebras of $B(H)$. For instance, the algebra $K(H)$ of \emph{compact operators} on a Hilbert space forms a $C^*$ algebra. Another example, given a family of operators $S \subset B(H)$, is the space $C^*(S)$, which is the smallest closed involutive unital subalgebra of $B(H)$ containing $S$. As a special case, for an operator $T \in B(H)$, is the algebra $C^*(T)$, which can be seen as the closure of non-commutative polynomial expressions in $T$ and $T^*$.
\end{example}

\begin{example}
    The pointwise conjugation map $f \mapsto \overline{f}$ is an involution on $L^\infty(X)$, and for any $f \in L^\infty(X)$,
    %
    \[ \| f \overline{f} \|_\infty = \| |f|^2 \|_\infty = \| f \|^2_\infty. \]
    %
    Thus $L^\infty(X)$ is a $C^*$ algebra. Similarily, $C_0(X)$ and $C_b(X)$ are $C^*$ algebras with the same involution. The central result of the study of commutative $C^*$ algebras will show that all such algebras are isometric to $C_0(X)$ for some $X$.
\end{example}

\begin{example}
    For $f \in A(\mathbf{D})$, if we define $f^*(z) = \overline{f(\overline{z})}$, then $*$ is an isometric involution on $A(\mathbf{D})$. But $A(\mathbf{D})$ is \emph{not} a $C^*$ algebra with this involution. To show this, let $f(z) = e^{iz}$. Then
    %
    \[ (ff^*)(z) = e^{iz} \overline{e^{i\overline{z}}} = e^{iz} e^{-iz} = 1 \]
    %
    Thus $\| f f^* \|_\infty = 1$, yet $f(-i) = f^*(i) = e > 1$, so
    %
    \[ \| f \|_\infty \| f^* \|_\infty > \| ff^* \|_\infty. \]
\end{example}

\begin{example}
    For any locally compact group $G$, the convolution algebra $L^1(G)$ has an involution $f^*(g) = \overline{f(g^{-1})}$. It also has the involution $\overline{f}(g) = \overline{f(g)}$. The first one turns out to be much more important to the theory of harmonic analysis on such groups. But neither give $L^1(G)$ the structure of a $C^*$ algebra. As a characteristic example, if $G = \ZZ$, if $f = \delta_1 + \delta_0 - \delta_{-1}$, then
    %
    \[ \overline{f} * f = f * f = \delta_2 + 2 \delta_1 - \delta_0 - 2 \delta_{-1} + \delta_{-2} \]
    %
    and
    %
    \[ f * f^* = - \delta_2 + 3 \delta_0 - \delta{-2}. \]
    %
    Thus $\| f \|_{L^1(\ZZ)} = 3$, whereas $\| \overline{f} * f \|_{L^1(\ZZ)} = 7$ and $\| f * f^* \|_{L^1(G)} = 5$, and neither of these quantities are equal to $\| f \|_{L^1(\ZZ)}^2 = 9$. Similar examples can be constructed on general locally compact groups by using bump functions.
\end{example}

The next result is clearly not true in an arbitrary non-unital Banach algebra (e.g. if $MN = 0$ for all $M$ and $N$).

\begin{prop}
    Let $A$ be a $C^*$ algebra. Then for any $M \in A$,
    %
    \[ \| M \| = \sup \{ \| MN \| : \| N \| \leq 1 \} = \sup \{ \| NM \| : \| N \| \leq 1 \}. \]
\end{prop}
\begin{proof}
    If $N = M^* \|M\|^{-1}$, $\| N \| = 1$, and
    %
    \[ \| MN \| = \| MM^* \| \| M \|^{-1} = \| M \| \]
    %
    Thus the suprema on the right is greater than or equal to $\| M \|$. But if $\| N \| \leq 1$, then
    %
    \[ \| MN \| \leq \| M \| \| N \| \leq \| M \| \]
    %
    so the suprema is also less than or equal to $\| M \|$.
\end{proof}

It follows that we may isometrically embed any $C^*$-algebra $A$ into $B(A)$, by either considering the left or right multiplications. This is known as the \emph{left} or \emph{right regular representation}. Thus if $A$ is a $C^*$ algebra without identity, then we may embed $A$ in $B(A)$, and then consider the smallest closed $C^*$-subalgebra of $B(A)$ which contains $A$ and the identity. The general representation theory of $C^*$ algebras, to be returned to at a later time. In particular, we will learn about the Gelfand Naimark Segal theorem, which gives rise to a universal representation of $A$ as a subalgebra of operators on a Hilbert space.

Given a non-unital Banach $*$ algebra $A$, there is a natural involution on $A^\#$ obtain by defining
%
\[ (\lambda + M)^* = \overline{\lambda} + M^* \]
%
If $A$ is a $C^*$ algebra, so is $A^\#$, provided that we give it the norm
%
\[ \| \lambda + M \| = \sup \{ \| (M + \lambda) N \| : N \in A, \| N \| \leq 1 \}, \]
%
i.e. the norm of $\lambda + M$ is the operator norm of $\lambda + M$ obtained by embedding $A^\#$ in $B(A)$ by the left regular representation. 
%We then calculate that
%
%\[ \| (\lambda + M)^* \| = \sup_N \| (\lambda + M)^* N \| = \sup_N \| N^* (\lambda + M) \| = \sup_N \| N (\lambda + M) \|. \]
%
%Now we have
%
%\[ \sup_N \| N (\lambda + M) \| \leq \| \lambda + M \|. \]
%
%Thus $\| (\lambda + M)^* \| \leq \| \lambda + M \|$. But now symmetry gives the opposite result, so that we have proved
%
%\[ \| (\lambda + M)^* \| = \| \lambda + M \|. \]
%
%Now to prove this gives a $C^*$ norm, we calculate that
%
%\[ \| (A + \lambda)^* (A + \lambda) \| \geq \| (A + \lambda)^* \| \| A + \lambda \| = \| A + \lambda \|^2. \]
%
%The converse follows because
%
%\[ \| (A + \lambda)^* (A + \lambda) N \| \leq \| (A + \lambda)^* \]
%
%\begin{align*}
%    \| (A + \lambda)^* (A + \lambda) \| &= \sup_{\| N \| \leq 1} (A^* A + \overline{\lambda} A + \lambda A + |\lambda|^2) N\\
%    &\geq \sup_{\| N \| \leq 1} N^* (A^* A + \overline{\lambda} A + \lambda A + |\lambda|^2) N\\
%    &= \sup_{\| N \| \leq 1} \| (AN + \lambda N)^* (AN + \lambda N) \|\\
%    &= \sup_{\| N \| \leq 1} \| (A + \lambda) N \|^2\\
%    &= \| A + \lambda \|^2.
%\end{align*}
%
%But the converse follows trivially since
%
%\[ (A + \lambda)^* (A + \lambda) \| \geq \| (A + \lambda)^* \| \| A + \lambda \| \]
%
The above proposition verifies that the embedding of $A$ in $A^\#$ is an isometry under this norm.

The theory of $C^*$ algebras is closely related to operator theory. The Gelfand Naimark Segal construction shows that all $C^*$ algebras are isometric to some $C^*$ subalgebra of bounded operators on a Hilbert space. Thus it makes sense to extend terminology from Hilbert space theory to the $C^*$ domain. We say $M \in A$ is \emph{self-adjoint} or \emph{hermitian} if $M^* = M$. The set of all such elements forms a real subalgebra of $A$, analogous to the inclusion of the real numbers in the complex numbers. Here is a result which provides further evidence that this analogy is apt.

\begin{prop}
    In any Banach $*$ algebra $A$, there is a unique way to write any $M \in A$ as $M = T + iS$, where $T$ and $S$ are self-adjoint elements of $A$.
\end{prop}
\begin{proof}
    Write
    %
    \[ T = \frac{M + M^*}{2}\ \ \ \ \ S = \frac{M - M^*}{2i} \]
    %
    Then $T + iS = M$, and they are trivially verified to be self-adjoint. Now suppose $M = N + iL$, where $N$ and $L$ are self-adjoint. Then
    %
    \[ 0 = (N - T) + i(L - S) \]
    %
    Taking adjoints on both sides thus gives
    %
    \[ (N - T) + i(L - S) = (N - T) - i(L - S) \]
    %
    And so $i(L - S) = -i(L - S)$, which can only occur when $L - S = 0$. But then $N - T = 0$ also.
\end{proof}

An element $M$ of a Banach $*$ algebra is \emph{normal} if
%
\[ M^* M = MM^* \]
%
The importance of this property results because if $M$ is normal, then the smallest involutive subalgebra of $A$ containing $M$ will be commutative. An element $M$ of a Banach $*$ algebra is \emph{unitary} if
%
\[ MM^* = M^*M = 1, \]
%
and the set of all such elements forms a closed subgroup $U(A)$ of $GL(A)$. A \emph{projection} in a Banach $*$ algebra is an element $P$ such that $P^2 = P$.






\chapter{Basic Spectral Theory of Banach Algebras}

We shall begin our study into Banach algebras by analyzing criteria for invertibility, which coincides with the spectral theory of these algebras. For obvious reasons, we restrict our attention to unital algebras, though one can extend all concepts involved to a non-unital algebra $A$ by embedding $A$ in $A^\#$. The set of all units in a unital algebra $A$ will be denoted $GL(A)$, and called the general linear group of $A$. The \emph{spectrum} and \emph{resolvent} of an element $M$ of a Banach algebra $A$ are defined respectively as
%
\[ \sigma_{A}(M) = \{ \lambda \in \mathbf{C} : \lambda - M \not \in GL(A) \} \]
%
and
%
\[ \rho_{A}(M) = \{ \lambda \in \mathbf{C} : \lambda - M \in GL(A) \} \]
%
The resolvent is the complement of the spectrum. When the underlying algebra is canonical, we just use $\sigma(M)$ and $\rho(M)$ to denote the spectrum. %One way to view the addition of a complex number to an operator as an `infinitisimal shift' in the effects of the operator. The spectrum tells us in which directions the operator goes bad under infinitisimal changes, and we shall find that knowledge of the spectrum will allow us to perturb operators more finely. A large number of theorems are based on relating these perturbations to the original operator.

\begin{example}
    Let $X$ be a compact topological space, and consider $f \in C(X)$. Then $\sigma(f) = f(X)$, because the function
    %
    \[ \frac{1}{\lambda - f(x)} \]
    %
    is well defined, bounded, and continuous if and only if $\lambda \not \in f(X)$. If $\Omega$ is a domain in $\CC$, then $\sigma_{A(\Omega)}(f) = \sigma_{C(\Omega)}(f)$, since under the assumptions of this example, the function $(\lambda - f(x))^{-1}$ will also be an analytic function. Similarily, if $X$ is a measure space, and $f \in L^\infty(X)$, then $\sigma(f) = \text{ess supp}(f)$, the \emph{essential support} of $f$.
\end{example}

\begin{example}
    Consider the algebra of $n \times n$ matrices $M_n$. An $n \times n$ matrix is invertible if and only if it is injective, so for an $n \times n$ matrix $M$, $\sigma(M)$ is \emph{precisely} the set of eigenvalues of $M$. On the other hand, in a Banach space $E$, it is no longer true that a bounded operator $T: E \to E$ is invertible if and only if it is injective. Thus the \emph{point spectra} $\sigma_p(T)$ of eigenvalues for $T$ is a proper subset of the spectrum $\sigma(T)$.
\end{example}

The next lemma is incredibly important, and is an extension of the power series formula
%
\[ \frac{1}{1 - z} = \sum_{k = 0}^\infty z^k \]
%
whose conclusion relies solely on the product and addition structure on the complex numbers, which all Banach algebras possess.

\begin{lemma}[Neumann Series]
    Let $A$ be a Banach algebra. If $\|M\| < 1$, then $1 - M \in GL(A)$, and
    %
    \[ (1 - M)^{-1} = \sum_{k = 0}^\infty M^k. \]
\end{lemma}
\begin{proof}
    The right side converges absolutely by the comparison test, since $\| M^k \| \leq \| M \|^k$, and $A$ is complete. This justifies the manipulation
    %
    \[ (1 - M) \sum_{k = 0}^\infty M^k = \sum_{k = 0}^\infty (1 - M)M^k = \sum_{k = 0}^\infty M^k - M^{k+1} = \lim_{n \to \infty} 1 - M^{n+1} \]
    %
    As $n \to \infty$, $M^{n+1} \to 0$, so the limit above tends to one. But $(1 - M)$ commutes with $\sum_{k = 0}^\infty M^k$ (work with limits), since $\sum_{k = 0}^\infty M^k$ is both a left and right inverse for $1 - M$.
\end{proof}

\begin{corollary}
    Let $A$ be a Banach algebra. If $M \in A$ and $\| 1 - M \| < 1$, then $M \in GL(A)$, and
    %
    \[ M^{-1} = \sum_{k = 0}^\infty (1 - M)^k \]
\end{corollary}

\begin{corollary}
    For any Banach algebra $A$, $GL(A)$ is an open subset of $A$.
\end{corollary}
\begin{proof}
    If $M \in GL(A)$, and if $\| M - N \| < 1/\| M^{-1} \|$, then
    %
    \[ \| 1 - M^{-1}N \| \leq \| M^{-1} \| \| M - N \|  < 1 \]
    %
    so $M^{-1}N \in GL(A)$, and thus $N \in GL(A)$.
\end{proof}

\begin{corollary}
    $\sigma(M)$ is a closed and bounded subset of $\mathbf{C}$, and $\rho(M)$ is open
\end{corollary}
\begin{proof}
    The map $f: \lambda \mapsto \lambda - M$ is a continuous operation, for
    %
    \[ \| (\lambda - M) - (\mu - M) \| = \| \lambda - \mu \| = | \lambda - \mu | \]
    %
    Since $GL(A)$ is open, $f^{-1}(GL(A)) = \rho(M)$ is open, hence $\sigma(M)$ is closed. If $|\lambda| > \|M\|$, then $\| M/\lambda \| < 1$, so $(1 - M/\lambda) \in GL(A)$, which means $\lambda - M$ is also invertible. Thus $\sigma(A)$ is closed and bounded, hence compact.
\end{proof}

If we define the \emph{spectral radius} $r(M)$ to be the maximum modulus of an element of $\sigma(M)$, then the last corollary proves that $r(M) \leq \| M \|$, though we need not have equality here (take a trivial Banach algebra $X$ and equip it with a unit, in which it is then true that $\sigma(x) = \{ 0 \}$ for each $x \in X$).

We shall see that the spectra of complex Banach algebras are never empty. This is why we mainly study complex algebras, rather than real algebras; there are even finite dimensional real operators with empty spectra.

\begin{example}
    Consider the matrix
    %
    \[ M = \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix} \]
    %
    as an element of the Banach algebra $M_2(\mathbf{R})$. The characteristic polynomial is calculated to be $\lambda^2 + 1$, so the matrix has no eigenvalues in the real numbers, and correspondingly, $\sigma(M) = \emptyset$. Over $M_2(\mathbf{C})$, we find $\sigma(M) = \{ i, -i \}$, so the spectrum is non-empty in the complex extension of the Banach algebra.
\end{example}

\begin{lemma}
    Inversion in an operator algebra $A$ is continuous.
\end{lemma}
\begin{proof}
    Let $(M_n)$ be a sequence in $GL(A)$ converging to an invertible element $M$. Then, by continuity, $M_nM^{-1} \to 1$. If $\| 1 - M_n M^{-1} \| < 1/2$, then
    %
    \[ M_n^{-1} M = (M^{-1}M_n)^{-1} = \sum_{k = 0}^\infty (1 - M^{-1}M_n)^k \]
    %
    It follows that
    %
    \begin{align*}
        \| M_n^{-1} \| &\leq \| M^{-1} \| \| M_n^{-1} M \| \leq \| M^{-1} \| \sum_{k = 0}^\infty \| (1 - M^{-1} M_n)^k \|\\
        &\leq \| M^{-1} \| \sum 2^{-k} = 2 \| M^{-1} \|
    \end{align*}
    %
    Finally, we obtain convergence of inverses,
    %
    \[ \| M_n^{-1} - M^{-1} \| = \| M_n^{-1} (M - M_n) M^{-1} \| \leq \| M_n^{-1} \| \| M - M_n \| \| M^{-1} \| \]
    %
    which tends to zero, since the first and third values are bounded, and the second converges to zero. Thus inversion is continuous.
\end{proof}

The invertible elements $GL(A)$ cannot form a closed set of $A$, for $A$ is path-connected, so there is a sequence of invertible elements $x_i$ which converge to a non-invertible element. Fortunately, Banach algebras have a way of telling us when things are about to go wrong; the norm of the operator blows up. If $A$ was an algebra of operators on a Banach space, one could prove this result from the uniform boundedness principle, and despite the fact we could prove the general result by embedding any Banach algebra into a space of operators of this form by taking an equivalent norm, we work more abstractly.

\begin{lemma}
    Let $A$ be a Banach algebra containing a convergent sequence $M_i \to M$, such that $M_i \in GL(A)$ for each $i$, and $M \not \in GL(A)$. Then $\| M_i^{-1} \| \to \infty$.
\end{lemma}
\begin{proof}
    If $\| M_n^{-1} \| \leq C$ for all $n$, then
    %
    \[ \| 1 - M_n^{-1} M \| \leq \| M_n^{-1} \| \| M_n - M \| \leq C \| M_n - M \|. \]
    %
    If we choose $n$ large enough that $\| M_n - M \| < 1/C$, then $\| 1 - M_n^{-1} M \| < 1$, which means $M_n^{-1} M \in GL(A)$, and thus $M \in GL(A)$.
\end{proof}

\begin{theorem}
    If $B$ is a closed subalgebra of $A$, and $M \in GL(B)$, then the entire connected component of $M$ in $B \cap GL(A)$ is contained in $GL(B)$.
\end{theorem}
\begin{proof}
    $GL(A)$ is open, so $B \cap GL(A)$ is open in $B$. So too is $GL(B)$. If $M_n \to M$, with $M_n \in GL(B)$, and $M \in GL(A)$, then $\| M_n^{-1} \| \to \| M^{-1} \| < \infty$, so by the last lemma, $M$ must be invertible in $B$. Thus we have argued that $GL(B)$ is a closed subset of $B \cap GL(A)$. But it is also open, and therefore must be a union of connected components of $B \cap GL(A)$.
\end{proof}

\begin{corollary}
    If $B$ is a closed subalgebra of $A$, then $\sigma_B(M)$ is obtained from $\sigma_A(M)$ by adding certain components of $\rho_A(M)$.
\end{corollary}

\begin{corollary}
    If $A \subset B$ are algebras, and $\sigma_B(M) \subset \mathbf{R}$, then $\sigma_A(M) = \sigma_A(M)$.
\end{corollary}
\begin{proof}
    $\sigma_B(M)$ is a bounded subset of $\mathbf{R}$, so $\rho_B(M)$ is connected in $\mathbf{C}$. Hence $\sigma_A(M) = \sigma_B(M)$, since $\sigma_A(M) \neq \mathbf{C}$, and if we added any more points to the spectrum we would have to add the entire component, which would have to be all of $\rho_B(M)$.
\end{proof}

\section{Resolvents}

The fundamental theorem of spectral theory relies on heavy complex analysis, hence its restricted application to Banach algebras over the complex numbers. Define the resolvent of $M$, defined on $\rho(M)$ by
%
\[ R(z; M) = (z - M)^{-1} \]
%
Fixing $M$, we obtain a function $R: \rho(M) \to A$. Formally speaking, we should expcet $R$ to be an analytic function with derivative $-(z - M)^{-2}$. This turns out to be the case.

\begin{lemma}
    $R: \rho(M) \to A$ is analytic, in the sense that
    %
    \[ R'(z) = \lim_{w \to 0} \frac{R(z + w) - R(z)}{w} \]
    %
    is a well defined, convergent limit for all $z \in \rho(M)$.
\end{lemma}
\begin{proof}
    The proof is a pure computation.
    %
    \begin{align*}
        \lim_{w \to 0} &\frac{R(z + w, M) - R(z,M)}{w}\\
        &= \lim_{w \to 0} \frac{(z + w - M)^{-1} - (z - M)^{-1}}{w}\\
        &= \lim_{w \to 0} (z + w - M)^{-1} \frac{(z - M) - (z + w - M)}{w} (z - M)^{-1}\\
        &= \lim_{w \to 0} -(z + w - M)^{-1} (z - M)^{-1}\\
        &= -(z - M)^{-2}. \qedhere
    \end{align*}
\end{proof}

In functional analysis, we call such a mapping \emph{strongly analytic}. A \emph{weakly analytic} function $f: \CC \to X$ is a map such that for each $\lambda \in X^*$, $\lambda \circ f$ is analytic. These notions are equivalent if $X$ is a Banach space (proved via methods such as the Banach-Steinhaus theorem). Applying Louiville's theorem to the spectrum, we shall now prove that the spectrum of any element of a Banach algebra is non-empty.

\begin{lemma}
    A bounded weakly analytic function $f: \mathbf{C} \to X$ is constant.
\end{lemma}
\begin{proof}
    Let $\phi \in X^*$. Then $\phi \circ f$ is an analytic function, and
    %
    \[ | (\phi \circ f)(z) | \leq \| \phi \| \| f \|_\infty \]
    %
    Louiville tells us there is $w \in \mathbf{C}$ such that $\phi \circ f = w$. Fix $x,y \in \mathbf{C}$. Then $\phi[f(x) - f(y)] = 0$ for all $\phi \in X^*$. By the Hahn Banach theorem, since $\phi$ was arbitrary, we must have $f(x) - f(y) = 0$, so $f(x) = f(y)$. Thus $f$ is a constant function.
\end{proof}

There is a deep relationship between complex analysis and the spectral theory of Banach algebras. We shall return to `holomorphic functional analysis' later.

\begin{theorem}[Gelfand-Mazur] If $A$ is a complex Banach algebra, then for any $M \in A$, $\sigma(M) \neq \emptyset$.
\end{theorem}
\begin{proof}
    Assume $\sigma(M)$ is empty. Then $R(z)$ is an analytic, entire function of $z$, and tends to zero at infinity, since if $|z| > N \| M \|$,
    %
    \begin{align*}
        \| R(z) \| &= \| (z - M)^{-1} \| = \frac{1}{|z|} \left\| \sum_{k = 0}^\infty \frac{M^k}{z^k} \right\| \leq \sum_{k = 0}^\infty \frac{\| M \|^k}{|z|^{k+1}}\\
        &\leq \frac{1}{|z|} \sum_{k = 0}^\infty \frac{1}{N^{k+1}} = \frac{1}{N(N - 1)\|M\|}
    \end{align*}
    %
    But this implies $R = 0$. But then
    %
    \[ R(z) = (z - M)^{-1} = 0, \]
    %
    and this is clearly impossible for any particular value of $z$.
\end{proof}

\begin{remark}
    We note how non-constructive this theorem is. In particular, this reflects the fact that, for a given explicit element $M$ of a Banach algebra, it is often very difficult to compute $\sigma(M)$ explicitly.
\end{remark}

A cute little theorem arises from this property of Banach algebras that will surprisingly find great use in the analysis of commutative Banach algebras.

\begin{corollary}
    Every complex Banach division algebra is isometric to $\mathbf{C}$.
\end{corollary}
\begin{proof}
    In any unital Banach algebra $A$, $\mathbf{C} \cdot 1$ is isometric to $\mathbf{C}$, for
    %
    \[ \| \lambda \cdot 1 \| = |\lambda| \| 1 \| = |\lambda| \]
    %
    Let $A$ be a complex division algebra, and fix $M \in A$. Pick some $\lambda \in \sigma(A)$. Then $\lambda - M \not \in GL(A)$, hence $\lambda - M = 0$, i.e. $M = \lambda$. Thus $A = \mathbf{C} \cdot 1 \cong \mathbf{C}$.
\end{proof}

The real case is much more complicated. There are three Banach division algebras over the real numbers, namely $\mathbf{R}$, $\mathbf{C}$, and $\mathbf{Q}$ (the quaternions), and it is much more difficult to show that these are the only three.

\begin{theorem}
    If $A$ is a Banach algebra for which $M$ exists such that
    %
    \[ \| x \| \| y \| \leq M \| x y \| \]
    %
    for all $x,y \in \mathbf{C}$, then $A$ is isometric to $\mathbf{C}$.
\end{theorem}
\begin{proof}
    Let $y \in \partial GL(A)$. Then there are $y_i \to y$ with $y_i \in GL(A)$. Thus $\| y_i^{-1} \| \to \infty$. But since
    %
    \[ \| y_i \| \| y_i^{-1} \| \leq M \]
    %
    we must have $\| y_i \| \to 0$, so $y = 0$. Suppose $z \not \in GL(A)$. Consider the line between $z$ and 1. Consider
    %
    \[ \lambda = \inf \{ \gamma \in [0,1] : \gamma + (1 - \gamma) z \in GL(A) \} \]
    %
    then $\gamma + (1 - \gamma) z \in \partial GL(A)$, so $\gamma + (1 - \gamma) z = 0$, and since $\gamma \neq 1$ (since $GL(A)$ is open), we conclude $z = \gamma/(1-\gamma)$, and since $z$ is not invertible, we must have $z = 0$. Thus $A$ is a division algebra, so $A$ is isometric to $\mathbf{C}$.
\end{proof}

\begin{theorem}
    $\sigma$ is `continuous', in the sense that for any open set $U \subset \mathbf{C}$ containing $\sigma(M)$, there is an open neighbourhood $V$ of $M$ such that if $N \in V$, $\sigma(N) \subset U$.
\end{theorem}
\begin{proof}
    The map $\lambda \mapsto \| (\lambda - M)^{-1} \|$ is a bounded continous function in $U^c$, with some bound $K$. If $\| N \| < 1/K$, and $\lambda \in U^c$, then
    %
    \[ \lambda - (M + N) = (\lambda - M)(1 - (\lambda - M)^{-1}N) \]
    %
    is invertible, since $(\lambda - M)$ is invertible, since $\| (\lambda - M)^{-1} N \| < 1$. It follows that $\sigma(M + N) \subset U$.
\end{proof}

\section{Spectral Radii}

The \emph{spectral radius} of an element $M \in A$ is defined to be
%
\[ r(M) = \sup \{ |\lambda| : \lambda \in \sigma(M) \} \]
%
It bounds where to look for the spectrum of $M$ occurs. What is amazing is that we can define the spectral radius without any algebraic reference; this is crazy, since if we enlarge our Banach algebra, more elements become invertible, and thus the spectrum shrinks in size. The radius formula says that there still exists points on the boundary of the spectrum.

\begin{lemma}
    Let $M \in A$, $n \in \mathbf{N}$. Then, if $\lambda \in \sigma(M)$, $\lambda^n \in \sigma(M^n)$.
\end{lemma}
\begin{proof}
    Suppose $\lambda \in \sigma(M)$. Then
    %
    \[ \lambda^n - M^n = (\lambda - M) \left(\sum \lambda^{n-1-k} M^k \right) = \left(\sum \lambda^{n-1-k} M^k \right) (\lambda - M) \]
    %
    If $\lambda - M$ was invertible, then $\lambda^n - M^n$ would also be invertible.
\end{proof}

\begin{theorem}[Spectral Radius Theorem]
    \[ r(M) = \lim_{n \to \infty} \| M^n \|^{1/n} \]
\end{theorem}
\begin{proof}
    If $\lambda \in \sigma(M)$, then $\lambda^n \in \sigma(M^n)$, and therefore by Neumann's lemma, $|\lambda|^n \leq \| M^n \|$. We conclude
    %
    \[ |\lambda| \leq \| M^n \|^{1/n} \]
    %
    taking extrema,
    %
    \[ r(M) = \sup_{\lambda \in \sigma(M)} |\lambda| \leq \liminf_{n \to \infty} \|M^n\|^{1/n} \]
    %
    Set $R = r(M)^{-1}$ (which can be $\infty$, if $r(M) = 0$), and $r = \|M\|^{-1}$. Let $\lambda$ be a complex number with modulus less than $R$. Then $1/|\lambda| > r(M)$, so $1 - \lambda M \in GL(A)$. If $\phi \in A^*$, define
    %
    \[ f: \lambda \mapsto \langle \phi, (1 - \lambda M)^{-1} \rangle \]
    %
    Then $f$ is holomorphic in the disk of radius $R$. If $|\lambda| < r$, then $\| \lambda M \| < 1$, $1 - \lambda M \in GL(A)$, and
    %
    \[ \phi \left( (1 - \lambda M)^{-1} \right) = \sum_{k = 0}^\infty \phi( \lambda^k M^k) \]
    %
    power series expansions are unique, hence this expansion should work in the whole disk of radius $R$. But $\phi$ was arbitrary, so the sequence $\lambda^k M^k$ must be bounded, by Banach Steinhaus. If $\lambda$ is fixed, then there is $C$ such that
    %
    \[ |\lambda^n| \|M^n\| \leq C \]
    %
    for all $n$, so
    %
    \[ \|M^n\|^{1/n} \leq \frac{C^{1/n}}{|\lambda|} \]
    %
    Hence
    %
    \[ \limsup_{n \to \infty} \|M^n\|^{1/n} \leq \lim_{n \to \infty} \frac{C^{1/n}}{\lambda} = \frac{1}{\lambda} \]
    %
    Letting $\lambda \to R$, we obtain that $\limsup \|M^k\|^{1/k} \leq r(a)$. We have shown
    %
    \[ \liminf \|M^k\|^{1/k} \geq r(M) \geq \limsup \|M^k\|^{1/k} \]
    %
    from which the theorem follows.
\end{proof}

\begin{corollary}
    The spectral radius of a Banach algebra element is invariant of which Banach algebra the element is in. If $B$ is a Banach subalgebra of $A$, with $M \in B$, then $r_{A}(M) = r_B(a)$.
\end{corollary}

\begin{example}
    We can isometrically embed $A(\mathbf{D})$ in $C(\mathbf{T})$ via the map $f \mapsto f|_\mathbf{T}$, so that we may view $A(\mathbf{D})$ as a closed subspace of $C(\mathbf{T})$. These properties follows simply from the maximum modulus principle. Let $z: \mathbf{T} \to \mathbf{C}$ be the identity map. Then
    %
    \[ \sigma_{A(\mathbf{D})}(z) = \mathbf{D} \supsetneqq \mathbf{T} = \sigma_{C_0(\mathbf{T})}(z|_\mathbf{T}) \]
    %
    while the spectrum are different, the spectral radius is the same. Note that enlarging the spectrum in this way is the \emph{only} possible change one can have in the spectrum, since we have to add a connected component of the resolvent when enlarging the algebra, and the interior of the unit disk is the only bounded connected component of the resolvent.
\end{example}

\begin{corollary}
    If $A$ is a $C^*$ algebra, and $M \in A$ is normal, then $r(M) = \| M \|$.
\end{corollary}
\begin{proof}
    We calculate that
    %
    \[ \| M^2 \| = \| (M^*)^2 M^2 \|^{1/2} = \| (M^* M) (M^* M)^* \|^{1/2} = \| M^* M \| = \| M \|^2. \]
    %
    More general, by induction we can prove that
    %
    \[ \| M^{2^n} \| = \| M \|^{2^n}. \]
    %
    But this means that
    %
    \[ r(M) = \lim_{n \to \infty} \| M^{2^n} \|^{1/2^n} = \| M \|. \]
    %
    Since $r(M)$ is defined completely algebraically, without any reference to the norm of $A$, this shows $\| M \|$ depends solely on the algebraic structure of $A$.
\end{proof}

\begin{corollary}
    Every $C^*$ algebra $A$ has a unique norm making it a $C^*$ algebra.
\end{corollary}
\begin{proof}
    For any $M \in A$, $M^* M$ is self-adjoint, so
    %
    \[ \| M \|^2 = \| M^*M \| = r(M^*M). \]
    %
    Thus the norm of $M$ is defined by the algebraic structure of $A$.
\end{proof}

\section{Bounded Approximate Identities}

We may not have an identity in a non-unital Banach algebra, but we may be able to `approximate' an identity in some sense. An \emph{approximate identity} for an algebra $A$ is a net $\{ E_\alpha \}$ such that for any $N \in A$,
%
\[ \lim E_\alpha N = N\ \ \ \ \ \ \ \ \lim N E_\alpha = N \]
%
Nets for which one of these equations hold are known as \emph{left} or \emph{right} approximate identities. A \emph{bounded approximate identity} is an approximate identity for which $\sup \| E_\alpha \| < \infty$.

\begin{example}
    Let $X$ be locally compact and Hausdorff. The set of all compact subsets is a directed, exhausting set. Using Urysohn's lemma, find $f_K$, for each compact $K$, such that $f_K |_K = 1$, and $\| f_K \|_\infty \leq 1$. Fix $g \in C_0(X)$. Pick $K$ such that $|g| \leq \varepsilon$ outside of $K$. Then, in $K$, $|f_K g - g| = 0$, and outside of $K$, $|f_K g - g| < 2 \varepsilon$. Thus $\| f_K g - g \|_\infty < 2 \varepsilon$. Thus $\lim_{K \to \infty} f_K g = g$, and so $\{ f_K \}$ is a bounded approximate identity in $C_0(X)$.
\end{example}

\begin{example}
    Consider the net $P_r(z) = \sum_\mathbf{Z} r^{|n|} z^n$ in the group algebra $L^1(S^1)$, for $0 < r < 1$. Given a continuous function $g$, it is well known that $(P_r * g) \to g$ uniformly. Since, for any $g \in L^1(S^1)$, $\| P_r * g \|_{L^1} \lesssim \| g \|_{L^1}$, uniformly in $r$, it follows that $g * P_r \to g$ in $L^1(S^1)$ for any $g \in L^1(S^1)$. Thus $\{ P_r \}$ is a bounded approximate identity.
\end{example}

\begin{example}
    Bounded approximate identities appear in advanced Banach space theory. For instance, they arise in the study of compact operators. Let $X$ be a Banach space, and let $A(X)$ be the closure of the family of all bounded finite rank operators on $X$. Then $A(X)$ is a right ideal in $B(X)$ contained in $K(X)$, the subalgebra of compact operators on $X$. We say $X$ has the \emph{approximation property} if $K(X) = A(X)$, i.e. all compact operators can be approximated in the operator norm by finite rank operators. We say $X$ has the \emph{bounded approximation property} if there exists a family of uniformly bounded finite rank operators $\{ E_\alpha \}$ converging pointwise to the identity map on $X$. Under these assumptions, it then follows that the family $\{ E_\alpha \}$ actually converges \emph{uniformly} on compact subsets of $X$, from which it follows that for any compact operator $T \in K(X)$, $E_\alpha T$ is a family of finite rank operators converging in the operator topology to $T$. Thus if $X$ has the bounded approximation property, then $X$ has the approximation property. If a Banach space is reflexive, it is a result of Grothendieck that a space has the bounded approximation property if and only if $A(X) = K(X)$, i.e. all compact operators on $X$ can be approximated by finite rank operators. We claim that $X$ has the bounded approximation property if and only if $A(X)$ has a bounded left approximate identity.

    If $X$ has the bounded approximation property, and $\{ E_\alpha \}$ are the uniformly bounded, finite rank operators converging uniformly to the identity on compact subsets of $X$, then it is simple to see that for any finite rank operator $T: X \to X$, $E_\alpha T$ converges in the operator norm to $T$. Indeed, if $Y$ is the finite dimensional image of $T$, and $B_Y$ is the closed unit ball in $Y$, then $B_Y$ is compact, so $E_\alpha$ converges to the identity uniformly on $B_Y$, so $E_\alpha|_Y$ converges in the norm topology to $\text{id}|_Y$, and thus means that
    %
    \[ \| E_\alpha T - T \|_{B(X)} = \| (E_\alpha - 1) T \|_{B(X)} \leq \| E_\alpha - 1 \|_{B(Y)} \| T \|_{B(X)} \to 0. \]
    %
    A density argument shows the same is true for any $T \in A(X)$, so $\{ E_\alpha \}$ is a bounded approximate identity for $A(X)$.

    Conversely, suppose $\{ E_\alpha \}$ is a bounded left approximate identity for $A(X)$. Without loss of generality, by a density argument we may assume each $E_\alpha$ is of finite rank. For each $x_0 \in X$, and $\phi_0 \in X^*$, the operator $x_0 \otimes \phi_0: X \to X$ defined by setting
    %
    \[ (x_0 \otimes \phi_0)(x) = \phi(x) \cdot x_0 \]
    %
    is clearly a finite rank operator. Thus we conclude that $E_\alpha (x_0 \otimes \phi_0)$ converges to $x_0 \times \phi_0$ in $B(X)$. But this means that
    %
    \[ |\phi_0(x)| \| E_\alpha x_0 - x_0 \| = o \left( \| x \| \right). \]
    %
    But this clearly means that $\| E_\alpha x_0 - x_0 \| \to 0$. Thus $\{ E_\alpha \}$ is a uniformly bounded family of operators converging pointwise to the identity, so $X$ has the bounded approximation property.

    Finding a Banach space without the approximation property proved to be a very challenging problem in 20th century mathematics. For instance, if $H$ is a Hilbert space, then $B(H)$ always has the bounded approximation property (consider the bounded approximate identity given by orthogonal projections onto finite dimensional subspaces), as do the classical sequence spaces $c_0$ and $l_p$. The problem of proving or disproving that all Banach spaces have the approximation property was proposed by Stanislaw Mazur in 1936, promising a reward of a live goose as a prize for the solution to this problem. Forty years later, the problem was solved by the Swedish mathematician Per Enflo in 1972, who finally recieved a live goose as a reward.
\end{example}



\begin{example}
    Let $X$ be a Banach space with the bounded approximation property. Let $\{ T_\alpha \}$ be a bounded net. If $S$ is a compact operator, then $S(\overline{B_X})$ is precompact, and
    %
    \[ \| T_\alpha S - S \| = \sup \{ \| T_\alpha Sx - Sx \| : x \in B_X \} \leq \sup \{ \| T_\alpha y - y \| : y \in \overline{S(B_X)} \} \]
    %
    The right side is the supremum over a compact set, and since $\{ T_\alpha \}$ uniformly tends to the identity on compact sets, $\| T_\alpha S - S \| \to 0$. This shows $\{ T_\alpha \}$ is a bounded left approximate identity, and that $A(X) = K(X)$.

    Now suppose $A(X)$ has a bounded left approximate identity $\{ T_\alpha \}$. Without loss of generality, we may assume each $T_\alpha$ is of finite rank, because if we choose, for each $\alpha$, a bounded net $W_{\alpha, \beta}$ of finite rank operators such that $\lim_\beta W_{\alpha, \beta} = T_\alpha$, then $\{ W_{\alpha, \beta} \}$ is a bounded left approximate identity. For $x \in X$, $\phi \in X^*$, let $x \otimes \phi : X \to X$ be defined by $(x \otimes \phi)(y) = \phi(y) x$. Then $x \otimes \phi$ obviously has finite rank. Pick $y$ for which $\langle \phi, y \rangle = 1$. Then
    %
    \[ \| T_\alpha x - x \| = \| T_\alpha (x \otimes \phi)(y) - (x \otimes \phi)(y) \| \leq \| y \| \| T_\alpha (x \otimes \phi) - (x \otimes \phi) \| \to 0 \]
    %
    So the nets converge pointwise, which implies convergence on compact sets. To summarize, $X$ has the bounded approximxation property if and only if $A(X)$ has a bounded left approximate identity if and only if $K(X)$ has a bounded left approximate identity belonging to $A(X)$.
\end{example}

\begin{lemma}
    If a space has the bounded left and right approximation properties then it has the two sided approximation property.
\end{lemma}
\begin{proof}
    Let $\{ M_\alpha \}$ be a left approximation identity, and $\{ N_\beta \}$ a right approximation identity. We contend $\{ M_\alpha + N_\beta - M_\alpha N_\beta \}$ is a two sided approximator. The limits below certainly converge, and the iterated limits must therefore equal the convergent factor, which is
    %
    \[ \lim_\alpha \lim_\beta L(M_\alpha + N_\beta - M_\alpha N_\beta) = \lim_\alpha LM_\alpha + L - LM_\alpha = L \]
    %
    \[ \lim_\beta \lim_\alpha (M_\alpha + N_\beta - M_\alpha N_\beta)L = \lim_\beta L + N_\beta L - N_\beta L = L \]
    %
    So twe have a two sided approximator.
\end{proof}

Before the logician Paul Cohen got into logic, he was a functional analyst who contributed to the theory of approximation identities. We shall prove the theorem he contributed to the field, generalized to work over arbitrary modules. Let $A$ be a Banach algebra. A \emph{left Banach $A$-module} is a Banach space $X$, which is a module over $A$, such that for any $M \in A$, and $x \in X$,
%
\[ \| Mx \| \leq \| M \| \| x \| \]
%
This makes the module operations continuous. As with Banach algebras, if module operations are continuous, then a Banach space can be made into a Banch $A$-module.

Now if $A$ is a unital Banach algebra, and $X$ is a left Banach $A$-module, then every $x \in A$ can be trivially written as $My$ for some $M \in A$ (just choose $M$ to be the identity). If $A$ is a non-unital Banach algebra, and $x \in \overline{A X}$, then we can write $x$ approximately as $My$. Cohen found that in an algebra with a bounded left approximate identity, we have $AX = \overline{AX}$, i.e. we can write $x$ precisely as $My$ for some $M \in A$ and $y \in X$.

\begin{theorem}[Cohen's Factorization Theorem]
    Let $A$ be a Banach algebra with a bounded left approximation identity $\{ E_\alpha \}$, bounded by some quantity $R > 0$. If $X$ is a Banach $A$-module, let $x \in \overline{AX}$, and let $\varepsilon > 0$. Then there is $M \in A$ with $\| M \| \leq R$, $y \in \overline{AE}$ with $\| y - x \| < \varepsilon$, for which $x = My$.
\end{theorem}
\begin{proof}
    If $A$ has an identity, then the proof is trivial. Since $\overline{AX}$ is a closed subset of $X$, we might as well assume $\overline{AX} = X$. We may extend $X$ to be an module over $A^\#$, by defining
    %
    \[ (M + \lambda) x = Mx + \lambda x \]
    %
    Pick $\lambda > 0$ small enough such that
    %
    \[ 0 < \frac{\lambda}{1 - \lambda} K < 1 \]
    %
    Define a net $\{ L_\alpha \}$ in $A^\#$ by letting
    %
    \[ L_\alpha = \lambda E_\alpha + (1 - \lambda) = (1 - \lambda) \left( 1 + \frac{\lambda}{1 - \lambda} E_\alpha \right) \]
    %
    Then $L_\alpha N \to N$ for all $N \in A$, so $\{ L_\alpha \}$ is a BLAI, and each $L_\alpha$ is invertible in $A^\#$ by the choice of $\lambda$, so
    %
    \[ L_\alpha^{-1} = \frac{1}{1 - \lambda} \sum_{k = 0}^\infty \left(- \frac{\lambda}{1 - \lambda} L_\alpha \right)^k \]
    %
    which implies
    %
    \begin{align*}
        \| L_\alpha^{-1} N - N \| &= \| L_\alpha^{-1} N - L_\alpha^{-1} L_\alpha N \| \leq \| L_\alpha^{-1} \| \| N - L_\alpha N \|\\
        &\leq \left( \frac{1}{1 - \lambda} \sum_{k = 0}^\infty \left( \frac{\lambda K}{1 - \lambda} \right)^k \right) \| N - L_\alpha N \| \to 0
    \end{align*}
    %
    Therefore $L_\alpha^{-1}$ is also a BLAI. Fix $x \in X$, and let
    %
    \[ \delta < 1, \frac{\varepsilon}{2 + \|x\|} \]
    %
    We will inductively construct a sequence $\alpha_n$ of indices such that the sequence $\{ L_{\alpha_n} \dots L_{\alpha_1} \}$ and $\{ L_{\alpha_1}^{-1} \dots L_{\alpha_n}^{-1} \}$ are convergent subnets. Choose $\alpha_1$ such that
    %
    \[ \| L_{\alpha_1}^{-1} x - x \| < \delta/2 \]
    %
    If $\alpha_1, \dots, \alpha_n$ has been chosen. There is a unique element $T_n \in A$ such that
    %
    \[  L_{\alpha_n} \dots L_{\alpha_1} = (1 - \lambda)^n + T_n \]
    %
    Pick $\alpha_{n+1}$ such that
    %
    \[ \| L_{\alpha_{n+1}} x - x \| < \frac{\delta}{\| L_{\alpha_1}^{-1} \dots L_{\alpha_n}^{-1} \| 2^{n+2}}\ \ \ \ \ \ \ \ \ \ \| L_{\alpha_{n+1}} T_n - T_n \| < 1/2^n \]
    %
    Then
    %
    \[ \| L_{\alpha_1}^{-1} \dots L_{\alpha_n}^{-1} L_{\alpha_{n+1}}^{-1} x - L_{\alpha_1}^{-1} \dots L_{\alpha_n}^{-1} x \| \leq \| L_{\alpha_1}^{-1} \dots L_{\alpha_n}^{-1} \| \| x - L_{\alpha_{n+1}}^{-1} x \| < \frac{\delta}{2^{n+2}} \]
    %
    This implies $\lim_{n \to \infty} L_{\alpha_1}^{-1} \dots L_{\alpha_n}^{-1} x = y$ exists, and
    %
    \begin{align*}
    \| y - x \| &= \left\| \sum_{n = 0}^\infty (L_{\alpha_1}^{-1} \dots L_{\alpha_{n+1}^{-1}} x - L_{\alpha_1}^{-1} \dots L_{\alpha_n}^{-1} x) \right\|\\
    &\leq \sum_{n = 0}^\infty \| L_{\alpha_1}^{-1} \dots L_{\alpha_{n+1}^{-1}} x - L_{\alpha_1}^{-1} \dots L_{\alpha_n}^{-1} x \| < \sum_{n = 0}^\infty \frac{\delta}{2^{n+2}} < \delta/2
    \end{align*}
    %
    By construction,
    %
    \[ L_{\alpha_{n+1}} (L_{\alpha_n} \dots L_{\alpha_1}) = (1 - \lambda)^n L_{\alpha_{n+1}} + L_{\alpha_{n+1}} T_n \]
    %
    Thus
    %
    \begin{align*}
        \| L_{\alpha_{n+1}} L_{\alpha_n} \dots L_{\alpha_1} - L_{\alpha_n} \dots L_{\alpha_1} \| &= \| (1 - \lambda)^n L_{\alpha_{n+1}} + L_{\alpha_{n+1}} T_n - T_n - (1-  \lambda)^n \|\\
        &\leq (1 - \lambda)^n \| L_{\alpha_{n+1}} - 1 \| + \frac{1}{2^n}
    \end{align*}
    %
    Thus $L_{\alpha_n} \dots L_{\alpha_1}$ converge to some element $M$. Since $A$ is closed in $A^\#$, $M \in A$, and moreover,
    %
    \[ x = \lim_{n \to \infty} (L_{\alpha_1} \dots L_{\alpha_n}) (L_{\alpha_1}^{-1} \dots L_{\alpha_n}^{-1} x) = My \]
    %
    Also, observe that
    %
    \begin{align*}
        T_{n+1} &= L_{\alpha_{n+1}} \dots L_{\alpha_1} - (1 - \lambda)^{n+1}\\
        &= L_{\alpha_{n+1}} T_n + L_{\alpha_{n+1}} (1 - \lambda)^n - (1 - \lambda)^{n+1}\\
        &= L_{\alpha_{n+1}} T_n + (1 - \lambda)^n \lambda e_{\alpha_{n+1}}
    \end{align*}
    %
    Thus
    %
    \[ \| T_{n+1} - T_n \| = \| (L_{\alpha_{n+1}} T_n - T_n + (1 - \lambda)^n \lambda e_{\alpha_{n+1}} \| \leq 1/2^n + (1 - \lambda)^n \lambda K \]
    %
    Since $M = \lim T_n$,
    %
    \[ \| M \| \leq \| T_1 \| + \left(\sum_{n = 1}^\infty (1 - \lambda)^n \lambda K \right) + \delta \leq \lambda K + (1 - \lambda) K + \delta = K + \delta \]
    %
    Let $M' = \frac{K}{K + \delta} M$, $y' = \frac{K + \delta}{K} y$. Then $M'y' = My = x$, and by the choice of $\delta$ (kept hidden all this time),
    %
    \[ \| x - y' \| \leq \frac{\| Kx - Ky \| + \| \delta y \|}{K} < \delta + \delta \| x \| + \delta^2 \leq \delta(2 + \|x\|) = \varepsilon \]
    %
    And we have verified what was needed.
\end{proof}

\begin{corollary}
    If $A$ is a Banach algebra with a bounded left approximate identity, then each $M \in A$ may be written $M = NL$, with $N,L \in A$.
\end{corollary}

Thus, for instance, for any locally compact group $G$, since $L^1(G)$ has an approximate identity, any $f \in L^1(G)$ can be written as $f = g * h$ for some $g,h \in L^1(G)$. Moreover, for any $\varepsilon > 0$, we can pick $g$ and $h$ such that $\| g \|_{L^1(G)} \leq 1$, and $\| f - h \|_{L^1(G)} < \varepsilon$, or vice versa.

\begin{corollary}
    Let $A$ be a Banach algebra with a BLAI, and let $\{ M_n \}$ be a sequence in $A$ converging to 0. Then there is $M \in A$ and $\{ N_m \}$ in $A$ tending to zero such that $M_k = M N_k$.
\end{corollary}
\begin{proof}
    Let $X$ be the set of all sequences in $A$ that converge to zero, with the $\| \cdot \|_\infty$ norm. Then $X$ is a left $A$ module, and $X = \overline{AX}$, since $A$ has a BLAI. Applying Cohen's theorem, we find that we may write
    %
    \[ (M_1, M_2, \dots) = M (N_1, N_2, \dots) \]
    %
    hence $M_i = M N_i$ for each $i$.
\end{proof}






\section{The Holomorphic Functional Calculus}

Given a polynomial $P(x) = \sum a_i x^i$, a Banach algebra $A$, and $M \in A$, it is natural to overload the definition of $P$ by setting $P(M) = \sum a_i M^i$. This does not really require any topological info about $A$. In this section, we use this topology to extend this so that we can consider $f(M)$, for a large family of analytic functions $f$. As a basic example of this, in matrix theory it is useful to define the exponential of $M \in A$ by
%
\[ e^M = \sum_{k = 0}^\infty \frac{M^k}{k!} \]
%
We may define $\sin(M)$ and $\cos(M)$ similarily. More generally, for any power series $f = \sum_{k = 0}^\infty c_k (X - \alpha)^k$ with a radius of convergence $R$, we may define
%
\[ f(M) = \sum_{k = 0}^\infty c_k (M - \alpha)^k \]
%
And this function is defined for any $M$ satisfying $\|M - \alpha \| < R$. For analytic functions not given in a power series representation, it is non-trivial to define them for elements of a Banach algebra, but very useful.

To do this, we utilize the Cauchy integral formula
%
\[ f(w) = \frac{1}{2\pi i} \int_\gamma \frac{f(z)}{z - w} \; dz, \]
%
where $\gamma$ is a simple positively oriented curve containing $w$ in it's interior. We use this formula to overload the function $f$. Given a Banach algebra $A$ containing an element $M$, a domain $\Omega \subset \CC$ containing $\sigma(M)$, an analytic function $f: \Omega \to A$, and a positively oriented curve $\gamma$ containing $\sigma(M)$ in it's interior, we define
%
\[ f(M) = \frac{1}{2 \pi i} \int_\gamma f(z) (z - M)^{-1}\; dz. \]
%
The right hand side is a vector-valued integral, whose integrand is smooth on it's compact domain. Thus we can define this integral using Riemann sums (i.e. using the \emph{Bochner integral}). We can reduce the study of such quantities to standard complex contour integrals by the fact that for any linear functional $\Lambda: A \to \CC$,
%
\[ \Lambda \left( \frac{1}{2 \pi i} \int_\gamma f(z) (z - M)^{-1}\; dz \right) = \frac{1}{2 \pi i} \int_\gamma \Lambda \left( f(z) (z - M)^{-1} \right)\; dz. \]
%
In particular, it follows from such a reduction that $f(M)$ is actually independent of the contour $\gamma$ used. We have thus laid the foundations for the holomorphic functional calculus.

\begin{theorem}
    The map $f \mapsto f(M)$ is the unique algebra homomorphism from the algebra $\mathcal{O}(\sigma(M))$ into $A$ extending the evaluation of polynomials, and which is continuous under the topology of locally uniform convergence.
\end{theorem}
\begin{proof}
    If the integrands of a Bochner integral converge uniformly, then the integrals converge. This gives continuity of the map $f \mapsto f(M)$. The map $f \mapsto f(M)$ is obviously linear. Next, we show that for any polynomial $f$, and any contour integral $\gamma$ around $\sigma(M)$,
    %
    \[ f(M) = \frac{1}{2 \pi i} \int_\gamma f(z) (z - M)^{-1}\; dz. \]
    %
    By linearity, we may assume $f(z) = z^k$. We may assume our contour is large enough that $\| (z - M)^{-1} \| < 1$ on $\gamma$. Then
    %
    \begin{align*}
        \frac{1}{2 \pi i} \int_\gamma z^n (z - M)^{-1} dz &= \frac{1}{2 \pi i} \int_\gamma z^{n-1} \sum_{k = 0}^\infty \frac{M^k}{z^k} dz\\
        &= \frac{1}{2 \pi i} \sum_{k = 0}^\infty M^k \int_\gamma z^{n - 1 - k} dz = M^n,
    \end{align*}
    %
    since $\int_\gamma z^{n-1-k} dz \neq 0$ only when $n - 1 - k = -1$ ($n = k$). Now we need to show the multiplicity of the evaluation map. This is done by brute calculation. Let $f,g \in \mathcal{O}(D)$ be given. Pick $\gamma$ be as required, and consider another $\tilde{\gamma}$ satisfying $\text{Int}\ \tilde{\gamma} \supset \overline{\text{Int}\ \gamma}$, $\mathbf{C} - D \subset \text{ext}\ \tilde{\gamma}$. Then
    %
    \begin{align*}
        f(M) g(M) &= \frac{-1}{4 \pi^2} \left( \int_\gamma f(z) (z - M)^{-1} dz \right) \left( \int_{\tilde{\gamma}} g(w) (w - M)^{-1} \right)\\
        &= \frac{-1}{4 \pi^2} \int_\gamma \int_{\tilde{\gamma}} f(z) g(w) (z - M)^{-1} (w - M)^{-1} dw\ dz\\
        &= \frac{-1}{4 \pi^2} \int_\gamma \int_{\tilde{\gamma}} f(z) g(w) \left( \frac{1}{w - z} \left((z - M)^{-1} - (w - M)^{-1} \right) \right) dw\ dz\\
        &= \frac{-1}{4 \pi^2} \int_\gamma f(z) \left( \int_{\tilde{\gamma}} \frac{g(w)}{w - z} dw \right) (z - M)^{-1} dz\\
        &\ \ \ + \frac{1}{4 \pi^2} \int_{\tilde{\gamma}} g(w) \left( \int_\gamma \frac{f(z)}{w - z} dz \right) (w - M)^{-1} dw\\
        &= \frac{1}{2 \pi i} \int_\gamma f(z) \left( \frac{1}{2 \pi i} \int_{\tilde{\gamma}} \frac{g(w)}{w - z} dw \right) (z - M)^{-1} dz\\
        &= \frac{1}{2 \pi i} \int_\gamma f(z) g(z) (z - M)^{-1} dz\\
        &= (fg)(M)
    \end{align*}
    %
    and thus we have an algebra homomorphism.

    Finally, we prove uniqueness. This would be trivial if $\Omega = \mathbf{C}$, since all functions are limits of polynomials. More generally, we apply Runge's theorem, which says that for any compact set $K$, if $S$ is a set containing at least one point from each bounded connected component of $\CC - K$, then every analytic function on $K$ is the uniform limit of a family of rational functions with poles solely in $S$. Now suppose $\Lambda: \mathcal{O}(\sigma(M)) \to A$ is another homorphism satisfying the properties above. Then for any two polynomials $P$ and $Q$, such that the zeroes of $Q$ are disjoint from $\sigma(M)$, $1/Q \in \mathcal{O}(\sigma(M))$, and so $\Lambda(1/Q) = \Lambda(Q)^{-1}$. Thus we calculate that
    %
    \[ \Lambda(P/Q) = \Lambda(P) \Lambda(1/Q) = \Lambda(P) \Lambda(Q)^{-1} = P(M) Q(M)^{-1} = (P/Q)(M). \]
    %
    Thus $\Lambda$ agrees with the holomorphic functional calculus on rational functions, which shows $\Lambda(f) = f(M)$ for all $f \in \mathcal{O}(\sigma(M))$ by continuity and Runge's theorem.
\end{proof}

\begin{example}
    Let $K$ be compact, and let $f \in C(K)$. Let $\Omega$ be an open neighbourhood of $\sigma(f) = f(K)$. Consider the map from $\mathcal{O}(\Omega) \to C(K)$ mapping $g$ to $g \circ f$. This map satisfies the properties of the holomorphic functional calculus, so it really is the holomorphic calculus in disguise.
\end{example}

\begin{theorem}
    Let $F: A \to B$ be a homomorphism between two Banach algebras, and suppose $M \in A$. Then $\sigma(F(M)) \subset \sigma(M)$, and if $g$ is holomorphic in a neighbourhood of $\sigma(M)$, then $(g \circ F)(M) = (F \circ g)(M)$.
\end{theorem}
\begin{proof}
    That $\sigma(f(M)) \subset \sigma(M)$ is trivial, for if $\alpha - M$ is invertible, then $f(\alpha - M) = \alpha - f(M)$ is invertible. If $g$ is a polynomial, then the theorem follows from the multiplicative property. But then the theorem is true for limits of polynomials, and hence for all $g$.
\end{proof}

The ultimatum of the holomorphic functional calculus is the proof of the spectral mapping theorem, that asserts that the `operation' of the spectrum commutes with holomorphic functions. To extend this theorem to non-commutative algebras, we need an algebraic trick. We introduce the center $Z(A)$ of an algebra, which is defined exactly how it is defined in all other algebraic structures. It is a closed subalgebra of $A$, for if $M_i \to M$, and each $M_i \in A$, then
%
\[ MN = \lim M_iN = \lim NM_i = NM \]
%
If $S$ is a subset of $A$, then we may consider
%
\[ Z(S) = \{ M \in A : (\forall N \in S: MN = NM) \} \]
%
which is a commutative subalgebra of $A$. For any subset $S$, the double centralizer $Z(Z(S))$ also contains $S$. $Z(Z(S))$ is commutative if $S \subset Z(S)$, for then $Z(Z(S)) \subset Z(S)$, and thus commutes with itself by definition.

Now consider the commutative subalgebra $B = Z(Z(\{ M \}))$ of an algbera $A$. If $\lambda - M$ is invertible in $A$, there is $N = (\lambda - M)^{-1}$. But then if $K \in Z(\{ M \})$, then $K$ commutes with $\lambda - M$, and
%
\[ KN = N(\lambda - M) K N = NK (\lambda - M) N = NK \]
%
which implies that $\lambda - M$ is invertible in $Z(Z(\{M\}))$, so $\sigma_A(M) = \sigma_B(M)$.

\begin{theorem}[Spectral Mapping Theorem]
    If $\sigma(M) \subset \Omega$, where $\Omega$ is open, and $f \in \mathcal{O}(\Omega)$, then $\sigma(f(M)) = f(\sigma(M))$.
\end{theorem}
\begin{proof}
    First, suppose $A$ is commutative. By Gelfand theory,
    %
    \[ \sigma(f(M)) = \widehat{f(M)}(\Phi_A) = f(\widehat{M}(\Phi_A)) = f(\sigma(M)) \]
    %
    If $A$ is not commutative, consider the commutative subalgebra
    %
    \[ B = Z(Z(\{M\})). \]
    %
    If $f$ is analytic in a neighbourhood of $\sigma_A(M)$, then $f(M) \in B$, for there is surely a homomorphism from analytic functions in a neighbourhood of $\sigma_A(M)$ into $B$, and the embedding of $B$ in $A$ produces the unique homomorphism required. Thus we have justified the computation
    %
    \[ \sigma_A(f(M)) = \sigma_B(f(M)) = f(\sigma_B(M)) = f(\sigma_A(M)) \]
    %
    and the spectral mapping theorem is proved in general.
\end{proof}

The holomorphic functional calculus is incredibly useful for it allows us to apply calculations on the complex plane to arbitary algebras. Here are some immediate uses.

\begin{theorem}
    Suppose $A$ is a Banach algebra, $M \in GL(A)$, and $\sigma(M)$ does not separate the origin from $\infty$. Then
    %
    \begin{itemize}
        \item For each $m$, there is $N$ such that $M = N^m$. If $\sigma(M) \subset \RR^+$, then we may choose the $N$ such that $\sigma(N) \subset \mathbf{R}^+$.
        \item There is $N$ such that $M = e^N$.
        \item if $B$ is the closed subalgebra of $A$ generated by $M$, then $M^{-1} \in B$.
    \end{itemize}
\end{theorem}
\begin{proof}
    By assumption, we may choose a branch $f(z)$ of $z^{1/m}$ which is holomorphic on a neighborhood of $\sigma(M)$, and if $\sigma(M) \subset \mathbf{R}^+$, we may choose $f(z)$ to be the principal branch. Thus we can define $N = f(M)$. Since $f^m(z) = z$ for all $z$ on a neighborhood of $\sigma(M)$, it follows that $N^m = f(M)^n = f^n(M) = M$. The spectral mapping theorem shows that $\sigma(N) = \sigma(f(M)) = f(\sigma(M))$, so if $f$ is the principal branch, and $\sigma(M) \subset \mathbf{R}^+$, then $\sigma(N) \subset \RR^+$. Similar calculations involving the logarithm allow us to define $N$ such that $M = e^N$. Finally, by Runge's theorem, for each $\varepsilon > 0$, we can find a polynomial $f(z)$ such that $|f(z) - 1/z| \leq \varepsilon$ on $\sigma(M)$. Then $\| f(M) - M^{-1} \| \leq \varepsilon$. Taking $\varepsilon \to 0$ gives the final point.
\end{proof}

One can apply this result, for instance, for unitary, or self adjoint elements, of $C^*$ algebras.

\begin{prop}
    In a $C^*$ algebra, if $M$ is unitary, then $\sigma(M) \subset \mathbf{T}$.
\end{prop}
\begin{proof}
    Because $M$ is normal, $r(M) = \| M \| = 1$. But $M^{-1} = M^*$ is also unitary, so $r(M^{-1}) = 1$, and by the spectral mapping theorem,
    %
    \[ \sigma(M^{-1}) = \sigma(M)^{-1} \]
    %
    which implies that both spectra lie on $\mathbf{T}$.
\end{proof}

\begin{prop}
    If $M$ is self-adjoint, then $\sigma(M) \subset \RR$.
\end{prop}
\begin{proof}
    The operator $e^{iM}$ is unitary, for
    %
    \[ e^{-iM} = \sum_{k = 0}^\infty \frac{(-i)^k M^k}{k!} = \sum_{k = 0}^\infty \left( \frac{(iM)^k}{k!} \right)^* = (e^{iM})^* \]
    %
    The spectral mapping theorem implies that $\sigma(e^{iM}) = e^{i \sigma(M)} \subset \mathbf{T}$, which implies $\sigma(M) \subset \mathbf{R}$.
\end{proof}

\begin{corollary}
    Let $A$ be a $C^*$ algebra, and let $B$ be a $C^*$ subalgebra. Then $GL(B) = B \cap GL(A)$.
\end{corollary}
\begin{proof}
    Suppose $N \in B \cap GL(A)$. Then $N^* \in B \cap GL(A)$, for $(N^{-1})^* = (N^*)^{-1}$. Thus $NN^* \in B \cap GL(A)$. Moreover, $NN^*$ is invertible, so $\sigma_A(NN^*) \subset \RR - \{ 0 \}$. Now $\sigma_B(NN^*)$ is obtained by adding bounded connected components of $\rho_A(NN^*)$ to $\sigma_A(NN^*)$, and since $\rho_A(NN^*)$ has no bounded connected components, it follows that $\sigma_B(NN^*) = \sigma_A(NN^*)$. But this means that $NN^* \in GL(B)$. But then $N$ has a right inverse in $B$. By symmetry $N^*N \in GL(B)$, so $N$ has a left inverse in $B$. But if $N$ has a left and right inverse, then $N \in GL(B)$.
\end{proof}

\begin{corollary}
    If $A$ is a $C^*$ algebra, and $B$ is a $C^*$-subalgebra, then for any $M \in B$, $\sigma_A(M) = \sigma_B(M)$.
\end{corollary}

\begin{corollary}
    If $A$ is a $C^*$ algebra, and $M \in GL(A)$ is normal, then $M^{-1}$ is approximable by expressions of the form
    %
    \[ N = \sum_{\alpha, \beta} a_{\alpha \beta} M^\alpha (M^*)^\beta, \]
    %
    i.e. for any $\varepsilon > 0$, we can find $N$ as above such that $\| M^{-1} - N \| \leq \varepsilon$.
\end{corollary}
\begin{proof}
    Let $B$ be the smallest $C^*$ subalgebra of $A$ containing $M$. Then the family of expressions of the form above is dense in $B$. By the result we have just proven, $M \in GL(B)$, and so the result immediately follows.
\end{proof}

Even for finite dimensional matrix algebras, the holomorphic functional calculus has nontrivial repercussions.

\begin{corollary}
    If $M \in GL_n(\mathbf{C})$, then for each $m$, there is $N \in GL_n(\mathbf{C})$ such that $M = N^m$, and there is $N$ such that $M = e^N$.
\end{corollary}

\begin{example}
    Now consider the exponential function $\exp: A \to A$. If $M$ and $N$ commute, it is easy to see from the power series representation that
    %
    \[ \exp(M + N) = \exp(M) \exp(N). \]
    %
    In particular, this implies that $\exp(M)$ is invertible, and $\exp(-M) = \exp(M)^{-1}$. Thus for each $M$, the image of the map
    %
    \[ t \mapsto \exp(tM) \]
    %
    is a one-parameter subgroup of $G(A)$, which gives us a path from the identity $\exp(0)$ to $\exp(M)$. The connected component of $G(A)$ containing the identity is the \emph{principal component}, denoted $G_0(A)$. Since $N \mapsto MNM^{-1}$ is a continuous map leaving the identity fixed, it maps $G_0(A)$ to itself, so $G_0(A)$ is a normal subgroup. The quotient group $G(A)/G_0(A)$ is sometimes called the \emph{index group}.
\end{example}

\begin{theorem}
    If $P(X) = (X - \alpha_1)^{m_1} \dots (X - \alpha_s)^{m_s}$, and $P(M) = 0$, then
    %
    \[ \sigma(M) \subset \{ \alpha_1, \dots, \alpha_n \}, \]
    %
    and if $\Omega$ is an open subset of the plane containing $\alpha_1, \dots, \alpha_n$, then for any $f \in \mathcal{O}(\Omega)$, there is a polynomial $Q$ of degree less than $m_1 + \dots + m_s$, such that $f - Q = gP$, for some $g \in \mathcal{O} (\Omega)$, so $f(M) = Q(M)$.
\end{theorem}
\begin{proof}
    Applying the spectral theorem, we find $\{ 0 \} = \sigma(P(M)) = P(\sigma(M))$, from which the spectrum of $M$ is restricted to lie in the roots of $P$. The Laurent series about each $\alpha_i$ gives constants $c_{it}$ such that
    %
    \[ g(z) = f(z)/P(z) - \sum_{i = 1}^s \sum_{k = 1}^{m_i} \frac{c_{it}}{(z - \alpha_i)^k} \]
    %
    is holomorphic in $\Omega$, and
    %
    \[ gP = f - \sum_{i = 1}^s \sum_{k = 1}^{m_i} c_{it} P (z - \alpha_i)^{-k} \]
    %
    constructing $Q$ with degree less than $n$.
\end{proof}

In non-unital algebras, we cannot evaluate any polynomial of the form $\sum a_i z^i$, since $a_0 \in \mathbf{C}$ does not necessarily have an interpretation in the algebra. If $a_0 = 0$, we can evaluate $\sum a_i z^i$, and thus any holomorphic function $f$, provided $f(0) = 0$. To verify this, one need only use the simple trick of switching from a non-unital algebra $A$ to a unital algebra $A^\#$.

For a Banach space $X$, a weaker version of the spectral mapping theorem applies to the \emph{point spectrum} $\sigma_p(T)$ of bounded operators $T: X \to X$.

\begin{theorem}
    Suppose $T: X \to X$ is a bounded operator, and $f \in \mathcal{O}(\sigma(T))$. Then $f(\sigma_p(T)) \subset \sigma_p(f(T))$, with equality if $f$ is not constant on any connected component of the domain of $f$.
\end{theorem}
\begin{proof}
    Suppose $Tx = \lambda x$. The function $g(z) = (f(z) - f(\lambda)) / (z - \lambda)$ is analytic, and so
    %
    \[ [f(T) x - f(\lambda) x] = [g(T) \circ (T - \lambda)] x = g(T) 0 = 0. \]
    %
    Thus $f(T) x = f(\lambda) x$. Now if $f(T) x = \gamma x$, and $f - \gamma$ does not vanish in any connected component of the domain of $f$, then $f(z) - \gamma$ has finitely many zeroes $\xi_1, \dots, \xi_n$, counted up to multiplicity. Thus the function
    %
    \[ g(z) = \frac{(z - \xi_1) \dots (z - \xi_n)}{f(z) - \gamma} \]
    %
    is analytic and non-vanishing on the domain of $f$, and so
    %
    \[ (T - \xi_1) \dots (T - \xi_n) x = g(T) [f(T) - \gamma] x = 0. \]
    %
    But this means that there is some nonzero $y$ with $(T - \xi_i) y = 0$, and so $\xi_i \in \sigma_p(T)$. Since $f(\xi_i) = \gamma$, this shows that $f(\sigma_p(T)) = \sigma_p(f(T))$ if $f$ is non constant on any connected component of it's domain.
\end{proof}

As mentioned briefly at the beginning of this section, given a non-unital algebra $A$, we extend the concepts of this chapter to elements of $A$ by embedding $A$ in $A^\#$. Thus, for instance, if $M \in A$, then
%
\[ \sigma_A(M) = \{ \lambda : \lambda - M\ \text{is not invertible in $A^\#$} \}. \]
%
The spectral radius formula remains true in this setting, as does the result that $\sigma(M)$ is non-empty (though it is rather trivial in this setting, since $0 \in \sigma(M)$ for any $M \in A$). If $M \in A$, and $f$ is holomorphic on a neighborhood of $\sigma(M)$, then we can define $f(M) \in A^\#$. Moreover, we have $f(M) \in A$ precisely when $f(0) = 0$, because if we consider the continuous linear functional $\Lambda(\lambda + M) = \lambda$ on $A^\#$, then $\Lambda(f(M)) = f(0)$ for any polynomial $f$, and we can apply a density argument. Thus we have a holomorphic functional calculus on $A$ provided we consider only holomorphic functions vanishing at the origin.








\chapter{Gelfand Theory}

Gelfand found a very powerful way of transforming commutative Banach algebras into Banach algebras of continuous functions over a compact space, which generalizes the spectral theory of commuting operators, as well as the Fourier transform. The ingenious trick to Gelfand theory is that one can recover the points in the set $K$ via the algebraic structure of $C(K)$ by studying the maximal ideals of $C(K)$. Thus given a general commutative Banach algebra $A$, we can study the set $K$ of maximal ideals of $A$, equipped this set with a topology, and then consider a natural homomorphism of $A$ into $C(K)$ which has powerful algebraic properties. For reasons which might become clear from the last chapter (the main reason being that the only complex Banach division algebras are fields), in this chapter we solely work over algebras defined on the complex numbers.

\section{Algebra Homomorphisms and Ideals}

Given an algebra $A$ containing an ideal $\IA$, we can consider the quotient ring $A/ \IA$. We can consider left, right, and two-sided ideals, and we'll denote them by gaudy letters such as $\IA$ and $\IB$. If $\IA$ is a \emph{closed} ideal in a Banach algebra, then $A/ \IA$ naturally has the structure of a Banach algebra, equipped with the norm defined, for any coset $S \in A/\IA$,
%
\[ \| S \|_{A/\IA} = \inf_{s \in S} \| s \|_A. \]
%It is simple to see that a proper left ideal cannot contain any left invertible elements, and a right ideal cannot contain any right invertible elements. It is also easy to verify that in a Banach algebra, the closure of any ideal is an ideal. In the commutative case, the quotient of an algebra by a maximal ideal is a field. Furthermore, Zorn's Lemma can be applied to show every left, right, or double sided ideal can be extended to a maximal ideal of the same type.
Ideals are naturally connected with homomorphisms $f: A \to B$ between algebras. Every ideal is the kernel of some homomorphism, and the kernel of every homomorphism is an ideal. Similarily, \emph{closed ideals} in a Banach algebra correspond to continuous homomorphisms. The most tractable homomorphisms to study are the \emph{characters}, homomorphisms from a ring $A$ to $\CC$. In this section, we will deduce their structure.

\begin{lemma}
    Let $A$ be a unital Banach algebra and let $\phi: A \to \CC$ be a character. Then $\| \phi \| \leq 1$.
\end{lemma}
\begin{proof}
    Let $\IA$ be the kernel of $\phi$. Then $\IA$ cannot be all of $A$. Thus $\IA$ does not contain any invertible elements of $A$. If $f(M) = \lambda$, then $\lambda - M \in \IA$, so $\lambda - M \in \sigma(M)$. This means $|\lambda| \leq r(M) \leq \| M \|$.
\end{proof}

\begin{corollary}
    Every character is a continuous linear map.
\end{corollary}

\begin{lemma}
    Every maximal ideal of a unital Banach algebra is closed.
\end{lemma}
\begin{proof}
    Let $\mathfrak{a}$ be a maximal ideal of an algebra $A$. It is easy to show the closure of any ideal is an ideal. It follows that either $\overline{\mathfrak{a}} = \mathfrak{a}$ (so that $\mathfrak{a}$ is closed), or $\mathfrak{a}$ is dense in $A$. Suppose the second option holds. Let $M \in GL(A)$ be chosen. Then there is $M_i \in \mathfrak{a}$ converging to $M$. But then the $M_i$ are eventually invertible, since $GL(A)$ is open, from which we conclude $\mathfrak{a} = A$, a contradiction.
\end{proof}

\begin{corollary}
    If $\mathfrak{a}$ is a maximal ideal in a commutative, unital Banach algebra $A$, then $A/\IA$ is isometric to $\CC$.
\end{corollary}
\begin{proof}
    For then $A/\mathfrak{a}$ is a Banach division algebra.
\end{proof}

Thus it follows that the kernel of every character on a commutative, unital Banach algebra $A$ is a maximal ideal in $A$, and conversely, every maximal ideal corresponds to the kernel of some character. Moreover, this character is uniquely determined.

\begin{lemma}
    If $\phi_1: A \to \CC$ and $\phi_2: A \to \CC$ have the same kernel, then $\phi_1 = \phi_2$.
\end{lemma}
\begin{proof}
    Fix $M \in A$. Then
    %
    \[ \phi_1(M - \phi_1(M)) = \phi_1(M) - \phi_1(M) = 0. \]
    %
    Thus $M - \phi_1(M)$ is in the kernel of $\phi_1$. But this means $M - \phi_1(M)$ is also in the kernel of $\phi_2$, so
    %
    \[ \phi_2(M - \phi_1(M)) = \phi_2(M) - \phi_1(M) = 0. \]
    %
    Thus $\phi_1(M) = \phi_2(M)$.
\end{proof}

We denote the set of characters of a Banach algebra by $A$ by $\Phi_A$, and call it the \emph{character space} of $A$. Gelfand's theory is the study of $\Phi_A$, and its relation to $A$, especially when $A$ is a commutative Banach algebra.

\begin{example}
    Let $K$ be a compact space, and consider the algebra $C(K)$ of continuous functions. Given a closed subset $C$ of $K$, the set
    %
    \[ \{ f \in C(K) : f|_C = 0 \} \]
    %
    is an ideal of $C(K)$. We contend these are all such closed ideals. Let $\IA$ be an arbitrary closed ideal of $C(K)$. It is easy to see that the set
    %
    \[ C = \{ x \in K : f(x) = 0\ \text{for all $f \in \IA$} \}. \]
    %
    are closed. Conversely, suppose $g \in C(K)$ and $g|_C = 0$. Fix $\varepsilon > 0$, and let $V = g^{-1}(B_\varepsilon(0))$. For each $y \in K - C$, there is a function $f_y \in \IA$ with $f_y(y) = g(y)$. Let $U_y = (f_y - g)^{-1}(B_\varepsilon(0))$. Then $\{ V \} \cup \{ U_y \}$ is an open cover of $K$, and thus has a finite subcover $V, U_{y_1}, \dots, U_{y_n}$. Let $h_V, h_{y_1}, \dots, h_{y_n}$ be a partition of unity subordinate to the cover. Consider the function $\tilde{g} = \sum h_{y_i} f_{y_i} \in \IA$. For $x \in K - V$,
    %
    \[ |\tilde{g}(x) - g(x)| = | \sum h_{y_i}(x) (f(x) - f_{y_i}(x)) | \leq \sum h_{y_i}(x) |f(x) - f_{y_i}(x)| < \varepsilon \]
    %
    For $x \in V$,
    %
    \[ |\tilde{g}(x) - g(x)| = | \sum h_{y_i}(x) (f(x) - f_{y_i}(x)) - h_V(x) f(x) | < \varepsilon + h_V(x) f(x) \leq 2 \varepsilon \]
    %
    Thus $\| \tilde{g} - g \|_\infty \leq 2 \varepsilon$. It follows that $g$ can be approximated to arbitrary precision by elements of $\IA$, so $g \in \IA$, since $\IA$ is closed. We conclude $\IA$ consists of all functions which vanish on $C$. Thus closed ideals of $C(K)$ are in one-to-one correspondence with closed sets of $K$. But this means that the maximal ideals are precisely in one to one correspondence with the points of $K$. Thus the points in the character space of $C(K)$ is in one-to-one correspondence with the points in $K$.
\end{example}

Given a unital Banach algebra $A$, and $M \in A$, by duality we obtain a function $\widehat{M}: \Phi_A \to \CC$, given by $\widehat{M}(\phi) = \phi(M)$. It is natural to equip $\Phi_A$ with the weakest topology such that each of the functions $\widehat{M}$ is continuous. Thus we obtain an algebra homomorphism $\Gamma: A \to C(\Phi_A)$, given by $\Gamma(M) = \widehat{M}$.

\begin{theorem}
    For any unital Banach algebra $A$, $\Phi_A$ is a compact, Hausdorff space.
\end{theorem}
\begin{proof}
    Let us first verify the Hausdorff condition. Given $\phi, \psi \in \Phi_A$, there is $M \in A$ such that $\phi(M) \neq \psi(M)$. If we pick disjoint neighborhoods $U_0$ and $V_0$ of $\phi(x)$ and $\psi(x)$, then
    %
    \[ U = \widehat{M}^{-1}(U_0)  \quad\text{and}\quad V = \widehat{M}^{-1}(V_0) \]
    %
    are disjoint open sets in $\Phi_A$ separating $\phi$ and $\psi$. Thus $\Phi_A$ is a Hausdorff space.

    Compactness is a little trickier. The space $\Phi_A$ can be viewed as a subset of $A^*$, since characters are linear maps, and the topology on $\Phi_A$ is the relative topology on $A^*$ when $A^*$ is equipped with the weak $*$ topology. Since $\| \phi \| \leq 1$ for each $\phi \in \Phi_A$, $\Phi_A$ is a bounded subset of $A^*$. If we can show it is also weak $*$ closed, then the Banach-Alaoglu theorem will imply that $\Phi_A$ is compact.

    To show $\Phi_A$ is weak-$*$ closed, it suffices to show any functional $\alpha$ in the weak $*$ closure of $\Phi_A$ is a character. Fix $M,N \in A$, and $\varepsilon > 0$. Consider
    %
    \[ W = \{ \psi \in A^* : | \psi - \alpha | (z) < \varepsilon\ \text{for}\ z \in \{ 1, M, N, MN \} \} \]
    %
    Then $W$ is a weak-$*$ neighbourhood of $\alpha$, and thus contains some $\phi \in \Phi_A$. Thus
    %
    \[ |\phi(1) - \alpha(1)| = | 1 - \alpha(1) | < \varepsilon. \]
    %
    Thus we find that $\alpha(1) = 1$, since $\varepsilon$ was arbitrary. Furthermore,
    %
    \begin{align*}
        \alpha(MN) -   \alpha(M) \alpha(N) &= [\alpha(MN) - \phi(MN)] + [\phi(M)\phi(N) - \alpha(M)\alpha(N)]\\
        &= [\alpha(MN) - \phi(MN)] + [\phi(N) - \alpha(N)] \phi(M)\\
        &\ \ \ + [\phi(M) - \alpha(M)] \alpha(N)
    \end{align*}
    %
    Hence, since $\phi$ is a complex homomorphism,
    %
    \[ | \alpha(MN) - \alpha(M) \alpha(N) | < (1 + |\phi(M)| + |\alpha(N)|) \varepsilon \leq (1 + \| M \| + \| \alpha \| \| N \|) \varepsilon \]
    %
    Letting $\varepsilon \to 0$, we find $\alpha(MN) = \alpha(M) \alpha(N)$. Thus $\alpha$ is a character, so $\Phi_A$ is closed, and thus compact.
\end{proof}

\begin{theorem}
    If $f: A \to B$ is an algebra homomorphism, then the map $f^*: \Phi_B \to \Phi_A$ defined such that for any $M \in A$,
    %
    \[ f^*(\phi)(M) = \phi(f(M)) \]
    %
    is continuous.
\end{theorem}
\begin{proof}
    The open sets in $\Phi_A$ are generated for $M \in A$, $\lambda \in \CC$, and $\varepsilon > 0$, by sets of the form
    %
    \[ U = \{ \phi \in \Phi_A: |\phi(M) - \lambda| < \varepsilon \} = \widehat{M}^{-1} \left( B_\varepsilon(\lambda) \right). \]
    %
    The inverse image of $U$ under $f^*$ is precisely
    %
    \[ \{ \psi \in \Phi_B : |(\psi \circ f)(M) - \lambda| < \varepsilon \} = \widehat{f(M)}^{-1} \left( B_\varepsilon(\lambda) \right). \]
    %
    The map is therefore continuous.
\end{proof}

Let us now list the main properties of the Gelfand transform, which make it so useful.

\begin{theorem}
    Let $A$ be a unital Banach algebra. Then for any $M \in A$,
    %
    \[ \widehat{M}(\Phi_A) \subset \sigma(M). \]
    %
    If $A$ is commutative, then
    %
    \[ \widehat{M}(\Phi_A) = \sigma(M). \]
\end{theorem}
\begin{proof}
    If $\phi: A \to \CC$, and $\phi(M) = \lambda$, then $\lambda - M$ lies in some maximal ideal of $A$, and thus cannot be invertible, so $\lambda \in \sigma(M)$. Conversely, if $A$ is commutative, and $\lambda \in \sigma(M)$, then $\lambda - M$ lies in some maximal ideal of $A$, and so there is some character $\phi: A \to \CC$ with $\phi(\lambda - M) = 0$, i.e. $\phi(M) = \lambda$.
\end{proof}

\begin{example}
    The Gelfand transform can be quite useless when studying a noncommutative algebra $A$. For instance, the algebra $M_n(\CC)$ of $n \times n$ matrices has only a single ideal, namely the set $\{ 0 \}$. To see this, suppose $\IA$ is an ideal of $M_n(\CC)$ containing some non-zero matrix $M$. It follows from the elementary theory of matrices that there are invertible matrices $N$ and $L$ such that $NML \in \IA$ is a diagonal matrix, non-zero somewhere on the diagonal. Multiplying on the left by an appropriate matrix, we may assume that there is only a single entry on the diagonal. Multiplying on the left by permutation matrices and then considering linera combinations of what we get then shows that $\IA$ contains all diagonal matrices in $M_n(\CC)$. But as we have seen above, this implies that $\IA$ contains \emph{all} matrices in $M_n(\CC)$. Thus $M_n(\mathbf{C})$ is a {\it simple} algebra, it contains no non-trivial ideals. Consequently, we find that every non-zero algebra homomorphism $F: M_n(\CC) \to A$ must be injective, for the kernel is a proper ideal for $M_n(\CC)$. Thus the character space of $M_n(\CC)$, for $n > 1$, must be $\emptyset$, because there is no injective algebra homomorphism from $M_n(\mathbf{C})$ to $\mathbf{C}$ (there is not even an injective linear map).
\end{example}

Since $\widehat{M}(\Phi_A) \subset \sigma(M)$ for any $M$ in a unital Banach algebra $A$, it follows that
%
\[ \| \Gamma(M) \|_{L^\infty(\Phi_A)} = \rho(M) \leq \| M \|. \]
%
Thus $\Gamma$ is a contractive algebra homomorphism from $A$ into $C(\Phi_A)$, whose kernel is the Jacobson radical of $A$, the intersection of all it's maximal ideals. Thus $\Gamma$ is injective if and only if $A$ is semisimple.

\begin{example}
    An example of a commutative Banach algebra which is not semisimple is the ring $A$ of all matrices of the form
    %
    \[ \begin{pmatrix} a & b \\ 0 & a \end{pmatrix}. \]
    %
    Any matrix of the form above, where $a \neq 0$, is invertible in $A$, and the complement of this set is an ideal. Thus this ring is local. The Gelfand space $\Phi_A$ in this case is just a single point, and a matrix of the form above is mapped to the function taking the value $a$ at this point.
\end{example}

For general Banach $*$ algebras, Gelfand theory does not necessarily react well with involution. We would want to have the property that
%
\[ \widehat{M^*} = \overline{\widehat{M}}, \]
%
for all $M \in A$. This condition is equivalent to $\Gamma: A \to C(\Phi_A)$ being a homomorphism of Banach $*$ algebras. A Banach $*$ algebra with this property is called \emph{symmetric}. An example consequence is that for any $M \in A$,
%
\[ \widehat{M^* M} = |\widehat{M}|^2, \]
%
and if $M$ is unitary, then $|\widehat{M}| = 1$.

\begin{lemma}
    Any unital $C^*$ algebra $A$ is symmetric.
\end{lemma}
\begin{proof}
    We have already seen that $\sigma(M) \subset \RR^+$ for any self-adjoint $M \in A$, so the theorem follows immediately from the holomorphic functional calculus and the fact that $\widehat{M}(\Phi_A) \subset \sigma(M)$. Alternatively, we can give a more elementary proof. Consider a self-adjoint element $M \in A$. If $\phi: A \to \CC$ is a character, and $\phi(M) = a + i b$, then $\phi(M + i t) = a + i(b + t)$. But we have
    %
    \begin{align*}
        |\phi(M + i t)|^2 &= a^2 + (b + t)^2\\
        &\leq \| M + it \|^2\\
        &= \| (M + it)(M - it) \|\\
        &= \| M^2 + t^2 \| \leq \| M \|^2 + t^2.
    \end{align*}
    %
    Subtracting $t^2$ from each side of the equation gives a uniform bound on $a^2 + b^2 + 2bt$ for all $t$, which is only possible if $b = 0$.
\end{proof}

\begin{remark}
    The algebra $A(\mathbf{D})$ with involution $f^*(z) = \overline{f}(\overline{z})$, is not symmetric. Neither is $L^1(\mathbf{Z})$, where $(a^*)_n = \overline{a}_n$. But if we instead consider the convolution $(a^*)_n = \overline{a_{-n}}$, then $L^1(\mathbf{Z})$ \emph{is} a symmetric algebra, which is one of the reaons why this involution is so much more useful than pointwise conjugation.
\end{remark}

\begin{theorem}
    If $A$ is a commutative, symmetric Banach $*$ algebra, then the image of the Gelfand transform is a dense subset of $C(\Phi_A)$.
\end{theorem}
\begin{proof}
    Let $B = \Gamma(A)$. Then $B$ is closed under conjugation because $A$ is symmetric. Since characters are determined by their values on elements of $A$, elements of $B$ separate points in $\Phi_A$. It also contains constants since $\widehat{1} = 1$. Thus by the Stone-Weirstrass theorem, $B$ is dense in $C(\Phi_A)$.
\end{proof}

If $A$ is a \emph{commutative} unital Banach algebra, the spectral radius formula implies that $\Gamma$ is an isometry if and only if for any $M \in A$,
%
\[ \| M^2 \| = \| M \|^2. \]
%
This is true, in particular, if $A$ is a commutative $C^*$ algebra. To see this, using the fact that $A$ is symmetric, we calculate that
%
\[ \| M \|^2 = \| M^* M \| = \| \widehat{M^* M} \|_{L^\infty(\Phi_A)} = \| |\widehat{M}|^2 \|_{L^\infty(\Phi_A)} = \| (\widehat{M})^2 \|_{L^\infty(\Phi_A)} = \| M^2 \|. \]
%
Thus \emph{every commutative, unital $C^*$ algebra is semisimple}. Moreover, the Gelfand transform $\Gamma: A \to C(\Phi_A)$ is then an \emph{isometric $*$-isomorphism} between $A$ and $C(\Phi_A)$. Thus \emph{every commutative $C^*$ algebra is naturally isomorphic to $C(K)$ for some compact space $K$}. This is the commutative case of the \emph{Gelfand-Naimark theorem}.

The commutative Gelfand Naimark theorem has two important applications. The first is a proof of the existence of the \emph{Stone-Cech compactification} of a completely regular topological space. Recall that a space $X$ is \emph{completely regular} if it is $T1$, and if for each closed $C$ and $x \in X - C$, there is $f \in C(X)$ which satisfies $f(x) = 1$, but vanishes on $C$. Urysohn's lemma tells us all normal spaces are completely regular.

\begin{theorem}
    Let $X$ be a completely regular space. Then there is a compact Hausdorff space $\beta X$ with an embedding $i: X \to \beta X$ onto a dense subset of $\beta X$ satisfying the following universal property: For any continuous map $j: X \to K$ into a compact Hausdorf space, there is a unique $F: \beta X \to K$ such that $j = F \circ i$. The universal property implies $\beta X$ is unique up to a homeomorphism, and it is called the \emph{Stone-Cech compactification of $X$}.
\end{theorem}
\begin{proof}
    The space $C_b(X)$ is a commutative unital $C^*$ algebra, so the Gelfand transform gives an isomorphism of $C_b(X)$ with $C(\beta X)$, where $\beta X = \Phi_{C_b(X)}$ is the Character space of $C_b(X)$. For each $x \in X$, the map $\phi_x(f) = f(x)$ is a character on $C_b(X)$, and thus corresponds to a unique point $i(x) \in \beta X$. We thus obtain an injective map $i: X \to \beta X$. We claim it is also continuous. Indeed, the open sets of $\beta X$ are generated, for each $f \in C_b(X)$, $\lambda \in \CC$, and $\varepsilon > 0$, by sets of the form
    %
    \[ U = \{ x \in \beta X: |\widehat{f}(x) - \lambda| < \varepsilon \}. \]
    %
    The inverse image of $U$ by $i$ is precisely
    %
    \[ \{ x \in X : |f(x) - \lambda| < \varepsilon \}, \]
    %
    which is certainly open. Thus $i$ is continuous. We will now show $i: X \to \beta X$ satisfies the required universal properties.

    To begin with, note that $\widehat{f} \circ i = f$ for any $f \in C_b(X)$. Since $C_b(X)$ is a $C^*$ algebra, $\Gamma: C_b(X) \to C(\beta X)$ is an isomorphism, so every continuous, scalar-valued function on $\beta X$ is the extension of a unique bounded, continuous function on $X$ with the same $L^\infty$ norm. It follows that $i(X)$ is dense in $\beta(X)$, e.g. by Urysohn's lemma.

    Next, we show $i: X \to \beta X$ is an embedding. Fix a net $\{ x_\alpha \}$ in $X$, let $x \in X$, and suppose $\{ x_\alpha \}$ does not converge to $x$. Thinning this net if necessary, we may find an open set $U$ containing $x$, but no elements of the net $\{ x_\alpha \}$. Since $X$ is completely regular, we may find $f \in C_b(X)$ whcih vanishes outside of $U$, but with $f(x) = 1$. Then $\widehat{f}(i(x_\alpha)) = 0$, but $\widehat{f}(I(x)) = 1$. Thus $\{ i(x_\alpha) \}$ cannot converge to $i(x)$. Thus we have shown $i$ is an embedding.

    Finally, let $K$ be any compact Hausdorff space, and let $j: X \to K$ be a continuous map. Then $j$ induces an algebra homomorphism $j^*: C(K) \to C_b(X)$, and thus a continuous map from the character space of $C_b(X)$ to the character space of $C(K)$, i.e. a continuous map $F: \beta X \to K$ such that if $x \in \beta X$ corresponds to a character $\phi: C_b(X) \to \CC$, then for any $g \in C(K)$,
    %
    \[ g(F(x)) = \phi(j^*(g)). \]
    %
    In particular, if $x \in X$, then
    %
    \[ g(F(i(x))) = \phi_x(j^*(g)) = j^*(g)(x) = g(j(x)). \]
    %
    Since $g$ was arbirary, it follows that $F \circ i = j$. That $F$ is the unique continuous map making the diagram commute follows because $i(X)$ is dense in $\beta X$. Thus we have verified the universal property.
\end{proof}

For normal elements of a $C^*$ algebra, we now use the Gelfand-Naimark theorem to extend the holomorphic functional calculus to a \emph{continuous functional calculus}. To begin with, note that $A$ is a commutative, unital Banach $*$ algebra containing an element $M$ such that one of three things is true:
%
\begin{itemize}
    \item $A$ is generatedf as a unital algebra by $M$.
    \item $M$ is invertible, and $A$ is generated as an algebra by $M$ and $M^{-1}$.
    \item $A$ is a symmetric algebra, and $A$ is generated as a unital algebra by $M$ and $M^*$.
\end{itemize}
%
Then $\widehat{M}: \Phi_A \to \sigma(M)$ is a homeomorphism. Since $\Phi_A$ is compact, it suffices to show that $\widehat{M}$ is injective. But this follows because, in each of the three cases above, if two characters agree at $M$, they agree on the entirety of $M$. A particular example of importance to us is, for an arbitrary $C^*$ algebra $A$ and a normal element $M \in A$, is $C^*(M)$, the smallest $C^*$ subalgebra generated by $M$. Since $M$ is normal, $C^*(M)$ will be commutative, and generated as a unital algebra by $M$ and $M^*$.

\begin{example}
    Consider $\delta^1 \in L^1(\ZZ)$. Then $\sigma(\delta^1) = \mathbf{T}$. We have $\sigma(\delta^1) \subset \mathbf{D}$ because $\| \delta^1 \|_{L^1(\ZZ)} = 1$. The function $\delta^1$ has an inverse $\delta^{-1}$, and by the spectral theorem and the fact that $\| \delta^{-1} \|_{L^1(\ZZ)} = 1$, $\sigma(\delta^1)^{-1} = \sigma(\delta^{-1}) \subset \mathbf{D}$, so we conclude $\sigma(\delta^1) \subset \mathbf{T}$. In fact, $\sigma(\delta^1)$ is equal to $\mathbf{T}$. To see this, let us try and invert $\lambda \delta^0 - \delta^1$, for some $|\lambda| = 1$. If $c * (\lambda \delta^0 - \delta^1) = \delta^0$, then
    %
    \[ \lambda c_n - c_{n-1} = \begin{cases} 1 & n = 0 \\ 0 & n \neq 0 \end{cases} \]
    %
    Solving these equations recursively, we find $c_n = \lambda^{-n} c_0$ for $n \geq 0$, and $c_{-n} = \lambda^{n-1} c_{-1}$. We must have $c \in L^1(\mathbf{Z})$, so that
    %
    \[ |c_0| \sum_{k = 0}^\infty 1 + |c_{-1}| \sum_{k = 1}^\infty 1 < \infty \]
    %
    Hence $c_0 = c_{-1} = 0$, so that $c = 0$, which is impossible if we also want $c * (\lambda \delta^0 - \delta^1) = \delta^0$ to hold. Thus $\sigma(\delta^1) = \mathbf{T}$.

    Since $\delta^1$ generates $L^1(\ZZ)$, it follows that the Gelfand space of $L^1(\ZZ)$ is homeomorphic to $\TT$. We have seen that $\Gamma(\sum a_n \delta^n) = \sum a_n z^n$. Taking uniform limits of this result gives that $\Gamma: L^1(\ZZ) \to C(\mathbf{T})$ is precisely the inverse Fourier transform on the Torus. Conversely, more sophisticated arguments show that for any locally compact abelian group $G$, the space $L^1(G)$, equipped with convolution (against the Haar measure) as multiplication has Gelfand space equal to $\widehat{G}$, the dual group of complex-valued characters on $G$, and $\Gamma: L^1(G) \to C(\widehat{G})$ is precisely the abstract Fourier transform of functions on $G$. Thus the Gelfand transform is one way to think of the Fourier transform.
\end{example}

Now suppose that $A$ is a commutative unital $C^*$ algebra of the form above. Then $\widehat{M}: \Phi_A \to \sigma(M)$ is a homeomorphism. We thus have an isometric $*$ isomorphism
%
\[ A \cong C(\Phi_A) \cong C(\sigma(M)). \]
%
For $f \in C(\sigma(M))$, we let $f(M) \in A$ denote the image of this map. Thus for any character $\phi: A \to \CC$,
%
\[ \phi(f(M)) = f(\phi(M)). \]
%
This equation implies that $\sigma(f(M)) = f(\sigma(M))$, so we have a continuous functional calculus. Thus we have a continuous functional calculus on $A$. It is simple to see that $z(M) = M$, and $1(M) = 1$, and $f \mapsto f(M)$ is the unique $*$ algebra homomorphism with this property, since functions of the form
%
\[ f(z) = \sum c_{\alpha \beta} z^\alpha \overline{z}^\beta \]
%
are dense in $C(\sigma(M))$, and we see that $f(M) = \sum c_{\alpha \beta} M^\alpha (M^*)^\beta$. In particular, we see that if $f(z) = \sum c_k z^k$, then $f(M) = \sum c_k M^k$, and so this calculus extends the holomorphic functional calculus.

\begin{example}
    Let $T: H \to H$ be a compact normal operator on a Hilbert space $H$. Then we have seen via the spectral theory of such operators that there exists a family of orthogonal projections $\{ P_\lambda \}$ which are mutually orthogonal to one another, such that
    %
    \[ T = \sum_\lambda \lambda P_\lambda. \]
    %
    We claim that for each $f \in C(\sigma(T))$, then
    %
    \[ f(T) = \sum_\lambda f(\lambda) P_\lambda. \]
    %
    Certainly this is true for any polynomial, and under this definition, for any $f,g \in C(\sigma(T))$, $\| f(T) - g(T) \| \leq \| f - g \|_{L^\infty(\sigma(T))}$. Thus this definition is continuous, so by density of polynomials and continuity, we conclude that both definitions give the same result.
\end{example}

The key to Gelfand theory is noticing that characters of Banach algebras naturally reflect the structure of the Banach algebra in question. Understanding the character space leads to a natural understanding of the invertibility of the elements of the algebra. For instance, here is a result on the invertibility of Fourier series initially due to Wiener, whose proof was incredibly difficult without the technology that Gelfand introduced.

\begin{example}
    Consider a formal trigonometric series on $\TT$ of the form
    %
    \[ f(z) = \sum a_n z^n \]
    %
    where $\sum |a_n| < \infty$. Then $f = \Gamma(a)$, where $a \in L^1(\ZZ)$ is given by the sequence $\{ a_n \}$. Now suppose $f$ is non-vanishing on $\TT$. Since $\sigma(a) = f(\TT)$, this means that $a$ is invertible in $L^1(\ZZ)$. Thus there exists $b \in L^1(\ZZ)$ such that $a * b = \delta$. Thus $\Gamma(b) = 1/f$, i.e.
    %
    \[ \frac{1}{f(z)} = \sum b_n z^n. \]
    %
    Similar results can be applied to non-continous functions by approximation, and to higher dimensional trigonometric series, in which case the Gelfand space are the higher dimensional torii $\TT^n$.
\end{example}

Here's another example from the complex analysis of several variables.

\begin{example}
    Consider the commutative algebra $A(\mathbf{D})$. We also contend that $\Phi_{A(\mathbf{D})}$ can be identified with $\mathbf{D}$. The identity function $z$ has norm 1 in this space, implying that if $\phi: A(\mathbf{D}) \to \mathbf{C}$ is any character, then $\phi(z) = w \in \mathbf{D}$. Then
    %
    \[ \phi(\sum_{k = 0}^N a_k z^k) = \sum_{k = 0}^N a_k w^k \]
    %
    and these polynomials are dense in $A(\mathbf{D})$, from which we conclude $\phi(f) = f(w)$. Thus any character is given by evaluation at a point in $\mathbf{D}$.

    Let us use this calculation to obtain an analytic version of the nullstellensatz in algebraic geometry. Consider $n$ functions $f_1, \dots, f_n \in A(\mathbf{D})$ whose common nullset $Z(f_1,\dots,f_n)$ is empty. Then we claim there are $g_1,\dots,g_n \in A(\mathbf{D})$ such that $f_1 g_1 + \dots + f_n g_n = 1$. It is clear that this is equivalent to the ideal relation
    %
    \[ (f_1, \dots, f_n) = A(\mathbf{D}) \]
    %
    If this were not true, then $(f_1, \dots, f_n)$ would be contained in a closed, maximal ideal, and would therefore be annihilated by some $\phi \in \Phi_{A(\mathbf{D})}$. But $\phi$ corresponds to evaluation at some $w \in \mathbf{D}$, so $f_i(w) = 0$ for all $i$, which gives a contradiction. One can apply Runge's theorem to classify the character spaces of arbitrary spaces of the form $A(\Omega)$, but we leave this as an exercise to the reader.
\end{example}

If $M$ and $N$ are operators such that $MN = NM$, then certainly $M^*N^* = N^*M^*$, but it is not necessarily true that $MN^* = N^* M$ or $M^* N = N M^*$. For instance, if $M$ is not a normal operator, and we take $M = N$. But if $M$ and $N$ are normal, then the result is true. In fact, much more is true.

\begin{theorem}[Fuglede-Putnam-Rosenblum]
    Let $A$ be a $C^*$ algebra, and consider normal elements $M,N \in A$, and a third element $L \in A$ such that $ML = LM$. Then $M^* L = L N^*$.
\end{theorem}
\begin{proof}
    Without loss of generality, assume $A$ is unital. If $ML = LN$, then for all $k > 0$, $M^kL = LN^k$, and so by taking power series, we conclude that for any $\lambda \in \CC$,
    %
    \[ e^{\lambda M} L = L e^{\lambda N}, \]
    %
    so that $L = e^{\lambda M} L e^{- \lambda N}$. Thus
    %
    \[ e^{\lambda M^*} L e^{- \lambda N^*} = e^{\lambda (M^* - M)} L e^{\lambda (N - N^*)}. \]
    %
    Note that $e^{\lambda(M^* - M)}$ and $e^{\lambda (N - N^*)}$ are both unitary, and so if we set
    %
    \[ f(\lambda) = e^{\lambda M^*} L e^{- \lambda N^*} = L + \lambda (M^* L - L N^*) + O(\lambda^2), \]
    %
    then we see $f: \CC \to A$ is an entire function of $\lambda$, and $\| f \|_{L^\infty(\CC)} \leq \| L \|$ is uniformly bounded. Thus we conclude $f$ is a constant function. But this means that the first order term of the power series vanishes, i.e. $M^* L = LN^*$.
\end{proof}

\section{The Non-Unital Gelfand Space}

Let us also address the non-unital case of Gelfand theory. In the Gelfand theory, we have a little trouble defining ideals. We call an ideal $\mathfrak{a}$ (an additive subgroup closed under multiplication) for a non-unital algebra $A$ \emph{modular} if there is $N \in A$ such that $MN - M, NM - M \in \mathfrak{a}$ for all $M \in A$. If $\mathfrak{a}$ is modular, it follows that $A/\mathfrak{a}$ contains an identity. Zorn can show us maximal modular ideals exist in any non-unital algebra. We need to edit the proof which shows a maximal ideal is closed, which relies on an algebraic trick.

\begin{lemma}
    A maximal modular ideal in an algebra $A$ is closed.
\end{lemma}
\begin{proof}
    If a maximal ideal $\mathfrak{a}$ was not closed, then we would have $\overline{\mathfrak{a}} = A$. Let $N$ be a right modular identity for $\mathfrak{a}$. Then there is $M \in \mathfrak{a}$ with $\| M - N \| < 1$, so
    %
    \begin{align*}
        N &= (N - M) + M = \sum_{k = 1}^\infty (N - M)^k - \sum_{k = 2}^\infty (N - M)^k + M\\
        &= \sum_{k = 1}^\infty (N - M)^k - \left[ \sum_{k = 1}^\infty (N - M)^k \right] (N - M) + M\\
        &= \underbrace{\left[ \left( \sum_{k = 1}^\infty (N - M)^k \right) M + M \right]}_{\in \mathfrak{a}} - \underbrace{\left[ \left( \sum_{k = 1}^\infty (N - M)^k \right) N - \left( \sum_{k = 1}^\infty (N - M)^k \right) \right]}_{\in \mathfrak{a}}
    \end{align*}
    %
    Given any $L \in A$, $LM - M \in \mathfrak{a}$, implying $M \in \mathfrak{a}$. This implies $\mathfrak{a} = A$, an impossibility.
\end{proof}

\begin{lemma}
    The kernel of any {\it nonzero} algebra homomorphism from $A$ to $\mathbf{C}$ is a maximal modular ideal, and in a commutative non-unital algebra, any maximal modular ideal is the kernel of some non-zero algebra homomorphism.
\end{lemma}
\begin{proof}
    If $\phi: A \to \mathbf{C}$ is a non-zero algebra homomorphism, then $\phi$ is surjective, for if $\phi(M) \neq 0$, then $\phi(\mathbf{C} \cdot x) = \mathbf{C}$. We claim that if $\mathfrak{a} = \ker(\phi)$, and if $\phi(M) = 1$, then $M$ is a modular identify for $\mathfrak{a}$, because
    %
    \[ \phi(MN - N) = \phi(M)\phi(N) - \phi(N) = \phi(N) - \phi(N) = 0 \]
    %
    Implying $MN - N \in \mathfrak{a}$ for each $N$, so $\mathfrak{a}$ is a modular ideal. The first isomorphism theorem gives us an isomorphism from $A/\mathfrak{a}$ to $\mathbf{C}$, so $\mathfrak{a}$ is maximal, since $A/\mathfrak{a}$ is a field.

    If $A$ is commutative, and $\mathfrak{a}$ is a maximal modular ideal, then $A/\mathfrak{a}$ is an algebra with identity, whose only ideals consists of $(0)$ and $A/\mathfrak{a}$, since $\mathfrak{a}$ is maximal, so $A/\mathfrak{a}$ is a field, and is therefore isometric to $\mathbf{C}$, giving us an algebra homomorphism.
\end{proof}

Given this result, we may therefore consider the Gelfand transform of a non-unital algebra, into the character space of all nonzero non-unital algebra homomorphisms. Unitizing $A$ and considering the Gelfand theory on $A^\#$ only adds a single extra character, which normally does not introduce any complications.

\begin{lemma}
    If $A$ is an algebra without identity, and $\phi$ is a character on $A$, then there is a unique non-zero character $\tilde{\phi}$ defined on $A^\#$ which extends $\phi$.
\end{lemma}
\begin{proof}
    Define $\tilde{\phi}(M + \lambda) = \phi(M) + \lambda$. Then $\tilde{\phi}$ is linear, $\tilde{\phi}(1) = 1$, and
    %
    \begin{align*}
        \tilde{\phi}(M + \lambda) \tilde{\phi}(N + \gamma) &= [\phi(M) + \lambda][\phi(N) + \gamma]\\
        &= \phi(MN) + \phi(\lambda N) + \phi(\gamma M) + \lambda \gamma\\
        &= \tilde{\phi}((M + \lambda)(N + \gamma))
    \end{align*}
    %
    The uniqueness of the extension follows from the fact any maximal modular ideal of $A$ can be uniquely extended to a maximal ideal on $A^\#$, for the projection $M + \lambda \mapsto M$ maps ideals to ideals, and the only ideals of $\mathbf{C}$ are $(0)$ and $(1)$.
\end{proof}

\begin{corollary}
    We have
    %
    \[ \Phi_{A^\#} = \Phi_A \cup \{ \phi_\infty \}, \]
    %
    where
    %
    \[ \phi_\infty(M + \lambda) = \lambda. \]
    %
    We have $\sigma(M) \subset \widehat{M}(\Phi_A) \cup \{ 0 \}$, with equality if $A$ is commutative.
\end{corollary}

We can equip $\Phi_A$ with the weak topology in the same way that we introduce a topology in the unital case. In this situation, however, $\Phi_A$ is only \emph{locally compact}, not compact, with $\Phi_{A^\#}$ corresponding to the one point compactification.

\begin{theorem}
    The Gelfand topology on $\Phi_A$ is Hausdorff and locally compact, and we have
    %
    \[ \Gamma: A \to C_0(\Phi_A). \]
\end{theorem}
\begin{proof}
    This follows from the fact that $\Phi_{A^\#}$ is Hausdorff compact, and $\Phi_A$ is obtained from $\Phi_{A^\#}$ by removing a single point. Certainly it follows from this that $\Phi_A$ is Hausdorff. Moreover, for any character $\phi$, we can find an open set $U \subset \Phi_{A^\#}$ containing $\phi$ and an open set $V \subset \Phi_{A^\#}$ containing $\phi_\infty$ such that the closures of $U$ and $V$ are disjoint. But this means that $\overline{U} \subset \Phi_A$ is a compact neighborhood of $\phi$. Thus $\Phi_A$ is locally compact.
%    Let $\psi_0 \in \Phi_A$ be arbitrary, and fix $M \in A$ such that $\psi_0(M) = 1$. We shall verify that
    %
%    \[ K = \{ \psi \in \Phi_A : |\psi(M)| \geq 1/2 \} \]
    %
%    is compact, in which case it's interior will be a compact neighborhood of $\psi_0$, proving that $\Phi_A$ is locally compact. Since characters on $A$ extended to characters on $A^\#$, it follows that $\| \psi(M) \| \leq \| M \|$ for any $M \in A$, and thus $\Phi_A$, and thus $K$ is bounded in $A^*$. It therefore suffices to show $K$ is weak-$*$ closed in $A^*$. Certainly $K$ is closed in $\Phi_A$ because $\widehat{M}: \Phi_A \to \CC$ is continuous. But $K$ is also closed in $\Phi_{A^\#}$ for the same reasons, since $\phi_\infty \not \in K$. Since $\Phi_{A^\#}$ is closed in $A^*$, it follows $K$ is closed in $A^*$.
%
%    But $\Phi_{A^\#} = $
%
%    the corresponding  $K$ is closed in $\Phi_{A^\#}$
%
%    Since $K = \psi_0^{-1}([1/2, \infty))$, where $\Lambda: A^* \to \mathbf{C}$ is the map $\phi \mapsto |\phi(M)|$, then $\Lambda$ is weak $*$ continuous, so $K$ is closed, hence compact. $K$ contains an open neighbourhood
    %
%    \[ U = \{ \psi \in \Phi_A : |\psi(M)| > 1/2 \} \]
    %
%    so $\Phi_A$ is locally compact. If $M \in A$, $\widehat{M} \in C_0(\Phi_A)$, since
    %
%    \[ \{ \phi \in \Phi_A : |\phi(M)| \geq \varepsilon \} \]
    %
%    is compact, and outside of this set $\widehat{M} \leq \varepsilon$.
\end{proof}

\begin{example}
    Suppose $X$ is a locally compact, but not compact, space. Then $C_0(X)$ is a non-unital $C^*$ algebra. It's unitization is then a unital $C^*$ algebra, and thus corresponds to $C(X^*)$ for some compact set $X^*$. We have already seen that $X^*$ is equal to the one point compactification of $X$.
\end{example}

As with the holomorphic functional calculus on non unital Banach algebras, given a normal element $M$ of a non-unital $C*$ algebra $A$, we can define $f(M)$ for any $f \in C(\sigma(M))$, provided that $f(0) = 0$.







\chapter{Positivity}

The model example of this chapter is a positive semidefinite operator on a Hilbert space.

\begin{theorem}
    Let $H$ be a Hilbert space, and suppose $T: H \to H$ is a bounded, self-adjoint operator. Then the following two conditions are equivalent:
    %
    \begin{itemize}
        \item $\sigma(T) \subset [0,\infty)$.
        \item For any $x \in H$, $\langle Tx, x \rangle \geq 0$.
    \end{itemize}
    %
    If these properties are true, we say $T$ is \emph{positive semidefinite}.
\end{theorem}
\begin{proof}
    Suppose $\sigma(T) \subset [0,\infty)$. If we let $f: \sigma(T) \to [0,\infty)$ be defined by setting $f(x) = \sqrt{x}$, and we set $S = f(T)$, then $S$ is also self-adjoint, and $T = S^2$. It follows that for any $x \in H$,
    %
    \[ \langle Tx, x \rangle = \langle S^2x, x \rangle = \langle Sx, Sx \rangle \geq 0. \]
    %
    Conversely, suppose that for all $x \in H$,
    %
    \[ \langle Tx, x \rangle \geq 0. \]
    %
    Then for any $\lambda < 0$, we have
    %
    \[ \langle (T - \lambda)x, x \rangle = \langle Tx, x \rangle - \lambda \| x \|^2 \geq - \lambda \| x \|^2 \gtrsim \ |x \|^2. \]
    %
    Since $| \langle (T - \lambda) x, x \rangle | \lesssim \| (T - \lambda) x \| \| x \|$, it follows that $\| (T - \lambda) x \| \sim \| x \|$, so $T - \lambda$ is an isomorphism of $H$. Thus $\sigma(T) \subset [0,\infty)$.
\end{proof}

Now let $A$ be an arbitrary $C^*$ algebra. A self-adjoint element $M$ is \emph{positive} if $\sigma(M) \subset [0, \infty)$. In shorthand, we write $M \geq 0$. The set of all positive elements in an algebra $A$ is denoted $A_+$. We write $M \leq N$ if $N - M \geq 0$. Thus identifying positive elements gives us a partial ordering on the set of self-adjoint elements of $A$.

\begin{example}
    Let $X$ be a locally compact space. Then $f \in C_0(X)$ is self-adjoint if and only if $f$ is real-valued, and positive if and only if $f \geq 0$, since $\sigma(f) = \{ 0 \} \cup f(X)$. For a measure space $X$, $f \in L^\infty(X)$ is self-adjoint if and only if $f$ is almost-everywhere real valued, and positive if and only if $f \geq 0$ almost everywhere.
\end{example}

\begin{prop}
    Let $A$ be a $C^*$ algebra. If $M \in A_+$, there is a unique $N \in A_+$ for which $M = N^2$, which we denote by $\sqrt{M}$.
\end{prop}
\begin{proof}
    Consider the positive square root map $f(z) = \sqrt{z}$. This is well defined on $\sigma(M)$ since $\sigma(M) \subset [0,\infty)$, and $f \in C(\sigma(M))$, so we may consider $N = f(M)$. Now
    %
    \[ N^2 = f(M)^2 = (f^2)(M) = (\text{id}_{\sigma(M)})(M) = M. \]
    %
    Denote this element by $\sqrt{M}$. If $N^2 = M$ for any other $N \in A_+$, then
    %
    \[ NM = N^3 = (N^2)N = MN \]
    %
    Thus $N$ commutes with $M$, and so it follows that $N$ commutes with $f(M)$ for any $f \in C(\sigma(M))$, since $f(M)$ lies in the unital $C^*$ algebra generated by $M$. Thus the smallest $C*$ algebra $B$ containing $N$ and $f(M)$ is commutative. Applying the Gelfand transform, since $N \in A_+$, $\widehat{N}(\Phi_B) \geq 0$, and $\widehat{N}^2 = M$. This means that $\widehat{N} = \sqrt{\widehat{M}} = \widehat{f(M)}$. Thus $\widehat{f(M)} = \widehat{N}$, so $f(M) = N$.
\end{proof}

If self-adjoint operators behave somewhat like real numbers, positive operators behave like positive real numbers.

\begin{prop}
    Let $A$ be a $C^*$ algebra, and let $M \in A$ be self adjoint. Then there are unique $M_+$, $M_- \in A_+$ for which $M = M_+ - M_-$, and $M_+ M_- = 0$.
\end{prop}
\begin{proof}
    Let $f_+(t) = \text{max}(t,0)$, and $f_-(t) = -\text{min}(t,0)$. Then $f_+$ and $f_-$ are continuous maps, and we may consider $f_+(M)$, $f_-(M)$. Since
    %
    \[ f_+ - f_- = \text{id}_{\sigma(M)} \]
    %
    $f_+(M) - f_-(M) = M$. By the spectral mapping theorem,
    %
    \[ \sigma(f_+(M)) = f_+(\sigma(M)) \subset [0,\infty)\ \ \ \ \ \sigma(f_-) = f_-(\sigma(M)) \subset [0,\infty) \]
    %
    so $f_+(M), f_-(M) \geq 0$. Since $f_+ f_- = 0$, $f_+(M) f_-(M) = 0$. Uniqueness follows by a Gelfand transform calculation analogous to the proof of the uniqueness of square roots.
\end{proof}

%\begin{example}
%    Suppose $M$ is a positive element of $B(H)$. Then, for any $x \in H$,
    %
%    \[ \langle Mx,x \rangle = \langle \sqrt{M}\sqrt{M}x,x \rangle = \langle \sqrt{M}x, \sqrt{M}x \rangle \geq 0 \]
    %
%    Conversely, let $M \in B(H)$ be an operator such that, for any $x \in H$,
    %
%    \[ \langle Mx, x \rangle \geq 0 \]
    %
%    Write $M = N + iL$, with $N$ and $L$ self adjoint.Then
    %
%    \[ \langle Mx, x \rangle = \langle Nx, x \rangle + i \langle Lx, x \rangle \geq 0 \]
    %
%    implying $\langle Lx, x \rangle = 0$ for all $x$, so $L = 0$, and $M = N$ is self-adjoint. The last proposition implies that we may write $M = M_+ - M_-$. Then
    %
%    \[ 0 \leq \langle MM_-x, M_-x \rangle = - \langle M_-^2 x, M_- x \rangle \leq 0 \]
    %
%    Since $H = \overline{M_-H} \oplus \ker M_-$, we find that
    %
%    \[ \langle M_-x, x \rangle = 0 \]
    %
%    for all $x$, implying $M_- = 0$, so $M = M_+$ is positive. Thus positive operators have a nice characterization on Hilbert spaces.
%\end{example}

\begin{lemma}
    Let $A$ be a $C*$ algebra. If $M$ is self-adjoint, $t \geq 0$, and $\| M - t \| \leq t$, then $M$ is positive. If $M$ is positive, and $\| M \| \leq t$, then $\| M - t \| \leq t$. Thus the positive elements of $A$ are a relatively open subset of the self-adjoint elements of $A$.
\end{lemma}
\begin{proof}
    Suppose $\| M - t \| \leq t$. Then $\sigma(M - t) \in [-t,t]$, so
    %
    \[ \sigma(M) = \sigma(M - t) + t \in [0, 2t] \]
    %
    Thus $M \in A_+$. Conversely, if $M \in A_+$ and $\| M \| \leq t$, then $\sigma(M) \subset [0,t]$, and
    %
    \[ \sigma(M - t) = \sigma(M) - t \subset [-t,0] \]
    %
    which means $\| M - t \| = r(M - t) \leq t$.
\end{proof}

The next result shows that $A_+$ is a cone in $A$.

\begin{prop}
    If $A$ is a $C^*$ algebra, then
    %
    \begin{enumerate}
        \item[(a)] $A_+$ is closed.
        \item[(b)] If $M,N \in A_+$, then $M + N \in A_+$.
        \item[(c)] If $M \in A_+$, and $t \geq 0$, then $tM \in A_+$.
        \item[(d)] $A_+ \cap (-A_+) = (0)$.
    \end{enumerate}
\end{prop}
\begin{proof}
    Suppose that $N \in \overline{A_+}$, choose $K > \| N \|$. Fix $\varepsilon > 0$ and pick $M \in A_+$ with $\| N - M \| < \varepsilon$. Then if $\varepsilon$ is chosen small enough, then $\| M \| < K$, and
    %
    \[ \| N - K \| = \| N - M \| + \| M - K \| \leq \varepsilon + K \]
    %
    Letting $\varepsilon \to 0$, we find $N \in A_+$. To prove (b), we apply the inequality in the lemma to conclude
    %
    \[ \| M + N - (\| M \| + \| N \| ) \| \leq \| M - \| M \| \| + \| N - \| N \| \| \leq \| M \| + \| N \| \]
    %
    So $M + N \in A_+$. Since $\sigma(\lambda M) = \lambda \sigma(M)$, we obtain (c). To prove (d), suppose $M \in A_+ \cap -A_+$. Then $\sigma(M) = \{ 0 \}$, and since $M$ is normal, this means that
    %
    \[ \| M \| = r(M) = 0. \qedhere \]
\end{proof}

The next corollary is an analogy of the result over the real numbers that if $|x| \leq 0$, then $x = 0$.

\begin{lemma}
    Let $A$ be a $C^*$ algebra. If $M \in A$, and $-M^*M \in A_+$, then $M = 0$.
\end{lemma}
\begin{proof}
    Since
    %
    \[ \sigma(MM^*) \cup \{ 0 \} = \sigma(M^*M) \cup \{ 0 \} \]
    %
    which can be proved by some algebraic tricks, $-MM^* \in A_+$ as well. If we write $M = T + iS$, where $T$ and $S$ are self-adjoint, then
    %
    \[ M^*M + MM^* = (T - iS)(T + iS) + (T + iS)(T - iS) = 2(T^2 + S^2) \in A_+ \]
    %
    Thus
    %
    \[ M^*M = 2(T^2 + S^2) - MM^* \in A_+ \]
    %
    which implies $M^*M = 0$.
\end{proof}

\begin{prop}
    Let $A$ be a $C^*$ algebra. The following are equivalent:
    %
    \begin{enumerate}
        \item[(a)] $M \in A_+$.
        \item[(b)] There is $N \in A_+$ such that $M = N^2$.
        \item[(c)] There is $N \in A$ such that $N^*N = M$.
    \end{enumerate}
\end{prop}
\begin{proof}
    We have already show (a) implies (b). The proof of (c) from (b) is trivial. To prove (a) from (c), note that if $M = N^*N$, then $M$ is certainly self-adjoint, so we may write $M = M_+ - M_-$. Let $L = NM_-$. Then
    %
    \[ - L^*L = - M_-N^*NM_- = - M_- M M_- = M_-^3 \in A_+ \]
    %
    Implying $NM_- = 0$, hence
    %
    \[ 0 = N^*L = N^*NM_- = -M_-^2 \in A_+ \]
    %
    This implies $M_- = 0$, so $M = M_+ \in A_+$.
\end{proof}

For $M$ lying in a $C*$ algebra $A$, we are thus inclined to define $|M| = \sqrt{MM^*}$, for any $M \in A$, which is a positive element.

\begin{prop}
    Let $A$ be a $C^*$ algebra. If $M \leq N$, then
    %
    \begin{enumerate}
        \item[(a)] $M + L \leq N + L$.
        \item[(b)] $L^*ML \leq L^*NL$.
    \end{enumerate}
    %
    If in addition, $0 \leq M \leq N$, then
    %
    \begin{enumerate}
        \item[(c)] $\| M \| \leq \| N \|$.
        \item[(d)] $\sqrt{M} \leq \sqrt{N}$.
    \end{enumerate}
    %
    and if $M, N \in GL(A)$, then
    %
    \begin{enumerate}
        \item[(e)] $0 \leq N^{-1} \leq M^{-1}$
    \end{enumerate}
\end{prop}
\begin{proof}
    (a) is trivial. To prove (b), note that
    %
    \[ L^*(N - M)L = L^*\sqrt{N - M}\sqrt{N - M}L = (\sqrt{N - M} L)^* (\sqrt{N - M} L) \in A_+ \]
    %
    Let us prove (c). If $N$ is positive, then $N \leq \| N \|$, which follows by Gelfand theory. Thus if $M$ and $N$ are positive, then
    %
    \[ 0 \leq M \leq N \leq \| N \| \]
    %
    But then $\phi(M) \leq \| N \|$ for each $\phi$, so $\| M \| \leq \| N \|$.

    Fix $\varepsilon > 0$. Write
    %
    \[ (\varepsilon + \sqrt{N} + \sqrt{M})(\varepsilon + \sqrt{N} - \sqrt{M}) = T \]
    %
    where $T,U \in A$ are self adjoint. By calculation
    %
    \[ T = \varepsilon^2 + 2 \varepsilon \sqrt{N} + N - M \geq \varepsilon^2 \]
    %
    Thus $T$ is positive and invertible, so $\varepsilon + \sqrt{N} - \sqrt{M}$ is left invertible and therefore invertible. Thus $\varepsilon \not \in \sigma(\sqrt{M} - \sqrt{N})$, and thus
    %
    \[ \sigma(\sqrt{M} - \sqrt{N}) \subset (-\infty, 0) \]
    %
    But this implies $\sqrt{N} - \sqrt{M}$ is positive.

    If $M \geq \varepsilon$, then $M^{-1} \leq 1/\varepsilon$, which follows by Gelfand theory, since $\phi(M^{-1}) = \phi(M)^{-1}$. Since
    %
    \[ 1 = \sqrt{M}^{-1}M\sqrt{M}^{-1} \leq \sqrt{M}^{-1} N \sqrt{M}^{-1} \]
    %
    this yields
    %
    \[ \sqrt{M} N^{-1} \sqrt{M} \leq 1 \]
    %
    and by conjugation again,
    %
    \[ N^{-1} = \sqrt{M}^{-1} \sqrt{M} N^{-1} \sqrt{M} \sqrt{M}^{-1} \leq M^{-1}. \qedhere \]
\end{proof}

\section{Positive Approximate Units}

We now show that all $C^*$ algebras have \emph{approximate units}, which are bounded approximate identities which are also increasing nets, in the sense of the positive ordering on the $C^*$ algebra.

\begin{lemma}
    If $0 \leq M \leq N$, then $M(1 + M)^{-1} \leq N(1 + N)^{-1}$.
\end{lemma}
\begin{proof}
    Note that
    %
    \[ M(1 + M)^{-1} = 1 - (1 + M)^{-1}\ \ \ \ \ N(1 + N)^{-1} = 1 - (1 + N)^{-1} \]
    %
    We know $1 + M \leq 1 + N$, so $(1 + N)^{-1} \leq (1 + M)^{-1}$, so
    %
    \[ N(1 + N)^{-1} = 1-(1 + N)^{-1} \leq 1-(1 + M)^{-1} = M(1 + M)^{-1} \]
    %
    which is exactly the inequality we needed.
\end{proof}

\begin{prop}
    For a $C^*$ algebra $A$, the net
    %
    \[ \{ M \in A_+ : \| M \| < 1 \} \]
    %
    ordered by the positivity of the algebra, is a bounded approximate unit.
\end{prop}
\begin{proof}
    The set is a net, because any $M,N \in A_+$ are less than or equal to $\max(\|M\|,\|N\|)$. Fix a self adjoint $M \in A$, and let $\varepsilon > 0$. Let $B$ be the (non-unital) $C^*$ algebra generated by $M$. Then $B$ is commutative, and we can consider the continuous function $m: \Phi_B \to \CC$ given by the Gelfand transform of $M$. Pick a function $i \in C_0(\Phi_B)$ such that $0 \leq i < 1$ and $\| (1 - i) m \| < \varepsilon$, and let $E = i(M)$. Then $\| E \| \leq 1$, $E \geq 0$, and $\| M - ME \| = \| (1 - i) m \|_{L^\infty} < \varepsilon$. If $E \leq E' < 1$, then since $\sigma(\sqrt{1 - E'}) \in [0,1]$, $\| \sqrt{1 - E'} \| < 1$, and so
    %
    \begin{align*}
        \| M - E'M \|^2 &= \| \sqrt{1 - E'}^2 M \|^2 \leq \| \sqrt{1 - E'}\ M \|^2 = \| M(1 - E')M \|\\
        &\leq \| M(1 - E)M \| \leq \| M \| \| (1 - E)M \| < \| M \| \varepsilon.
    \end{align*}
    %
    Decreasing $\varepsilon$ is arbitrary, we find $\lim LM = M$. Similar results show $\lim ML = M$. But this proves the result in general, since any $M \in A$ and be written as $M_1 + i M_2$ for appropriate self-adjoint elements $M_1,M_2 \in A$.
\end{proof}

\begin{remark}
    If $A$ is separable, then we can take a subsequence of the net above which is also an approximate unit. To see this, for any self-adjoint $M \in A$, if $B$ is as in the last proof, then $B$ is separable, $C_0(\Phi_B)$ is separable, and so using the functional calculus, we can find a countable sequence $\{ F_i \in A_+ \}$ such that $M = \lim MF_i = \lim F_iM$. If we let $\{ M_j \}$ be a countable, dense subset of self-adjoint elements of $A$, and let $\{ F_{ji} \in A_+ \}$ be the elements such that $M_j = \lim_i F_{ji} M_j = \lim_i M_j F_{ji}$. If we pick some sequence $\{ E_k \in A_+ \}$ such that $E_k \geq E_{ji}$ for all $i,j \leq k$, then $M_j = \lim_k E_k M_j = \lim_k M_j E_k$ for all $j$, and by density, this implies that $M = \lim_k E_k M = \lim_k M E_k$ for all self-adjoint $M$. Thus any separable $C^*$ algebra has an approximate unit which is a sequence.
\end{remark}

If $A$ is an abelian $C^*$ algebra, then the family of self adjoint elements in $A$ forms a real Riesz lattice, which can be easily seen from an isometric representation $A \cong C(X)$. Thus if $0 \leq M \leq N + L$, where $N,L \in A_+$, then there are $N_0,L_0$ such that $N_0 \leq N$, $L_0 \leq L$, and such that $M = N_0 + L_0$. This theorem does not hold in general nonabelian $C^*$ algebras, but there is a weaker decomposition theorem which does hold.

\begin{prop}
    If $A$ is a $C^*$ algebra, and
    %
    \[ \sum_{i = 0}^m M_i^* M_i = \sum_{j = 0}^n N_i^* N_i \]
    %
    then there are $L_{i,j}$ such that
    %
    \[ M_i^* M_i = \sum_j L_{i,j}^* L_{i,j}\ \ \ \ \ \ N_j^* N_j = \sum_i L_{i,j}^* L_{i,j} \]
\end{prop}
\begin{proof}
    We may assume $A$ is unital. TODO
\end{proof}







\section{Ideals and Quotients of $C^*$ Algebras}

If we quotient a $C^*$ algebra by a closed ideal, we certainly get a Banach algebra. But does this algebra still have an involution? Or do we need to apply additional structure to our ideals? Such discussions will naturally lead to the Gelfand Naimark construction, establishing that every $C^*$ algebra is isometric to some subalgebra of bounded operators on a Hilbert space. Thus this endeavor will prove very fruitful. Unless stated otherwise, by an ideal of a $C^*$ algebra we mean a closed, two-sided ideal. Left and right ideals are also assumed to be closed unless stated otherwise.

\begin{lemma}
    If $\IA$ is a closed left/right ideal in a $C^*$ algebra $A$ containing a self-adjoint element $M$, and if $f \in C(\sigma(M))$ satisfies $f(0) = 0$, then $f(M) \in \mathfrak{a}$.
\end{lemma}
\begin{proof}
    We may approximate $f(M)$ on $C(\sigma(M))$ by polynomial expressions of the form $a_1 M + \dots + a_n M^n$. Such expressions all lie in $\IA$, and because $\IA$ is closed, this implies $f(M)$ is closed.
\end{proof}

\begin{corollary}
    If $M \in \IA$ is self adjoint, then $M_+$, $M_-$, $|M|$, and $\sqrt{M}$ are all in $\IA$.
\end{corollary}

\begin{lemma}
    Let $A$ be a $C^*$ algebra. Then every ideal of $A$ is self-adjoint.
\end{lemma}
\begin{proof}
    Let $\mathfrak{a}$ be an ideal of $A$. Then $\mathfrak{a}^*$ is also an ideal. Let $\mathfrak{b} = \mathfrak{a} \cap \mathfrak{a}^*$. Then $\mathfrak{b}$ is a $C^*$ subalgebra of $A$ containing $\mathfrak{a} \mathfrak{a}^*$. We therefore know from the last section that $\mathfrak{b}$ has an approximate unit $\{ E_\alpha \}$. But this means that for any $M \in \mathfrak{a}$,
    %
    \begin{align*}
        \| M^* - M^* E_\alpha \|^2 &= \| (M - E_\alpha M) (M^* - M^* E_\alpha) \|\\
        &= \| MM^*(1 - E_\alpha) + (1 - E_\alpha) MM^* E_\alpha \|\\
        &\leq \| MM^* (1 - E_\alpha) \| + \| (1 - E_\alpha) MM^* \|.
    \end{align*}
    %
    Both of these quantities converge to zero as $\alpha \to \infty$, since $MM^* \in \mathfrak{b}$. Thus we find that $M^* = \lim_\alpha M^* E_\alpha$. But $M^* E_\alpha \in \mathfrak{a}$, so $M^* \in \mathfrak{a}$. But this means $\mathfrak{a}$ is self-adjoint.
\end{proof}

It follows from this result that if $A$ is a $C^*$ algebra, and $\mathfrak{a}$ is an ideal, then $A / \mathfrak{a}$ will be a Banach-$*$ algebra, since the involution naturally respects equivalence classes, and moreover, because $\mathfrak{a}$ is self-adjoint,
%
\[ \| M + \mathfrak{a} \| = \inf_{N \in A} \| M + N \| = \inf_{N \in A} \| M^* + N^* \| = \inf_{N \in A} \| M^* + N \| = \| M^* + \mathfrak{a}. \]
%
Thus the involution is an isometry.

A subalgebra $B$ of a $C^*$ algebra $A$ is called \emph{hereditary} if whenever $M \in B_+$ and $M \geq N$ for some $N \in A_+$, we have $N \in B$. To show that quotients of $C^*$ algebras remain $C^*$ algebras, we shall now show that all ideals in a $C^*$ algebra are hereditary.

\begin{lemma}
    Let $A$ be a $C^*$ algebra. If $|M|^2 \leq N$, then there is $L \in A$ with $\| L \| \leq \| N \|^{1/4}$ such that $M = LN^{1/4}$.
\end{lemma}
\begin{proof}
    Let
    %
    \[ L_n = M(N + 1/n)^{-1/2} N^{1/4}, \]
    %
    which is well-defined in $A$ even if $A$ is non-unital. Nonetheless, the following calculation will be done in the unitization of $A$. If
    %
    \[ D_{nm} = (N + 1/n)^{-1/2} - (N + 1/m)^{-1/2}, \]
    %
    we find that
    %
    \begin{align*}
        \| L_n - L_m \|^2 &= \| M D_{nm} N^{1/4} \|^2\\
        &= \| N^{1/4} D_{nm} M^* M D_{nm} N^{1/4} \|\\
        &\leq \| N^{1/4} D_{nm} N D_{nm} N^{1/4} \|\\
        &= \| D_{nm} N^{3/4} \|^2\\
        &= \| f_n(N) - f_m(N) \|^2\\
        &\leq \| f_n - f_m \|_{L^\infty(\sigma(N))},
    \end{align*}
    %
    where
    %
    \[ f_n(t) = (t + 1/n)^{-1/2} t^{3/4}. \]
    %
    But $f_n$ converges locally uniformly on $[0,\infty)$ to $t^{1/4}$, which means that $\| L_n - L_m \|$ is Cauchy. Thus we have $L = \lim L_n$ for some $L \in A$. Similarily, we find that $(N + 1/n)^{-1/2} N^{1/2}$ converges locally uniformly to the identity, so that
    %
    \[ LN^{1/4} = \lim_n L_n N^{1/4} = \lim_n M (N + 1/n)^{-1/2} N^{1/2} = M. \qedhere \]
\end{proof}

\begin{theorem}
    Let $A$ be a $C^*$ algebra. Then any ideal $\mathfrak{a}$ of $A$ is hereditary.
\end{theorem}
\begin{proof}
    Suppose $0 \leq M \leq N$, and $N \in \mathfrak{a}$. Then we can write $M = T^*T$ for some $T \in A$. The Lemma above finds that $T = LN^{1/4}$ for some $L \in A$. Since $N^{1/4} \in \mathfrak{a}$, $T \in \mathfrak{a}$, and thus $M = T^*T \in \mathfrak{a}$.
\end{proof}

\begin{theorem}
    If $A$ is a $C^*$ algebra, and $\IA$ is an ideal, then $A / \IA$ is a $C^*$ algebra.
\end{theorem}
\begin{proof}
    All that remains to be checked is that for any $M \in A$,
    %
    \[ \| M^* M + \IA \| = \| M + \IA \|^2. \]
    %
    If $\{ E_\alpha \}$ is an approximate identity for $\IA$, then we claim that for any $N \in A$, $\lim_\alpha \| N(1 - E_\alpha) \|$ converges, and
    %
    \[ \| N + \IA \| = \lim_\alpha \| N(1 - E_\alpha) \|. \]
    %
    To see the limit exists, note that for $\alpha_1 \leq \alpha_2$, using the construction of the approximate identity using the functional calculus, we can write $(1 - E_{\alpha_2}) = (1 - E_{\alpha_1}) L$ for some $L \in A$ with $\| L \| \leq 1$, and then we conclude that
    %
    \[ \| N(1 - E_{\alpha_2}) \| = \| N (1 - E_{\alpha_1}) L \| \leq \| N (1 - E_{\alpha_1}) \|. \]
    %
    Thus the net is monotonically decreasing and bounded from below, and thus converges. Now we certainly have $\| N + \IA \| \leq \lim_\alpha \| N(1 - E_\alpha) \|$ since $N E_\alpha \in \IA$. On the other hand, for any $\varepsilon > 0$, we can find $L \in \IA$ such that
    %
    \[ \| N + \IA \| \geq \| N + L \| - \varepsilon. \]
    %
    But then
    %
    \begin{align*}
        \lim_\alpha \| N(1 - E_\alpha) \| &\leq \lim_\alpha \| (N - L)(1 - E_\alpha) \| + \| L(1 - E_\alpha) \|\\
        &\leq  \| N - L \| + 0\\
        &\leq \| N + \IA \| + \varepsilon.
    \end{align*}
    %
    Taking $\varepsilon \to 0$ completes the proof of the claim. And now we see that
    %
    \begin{align*}
        \| M^* M + \IA \|^2 = \lim_\alpha \| M^* M (1 - E_\alpha) \|^2\\
        &\geq \lim_\alpha \| (1 - E_\alpha) M^* M (1 - E_\alpha) \|^2\\
        &= \lim_\alpha \| M (1 - E_\alpha) \|^4\\
        &= \| M + \IA \|^4.
    \end{align*}
    %
    Taking square roots gives the required condition.
\end{proof}

Thus if $\mathfrak{a}$ is a closed ideal in a $C^*$-algebra $A$, then $A/\mathfrak{a}$ is verified to be a Banach $*$ algebra. It is only a little more work now to verify that $A/\mathfrak{a}$ is a $C^*$ algebra.

\begin{lemma}
    If $\mathfrak{a}$ is a closed ideal of $A$, and $\{ E_\alpha \}$ is the positive BAI for $\mathfrak{a}$, then $A/\mathfrak{a}$, then for any $M \in A$,
    %
    \[ \| M + \mathfrak{a} \| = \lim \| M - E_\alpha M \| = \lim \| M - M E_\alpha \| \]
\end{lemma}
\begin{proof}
    Fix $M \in A$, and let $\{ E_\alpha \}$ be the positive BAI for $\mathfrak{a}$. Choose $\varepsilon > 0$, and let $N \in \mathfrak{a}$ satisfy $\| M + N \| \leq \| M + \mathfrak{a} \| + \varepsilon$. Then
    %
    \begin{align*}
        \limsup_\alpha \| M - E_\alpha M \| &\leq \limsup_\alpha \| (1 - E_\alpha)(M + N) \| + \| E_\alpha N - N \|\\
        &\leq \limsup_\alpha \| (1 - E_\alpha)(M + N) \| + \limsup_\alpha \| N - E_\alpha N \|\\
        &= \limsup_\alpha \| (1 - E_\alpha) (M + N) \|\\
        &\leq \| M + N \| \leq \| M + \mathfrak{a} \| + \varepsilon
    \end{align*}
    %
    and $\varepsilon$ was arbitrary, so
    %
    \[ \limsup_\alpha \| M - E_\alpha M \| \leq \| M + \mathfrak{a} \| \]
    %
    But $E_\alpha M \in \mathfrak{a}$, so for each $\alpha$,
    %
    \[ \| M - E_\alpha M \| \geq \| M + \mathfrak{a} \| \]
    %
    and since we have bounded the $\limsup$ above and below by the same value, it is in fact a limit, and has value $\| M + \mathfrak{a} \|$. Similar results hold for $\| M - M E_\alpha \|$.
\end{proof}

The lemma makes sense, for $E_\alpha$ approximates elements of $\mathfrak{a}$ as best as possible, so subtracting $E_\alpha M$ subtracts the best approximation of $M$ in $\mathfrak{a}$, thus giving us the quotient norm.

\begin{theorem}
    If $\mathfrak{a}$ is a closed ideal of a $C^*$ algebra $A$, then $A/\mathfrak{a}$ is a $C^*$ algebra.
\end{theorem}
\begin{proof}
    Letting $\{ E_\alpha \}$ be the positive approximate identity for $\mathfrak{a}$, we find
    %
    \begin{align*}
        \| M + \mathfrak{a} \|^2 &= \lim_\alpha \| M (1 - E_\alpha) \|^2\\
        &= \lim_\alpha \| (1 - E_\alpha) M^*M (1 - E_\alpha) \|\\
        &\leq \lim_\alpha \| (1 - E_\alpha) M^*M \|\\
        &= \| M^*M + \mathfrak{a} \|\\
        &\leq \| M^* + \mathfrak{a} \| \| M + \mathfrak{a} \|
    \end{align*}
    %
    And it is easy to see that $\| M^* + \mathfrak{a} \| = \| M + \mathfrak{a} \|$, for $\mathfrak{a}$ is closed under involution, so we find
    %
    \[ \| M + \mathfrak{a} \|^2 = \| M^*M + \mathfrak{a} \| \]
    %
    and this is exactly the $C^*$ identity for the quotient algebra.
\end{proof}

Thus every $*$-morphism is continuous. We shall find that injective $*$-morphisms are isometries. To prove this, we must first verify a corresponding theorem about continuous functions on a locally compact space, which says that the uniform norm on $C(X)$ is the sharpest algebra norm we can have.

\begin{lemma}
    Let $X$ be a compact Hausdorff space, and let $\vvvert \cdot \vvvert$ be a submultiplicative norm on $C(X)$. Then for any $f \in C(X)$,
    %
    \[ \| f \|_{L^\infty(X)} \leq \vvvert f \vvvert. \]
\end{lemma}
\begin{proof}
    Let $A$ be the Banach algebra formed by completing $C(X)$ with respect to the norm $\vvvert \cdot \vvvert$. Then $A$ is a Banach algebra, and since $C(X) \subset A$, for any $f \in C(X)$, $r_{C(X)}(f) \leq r_A(f)$. Thus
    %
    \[ \| f \|_{L^\infty(X)} = r_{C(X)}(f) \leq r_A(f) \leq \vvvert f \vvvert. \qedhere \]
\end{proof}

\begin{lemma}
    Let $A$ and $B$ be $C^*$ algebras (possibly without unit), and let $\pi: A \to B$ be a nont-necessarily continuous non-zero $*$-algebra homomorphism (in particular, if $\pi$ is a unital $*$-algebra homomorphism). Then $\pi$ is continuous, $\| \pi \| = 1$, $\pi(A)$ is a $C^*$ subalgebra of $B$, and $\pi$ induces an isometry from $A/\IA$ to $B$, where $\IA$ is the kernel of $\pi$.
\end{lemma}
\begin{proof}
    For $M \in A$, we have
    %
    \[ \| \pi(M) \| = \sigma(\pi(M)) \leq \sigma(M) = \| M \|, \]
    %
    so $\| \pi \| \leq 1$. Thus $\pi$ is continuous. Thus the kernel $\IA$ of $\pi$ is closed, and $\pi$ factors through $A/\IA$. Thus for the remainder of the argument, we can switch $A$ with $A / \IA$ and assume $\pi$ is injective. If $\pi$ was not an isometry, there would be $M \in A$ such that $t = \| \pi(M) \| < \| M \| = r$. Then $\| \pi(M^*M) \| < \| M^*M \|$, so we may assume without loss of generality that $M \in A_+$. Let $f: \RR \to \RR$ is a continuous function vanishing on $[0, t]$ and with $f(r) = 1$. Then $f(\pi(M)) = 0$, but $f(M) \neq 0$ since $\sigma(f(M)) = f(\sigma(M)) \neq \{ 0 \}$, contradiction the fact that $\pi$ is injective. Thus $\pi$ is an isometry, from which it follows that $\pi(A)$ is closed, and thus a $C^*$ subalgebra of $B$, and that $\| \pi \| = 1$.
\end{proof}

Thus to calculate the norm on $A/\IA$, it is often easier to find a homomorphism $\pi: A \to B$ with $\IA$ the kernel of $\pi$, so that $\| M + \IA \| = \| \pi(M) \|$.

\begin{example}
    We have calculated that all closed ideals of $C(X)$ are of the form
    %
    \[ \mathfrak{a}_K = \{ f \in C(X) : (\forall x \in K: f(x) = 0) \} \]
    %
    for some closed set $K$. Then $C(X)/\mathfrak{a}_K$ is isometric to $C(K)$, because we have the $C^*$ homorphism $f \mapsto f|_K$, and $\mathfrak{a}_K$ is the kernel.
\end{example}

\begin{example}
    Let $H$ be a Hilbert space, and $\mathfrak{a}$ a closed ideal of $B(H)$. If $\mathfrak{a} \neq (0)$, we claim $K(H) \subset \mathfrak{a}$. For any $x,y \in H$, let $x \otimes y \in B(H)$ be the map
    %
    \[ z \mapsto \langle z, y \rangle x \]
    %
    Because $H$ satisfies the approximation property, $K(H)$ is the closed span of the family $\{ x \otimes y: x,y \in H \}$. If $T \in \mathfrak{a}$ and $T \neq 0$, then there are $v,w$ such that $\langle Tv, w \rangle = 1$. Then
    %
    \[ (x \otimes y) = (x \otimes w) \circ (Tv \otimes y) = (x \otimes w) \circ T \circ (v \otimes y) \in \mathfrak{a} \]
    %
    from which we obtain, since $x \otimes y$ was arbitrary, that $K(H) \subset \mathfrak{a}$. As a special case of this, if $H$ is finite dimensional, with some $n = \dim(H)$, we see that $B(H)$, which can be identified with the algebra $M_n(\CC)$ of $n \times n$ matrices, is simple, i.e. the algebra has no non-trivial ideals. The standard proof that $M_n(\CC)$ is simple, which we briefly described in the section on Gelfand theory in these notes, is essentially a version of this argument stated in the language of matrices.

    In a separable Hilbert space $H$, the \emph{only} closed ideals of $B(H)$ are $\{ 0 \}$, $K(H)$, and $B(H)$. To see this, let $\IA$ be an ideal properly containing $K(H)$, and let $T: H \to H$ be a non-compact operator in $\IA$. Without loss of generality, we may assume $T$ is positive semidefinite. TODO: WHEN BETTER AT SPECTRAL THEORY.

    Thus we can consider the \emph{Calkin Algebra} $C(H) = B(H)/K(H)$. This is a $C^*$ algebra that has no nontrivial closed ideals. What makes this algebra interesting is that it has no natural representation as a $C^*$ subalgebra of bounded operators on a Hilbert space (one cannot represent it as a $C^*$ subalgebra of operators on a separable Hilbert space), so it is some sense, an `abstract $C^*$ algebra'.
\end{example}



\begin{lemma}
    If $\pi: A \to B$ is an injective $*$-morphism between abelian $C^*$ algebras, then the dual map $\pi^*$ is surjective.
\end{lemma}
\begin{proof}
    Let
    %
    \[ K = \{ \phi|_A : \phi \in \Phi_B \]
    %
    If $K \neq \Phi_A$, then using the continuous functional calculus, we may find non-zero $f$ and $g$ which vanish at $K$, for which $f = fg$. Then $1 \in \sigma(g)$, so there is $\phi \in \Phi_B$ such that $\phi(g) = 1$. But this is clearly impossible.
\end{proof}

\begin{prop}
    An injective, continuous morphism $\pi: A \to B$ from a $C^*$ algebra to a Banach algebra satisfies
    %
    \[ \| M \| \leq \| \pi \| \| \pi(M) \| \]
\end{prop}
\begin{proof}
    Given $M$, let $A_0 = C^*(M^*M)$. Define a norm on $A_0$ by letting
    %
    \[ \vvvert{N} = \| \pi(N) \| \]
    %
    Then $\| \cdot \| \leq \vvvert{\cdot}$, and
    %
    \[ \| M \|^2 = \| M^*M \| \leq \vvvert{M^*M} = \| \pi(M^*M) \| \leq \| \pi \| \| M \| \| \pi(M) \| \]
    %
    which yields the claim.
\end{proof}

\begin{corollary}
    Let $\pi: A \to B$ be a $*$ homomorphism between $C^*$ algebras. Then $\pi(A)$ is a closed $C^*$ algebra, and $\pi$ is an isometry if it is injective.
\end{corollary}
\begin{proof}
    If $\pi$ is injective, it certainly has closed range by the last proposition. But in general, letting $\mathfrak{a} = \ker(\pi)$, $\pi$ induces an injective map
    %
    \begin{center}
    \begin{tikzcd}
        A \arrow{r}{\pi} \arrow{d}{} & B \\
        A/\mathfrak{a} \arrow{ru}{\tilde{\pi}}
    \end{tikzcd}
    \end{center}
    %
    and $\pi(A) = \tilde{\pi}(A)$ is closed. Thus if $\pi$ is injective we obtain an inverse $*$ homomorphism $\pi^{-1}: \pi(A) \to A$, and both must be contractible, hence isometries.
\end{proof}

\begin{prop}
    If $B$ is a $C^*$ subalgebra of $A$, and $\mathfrak{a}$ a closed ideal of $A$, then $B + \mathfrak{a}$ is a $C^*$ subalgebra of $A$.
\end{prop}
\begin{proof}
    Completeness is a three space property, and since $\mathfrak{a}$ is complete, we need only show that $(B + \mathfrak{a})/\mathfrak{a}$ is complete, and from this we will obtain completeness. Let $\pi: A \to A/\mathfrak{a}$ be the quotient map. But the composed $*$ morphism $\psi$ defined by
    %
    \[ B \to A \to A/\mathfrak{a} \]
    %
    tells us that the image of $B$ is closed, and
    %
    \[ \psi(B) = (B + \mathfrak{a})/\mathfrak{a} \]
    %
    so $(B + \mathfrak{a})/\mathfrak{a}$ is complete.
\end{proof}




\subsection{Positive Operators}

Every $C^*$ algebra is essentially a ring of bounded operators on a Hilbert space. The challenge is to construct a canonical Hilbert space from a $C^*$ algebra. The study of a certain subclass of operators will be essential. A \emph{positive} operator $\phi: A \to \mathbf{C}$ on a $C^*$ algebra maps $A_+$ into $[0,\infty)$. Certainly the set of positive maps form a real subspace of $A^*$.

\begin{example}
    If $\phi \in \Phi_A$, then $\phi$ is positive, for
    %
    \[ \phi(M^*M) = |\phi(M)|^2 \]
    %
    and every positive element can be written in the form $M^*M$.
\end{example}

\begin{example}
    If $X$ is locally compact and Hausdorff, and $\mu$ is a complex Borel measure on $X$, then the functional
    %
    \[ f \mapsto \int f d\mu \]
    %
    from $C_0(X)$ to $\mathbf{C}$ is positive if and only if $\mu$ is a positive measure.
\end{example}

\begin{example}
    Consider the trace map $\text{tr}: M_n(\mathbf{C}) \to \mathbf{C}$. Any positive matrix $M$ can be, by a change of basis, put into the form
    %
    \[ \begin{pmatrix} \lambda_1 & 0 & \dots & 0 \\ 0 & \lambda_2 & \dots & 0 \\ 0 & 0 & \ddots & 0 \\ 0 & 0 & \dots & \lambda_n \end{pmatrix} \]
    %
    where $\lambda_1, \dots, \lambda_n \geq 0$. The trace is invariant of a change in basis, and thus
    %
    \[ \text{tr}(M) = \sum \lambda_i \geq 0 \]
    %
    Thus the trace is positive.
\end{example}

\begin{example}
    For a fixed $x \in H$, the map $T \mapsto \langle Tx, x \rangle$ is positive.
\end{example}

One need not verify continuity for positive operators, for we obtain this automatically.

\begin{prop}
    A positive operator $\phi:A \to \mathbf{C}$ is continuous.
\end{prop}
\begin{proof}
    We claim the supremum
    %
    \[ K = \sup \{ \phi(M): M \in A_+, \| M \| \leq 1 \} \]
    %
    is finite. Otherwise, we may pick a sequence $M_1, M_2, \dots$ with $\| M_i \| \leq 1$ and $\phi(M_i) \geq 4^n$. Let
    %
    \[ M = \sum \frac{M_i}{2^i} \]
    %
    Then for each $n$, $2^n M \geq M_n$, so
    %
    \[ \phi(M) \geq \frac{\phi(M_n)}{2^n} \geq \frac{4^n}{2^n} = 2^n \]
    %
    which yields an immediate contradiction.

    Now suppose that $M$ is an arbitrary operator, choose self adjoint $N$ and $L$ in $A$ such that
    %
    \[ M = N + iL = N_+ - N_- + iL_+ - iL_- \]
    %
    Then if $\| M \| \leq 1$,
    %
    \[ |\phi(M)| = |\phi(N_+) - \phi(N_-) + i\phi(L_+) - i\phi(L_-)| \leq 4K \]
    %
    which shows that $\phi$ is continuous.
\end{proof}

\begin{theorem}
    The following are equivalent
    %
    \begin{enumerate}
        \item $\phi$ is positive operator.
        \item For every BAI $E_\alpha$ contained within the positive identity on $A$,
        %
        \[ \| \phi \| = \lim_\alpha \phi(E_\alpha) \]
        \item There is a BAI $E_\alpha$ in the positive identity on $A$ for which
        %
        \[ \| \phi \| = \lim_\alpha \phi(E_\alpha) \]
    \end{enumerate}
\end{theorem}
\begin{proof}
    Without loss of generality, assume $\| \phi \| = 1$. Suppose $\phi$ is positive. Fix an approximate identity $E_\alpha$. Then
    %
    \[ \limsup_\alpha \phi(E_\alpha) \leq 1 \]
    %
    On the other hand, when $\| M \| \leq 1$,
    %
    \[ |\phi(M)|^2 = \lim |\phi(E_\alpha M)|^2 \leq \liminf_\alpha \phi(E_\alpha^2) \phi(M^*M) \]
\end{proof}







\chapter{Spectral Theory for Normal Operators}

We now use Gelfand theory to extend the spectral decomposition of compact, normal operators on a Hilbert space to a spectral decomposition for general normal operators. Since $\sigma_c(T)$ is not necessarily empty for a bounded normal operator $T: H \to H$, this decomposition must also describe the almost eigenvalues of the operator. Instead of a decomposition of the form
%
\[ T = \sum_\lambda \lambda P_\lambda, \]
%
where $P_\lambda: H \to H$ is the orthogonal projection onto $H_\lambda$, we instead have to write
%
\[ T = \int \lambda \cdot dP(\lambda), \]
%
where we are now integrating with respect to a `projection' valued measure $P$. This allows for the existence of almost eigenvectors, since for any ball $B_\varepsilon(\lambda_0)$, the operator $P_{\lambda_0,\varepsilon} = P(B_\varepsilon(\lambda_0))$ will be an orthogonal projection onto a closed subspace $H_{\lambda_0,\varepsilon}$ such that for any $x \in H_{\lambda_0,\varepsilon}$,
%
\[ \| Tx - \lambda_0 x \| \leq \varepsilon \| x \|, \]
%
and if $x \in H_{\lambda_0,\varepsilon}^\perp$, which is the image of the orthogonal projection $P(B_\varepsilon(\lambda_0)^c)$, then we will have
%
\[ \| Tx - \lambda_0 x \| \geq \varepsilon \| x \|. \]
%
Thus $H_{\lambda_0,\varepsilon}$ is the space of `almost eigenvectors' with eigenvalue $\lambda_0$ up to an error $\varepsilon$.

\section{Resolutions of the Identity}

Let us work more precisely. Given a measurable space $\Omega$, we defnie a \emph{spectral resolution} on $\Omega$ over a Hilbert space $H$ to be a function $E$ associating with each measurable subset $\omega$ of $\Omega$, a bounded orthogonal projection $E(\omega): H \to H$, such that
%
\begin{itemize}
    \item $E(\emptyset) = 0$ and $E(\Omega) = 1$.
    \item If $\omega$ is the disjoint union of two measurable sets $\omega_1$ and $\omega_2$, then
    %
    \[ E(\omega_1 \cup \omega_2) = E(\omega_1) + E(\omega_2). \]

    \item If $\omega_1$ and $\omega_2$ are measurable sets, then
    %
    \[ E(\omega_1 \cap \omega_2) = E(\omega_1) E(\omega_2) = E(\omega_2) E(\omega_1). \]

    \item For each $x_1,x_2 \in H$, the function $E_{x_1,x_2}(\omega) = (E(\omega) x_1, x_2)$ is a complex measure on $\Omega$.
\end{itemize}
%
If $\Omega$ is a topological space equipped with the family of Borel measurable sets, we will assume that all of the measures $E_{x_1,x_2}$ are \emph{regular}.

One consequence of the definition is that if $\omega_1$ and $\omega_2$ are disjoint measurable sets in $\Omega$, then $E(\omega_1)$ and $E(\omega_2)$ are projections onto subspaces of $H$ which are orthogonal to one another. Another consequence is that if $x \in H$, then $E_{x,x}$ is a \emph{positive} measure with total variation $\| x \|^2$. More generally, $E_{x_1,x_2}$ has total variation at most $\| x_1 \| \| x_2 \|$, since if we write $\Omega$ as the disjoint union of sets $\omega_1, \dots, \omega_n$, then by Cauchy Schwartz and orthogonality,
%
\begin{align*}
    &|(E(\omega_1) x_1, x_2)| + \dots + |(E(\omega_N) x_1, x_2)|\\
    &\quad\leq \| E(\omega_1) x_1 \| \| E(\omega_1) x_2 \| + \dots + \| E(\omega_N) x_1 \| \| E(\omega_N) x_2 \| \\
        &\quad\leq \| x_1 \| \| x_2 \|.
\end{align*}
%
Now $E$ is certainly finitely additive, but except in trivial situations, $E$ is not countable additive in the norm topology of $B(H)$, since if we have a strictly increasing family of sets $\{ \omega_n \}$, the projections $\{ E(\omega_n) \}$ have ranges over a strictly increasing family of subspaces $\{ H_n \}$ of $H$, and so $\| E(\omega_n) - E(\omega_m) \| \geq 1$ for $n \neq m$, so this sequence cannot possibly converge. On the other hand, for any $x \in H$, since $E_{x,x}$ is countably additive,
%
\begin{align*}
    \lim_{n \to \infty} \| E(\omega) x - E(\omega_n) x \| &= \lim_{n \to \infty} (E(\omega) x, x) - (E(\omega_n) x, x)\\
    &= \lim_{n \to \infty} E_{x,x}(\omega) - E_{x,x}(\omega_n) = 0.
\end{align*}
%
Thus for any $x \in H$, $E(\omega_n) x \to E(\omega) x$, and so it follows that $E$ is countable additive \emph{in the strong operator topology}. This is equivalent to saying that if $x \in H$, then the function $\omega \mapsto E(\omega) x$ is a countably additive $H$-valued measure on $\Omega$.

We define a set $\omega$ to have \emph{measure zero} with respect to the resolution $E$ if $E(\omega) = 0$. Because of the continuity in the strong operator topology, the family of sets of measure zero is closed under countable unions. Given a spectral resolution $E$ on a measurable space $\Omega$, and a measurable function $f: E \to \CC$, we define the essential range of $f$ to be the smallest closed subset $C$ of $\CC$ such that $f^{-1}(C^c)$ has measure zero. We the define the $L^\infty(E)$ norm $\| f \|_{L^\infty(E)}$ to be the infinum of $\lambda > 0$ such that the essential range of $f$ is contained in the ball of radius $\lambda$ centred at the origin. The space $L^\infty(E)$ of functions with finite $L^\infty(E)$ then becomes a $C^*$ algebra if we identify functions that agree up to a set of measure zero.

For $f \in L^\infty(E)$, we can define a bounded operator on $H$, denote
%
\[ I(f) = \int f(\lambda) dE(\lambda), \]
%
which is a vector valued integral in the sense that for each $x_1,x_2 \in H$,
%
\[ \left( I(f) x_1, x_2 \right) = \int f(\lambda) dE_{x_1,x_2}(\lambda). \]
%
The right hand side is a well defined, bounded sesquilinear function in the variables $x_1$ and $x_2$, which proves the existence of $I(f)$ for each $f \in L^\infty(E)$. We have $I(f_1 f_2) = I(f_1) I(f_2)$ for any $f_1, f_2 \in L^\infty(E)$, which proves that $I(f)$ is a normal operator for any $f \in L^\infty(E)$. We have
%
\begin{align*}
    \| I(f) \| &= \sup_{\| x \| \leq 1} (I(f) x, I(f) x)\\
    &= \sup_{\| x \| \leq 1} \left( \int |f(\lambda)|^2 dE_{x,x}(\lambda) \right)^{1/2} = \| f \|_{L^\infty(E)},
\end{align*}
%
which follows because if $\omega$ is a measurable set such that $|f(\lambda)| \geq \| f \|_{L^\infty(E)} - \varepsilon$, and $x \in H$ lies in the range of $E(\omega)$, then $E_{x,x}$ is supported on $\omega$ and so
%
\[ \int |f(\lambda)|^2 dE_{x,x}(\lambda) \geq \int \left( \| f \|_{L^\infty(E)} - \varepsilon \right) dE_{x,x}(\lambda) = \| f \|_{L^\infty(E)} - \varepsilon, \]
%
and taking $\varepsilon \to 0$ gives equality. Thus $I: L^\infty(E) \to B(H)$ is a $*$ isometry between the two spaces. If $\{ f_n \}$ is a sequence in $L^\infty(E)$ which converges pointwise almost everywhere to some $f \in L^\infty(E)$, and $\sup_n \| f_n \|_{L^\infty(E)} < \infty$, then it follows by the dominated convergence theorem that for any $x_1,x_2 \in H$, by the dominated convergence theorem,
%
\[ ( I(f_n) x_1, x_2 ) = \int f_n(\lambda) dE_{x_1,x_2}(\lambda) \to \int f(\lambda) dE_{x_1,x_2}(\lambda) = (I(f) x_1,x_2). \]
%
Thus $I(f_n)$ converges to $I(f)$ in the weak operator topology.

\section{Decomposition of Normal Operators}

The main use of spectral resolutions for our purposes is the construction, for each bounded normal operator $T: H \to H$, of a Borel measurable spectral resolution $E$ on $\sigma(T)$ valued in $H$, such that
%
\[ T = \int \lambda dE(\lambda). \]
%
In fact, we will be able to obtain a more general result which is a consequence of a spectral theorem for commutative $C^*$ subalgebras of $B(H)$.

\begin{theorem}
    Let $A$ be a unital commutative $C^*$ subalgebra of $B(H)$. If $\Phi_A$ is the Gelfand space of $A$, then there is a unique Borel measurable spectral resolution $E$ on $\Phi_A$ such that for any $T \in A$,
    %
    \[ T = \int \widehat{T} dE, \]
    %
    where $\widehat{T}$ is the Gelfand transform of $T$, and for any nonempty open subset $\omega$ of $\Phi_A$, $E(\omega) \neq 0$.

    In particular, this means that the inverse of the Gelfand transform extends to an isometric $*$ isomorphism of $L^\infty(E)$ onto a closed $C^*$ subalgebra $B$ of $B(H)$, which is the closed span of the projections $\{ E(\omega) \}$.
\end{theorem}
\begin{proof}
    To see that $E$ must be unique, let us see how we can recover it. For any $f \in C(\Phi_A)$, $f$ must certainly be in $L^\infty(E)$ since it is bounded everywhere, and so for $x_1,x_2 \in H$,
    %
    \[ ( \Gamma^{-1}(f) x_1, x_2) = \int f(\lambda) dE_{x_1,x_2}(\lambda). \]

    %
    The left hand side is independant of $E$, and since $f$ is allowed to range over all continuous functions on $\Phi_A$, we see that $E_{x_1,x_2}$ is uniquely determined by this formula for all $x_1,x_2 \in H$. But this uniquely determines $E$. To obtain $E$, since the map
    %
    \[ f \mapsto (\Gamma^{-1}(f) x_1, x_2) \]
    %
    is continuous, linear, and bounded, the Riesz representation theorem shows there is a regular complex Borel measure $E_{x_1,x_2}$ such that the integral formula above holds.

    To find the spectral resolution $E$ from the family of measures $\{ E_{x_1,x_2} \}$, we have to do some legwork. Since we have the complex Borel measures $E_{x_1,x_2}$, we can then define
    %
    \[ \int f(\lambda) dE_{x_1,x_2}(\lambda) \]
    %
    for \emph{any} bounded Borel measurable function $f$, which is bounded and sesquilinear in $x_1$ and $x_2$, and thus corresponds to an operator $\Phi(f) \in B(H)$ with $\| \Phi(f) \| \leq \| f \|_{L^\infty(\Phi_A)}$. The fact that for continuous $f$, $f$ is real-valued if and only if $\Gamma^{-1}(f)$ is self-adjoint implies that $\mu_{x_2,x_1} = \overline{\mu_{x_1,x_2}}$, and thus if $f$ is bounded and Borel measurable,
    %
    \begin{align*}
        (\Phi(f) x_1, x_2) &= \int f(\lambda) dE_{x_1,x_2}(\lambda)\\
        &= \overline{ \int \overline{f(\lambda)} dE_{x_2,x_1}(\lambda) }\\
        &= \overline{ (\Phi(f^*) x_2, x_1) }\\
        &= (x_1, \Phi(f^*) x_2).
    \end{align*}
    %
    Thus $\Phi(f^*) = \Phi(f)^*$. Next, we prove that $\Phi(fg) = \Phi(f)\Phi(g)$ for any bounded Borel measurable functions $f$ and $g$. If $f$ and $g$ are continuous, then
    %
    \[ \int f(\lambda) g(\lambda) dE_{x_1,x_2}(\lambda) = (\Gamma^{-1}(f) \Gamma^{-1}(g) x_1, x_2) = \int f(\lambda) dE_{\Gamma^{-1}(g) x_1, x_2}. \]
    %
    This implies that the measures $g E_{x_1,x_2}$ and $E_{\Gamma^{-1}(g) x_1,x_2}$ are equal to one another, and thus this integral formula remains true if $f$ is replaced by a bounded Borel measurable function, and in this case,
    %
    \[ \int f(\lambda) g(\lambda) dE_{x_1,x_2} = (\Phi(f) \Gamma^{-1}(g) x_1, x_2) = (\Gamma^{-1}(g) x_1, \Phi(f^*) x_2) = \int g(\lambda) dE_{x_1, \Phi(f^*) x_2}. \]
    %
    Thus the measure $f E_{x_1,x_2}$ is equal to $E_{x_1, \Phi(f^*) x_2}$, and so it follows that the integral formula above remains true if $g$ is replaced with a bounded Borel measurable function. But this means precisely that
    %
    \[ (\Phi(fg) x_1, x_2) = (\Phi(g) x_1, \Phi(f^*) x_2) = (\Phi(f) \Phi(g) x_1, x_2). \]
    %
    Thus $\Phi(fg) = \Phi(f) \Phi(g)$.

    But now we can define $E(\omega) = \Phi(1_\omega)$ for any measurable subset $\omega$ of $A$, and it is simple to verify from the fact that $\phi(f^*) = \phi(f)^*$ and that $\Phi(fg) = \Phi(f) \Phi(g)$ that this gives a spectral resolution, and integrating against this spectral resolution gives the operator $\Phi$. For any open set $\omega$, we can find a non-negative, non-zero continuous function $f \in C(\Phi_A)$ supported on $\omega$, and then the fact that
    %
    \[ \| \int f(\lambda) dE(\lambda) \| = \| f \|_{L^\infty(\Phi_A)} \neq 0 \]
    %
    implies that we cannot have $E(\omega) = 0$.
\end{proof}

In particular, if $T \in B(H)$ is a normal operator, then there exists a unique resolution of the identity on Borel subsets of $\sigma(T)$ such that
%
\[ T = \int_{\sigma(T)} \lambda\; dE(\lambda) \]
%
and if $S$ commutes with $T$, every projection in the image of $E$ commutes with $S$. We can extend $E$ to a resolution on $\CC$ by setting $E(S) = 0$ if $S \cap \sigma(T) = \emptyset$. We call $E$ in this case the \emph{spectral decomposition} of $T$. Intuitively, we can think of the measure $dE(\lambda)$ as giving an `infinitisimal projection' onto the eigenspace corresponding to the eigenvalue $\lambda$. In particular, one consequence of this calculation is that we can now extend the \emph{continuous functional calculus} of $T$ to a \emph{bounded functional calculus}, associating with each $f \in L^\infty(\sigma(T))$ an operator $f(T) \in B(H)$ by the formula
%
\[ f(T) = \int_{\sigma(T)} f(\lambda)\; dE(\lambda). \]
%
We have already seen in the proof above that this definition extends the continuous functional calculus. It is sometimes simpler to work with the following weaker result.

Let us consider some applications of this symbolic calculus to the theory of normal operators.

\begin{theorem}
    If $T \in B(H)$ is normal, then
    %
    \[ \| T \| = \sup_{\|x\| \leq 1} |\langle Tx, x \rangle|. \]
\end{theorem}
\begin{proof}
    If scalars in $\sigma(T)$ always corresponded to eigenvectors, then this result would follow from the fact that $\| T \| = \sigma(T)$. But elements of $\sigma(T)$ do not always correspond to eigenvectors. On the other hand, we will use the spectral theorem to produce `almost eigenvectors' for $T$, which will suffice for our purposes. Since $T$ is normal, we know that $\| T \|$ is equal to the spectral radius of $T$. Thus for any $\lambda_0 \in \sigma(T)$ with $|\lambda_0| = \| T \|$. let $B_\varepsilon$ be a ball of radius $\varepsilon$ centred at $\lambda_0$. Then if $E$ is the resolution associated with $T$, we know that $E(B_\varepsilon) \neq 0$. Since $E(B_\varepsilon)$ is a projection, we may find $x \in H$ such that $E(B_\varepsilon) x = x$ and such that $\| x \| = 1$. If $f(\lambda) = (\lambda - \lambda_0) 1_{B_\varepsilon}$, then
    %
    \[ f(T) = (T - \lambda_0) E(B_\varepsilon) \]
    %
    and so $f(T) x = Tx - \lambda_0 x$. Now $\| f \|_{L^\infty} \leq \varepsilon$, so it follows that $\| f(T) x \| \leq \varepsilon$, which implies that
    %
    \[ |\langle Tx, x \rangle - \lambda_0| = |\langle f(T)x, x \rangle - \lambda_0| \leq \| f \|_{L^\infty} \leq \varepsilon. \]
    %
    This implies the required result.
\end{proof}

Similar arguments, left as an exercise, show that a normal operator $T \in B(H)$ is self-adjoint if and only if $\sigma(T) \subset \RR$, and unitary if and only if $\sigma(T)$ is contained on the unit circle. This need not be true for general operators (e.g. non orthogonal projections are not self-adjoint).

\begin{theorem}
    Let $T \in B(H)$ be normal, and let $E$ be it's spectral decomposition. If $f \in C(\sigma(T))$ and $S = f^{-1}(0)$, then the kernel of $f(T)$ is equal to the range of $E(S)$.
\end{theorem}
\begin{proof}
    If $g = 1_S$, then $f \cdot g = 0$, and so $f(T) g(T) = f(T) E(S) = 0$, which shows the range of $E(S)$ is contained in the kernel of $f(T)$. Conversely, suppose $x \in H$ and $f(T) x = 0$. For each $\varepsilon > 0$, let
    %
    \[ g_\varepsilon(x) = \frac{1}{f(x)} \cdot 1_{S_\varepsilon^c}(x). \]
    %
    Since $f$ is continuous, $g_\varepsilon \in L^\infty(\sigma(T))$ for all $\varepsilon > 0$. And $f \cdot g_\varepsilon = 1_{S_\varepsilon^c}$, so that $E(S_\varepsilon^c) x = g_\varepsilon(T) f(T) x = 0$. Countable additivity shows that $E(S^c) x = 0$, which shows by orthogonality of $E(S)$ and $E(S^c)$ that $x$ is in the range of $E(S)$.
\end{proof}

It follows that if $\lambda \in \sigma(T)$, and $E = E(\lambda)$, then the range of $E$ is precisely the kernel of $T - \lambda_0$, i.e. the set of eigenvectors for $T$ with eigenvalue $\lambda$. Since $E(U) \neq 0$ for any open $U$, any isolated point of $\sigma(T)$ is an eigenvalue for some eigenvector. More generally, we see that if $\lambda \in \sigma(T)$, then for any $\varepsilon > 0$, $E(B_\varepsilon(\lambda)) \neq 0$. If the range of this projection is the closd subspace $H_{\lambda,\varepsilon}$ of $H$, then we see that for $x \in H_{\lambda,\varepsilon}$,
%
\[ \| Tx - \lambda x \| = \left\| \int_{B_\varepsilon(\lambda)} (\gamma - \lambda) dE_{x,x}(\gamma) \right\| \leq \varepsilon \| x \|. \]
%
Conversely, if $x \in H_{\lambda,\varepsilon}^\perp$, then
%
\[ \| Tx - \lambda x \| = \left\| \int_{B_\varepsilon(\lambda)^c} (\gamma - \lambda) dE(\gamma) \right\| \geq \varepsilon \| x \|, \]
%
which follows because $|\gamma - \lambda| \geq \varepsilon$ on the support of the integral.

If $T: H \to H$ is normal, and $\sigma(T)$ is countable, with $\sigma(T)$, then it follows that
%
\[ H = \bigoplus_{\lambda \in \sigma(T)} H_\lambda, \]
%
where $H_\lambda$ is the set of $x \in H$ with $Tx = \lambda x$. Thus we see that the theory of compact normal operators follows simply from this more general spectral theory.

\begin{theorem}
    A normal operator $T \in B(H)$ is compact if and only if $\sigma(T)$ has no limit point but the origin (which implies $\sigma(T)$ is countable), and for any $\lambda \neq 0$, then $T - \lambda$ has finite dimensional kernel.
\end{theorem}
\begin{proof}
    To show this is sufficient, if $f_\varepsilon(x) = x \cdot \mathbf{I}(|x| \geq \varepsilon)$, then $f_\varepsilon(T)$ is a finite sum of finite rank operators. Thus $f_\varepsilon(T)$ is compact for each $\varepsilon > 0$. On the other hand,
    %
    \[ \| T - f_\varepsilon(T) \| \leq \| x - f_\varepsilon \|_{L^\infty} \leq \varepsilon, \]
    %
    which shows $T$ is the limit of a family of compact operators, and is therefore compact.
\end{proof}

It follows that if $T$ is normal and compact, then $T$ has an eigenvalue $\lambda$ with $\| T \| = |\lambda|$, and $f(T)$ is compact for any $f \in C(\sigma(T))$ with $f(0) = 0$, but $f(T)$ is not compact if $H$ is infinite dimensional and $f(0) \neq 0$.

\begin{theorem}
    If $T \in B(H)$, then we have
    %
    \[ \langle Tx, x \rangle \geq 0 \]
    %
    for each $x \in H$ if and only if $T$ is self-adjoint and $\sigma(T) \subset [0,\infty)$. We have
    %
    \[ \langle Tx, x \rangle > 0, \]
    %
    for each $x \neq 0$ if and only if $T$ is self-adjoint and $\sigma(T) \subset (0,\infty)$. We have
    %
    \[ \langle Tx, x \rangle \geq \lambda \| x \|^2 \]
    %
    if and only if $T$ is self-adjoint and $\sigma(T) \subset [\lambda,\infty)$.
\end{theorem}
\begin{proof}
    Suppose $\langle Tx,x \rangle \geq 0$ for each $x \in H$. Then because $\langle Tx, x \rangle$ is real-valued, $\langle Tx, x \rangle = \langle x, Tx \rangle = \langle T^* x, x \rangle$, so that $\langle (T - T^*) x, x \rangle = 0$ for all $x \in H$. Since $T - T^*$ is normal, it follows that $T - T^* = 0$, from which it follows that $T$ is self adjoint. For any $\lambda < 0$, for $x \neq 0$ we have
    %
    \[ -\lambda \| x \|^2 = \langle -\lambda x, x \rangle \leq \langle (T-\lambda) x, x \rangle \leq \| (T - \lambda) x \| \| x \|, \]
    %
    from which it follows that $\| (T - \lambda) x \| \geq - \lambda \| x \|$, so $T - \lambda$ is an isomorphism, hence $\lambda \not \in \sigma(T)$. Conversely, if $T$ is self adjoint, and $\sigma(T) \subset [0,\infty)$, then
    %
    \[ \langle Tx, x \rangle = \int_0^\infty \lambda dE_{x,x}. \]
    %
    Since $E_{x,x}$ is a positive measure, and $\lambda \geq 0$, it follows that $\langle Tx,x\rangle \geq 0$. The remaining cases are treated similarily.
\end{proof}

\section{Von Neumann Algebras}

A \emph{Von Neumann Algebra} is a unital $C^*$ subalgebra of $B(H)$ which is closed in the weak operator topology, or equivalently, in the strong operator topology. A major example includes $L^\infty(X)$, viewed as a subalgebra of $B(L^2(X))$ acting via multiplication. We will use the theory of Von Neuman algebras to extend the bounded functional calculus to more general algebras.

For $\mathcal{S}$ We denote by $W^*(\mathcal{S})$ the smallest Von Neumann algebra containing $\mathcal{S}$. If $\mathcal{S} \subset B(H)$, we define the \emph{commutatant} $\mathcal{S}'$ to be the set of all $T \in B(H)$ such that $TS = ST$ for all $S \in \mathcal{S}$. If $\mathcal{S}$ is closed under taking adjoints, then $\mathcal{S}'$ is a Von Neumann algebra. The following very important result is called the \emph{Von Neumann Double Commutant Theorem}.

\begin{theorem}
    Suppose that $A$ is a $C^*$ subalgebra of $B(H)$ with trivial nullspace. Then $W^*(A) = A''$.
\end{theorem}
\begin{proof}
    Clearly $A''$ is a Von Neumann algebra containing $A$, so $W^*(A) \subset A''$. Since $W^*(A)$ is just the closure of $A$ in the strong operator topology, it suffices to show that any $T \in A''$ lies in this closure. This is equivalent to showing that for any $x_1,\dots,x_n \in H$, there is $S \in A$ such that
    %
    \[ \sum_{i = 1}^n \| (T - S) x_i \|^2 < 1. \]
    %
    Suppose first that $n = 1$, so we have a single $x \in H$. Let $P$ be the orthogonal projection onto the closure of $Ax$, which we denote by $H_0$. Then $P \in A'$, since $A(PH) \subset PH$ and so $PSP = SP$ for every $S \in A$, which means that
    %
    \[ PS = (S^* P)^* = (PS^*P)^* = PSP = SP. \]
    %
    If $x' \in H_0^\perp$, then
    %
    \[ Ax' = A(P^\perp x') = P^\perp(Ax') = \{ 0 \}. \]
    %
    But since $A$ has trivial nullspace, this means that $x' = 0$. Thus $x \in Ax$. But this means that there is $S \in A$ such that $\| (T - S) x \| < 1$.


\end{proof}

TODO: More stuff (see book $C^*$ Algebras By Example)

TODO: If $\{ T_\alpha: H \to H \}$ is an \emph{increasing net} of self-adjoint operators on a Hilbert space $H$, which is bounded from above by some operator $M$, then $\lim_{\alpha \to \infty} T_\alpha$ exists in the strong operator topology, and is the least upper bound of the net $\{ T_\alpha \}$.







\section{Representations of $C^*$ Algebras}

A \emph{representation} of a $C^*$ algebra $A$ on a Hilbert space $H$ is a $*$ homorphism $\pi: A \to B(H)$. We say $\pi$ is \emph{irreducible} if $\pi(A)$ has no proper closed invariant subspaces in $H$.

\begin{theorem}
    If $\pi: A \to B(H)$ is an irreducible representation of a $C^*$ algebra $A$, then $\pi(A)$ is irreducible if and only if $\pi(A)'$ consists only of scalars.
\end{theorem}
\begin{proof}
    If $\pi(A)'$ does not solely contain scalars, then it is a nontrivial Von Neumann algebra and thus contains a non-scalar positive operator, and thus a proper projection $P$. If $H_0$ is the range of $P$, then $H_0$ is an invariant subspace. Conversely, if $\pi$ is irreducible, and $H_0$ is an invariant subspace, then $\pi(M) P = P \pi(M) P$ for all $M \in A$, which means that
    %
    \[ P \pi(A) = (\pi(A^*) P)^* = (P \pi(A^*) P)^* = P \pi(A) P = \pi(A) P. \]
    %
    Thus $P$ is a non-scalar element of $\pi(A)'$.
\end{proof}

TODO

Here is an application of this representation theory to the commutative theory.

\begin{theorem}
    If $A$ is a closed unital $C^*$ subalgebra of $B(H)$ for some Hilbert space $H$, then there exists a semifinite measure space $X$ and a unitary map $U: H \to L^2(X)$ and a $*$ homomorphism $\Phi: A \to L^\infty(X)$ such that
    %
    \[ UTU^{-1}(f) = \Phi(T) \cdot f \]
    %
    for all $f \in L^2(X)$.
\end{theorem}
\begin{proof}
    Suppose first that $A$ is cyclic, i.e. there is $x_0 \in H$ such that the subspace $\{ Tx_0 : T \in A \}$ of $H$ is dense. Set $X = \Phi_A$, and equip it with the measure $E_{x_0,x_0}$. For any $T \in A$, we have
    Then
    %
    \[ \| Tx_0 \|^2 = (T^*T x_0, x_0) = \int |\Phi(T)|^2\; dE_{x_0,x_0}. \]
    %
    Thus $\Phi(T) \in L^2(X)$ and $\| \phi(T) \|_{L^2(X)} = \| Tx_0 \|$. We define $U$ on the dense set $\{ Tx_0 : T \in A \}$ by setting $U(Tx_0) = \Phi(T)$. This is well define because if $T_1 x_0 = T_2 x_0$, then $\| \Phi(T_1 - T_2) \|_{L^2(X)} = 0$, so $\Phi(T_1) = \Phi(T_2)$ almost everywhere in $X$. It is easy to see that $U$ is an isometry on this dense subspace, so it extends uniquely to an isometry from $H$ to $L^2(X)$. The range of this isometry is a closed subspace of $L^2(X)$ containing $C(X)$, which implies the isometry is surjective, so $U$ is a unitary map. And we verify that for any $f \in C(X)$ equal to $\Phi(T_1)$ for some $T_1 \in A$,
    %
    \[ UTU^{-1}(f) = UT(T_1 x_0) = U(TT_1 x_0) = \Phi(TT_1) = \Phi(T) \Phi(T_1) = \Phi(T) \cdot f. \]
    %
    which proves the result in this case.

    In general, we can write $H = \bigoplus H_\alpha$, where $A$ is cyclic on $H_\alpha$. Thus for each $\alpha$, we can find a unitary map $U_\alpha: H_\alpha \to L^2(X_\alpha)$ and $\Phi_\alpha: A \to L^\infty(X)$ such that for $f \in L^2(X_\alpha)$,
    %
    \[ U_\alpha T U_\alpha^{-1} f = \Phi(T) \cdot f. \]
    %
    Setting $U = \bigoplus U_\alpha$ and $\Phi = \bigoplus \Phi_\alpha$ completes the proof since $\bigoplus_\alpha L^2(X_\alpha) = L^2(\bigoplus X_\alpha)$.
\end{proof}

We previously saw that if $T \in B(H)$ was a normal operator, and $\{ f_n \}$ was a sequence of bounded measurable functions on $\sigma(T)$ which converge pointwise to some bounded measurable function $f$, and satisfy $\sup \| f_n \|_{L^\infty} < \infty$, then $f_n(T)$ converges to $f(T)$ in the weak operator topology. But we can now prove that the convergence is also in the strong operator topology, because the result is clear if $H = L^2(X)$ from the dominated convergence theorem, and we can then apply the last result.

















\chapter{Unbounded Operators}

It is very desirable to have a spectral theory of operators that are not necessarily bounded from a space to itself, especially in the theory of partial differential equations, since the derivative is not a bounded function in most of the natural norm spaces we want to deal with. In fact, in some sense, there is \emph{no} Banach space upon which all differential operators of the form
%
\[ \sum p_\alpha(x) D^\alpha x \]
%
are bounded, where the $\{ p_\alpha \}$ are polynomials, which makes a spectral theory of unbounded operators essential in this setting. In one dimension, this is equivalent to having a Banach space of functions $M$ supporting a bounded derivative operators $D: M \to M$ and a bounded multiplier operator $X: M \to M$ given formally by $Df = f'$ and $Mf = xf$. Applying the product rule would show that $[D,M] = DM - MD$ is the identity operator. But the next lemma shows this is not possible.

\begin{lemma}
    There does not exist a unital normed algebra $A$ supporting $x,y \in A$ with $xy - yx = 1$.
\end{lemma}
\begin{proof}
    If $x,y \in A$ were found with $xy - yx = 1$, then it would follow that $x^n y - yx^n = nx^{n-1}$ (easily proved e.g. by induction). But this means that
    %
    \[ n \| x^{n-1} \| = \| x^n y - yx^n \| \leq 2 \| x^n \| \| y \| \leq 2 \| x^{n-1} \| \| x \| \| y \|, \]
    %
    so that $\| x \| \| y \| \geq n/2$ for every integer $n$, which is impossible.
\end{proof}

Thus we cannot find a common function space upon which to analyze differentiation and multiplication as bounded operators. Fortunately, we can obtain a theory of \emph{unbounded operators} which allow us to view $D$ and $M$ as operators with a common domain, e.g. $L^2(\RR)$. The key to this theory is to allow operators that are only defined on dense subsets of space.

Let $X$ and $Y$ be Banach spaces. An \emph{unbounded operator} $T: X \to Y$ is a linear map $T: \mathcal{D}(T) \to Y$ defined on a subspace $\mathcal{D}(T) \subset X$, which we call the \emph{domain} of the operator. Two unbounded operators are equal if they have the same domain, and are equal as functions. We do not assume this operator has any topological properties in general - if the operator was bounded the Hahn-Banach theorem would imply that it was just the restriction of a bounded operator $T: X \to Y$, so there would be no real importance to consider the operator on a smaller subspace.

\begin{example}
    Consider the operator $\Delta$ on $L^2(\Omega)$, where $\Omega$ is some domain in $\RR^d$ which, for simplicity, we might as well assume has smooth boundary. One domain we could take for $\Delta$ is $C^\infty(\Omega)$, or the space of all smooth, periodic functions and these will be different unbounded operators. Of more relevance in what follows, we can consider $\Delta$ either as an operator with domain $\mathcal{D}_D$ (the space of functions with \emph{Dirichlet boundary conditions} boundary conditions) consisting of all functions $f \in C^2(\Omega)$ vanishing on the boundary, or the domain $\mathcal{D}_N$ of functions $f \in C^2(\Omega)$ such that $\nabla f \cdot \nu$ vanishes on the boundary, where $\nu$ is the normal vector to the boundary. Thus we obtain two different unbounded operators, $\Delta_D$, and $\Delta_N$. The spectral theory of operators on the two domains can certainly differ, e.g. for $\Omega = [0,1]$, the eigenfunctions of $\Delta_D$ are precisely scalar multipliers of the functions
    %
    \[ f_n(x) = \sin(2 \pi n x) \]
    %
    which have eigenvalues $- 4 \pi^2 n^2$, for $n > 0$. The eigenfunctions for $\Delta_N$, on the other hand, are scalar multiples of the functions
    %
    \[ f_n(x) = \cos(2 \pi n x) \]
    %
    which also have eigenvalues $- 4 \pi^2 n^2$, for $n \geq 0$. Thus $0$ is an eigenvalue for $\Delta_N$, but not for $\Delta_D$. The differences between Neumann and Dirichlet boundary conditions only get more distinct in higher dimensions.
\end{example}

For any unbounded operator $T: X \to Y$, we can consider the graph $\Gamma(T) = \{ (x,Tx): x \in \mathcal{D}(T) \}$. If $\Gamma(T)$ is a closed subset of $X \times Y$, we say that $T$ is \emph{closed}. The closed graph theorem implies that if $T$ is closed and $\mathcal{D}(T) = X$, then $T$ is actually a bounded operator. For any closed operator $T$, we can give $\mathcal{D}(T)$ the structure of a Banach space by defining
%
\[ \| x \|_T = \sqrt{\| x \|^2 + \| Tx \|^2}, \]
%
and in this topology $T$ will then be a bounded operator from $\mathcal{D}(T)$ to $Y$.

The importance of being closed is \emph{essential} to the spectral theory of unbounded operators. Given an unbounded operator $T: X \to Y$, we define the resolvent $\rho(T)$ of $T$ to be the set of all complex numbers $\lambda \in \CC$ such that $T - \lambda$ has a bounded everywhere defined inverse $S: X \to \mathcal{D}(T)$, i.e. such that $(T - \lambda) S = I_X$, and $S (T - \lambda) = I_{\mathcal{D}(T)}$, which we will denote by $(T - \lambda)^{-1}$. The spectrum $\sigma(T)$ is then the complement of the resolvent of $T$.

\begin{lemma}
    If $\sigma(T) \neq \CC$, then $T$ is closed.
\end{lemma}
\begin{proof}
    Suppose $\lambda \not \in \sigma(T)$, and assume without loss of generality that $\lambda = 0$. Let $S: X \to \mathcal{D}(T)$ be the inverse of $T$. If $x_i \to x$ in $\mathcal{D}(X)$, and $T x_i \to y$ in $X$, then by continuity, $x_i = STx_i \to Sy$, so $x = Sy$. But this means that $Tx = TSy = y$. Thus $T$ is closed.
\end{proof}

Like with bounded operators, $\sigma(T)$ is a closed set, but it need not be compact. The proof is roughly the same.

\begin{lemma}
    If $\lambda \in \rho(T)$, and $L^{-1} = \| (T - \lambda)^{-1} \|$, then $B(\lambda,L) \subset \rho(T)$.
\end{lemma}
\begin{proof}
    Assume without loss of generality that $\lambda = 0$. Let $S = T^{-1}$. Then
    %
    \[ (T - \lambda)^{-1} = \sum_{n = 0}^\infty (-\lambda)^n  \]
    %
    \[ (T - \lambda)^{-1} = \sum_{n = 0}^\infty (-\lambda)^n T^n \]
    %

\end{proof}

We wish to extend the natural operations on the class of operators to the class of unbounded operators. Given a scalar $\lambda$, and an unbounded operator $T: X \to Y$, we can define an unbounded operator $\lambda T: X \to Y$ with $\mathcal{D}(\lambda T) = \mathcal{D}(T)$. For two unbounded operators $T: \mathcal{D}(T) \to Y$ and $S: \mathcal{D}(S) \to Y$, we can define $T + S$ as an unbounded operator defined on $\mathcal{D}(T) \cap \mathcal{D}(S)$. Similarily, we can define $T - S$. If $T: \mathcal{D}(T) \to Y$ and $S: \mathcal{D}(S) \to Z$ are unbounded operators, we can define $ST$ as an unbounded operator defined on $\mathcal{D}(T) \cap T^{-1}(\mathcal{D}(S))$. The distributive law $(S + U)T = ST + UT$ continues to hold since
%
\[ \mathcal{D}(T) \cap T^{-1}(\mathcal{D}(S) \cap \mathcal{D}(U)) = \left[ \mathcal{D}(T) \cap T^{-1}(\mathcal{D}(S)) \right] \cap \left[ \mathcal{D}(T) \cap T^{-1}(\mathcal{D}(U)) \right], \]
%
but the distributive law $T(S + U) = TS + TU$ might \emph{not} hold in this scheme. If $T: X \to Y$ is an injective unbounded operator, we can define $T^{-1}: Y \to X$ to be the unbounded operator whose domain is the image of $T$.



An unbounded operator is \emph{densely defined} if $\mathcal{D}(T)$ is a dense subset of $X$. Given an unbounded densely defined operator $T: \mathcal{D}(X) \to Y$, we can define an unbounded operator $T^*$ with domain
%
\[ \mathcal{D}(T^*) = \{ \gamma \in Y^*: \gamma \circ T \in \mathcal{D}(T)^* \}, \]
%
where $\mathcal{D}(T)$ is given the subspace topology inherited from $X$. We then set $T^* \gamma$ to be the unique $\lambda \in X^*$ extending the continuous functional $\gamma \circ T$. In inner product notation, we then have
%
\[ \langle T^* \gamma, x \rangle = \langle \gamma, Tx \rangle \]
%
for any $x \in \mathcal{D}(T)$ and $\gamma \in \mathcal{D}(T^*)$. Note that if $T$ is not densely defined, this extension might not be unique, and so the adjoint is not well defined. If $T: X \to Y$ and $S: Y \to Z$ are densely defined unbounded operators such that $ST$ is also densely defined, then $(ST)^*$ is an extension of $T^* S^*$. If $S$ is bounded, then the domains are actually equal to one another, and so we get the familiar identity $T^* S^* = (ST)^*$. We also note that if $T: X \to Y$ is densely defined and unbounded, and $S: Y^* \to X^*$ is another unbounded operator such that $\gamma(Tx) = (S\gamma)(x)$ for any $x \in \mathcal{D}(T)$ and $\gamma \in \mathcal{D}(S)$, then $\mathcal{D}(S) \subset \mathcal{D}(T^*)$, because $\gamma \circ T$ is a bounded functional on $\mathcal{D}(T)$ for any $\gamma \in \mathcal{D}(S)$.

%If $T^* S^* \eta$ is well defined, then firstly, $|\eta(Sy)| \lesssim \| y \|_Y$ for $y \in \mathcal{D}(S)$. Thus there exists $\gamma \in Y^*$ such that $\gamma y = \eta(Sy)$ for all $y \in \mathcal{D}(S)$. Moreover, we know that $|\gamma(Tx)| \lesssim \| x \|$ for all $x \in \mathcal{D}(T)$. Thus we can find $\lambda \in X^*$ such that $\lambda x = \gamma(Tx)$. But then
%
%\[ |\eta(STx)| = |\gamma(Tx)| \lesssim \| x \| \]
%
% So \eta is in the domain of T^* S^*.

If $T: H_1 \to H_2$ is an unbounded operator between two Hilbert spaces $H_1$ and $H_2$, the unbounded adjoint can be naturally identified with an unbounded adjoint $T^*: H_2 \to H_1$, which we will do whenever we are talking about Hilbert spaces. If $T: H \to H$ is unbounded, and $T^*$ is an extension of $T$, then we say $T$ is \emph{symmetric}. This means that for any $x_1,x_2 \in \mathcal{D}(T)$, $\langle Tx_1, x_2 \rangle = \langle x_1, Tx_2 \rangle$. If we actually have $T^* = T$, which means that in addition to the previous property, $x \mapsto \langle Tx, y \rangle$ is continuous if and only if $y \in \mathcal{D}(X)$, then we say $T$ is \emph{self adjoint}. However, it is possible for $T$ to be a strict extension of $T^*$, as the next example shows, and we do not say this operator is symmetric nor self-adjoint. 

\begin{example}
    Let $T: L^2[0,1] \to L^2[0,1]$ be the unbounded operator whose domain consists of all absolutely continuous functions $f: [0,1] \to \CC$ such that $f' \in L^2[0,1]$, and for such functions, let
    %
    \[ Df = if'. \]
    %
    If $g \in L^2[0,1]$ satisfies
    %
    \[ \left| \int f'(x) g(x)\; dx \right| \lesssim \| f \|_{L^2[0,1]} \]
    %
    for all $f \in \mathcal{D}(T)$, then we claim that $g$ is absolutely continuous, and $g(0) = g(1) = 0$. Conversely, if $g$ is absolutely continuous, then
    %
    \[ \int f'(x) g(x)\; dx = f(1) g(1) - f(0) g(0) - \int f(x) g'(x)\; dx, \]
    %
    and we see that this quantity is $\lesssim \| f \|_{L^2[0,1]}$ precisely when $g(0) = g(1) = 0$. All that remains is to argue that $g$ is absolutely continuous, but this follows because $\mathcal{D}(T)$ contains the restriction of $C_c^\infty(0,1)$, so the boundedness equation above implies that the distributional derivative $g'$ of $g$ in $\mathcal{D}(0,1)^*$ is square integrable, which implies that $g$ lies in $\mathcal{D}(T)$, i.e. $g$ is absolutely continuous and $g' \in L^2[0,1]$. Thus we see that, even though we have $\langle Tx, y \rangle = \langle x, Ty \rangle$ for $x \in \mathcal{D}(T)$ and $y \in \mathcal{D}(T^*)$, $T^*$ is only defined on a proper subset of the domain of $T$. On the other hand, the operator $ S = T^*$, which extends $T$, is \emph{symmetric}, but not self adjoint, since one can verify that $S^* = T^{**} = T$, and that $S^*$ therefore extends $S$. The operator $T'$ defined as the restriction of $T$ to functions $f$ with $f(0) = f(1)$, is \emph{self adjoint}, since once can easily verify from the integration by parts formula that $\mathcal{D}((T')^*) = \mathcal{D}(T')$.
\end{example}

We now come to an interesting result which relates the graph of $T$ and $T^*$.

\begin{theorem}
    Let $T$ be a densely defined unbounded operator on a Hilbert space $H$. Then $\Gamma(T^*) = [V \Gamma(T)]^\perp$, where $V(x,y) = (-y,x)$, and $H \times H$ is viewed as a Hilbert space. In particular, $T^*$ is \emph{always} a closed unbounded operator, all self-adjoint operators are closed.
\end{theorem}
\begin{proof}
    The following four statements are equivalent to one another:
    %
    \begin{itemize}
        \item $(y,z) \in \Gamma(T^*)$
        \item $\langle y, Tx \rangle = \langle z, x \rangle$ for every $x \in \mathcal{D}(T)$.
        \item $\langle (y,z), V(x,Tx) \rangle = 0$ for every $x \in \mathcal{D}(T)$.
        \item $(y,z)$ lies in $V(\Gamma(T))^\perp$.
    \end{itemize}
    %
    Thus the proof is complete.
\end{proof}

Here are some corollaries for a symmetric unbounded operator $T: H \to H$:
%
\begin{itemize}
    \item If $\mathcal{D}(T) = H$, then $T$ is a bounded, self-adjoint operator.
    \item If $T$ is injective and self-adjoint, then it has dense image, and $T^{-1}$ is self adjoint.
    \item If the image of $T$ is dense, then $T$ is injective.
    \item If $T$ is surjective (so that (c) applies and $T$ is actually injective), then $T$ is self-adjoint, and $T^{-1}$ is bounded.
\end{itemize}
\begin{proof}
    To see (a), if $\mathcal{D}(T) = H$, then because $T^*$ extends $T$, $T^* = T$, and since $T^*$ is closed, it follows that $T$ is bounded and self-adjoint. 

    To obtain (b), to show the image is dense let $x$ be perpendicular to the image of $T$. Then $x \in \mathcal{D}(T^*)$ because $\langle x, Tx' \rangle = 0$ for all $x' \in \mathcal{D}(T)$, and is thus trivially continuous. But this implies that $x \in \mathcal{D}(T^*) = \mathcal{D}(T)$, and $Tx = T^*x = 0$. Thus $x = 0$, which shows the image of $T$ is dense. Thus $T^{-1}$ is a densely defined unbounded operator, and so we can consider it's adjoint. We claim that $[V\Gamma(T^{-1})]^\perp = \Gamma(T^{-1})$, which would show that $T^{-1}$ is self adjoint. But $V(\Gamma T^{-1}) = \Gamma(-T)$ and $V \Gamma(-T)$, and since $T$ is self-adjoint, $-T$ is self adjoint, and so $\Gamma(-T)^\perp = V \Gamma(-T) = \Gamma(T^{-1})$, which completes the argument.

    To prove (c), we suppose that $Tx = 0$. Then for any $x' \in \mathcal{D}(T)$, $\langle x, Tx' \rangle = \langle Tx, x' \rangle = 0$. Thus $x$ is orthogonal to the image of $T$. But this implies $x = 0$.

    To prove (d), we note that $T^{-1}$ is symmetric, because if $x_1,x_2 \in H$ then $x_i = Ty_i$ for some $y_i \in \mathcal{D}(T)$, and so applying the fact that $T$ is symmetric, we find that
    %
    \[ \langle T^{-1} x_1, x_2 \rangle = \langle y_1, Ty_2 \rangle = \langle Ty_1, y_2 \rangle = \langle x_1, T^{-1} x_2 \rangle. \]
    %
    Since $T^{-1}$ is self adjoint, injective, and has dense iamge, this implies $T = (T^{-1})^{-1}$ is self adjoint. But (a) also implies that $T^{-1}$ is bounded.
\end{proof}

A powerful consequence of this result is that if $T$ is a closed operator, then we have an orthogonal decomposition
%
\[ H \times H = V \Gamma(T) \oplus \Gamma(T^*) \]
%
Thus for any $a,b \in H$, there exists a unique pair $(x,y) \in \mathcal{D}(T) \times \mathcal{D}(T^*)$ such that $a = y - Tx$ and $b = x + T^* y$. Moreover, for this pair we have
%
\[ \| a \|^2 + \| b \|^2 = \| x \|^2 + \| Tx \|^2 + \| y \|^2 + \| T^* y \|^2. \]
%
In particular, setting $a = 0$, we find that with any closed, unbounded operator $T$, we can associate a pair of bounded operators $A_1: H \to \mathcal{D}(T)$ and $A_2: H \to \mathcal{D}(T^*)$, with $\| A_1 \| \leq 1$ and $\| A_2 \| \leq 1$, such that $TA_1 = A_2$, $I = A_1 + T^* A_2$, and for any $x \in H$,
%
\[ \| x \|^2 = \| A_1 x \|^2 + \| TA_1 x \|^2 + \| A_2 x \|^2 + \| T^* A_2 x \|^2. \]
%
In particular, if $Q = I + T^*T$, then $I = QA_1$, so $Q$ is surjective onto $H$. Moreover, $Q$ is also injective, since if $x \in \mathcal{D}(Q)$, $Tx \in \mathcal{D}(T^*)$, and so
%
\[ \langle x, x \rangle + \langle Tx, Tx \rangle = \langle x, x \rangle + \langle T^*Tx, x \rangle = \langle Qx, x \rangle \]
%
which shows by Cauchy-Schwartz that $\| x \|^2 \leq \| x \| \| Qx \|$, so that $\| Qx \| \geq \| x \|$, and thus $Q$ is injective. Thus $Q$ is a bijection from $\mathcal{D}(Q)$ to $H$, which means that $I$ is an extension of $A_1Q$. The fact that $\langle Qx, x \rangle \geq 0$ for all $x \in \mathcal{D}(Q)$, means $\langle A_1 x, x \rangle \geq 0$ for all $x \in H$, i.e. $A_1$ is positive semidefinite. Since $A_1$ is injective and self-adjoint, this implies $Q = A_1^{-1}$ is self-adjoint. And this means $T^*T = Q - I$ is self adjoint for any closed unbounded operator $T$.

\begin{theorem}
    If $T: H \to H$ is a densely defined closed unbounded operator, then $\mathcal{D}(T^*)$ is dense, and $T^{**} = T$.
\end{theorem}
\begin{proof}
    We use the fact that $H \times H = V \Gamma(T) \oplus \Gamma(T^*)$. Suppose $y \perp \mathcal{D}(T^*)$. Then $(y,0)$ is perpendicular to $\Gamma(T^*)$. This means that $(y,0) \in V \Gamma(T)$, so $(0,-y) \in \Gamma(T)$, which implies $y = 0$. Thus $\mathcal{D}(T^*)$ is dense. To show $T^{**} = T$, we simply note that we can also write $H \times H = V \Gamma(T^{**}) \oplus \Gamma(T^*)$, from which it follows that $V \Gamma(T^{**}) = V \Gamma(T)$, and so $T = T^{**}$.
\end{proof}

A symmetric operator $T$ is \emph{maximally symmetric} if it has no proper symmetric extension. Any self-adjoint operator $T$ is maximally symmetric, for if $S$ is symmetric and extends $T$, then $T = T^*$ extends $S^*$, which means $S$ extends $S^*$, and since $S$ is symmetric, $S^*$ extends $S$, hence we conclude $S = S^*$, and as a result, $S = T$. On the other hand, maximally symmetric operators need not be self-adjoint.

\section{The Cayley Transform}

One trick to understanding symmetric operators $T$ is to consider the operator $T + i$. This is because if $T$ is symmetric, a simple calculation shows that for $x \in \mathcal{D}(T)$,
%
\[ \| Tx + i x \|^2 = \| x \|^2 + \| Tx \|^2. \]
%
Thus $\Gamma(T)$ is isometric to the image of $T + i$. Thus $T + i$ is injective, $T$ is closed if and only if the range of $T + i$ is closed, and if $T + i$ is onto $H$, then $T$ is maximally symmetric, because otherwise $T + i$ would have a symmetric extension which was still injective, which is impossible. Exactly the same results are true if we replace $i$ with any other purely imaginary quantity.

The mapping
%
\[ t \mapsto \frac{t - i}{t + i} \]
%
is a one-to-one-correspondence between $\RR$ and the unit circle in $\CC$ (except for one). It follows from the continuous functional calculus that the map
%
\[ T \mapsto (T - i)(T + i)^{-1} \]
%
is a one-to-one-correspondence between the family of self-adjoint bounded operator $T$ on a Hilbert space $H$, and the family of unitary operators on $H$ which do not contain $1$ in their spectrum. We wish to work with `unbounded' isometries, which will extend the theory of the Cayley transform to unbounded symmetric operators. An unbounded isometry is an unbounded operator $U: H \to H$ such that $\| Ux \| = \| x \|$ for each $x \in \mathcal{D}(U)$. Equivalently, this holds if and only if $\langle Ux, Uy \rangle = \langle x, y \rangle$ for each $x,y \in \mathcal{D}(U)$. If $U$ is an unbounded isometry, and $I - U$ has dense range, then $I - U$ is injective, for if $Ux = x$, then for any $y \in \mathcal{D}(U)$,
%
\[ \langle x, (I - U)y \rangle = \langle x, y \rangle - \langle x, Uy \rangle = \langle x,y \rangle - \langle Ux, Uy \rangle = 0. \]
%
Because the range of $I - U$ is dense, $x = 0$. It is similarily easy to prove that the three sets $\mathcal{D}(U)$, $U(\mathcal{D}(U))$, and $\Gamma(U)$ are all closed, or all not closed.

Let us now show the Caylet transform gives a correspondence between symmetric unbounded operators on $H$, and unbounded isometries on $H$ (unbounded operators $U: H \to H$ such that $\| Ux \| = \| x \|$ for each $x \in \mathcal{D}(U)$), or equivalently, $\langle Ux, Uy \rangle = \langle x, y \rangle$ for $x,y \in \mathcal{D}(U)$). For any symmetric operator $T$, since $\| Tx + ix \|^2 = \| x \|^2 + \| Tx \|^2$, the map $U = (T - i)(T + i)^{-1}$ is an unbounded isometry with domain is the image of the map $T + i$. The following properties then holds

\begin{theorem}
    Suppose $U$ is the Cayley transform of $T$.
    %
    \begin{itemize}
        \item $U$ is closed if and only if $T$ is closed.
        \item The map $I - U$ is injective from $\mathcal{D}(U)$ onto $\mathcal{D}(T)$, and $T = i(I + U)(I - U)^{-1}$.
        \item $U$ is unitary if and only if $T$ is self-adjoint.
    \end{itemize}
    %
    Conversely, any unbounded isometry on $H$ such that $I - U$ is injective is the Cayley transform of a symmetric operator.
\end{theorem}
\begin{proof}
    TODO
\end{proof}

If $U_1$ and $U_2$ are Cayley transforms of symmetric operators $T_1$ and $T_2$, then $T_1$ extends $T_2$ if and only if $U_1$ extends $U_2$. Thus problems about extensions of symmetric operators reduce to problems about extensions of isometries. Given an unbounded symmetric closed operator $T$, the range of $T + i$ and $T - i$ are closed, and $U$ is an isometry from the image of the first map to the image of the second map. The dimensions of $(T + i)^\perp$ and $(T - i)^\perp$ are called the \emph{deficiency indices} of the map $T$. Then it follows from the properties above that $T$ is self-adjoint if and only if both of its deficiency indices are zero, $T$ is maximally symmetric if and only if at least one of its deficiency indices is zero, and $T$ has a self-adjoint extension if and only if its two deficiency indices are equal.

\begin{example}
    Let $V$ be the right shift operator on $l^2$. Then $V$ is an isometry, and $I - V$ is injective. Thus $V$ is the Cayley transform of a symmetric operator $T$ on $l^2$. Since $V$ has domain $l^2$, and its range has codimension one, the deficiency indices of $T$ are zero and one. Thus $T$ is an example of a maximally symmetric operator which is not self adjoint.
\end{example}

\section{The Spectral Theory of Unbounded Symmetric Operators}

s






\section{Operator Semigroups}

TODO






\chapter{Von Neumann Algebras}





\section{Spectral Theorem for Normal Operators}

In finite dimensional theory, a normal linear transformation $T: \mathbf{C}^n \to \mathbf{C}^n$ can be written
%
\[ T = \sum \lambda_i P_i \]
%
where $P_i$ is projection onto the eigenspace corresponding to the eigenvalue $\lambda_i$. In this section we apply the theory of $C^*$ algebras to extend this theorem to arbitrary normal bounded transformations from a Hilbert space to itself. The trick is to `integrate' rather than sum up the subspaces.

A \emph{spectral measure} on a measure space $(\Omega, \mathcal{F})$ is a function $E: \mathcal{F} \to B(H)$, for which
%
\[ E(\Omega) = \text{id}_H \]
%
such that $E(S)$ is a projection for each $S$, and such that for a disjoint family of sets $\{ S_1, S_2, \dots \}$ in $\mathcal{F}$, then
%
\[ E \left( \bigcup S_i \right) = \sum E(S_i) \]
%
with convergence pointwise (the strong operator topology), rather than in the operator norm. It follows that $E(\emptyset) = 0$.

\begin{lemma}
    If $S \subset W$, $E(S) \leq E(W)$.
\end{lemma}
\begin{proof}
    Since $E(W - S)$ is a projection, $E(W - S) \geq 0$, so
    %
    \[ E(W) = E(W - S) + E(S) \geq E(S) \]
    %
    Hence a spectral measure is monotone on projections.
\end{proof}

\begin{lemma}
    If $S$ and $W$ are disjoint sets, then $E(S)E(W) = 0$.
\end{lemma}
\begin{proof}
    $E(S \cup W) = E(S) + E(W)$, so
    %
    \[ E(S \cup W) = E(S \cup W) E(S \cup W) = E(S) + E(W) E(S) + E(S)E(W) + E(W) \]
    %
    which implies
    %
    \[ E(S) + E(W) = E(S) + E(W) E(S) + E(S) E(W) + E(W) \]
    %
    hence $E(W) E(S) + E(S) E(W) = 0$.
\end{proof}

\begin{lemma}
    $E(S \cap W) = E(S) \circ E(W)$.
\end{lemma}
\begin{proof}
    We have
    %
    \[ E(S \cap W) + E(S - W) = E(S)\ \ \ \ \ \ \ \ E(S \cap W) + E(W - S) = E(W) \]
    %
    and
    %
    \[ E(S \cup W) = E(S \cap W) + E(S - W) + E(W - S) \]
    %
    From which it follows that
    %
    \begin{align*}
        E(S) + E(W) &= 2E(S \cap W) + E(S - W) + E(W - S)\\
        &= E(S \cap W) + E(S \cup W)
    \end{align*}
    %
    Now multiply both sides of the equation on the right by $E(W)$ gives
    %
    \[ E(S) E(W) + E(W)^2 = E(S \cap W) E(W) + E(S \cup W) E(W) \]
    %
    Since $S \subset S \cup W$, $E(S) \leq E(S \cup W)$, and
    %
    \[ E(S \cup W) E(S) = E(S) \]

    Now $E(S \cup W) E(S) = E(S)^2 + E(W - S) E(S) = E(S)$
\end{proof}

\begin{example}
    If $H = \mathbf{C}$, then $B(H) \cong \mathbf{C}$, and any $\{ 0, 1 \}$ valued measure $\mu$ is a spectral measure, provided $\mu(\Omega) = 1$. If $\mu(S_1) = \mu(S_2) = 1$, then
    %
    \[ 1 = \mu(S_1) = \mu(S_1 \cap S_2) + \mu(S_1 - S_2) \]
    %
    \[ 1 = \mu(S_2) = \mu(S_1 \cap S_2) + \mu(S_2 - S_1) \]
    %
    If $\mu(S_1 \cap S_2) = 0$, then $\mu(S_1 - S_2) = \mu(S_2 - S_1) = 1$, which implies
    %
    \[ \mu((S_1 - S_2) \cup (S_2 - S_1)) = \mu(S_1 - S_2) + \mu(S_2 - S_1) = 2 \]
    %
    an impossibility. Thus $\mu(S_1 \cap S_2) = 1$. If $\mu(S_1) = 0$, then $S_1 \cap S_2 \subset S_1$, so $\mu(S_1 \cap S_2) \leq \mu(S_1) = 0$. The same holds if $\mu(S_2) = 0$. Thus we have verified that $\mu(S_1 \cap S_2) = \mu(S_1) \circ \mu(S_2)$ on a case by case basis. The countable summation property holds by the property of the measure itself.
\end{example}

\begin{example}
    If $H$ is infinite dimensional, and $N \in K(H)$, then
    %
    \[ \sigma(N) = \{ \lambda_0, \lambda_1, \dots \} \]
    %
    where $\lambda_0 = 0$, and each $\lambda_i$ is an eigenvalue. For each $n > 0$, let $P_n$ be orthogonal projection onto $\text{ker}(\lambda_n - N)$, and let $P_0$ project onto the orthogonal complement of the closed linear space of the images of $P_1, P_2, \dots$. For $S \subset \sigma(N)$, let
    %
    \[ E(S) = \sum_{\lambda_n \in S} P_n \]
    %
    In the sense that the sum on the right is interpreted pointwise. Then $E$ is a spectral measure on $(\sigma(N), \mathcal{P}(\sigma(N)))$. Surely
    %
    \[ E(\sigma(N)) = \sum_{n = 0}^\infty P_n = \text{id}_H \]
    %
    And since $P_i \circ P_j = 0$ if $i \neq j$,
    %
    \begin{align*}
        E(S_1) \circ E(S_2) &= \left( \sum_{\lambda_n \in S_1} P_n \right) \circ  \left( \sum_{\lambda_m \in S_2} P_m \right)\\
        &= \sum_{\lambda_n \in S_1, \lambda_m \in S_2} P_n P_m\\
        &= \sum_{\lambda_n \in S_1 \cap S_2} P_n = E(S_1 \cap S_2)
    \end{align*}
    %
    And the countable additivity follows by the pointwise definition of the sum of operators.
\end{example}

\begin{example}
    If $\Omega$ is a $\sigma$-finite measure space with measure $\mu$, and $H = L^2(\Omega)$, define
    %
    \[ E(S) \xi = \chi_S \xi \]
    %
    Then $E$ is a spectral measure. Surely $E(\Omega) = \text{id}_H$, $E(\emptyset) = 0$, because $\chi_\Omega = 1$, $\chi_\emptyset = 0$. Now $E(S_1 \cap S_2) = E(S_1) \circ E(S_2)$ follows because $\chi_{S_1 \cap S_2} = \chi_{S_1} \chi_{S_2}$. Similarily, $\chi_{\bigcup S_i} = \sum \chi_{S_i}$ pointwise if the $S_i$ are disjoint.
\end{example}

\end{document}
