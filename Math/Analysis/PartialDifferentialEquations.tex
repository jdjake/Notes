\input{../../style.tex}

\title{Partial Differential Equations}
\author{Jacob Denson}

\begin{document}

\pagenumbering{gobble}

\maketitle

\tableofcontents

\pagenumbering{arabic}

\part{Classical PDEs}

\chapter{Introduction}

Physicists were the first to consider partial differential equations. Physical problems provided the insight for many of the basic techniques to solving partial differential equations, and it removes something from the subject to forget the physical motivations for partial differential equations. In this chapter, we derive the four main partial differential equations which began the study of partial differential equations. Most involve the \emph{Laplacian} operator
%
\[ \Delta f = \sum_{i = 1}^d \frac{\partial^2 f}{\partial x_i^2} = \nabla \cdot (\nabla f) \]
%
which measures the local average deviation of a function about a point.

\section{The Transport Equation}

The \emph{Transport}, or \emph{Continuity} equation models the evolution of a flowing distribution of some quantity, for instance, mass, energy, or electrical charge. We might describe the distribution of a continuous distribution by a density function $u: \RR^n_x \times \RR_t \to \RR$. To describe the movement of this density, we introduce a a vector field $v: \RR^d_x \times \RR_t \to \RR^d_x$ which gives the direction and rate that the quantity is moving at a given position and time. To derive an equation for the evolution of $u$ over time, we fix a region $\Omega$, and define
%
\[ A(t) = \int_\Omega u(x,t)\; dx \]
% u units of M / X^d
% x units of X
% A units of M
% v units of X/S
% u(x + sv,t + s) = u(x,t)
to be the total amount of $u$ in $\Omega$ at time $t$. The rate at which mass leaves $A$ can be measured by the equation
%
\[ \frac{dA}{dt} = \int_{\partial \Omega} u [- n \cdot v]\; dS, \]
%
where $n$ is the outward pointing normal vector to $\Omega$. Indeed, mass supported in an infinitisimal neighborhood of a point $x_0 \in \partial \Omega$ will move at time $t_0$ in the direction given by the vector $v(x_0,t_0)$, and the volume of this neighborhood which is contained in $\Omega$ will increase / decrease at a rate proportional $- n \cdot v$. Applying the divergence theorem, we find that
%
\[ \int_{\partial \Omega} u [-n \cdot v]\; dS = \int_\Omega - \nabla \cdot (uv)\; dx. \]
%
But this means that for any region $\Omega$,
%
\[ \int_\Omega \frac{\partial u}{\partial t}\; dx = \frac{dA}{dt} = \int_\Omega - \nabla \cdot (uv)\; dx. \]
%
Since $\Omega$ was arbitrary, we conclude that
%
\[ \frac{\partial u}{\partial t} = - \nabla \cdot (uv). \]
%
Often in physics we introduce the quantity $J = uv$, called the \emph{current density}, and write the equation as
%
\[ \frac{\partial u}{\partial t} = - \nabla \cdot J. \]
%
In the particular case where $v$ is \emph{incompressible}, i.e. $\nabla \cdot v = 0$, the equation simplifies to the homogeneous equation
%
\[ \frac{du}{dt} = - (\nabla u) \cdot v, \]
%
which occurs, in particular, if $v$ is a constant vector field.

Now suppose that in addition, there is a \emph{source / sink function} $f: \RR^d_x \times \RR^d_t \to \RR$, which gives the rate at which mass is produced / removed at each location. Then we should expect that, with $A$ defined as above,
%
\[ \frac{dA}{dt} = \int_\Omega f - \int_{\partial \Omega} u \cdot [n \cdot v], \]
%
and we obtain a non-homogeneous partial differential equation
%
\[ \frac{du}{dt} = f - \nabla \cdot (uv), \]
%
which is only slightly harder to solve.

\section{The Heat Equation}

Consider a region $\Omega$ in space, where we fix a temperature on the boundary, and then allow temperature to fluctuate on the interior on its own volition. Let $u(t,x)$ represent a density for the temperature at $x \in \Omega$ at time $t$. Given a subregion $D \subset \Omega$, the value
%
\[ \frac{d}{dt} \int_D u(t,x)\ dx = \int_D \frac{\partial u}{\partial t}(t,x)\ dx \]
%
represents the rate of energy entering $D$. Newton's law of cooling says that the rate of energy leaving $D$ is proportional to the difference in temperatures between the boundary of the body and its immediate surroundings. To a first order, this is approximated by
%
\[ \int_{\partial D} (\nabla u \cdot \nu)(x)\; dS(x) \]
%
where $\nu$ is the outward pointing unit vector varying about the boundary of $D$. The divergence theorem implies that
%
\[ \int_{\partial D} (\nabla u \cdot \nu)(x)\ dS(x) = \int_D (\nabla \cdot \nabla u)(x)\ dx = \int_D \Delta u(x)\ dx \]
%
By conservation of energy, we therefore find that
%
\[ \int_D \frac{\partial u}{\partial t}(t,x)\ dx = k \int_D (\Delta u)(x)\ dx \]
%
where $k > 0$ is some constant of proportionality. Since this holds for any domain $D$ with smooth boundary, we conclude that heat must satisfy the partial differential equation
%
\[ \frac{\partial u}{\partial t} = k \Delta u \]
%
throughout the entire domain $\Omega$. This is the {\bf heat equation}. More generally, this equation describes the evolution of some density function $u$ which

We note that the equation can also be applied to understand the diffusion of anything over time in some region, provided the rate of diffusion is linearly related to the change of concentration.

\section{Wave Equation}

Consider a one dimensional string in two dimensions, or a two dimensional membrane in three dimensions, which is fixed on in boundary, but allowed to vibrate up or down on its interior. Let $\Omega$ denote the region upon which the object lies before it begins vibrating. Let $u(t,x)$ denote the displacement of a point $x \in \Omega$ at time $t$ from the plane. The average displacement over a region $D$ is therefore equal to
%
\[ \fint_D u(t,x)\ dx \]
%
For a single point mass, Hooke's law tells us that a point under tension will experience a force proportional to its distance from equilibrium. Here, our membrane has no equilibrium position, but if we fix $x$, and consider a small neighbourhood around $x$, the force at $x$ should be proportional to the difference between $u(t,x)$ and the average of $u(t,y)$ on a small neighbourhood $D$ around $x$. Using the fact that this difference is proportional to $|D|^2 (\Delta u)(x)$ for nice enough small neighbourhoods $D$, we can absorb constants and we find that the average force in a small region $D$ is equal to
%
\[ \fint_D k |D|^2 \Delta u(x) = k |D| \int_D k \Delta u(x) \]
%
Applying Newton's law, assuming that $u(t,x)$ has constant density $\rho$, we conclude that the average force in $D$ is also equal to
%
\[ \rho |D| \int_D \frac{\partial^2 u}{\partial t^2}(t,x) \ dx \]
%
And therefore
%
\[ \rho |D| \int_D \frac{\partial^2 u}{\partial t^2}(t,x) = k |D| \int_D (\Delta u)(t,x) \]
%
Dividing by $|D|$, and then taking the equation over all subregions $D$ of $\Omega$, we conclude that
%
\[ \rho \frac{\partial^2 u}{\partial t^2} = k \Delta u \]
%
This is the {\bf wave equation}.

\section{Laplace and Poisson's equations}

A special case to solutions of both the heat equation and the wave equation is Laplace's equation $\Delta u = 0$, which finds functions whose average difference is essentially equal to zero. Twice continously differentiable solutions to Laplace's equation are known as {\bf harmonic}. In terms of the heat equation, $u$ gives {\it steady state} temperature distributions which stay constant throughout time. In terms of the wave equation, $u$ also gives steady state wave distributions, because we assume the membrane upon which $u$ is defined is fixed at the boundary, so that if
%
\[ \frac{\partial^2 u}{\partial t^2} = 0 \]
%
then $u$ is in fact constant. More generally, we can try and solve the partial differential equation
%
\[ - \Delta u = f \]
%
where $f$ is some given function. This generalization of Laplace's equation is known as {\bf Poisson's equation}, and arises in many contexts, like in Electrostatics, where solving the Poisson equation amounts to finding a charge potential $u$ for a given charge distribution $f$.

\section{The Field of Partial Differential Equations}

In general, a partial differential equation is an equation involving a function and some of its partial derivatives. The utopian goal of the theory of partial differential equations is to {\it solve} a given partial differential equation, which means to find all functions which solve the equation, possibly assuming some additional restriction on the class of solutions, such as how the solution behaves at the boundary of the functions definition. In the best scenario, we can find explicit formulae for all of the solutions to the partial differential equations, but even in the case of ordinary differential equations, we know that this is not possible, so we instead deduce qualitative properties of the solutions.

For ordinary differential equations, there is a conclusive {\it existence and uniqueness} theory guaranteeing that a given differential equations is solvable, and uniquely solvable given some initial conditions. For a general partial differential equation, we have no such theory. Instead, we must argue for ourself whether a partial differential equation given to us is {\it well posed}, in the sense that it has a solution, the solution is unique given certain conditions, and whether the solution depends continuously based on the initial conditions. In some cases, we may have to enlarge the variety of functions we consider to solve the equation. As an example, the PDE
%
\[ \frac{\partial u}{\partial t} + \frac{\partial (f \circ u)}{\partial x} = 0 \]
%
governs the motion of shock waves in physics. In real life, the behavior of these waves is highly discontinuous, and as such we should not expect a solution to this equation to even be differentiable. It is true that this equation has no differentiable solutions, but if we enlarge the class of functions which can solve this equation to the class of {\it generalized} or {\it weak solutions}, then we do have a uniqueness theory. To summarize, the theory of partial differential equations focuses on three phenomena:
%
\begin{itemize}
    \item (Existence) Does a solution to a partial differential equation exist, and if so, is it possible to express the solution in a formula.

    \item (Uniqueness) Given initial conditions, is the solution to a given partial differential equation unique?

    \item (Regularity) Are the solutions to a given differential equation differentiable, and to what extent?
\end{itemize}
%
In these notes we discuss our ability to answer these types of questions.



\chapter{Solving Classical Partial Differential Equations}

\section{The Transport Equation}

The easiest PDE to analyze is the {\bf transport equation}
%
\[ \frac{\partial u}{\partial t} + a \cdot \nabla u = 0 \]
%
It is essentially an exercise in basic multivariate calculus to show the uniqueness and existence theory of this PDE. Intuitively, $u$ should just propogate in the direction of the vector $a$. If $u$ is any differentiable solution to this equation, and we define $z(s) = u(t+s, x + as)$, then we find that $z$ satisfies the differential equation $z'(s) = u_t(t+s,x + as) + a \cdot \nabla u(t+s, x - as) = 0$, so $z$ is constant, and we conclude $u$ is constant on each line parallel to the hyperplane through the origin generated by $(1,a)$. Conversely, any differentiable function constant on these hyperplanes is a solution. If we specify a set of values on any hyperplane not parallel to $v$, there is a unique differentiable solution $u$ extending these values to the entire plane. If the values are not smooth, then $u$ will not be smooth, but we shall find that we can still view $u$ as a {\it weak} solution to the equation, as we will find later. Thus even the most basic partial differentiable equations have non-differentiable solutions.

Now how do we solve the non-homogenous equation
%
\[ \frac{\partial u}{\partial t} + a \cdot \nabla u = f \]
%
By linearity of the physical situation, it makes sense that the initial mass of the equation should propogate independently of the source mass produced. Thus, given any particular solution $u$, and a fixed $x$, we consider the function $z(s) = u(t+s, x + as)$. Then
%
\[ z'(s) = \frac{\partial u}{\partial t} + a \cdot \nabla u = f(t + s, x - as) \]
%
It therefore follows that
%
\[ u(t + s, x - as) = u(t,x) + \int_0^s f(t+y,x-ay)\ dy \]
%
If we are given initial values $u(0,x) = g(x)$, then we have unique solution solving the nonhomogenous equation given by
%
\[ u(t,x) = g(x-ta) + \int_0^t f(x + (s-t)a,s)\ ds \]
%
So the initial values $g$ propogate throughout the equation at a velocity $a$, as well as build up occuring based on the function $f$. In this case, the regularity of $u$ depends on the smoothness of $g$ and $f$.

\section{The Laplacian and Fundamental Solutions}

Provided a partial differential equation is linear, then a linear combination of solutions to the partial differential equation is also a solution to the partial differential equation. A good strategy to finding {\it all} solutions to PDEs of this form is to find a set of explicit solutions to the PDE, and then to find arbitrary solutions by taking arbitrary linear combinations of explicit solutions. We begin by analyzing Laplace's equation $\Delta u = 0$, or more generally, Poisson's equation $\Delta u = f$. It will help to note that the Laplacian is invariant under translations.

\begin{theorem}
    $(\Delta u) \circ T = \Delta (u \circ T)$ for any rotation $T$.
\end{theorem}
\begin{proof}
    $\Delta u(x)$ is the trace of the Hessian operator
    %
    \[ (Hu)(x) = \left( \frac{\partial^2 u}{\partial x_ix_j} \right) = D(\nabla u)(x) \]
    %
    Now the chain rule implies $D(u \circ T)(x) = (Du)(Tx) \cdot T$, so
    %
    \begin{align*}
        \nabla (u \circ T)(x) &= [D(u \circ T)(x)]^T = [(Du)(Tx) \cdot T]^T\\
        &= T^T \cdot (Du)(Tx)^T = T^T \cdot (\nabla u)(Tx) = (T^{-1} \circ \nabla u \circ T)(x)
    \end{align*}
    %
    The equation $\nabla (u \circ T) = T^{-1} \circ \nabla u \circ T$ essentially says that the gradient respects a coordinate change obtained by a rotation matrix (Remark: This is one of the main reasons why we can define the gradient unambiguously on a Riemannian manifold). Now
    %
    \begin{align*}
        H(u \circ T)(x) &= D(\nabla (u \circ T))(x) = D(T^{-1} \circ \nabla u \circ T)(x)\\
        &= T^{-1} \cdot D(\nabla u)(Tx) \cdot T = T^{-1} \cdot (Hu)(Tx) \cdot T
    \end{align*}
    %
    The traces of two similar matrices are equal, so the traces of $H(u \circ T)(x)$ and $Hu(Tx)$ are equal, so $\Delta (u \circ T)(x) = (\Delta u)(Tx)$.
\end{proof}

We now illustrate how to solve Poisson's equation using the distributional method of fundamental solutions. Given the equation $\Delta u = f$, we find a tempered distribution $\Phi$ whose Laplacian $\Delta \Phi$ is the Dirac delta function $\delta$. Then for any Schwarz function $f$, the Schwarz function $\Phi * f$ satisfies $\Delta(\Phi * f) = (\Delta \Phi) * f = \delta * f = f$, so via convolution we can find a solution to virtually any smooth Poisson's equation we require. A function $\Phi$ for such an equation is known as a {\bf fundamental solution}.

To determine if a fundamental solution exists, we can determine it's structure by using the Fourier transform, since $\Phi$ is tempered. This gives
%
\[ - 4 \pi^2 |\xi|^2 \widehat{\Phi} = (\Delta \Phi)^\ft = \delta^\ft = 1 \]
%
so we can divide both hand sides of the equations to conclude
%
\[ \widehat{\Phi}(\xi) = \frac{-1}{4 \pi^2 |\xi|^2} \]
%
The right hand side is bounded away from the origin, and locally integrable for $d \geq 3$, so is actually a tempered distribution, and we find by taking the inverse Fourier transform that
%
\[ \Phi(x) = \frac{- \Gamma(d/2 - 1)}{4 \pi^{d/2} |x|^{d - 2}} \]
%
Notice that the fundamental solution is radially symmetric, reflecting the fact that the Laplacian involves dispersion from a point. Since we have the inequality
%
\[ |\Phi(\phi)| \lesssim \int \frac{\phi(x)}{|x|^{d-2}} \lesssim \| \phi \|_\infty + \| x^3 \phi \|_\infty \]
%
the distribution $\Phi$ extends continuously to the Banach space of all measurable functions for which $f, x^3 f \in L^\infty(\mathbf{R})$. In particular, we can define
%
\[ (\Phi * f)(x) = \Phi(T_x f^*) \]
%
where $f^*(y) = f(-y)$. We find
%
\[ \int \Phi(T_x f^*) \frac{\phi(x+h) - \phi(x)}{h}\; dx = \int \frac{\Phi(T_{x-h} f^*) - \Phi(T_x f^*)}{h} \phi(x)\; dx \]
%
TODO: Extend this to show that the convolution satisfies Poisson's equation when $f$ is in a more general space.

\begin{example}
    Suppose $\Lambda$ is a tempered distribution with $\Delta \Lambda = 0$. Then
    %
    \[ 0 = \widehat{\Delta \Lambda}(\xi) = - 4 \pi^2 |\xi|^2 \widehat{\Lambda}(\xi) \]
    %
    Thus $\widehat{\Lambda}$ has support at the origin. But if $\widehat{\Lambda} = \sum \lambda_\alpha D^\alpha$, then combined with the fact that
    %
    \[ (D^\alpha f^\vee)(0) = ((-2 \pi i x)^\alpha f)^\vee(0) = \int (- 2 \pi i x)^\alpha f(x)\; d\xi \]
    %
    so $(D^\alpha)^\vee = (- 2 \pi i \xi)^\alpha$, so
    %
    \[ \Lambda(x) = \sum \lambda_\alpha (D^\alpha)^\vee(x) = \sum \lambda_\alpha (- 2 \pi i x)^\alpha \]
    %
    Thus the only harmonic tempered distributions are the polynomials.
\end{example}

Since $\Delta u = 0$ has rotational symmetry, it makes sense to begin by finding solutions to Laplace's equation which are rotationally symmetric. These will form our explicit solutions we can put into superposition to construct other solutions. So suppose we have a rotationally symmetric function $u(x) = f(r)$, where $r = |x|$. In this case, we find
%
\[ D_iu(x) = f'(r) (x_i/r)\ \ \ \ \ D_{ii} u(x) = f''(r) (x_i/r)^2 + f'(r)(1/r - x_i^2/r^3) \]
%
Summing up, we find
%
\[ \Delta u(x) = f''(r) + f'(r) \frac{d - 1}{r} \]
%
If $\Delta u$ vanishes, this means
%
\[ \frac{f''(r)}{f'(r)} = \frac{1 - d}{r} \]
%
So, integrating both sides with respect to $r$, we find $f'(r)$ is proportional to $1/r^{d-1}$, and therefore there are constants $A$ and $B$ such that
%
\[ f(x) = \begin{cases} A|x|^{2-d} + B & : d > 2 \\ A \log r + B & : d = 2 \end{cases} \]
%
These solutions motivate the {\bf fundamental solution of the Laplacian}
%
\[ \Phi(x) = \begin{cases} \frac{1}{d(d-2) \alpha(d)} \frac{1}{|x|^{d-2}} & : d > 2 \\ \frac{-1}{2 \pi} \log |x| & : d = 2 \end{cases} \]
%
where $\alpha(d)$ is the volume of the unit ball in $d$ dimensions. Certainly these two equations solve Laplace's equation for $x \neq 0$, but we have a singularity at the origin. The integral
%
\[ \int_{\mathbf{R}^n} f(x) \Phi(x) \]
%
is well defined for any $f$ bounded in a ball around the origin, and with a suitable decay for the terms of the limit to exist (for instance, any decay on the order of $O(1/|x|^\delta)$ will do, for some $\delta$), because using the fact that $r^{d-1} \Phi(r) = O_\varepsilon(r^{1 + \varepsilon})$ for all $\varepsilon > 0$, we conclude that
%
\begin{align*}
    \left| \int_{\mathbf{R}^d} f(x) \Phi(x) \right| &= \left| \int_0^\infty r^{d-1} \Phi(r) \int_{S_1} f(ry)\; dy \right|\\
    &\lesssim_{d,\varepsilon} \left| \int_0^\infty r^{1 + \varepsilon} \| f \|_{L^\infty(S_r)}\; dr \right|\\
    &= \left| \int_0^N r^{1 + \varepsilon} \| f \|_{L^\infty(S_r)}\; dr \right| + \left| \int_N^\infty r^{1 + \varepsilon} \| f \|_{L^\infty(S_r)}\; dr \right|\\
    &\lesssim N^{2 + \varepsilon} \| f \|_{L^\infty(\mathbf{R}^d)} + \| x^3 f \|_{L^\infty(\mathbf{R}^d)}
\end{align*}
%
Thus $\Phi$ can be interpreted as a tempered distribution. In particular, we can calculate $\Delta \Phi$ in a distribution sense. We find that $\Delta \Phi = -\delta$, where $\delta$ is the Dirac delta function at the identity. As a consequence, if $f$ is any Schwartz function, then the function
%
\[ u(x) = (\Phi * f)(x) = \int \Phi(y) f(x - y)\; dy \]
%
is $C^\infty$ and satisfies $\Delta u = - f$.

\begin{theorem}
    If $\Phi$ is the fundamental solution, then $\Delta \Phi = - \delta$. More generally, if $f$ is a function with bounded continuous first and second derivatives, and with a Laplacian with enough decay at $\infty$ to define the integral below, then
    %
    \[ \int (- \Delta f)(x) \Phi(x) = f(0) \]
\end{theorem}
\begin{proof}
    We note that the first partial derivatives of $\Phi$ are $\lesssim 1/|x|^{d-1}$, and the second derivatives are $\lesssim 1/|x|^d$. If $f$ is a given Schwartz function, we consider
    %
    \[ \int_{\mathbf{R}^n} \Phi(x) \Delta f(x) = \int_{|x| \leq \varepsilon} \Phi(x) \Delta f(x) + \int_{|x| > \varepsilon} \Phi(x) \Delta f(x) = A_\varepsilon + B_\varepsilon \]
    %
    Now $|A_\varepsilon| \leq \| \Delta f \|_{L^\infty(B_\varepsilon)} \| \Phi \|_{L^1(B_0(\varepsilon))} = o(1)$ as $\varepsilon \to 0$. An integration by parts implies that
    %
    \[ B_\varepsilon = \int_{|x| > \varepsilon} \Phi(x) \Delta f(x) = - \int_{S_\varepsilon} \Phi [\nabla f \cdot \eta] - \int_{|x| > \varepsilon} \nabla \Phi \cdot \nabla f = C_\varepsilon + D_\varepsilon \]
    %
    Again, the boundedness of $\nabla f$ implies that $C_\varepsilon = o(1)$, and a further integration by parts implies that
    %
    \[ D_\varepsilon = - \int_{|x| > \varepsilon} \nabla \Phi \cdot \nabla f = \int_{S_\varepsilon} f (\nabla \Phi) \cdot \eta + \int_{|x| > \varepsilon} f \Delta \Phi = \int_{S_\varepsilon} f (\nabla \Phi \cdot \eta) \]
    %
    Now we use the actual features of $\Phi$ other than its decay. Indeed, we know
    %
    \[ (\nabla \Phi)(x) = [-1/d\alpha(d)] x/|x|^d \]
    %
    so
    %
    \[ (\nabla \Phi)(x) \cdot \eta = (\nabla \Phi)(x) \cdot x/|x| = \frac{-1}{d\alpha(d)|x|^{d-1}} \]
    %
    so
    %
    \[ \int_{S_\varepsilon} f\; \nabla \Phi \cdot \eta = \frac{-1}{d\alpha(d) \varepsilon^{d-1}} \int_{S_\varepsilon} f(x) = - \fint_{S_\varepsilon} f(x) = - f(0) + o(1) \]
    %
    This completes the calculation.
\end{proof}

Harmonic functions possess an incredibly useful integral formula which makes them as regular as we desire, known as the mean value formula.

\begin{theorem}
    If $f$ is harmonic in a region $D$ containing a closed ball $B$ centered at $x$, then
    %
    \[ f(x) = \fint_{\partial B} f(y)\; dy \]
\end{theorem}
\begin{proof}
    Set
    %
    \[ \phi(r) = \fint_{\partial B_r(x)} f(y)\; dy = \fint_{\partial B_1(x)} f(x + ry)\; dy \]
    %
    Then it is easy to see that
    %
    \[ \phi'(r) = \fint_{\partial B_1(x)} (\nabla f(x + ry) \cdot y)\; dy = \fint_{\partial B_r(x)} (\nabla f(y) \cdot (y-x))\; dy \]
    %
    But Green's formula implies
    %
    \[ \int_{\partial B_r(x)} (\nabla f(y) \cdot (y-x))\; dy = r \int_{\partial B_r(x)} (\nabla f(y) \cdot \eta) dy = r \int_{B_r(x)} \Delta f(y) = 0 \]
    %
    so $\phi' = 0$, and so $\phi$ is constant. Since $\phi(r) \to f(x)$ as $r \to 0$, we find $\phi(r) = f(x)$ for all $r$. This completes the proof.
\end{proof}

A simple corollary is that if $f$ is harmonic, then
%
\begin{align*}
    \fint_{B_r(x)} f(y)\; dy &= \frac{1}{r^d \alpha(d)} \int_0^r \int_{\partial B_s(x)} f(y)\; dy\; ds\\
    &= d\alpha(d) f(x) \int_0^r\; s^{d-1} ds = r^d d \alpha(d) f(x)
\end{align*}
%
This is a suitable generalization of the Cauchy integral formula to higher dimensions, which enables us to try and extend the results of complex analysis to a higher dimensional setting. The easiest argument to generalize, which passes through without any change, is the maximal principle.

\begin{theorem}
    If $u$ is harmonic in an open precompact set $U$, and continuous on $\overline{U}$, then the maximum of $u$ over $\overline{U}$ occurs on the boundary of $U$. If, in addition, $U$ is connected, and $u$ attains it's supremum in the interior of $U$, then $u$ is constant.
\end{theorem}
\begin{proof}
    Let $u(x_0)$ be a maximum for $U$, so $u(y) \leq u(x_0)$ for all $y$ in a ball $B$ around $x_0$ in $U$. Then the mean value formula just proved shows
    %
    \[ u(x_0) = \fint_{\partial B} u(y)\; dy \leq u(x_0) \]
    %
    and this can only be an equality if $u(y) = u(x_0)$ for all $y \in \partial B$. Thus $u$ is locally constant around every local maximum for $U$. In particular, $u$ is constant on the connected component of $U$ containing $x_0$, and in particular, any point on the boundary of this connected component is also a maximum.
\end{proof}

The initial claim is known as the weak maximum principle, and the second claim the strong maximum principle. Variants of these principles will hold for more general solutions to differential equations, which we will discuss in time. Note that if $u$ is harmonic on $U$, and is positive on $\partial U$, then $u$ is positive everywhere on $U$, because by negating $u$ we obtain a `minimum principle'.

The maximum principles are useful because they allow us to show a {\it uniqueness result} for Laplace's equation.

\begin{theorem}
    If $f$ and $g$ are continuous functions defined on $U$ and $\partial U$ respectively, then there is at most one twice differentiable function $u$ with $\Delta u = f$ on $U$, continuous on  and $u = g$ on $\partial U$.
\end{theorem}
\begin{proof}
    Let $u_1$ and $u_2$ be two functions satisfying the required conditions. Then $u_1 - u_2$ is harmonic on $U$, and vanishes on the boundary. But this implies by the maximum principle that $u_1 - u_2$ vanishes on $U$ as well, so $u_1 = u_2$.
\end{proof}

Next, we prove the {\it regularity} of solutions to Laplace's equation.

\begin{theorem}
    If $u$ is continuous in $U$, and satisfies the mean value formula, then $u$ is actually infinitely differentiable in $U$.
\end{theorem}
\begin{proof}
    Let $\eta_\varepsilon$ be a {\it mollifier} in $U$. 
\end{proof}









\chapter{Sobolev Spaces}

The theory of Sobolev spaces is most effective in enabling us to apply the methods of functional analysis to the study of partial differential equations. In order to apply these techniques, we need 









\chapter{Dispersive Equations}

Consider a linear partial differential equation $\partial_t u = Lu$, where $L$ is a constant coefficient differential operator of order $k$. Write
%
\[ Lu = \sum_{|\alpha| \leq k} c_\alpha D^\alpha u. \]
%
A pure wave $u(x,t) = e^{2 \pi i (x \cdot \xi + \omega t)}$ satisfies this differential equation precisely when
%
\[ \omega = \sum_{|\alpha| \leq k} c_\alpha (2 \pi i)^{|\alpha| - 1} \xi^\alpha. \]
%
We write the right hand side as $h(\xi)$, and call the equation $\omega = h(\xi)$ the \emph{dispersion relation} of the equation. Let us focus on the case where the coefficients $c_\alpha (2 \pi i)^{|\alpha| - 1}$ are real-valued, in which case the waves we consider do not decay or grow exponentially in time.

Applying linearity and the Fourier transform, one might expect that a solution to the equation above can be written as a superposition of solutions applied to waves, i.e. a solution to the equation with initial conditions $u_0(x)$ can be written as
%
\[ u(x,t) = \int e^{2 \pi i (x \cdot \xi + h(\xi) t)} \widehat{u_0}(\xi)\; d\xi. \]
%
This is true, when the integral on the right is interpreted in a distributional sense. Thus to understand the regularity theory of this PDE, one must understand the interactions of waves travelling at different spatial and temporal frequencies. For the purpose of notational convenience, we also denote the above quantity be $(e^{tL} u_0)(x)$, where $e^{tL}$ is the \emph{propogation operator} of the equation. Intuitively, a partial differential equation exhibits \emph{dispersive behaviour} if the spatial uncertainty of $e^{tL} u_0$ increases over time much larger than the frequential uncertainty. In this case, we can usually obtain some kind of $L^p$ improving type behaviour over time.

\begin{example}
    Consider the phase-rotation equation $\partial_t u = 2 \pi i \omega_0 \cdot u$, where $\omega_0 \in \RR$. The dispersion relation for this equation is $h(\xi) = \omega_0$. Thus this is a fairly degenerate dispersive partial differential equation, since waves at different spatial frequencies travel at the same temporal frequency, and thus do not interact. We can solve this equation, writing $u(x,t) = e^{2 \pi i \omega_0 t} u_0(x)$, and we see that $\| e^{tL} u_0 \|_{L^p(\RR^d)} = \| u_0 \|_{L^p(\RR^d)}$ for all $0 < p \leq \infty$. Thus there is no constructive or deconstructive interference between waves, so there is not much dispersive behaviour in the PDE.
\end{example}

\begin{example}
    Consider the transport equation $\partial_t u = - v_0 \cdot \nabla u$, where $v_0 \in \RR^d$. The dispersion relation of this equation is $h(\xi) = -v_0 \cdot \xi$. The solution to this equation are $u(t,x) = u_0(x-tv_0)$. Again we see $\| e^{tL} u_0 \|_{L^p(\RR^d)} = \| u_0 \|_{L^p(\RR^d)}$. (TODO CAN WE EXPLAIN THIS VIA INTERACTIONS OF DIFFERENT FREQUENCIES)
\end{example}

\begin{example}
    Consider the free Schr\"{o}dinger equation $\partial_t u = - (\hslash / 4 \pi i m) \Delta u = 0$. The dispersion relation of this equation is $h(\xi) = -(\hslash/2 m)|\xi|^2$. TODO: WHY DO WE EXPECT DISPERSIVE BEHAVIOUR HERE.
\end{example}

\begin{example}
    Consider the Airy equation $\partial_t u = - (1/4\pi^2) \partial_{xxx} u$ on the real line, with dispersion relation $h(\xi) = \xi^3$.
\end{example}

For linear systems of first order differential equations, we can consider very similar relations. Given $u = (u_1,\dots,u_n)$, and a system of linear operators
%
\[ L_a u = \sum c_{\alpha ab} u_b \]
%
for $a \in \{ 1, \dots , a \}$, and consider the system of partial differential equations $\partial_t u_a = L_a u$. A pure wave with
%
\[ u_a(x,t) = e^{2 \pi i (\omega_a t + \xi_a \cdot x)} \]
%
solves this system of equations provided that
%
\[ 2 \pi i \omega_a = \sum (2 \pi i)^{|\alpha|} c_{\alpha ab} \xi_b^\alpha. \]
%
If we define $h(\xi_1,\dots,\xi_n)_a = \sum (2 \pi i)^{|\alpha| - 1} c_{\alpha ab} \xi_b^\alpha$, then we have a multivariate dispersion equation $\omega = h(\xi)$ measuring solutions to the linear system.

\begin{example}
    Consider the wave equation $\partial^2_t u = c^2 \Delta u$, which is a second order equation. We can write it as a first order system of equations by consider the set of solutions $(u_1,u_2)$ to the system of equations $\partial_t u_1 = u_2$, and $\partial_t u_2 = c^2 \Delta u_1$. Thus the dispersion equation is given by
    %
    \[ \omega_1 = 1/2\pi i \quad \omega_2 = (2 \pi i) c^2 |\xi_1|^2 \]
\end{example}

To get some intuition about dispersive behaviour, fix $\phi \in C^\infty(\RR^d)$, $\xi_0 \in \RR^d$, and a large parameter $R > 0$, and consider the function
%
\[ u_R(x) = e^{2 \pi i \xi_0 \cdot x} \psi(x), \]
%
where $\psi(x) = \text{Dil}_R \phi(x) = \phi(x/R)$. As $R$ increases, we can think of $u_R$ as being a model case of a function with large spatial uncertainty and small frequency uncertainty, i.e. the majority of it's mass in phase space is concentrated on a radius $\approx R$ ball around the origin, and it's mass in frequency space is mainly concentrated on a radius $\approx 1/R$ ball centred at $\xi_0$. Now
%
\[ e^{tL} u_R(x) = R^d \int e^{2 \pi i (\xi \cdot x + h(\xi) t)} \widehat{\phi}(R(\xi - \xi_0) )\; d\xi. \]
%
We see from this equation that $e^{tL} u_R$ has frequency uncertainty proportional to the frequency uncertainty of $u_R$. To understand the properties of the function in phase space, we perform a Taylor expansion, writing
%
\[ h(\xi) \approx h(\xi_0) + \nabla h (\xi_0) \cdot (\xi - \xi_0), \]
%
and then plugging this into the formula for $e^{tL} u_R$, we might expect that
%
\[ e^{tL} u_R(x) \approx e^{2 \pi i(\xi_0 \cdot x + h(\xi_0) t)} \psi(x + t \nabla h(\xi_0)). \]
%
Indeed, we calculate that if $D^\alpha \phi \in L^\infty(\RR^d)$ for each $|\alpha| \leq k$,
%
\[ \left| (\partial_t - L) \left\{ e^{2 \pi i(\xi_0 \cdot x + h(\xi_0) t)} \psi(x + t \nabla h(\xi_0)) \right\} \right| \lesssim 1/R^2 \]
%
Thus the spatial uncertainty is also preserved. The idea behind dispersive partial differential equations is that if $\nabla h(\xi_0)$ varies as $\xi_0$ varies, then the superposition of waves travelling at different frequencies will cancel out, leading to 
%, if $\| \nabla \phi \|_{L^1(\RR^d)} < \infty$, we can calculate that
%
%\[ |e^{tL} u_R(x) - e^{2 \pi i(\xi_0 \cdot x + h(\xi_0) t)} \psi(x + t \nabla h(\xi_0))| \lesssim t/R. \]
%
%Alternatively, assuming that $D^\alpha \phi \in L^\infty(\RR^d)$ for all $|\alpha| \leq k$, we calculate that
%
%\[ \left|(\partial_t - L) \left\{ e^{2 \pi i(\xi_0 \cdot x + h(\xi_0) t)} \psi(x + t \nabla h(\xi_0)) \right\} \right| \lesssim 1/R^2. \]
%
%It thus follows that for any $|\alpha| \leq k$,
%
%\[ \left| D^\alpha \left\{ e^{2 \pi i(\xi_0 \cdot x + h(\xi_0) t)} \psi(x + t \nabla h(\xi_0)) - e^{tL} u_R(x) \right\} \right| \lesssim e^{Ct}/R^2. \]








%\section{Appendix: The Laplacian}

%Using Taylor's theorem, we write
%
%\begin{align*}
%    f(x + y) &= f(x) + \langle (\nabla f)(x), y \rangle + \frac{1}{2} \sum_{i \leq j} y_iy_j f_{x_ix_j}(x) + \sum_{i \leq j \leq k} h_{ijk}(y) y_iy_jy_k
%\end{align*}
%
%where $h_{ijk}(y) \to 0$ as $y \to 0$. Note that since $y_i$ and $y_iy_j$ are odd functions of $i$ for $i \neq j$, the average value over a region symmetric in the $i$'th axis is equal to zero, and in particular for a ball of radius $r$, we find
%
%\[ \int_{B_r} y_i = \int_{B_r} y_iy_j = 0 \]
%
%If we let $\alpha(n)$ denote the volume of an $n$ dimensional unit ball, which can be calculated recursively by the formula
%
%\[ \alpha(n) = \alpha(n-1) \int_{-\pi/2}^{\pi/2} \cos^n(u)\ du \]
%
%with $\alpha(1) = 2$. We then find that
%
%\begin{align*}
%    \frac{1}{v(B^n_r)} \int_{B^n_r} [f(x+y) - f(x)] dy &= \sum \frac{f_{x_i}^2(x)}{2 v(B^n_r)} \int_{B^n_r} y_i^2 + \sum_{i \leq j \leq k} \frac{1}{v(B^n_r)} \int_{B^n_r} h_{ijk}(y) y_iy_jy_k\\
%    &= \sum \frac{f_{x_i^2}(x)}{v(B_r^n)} \int_0^r v\left(B^{n-1}_{\sqrt{1-y^2}}\right) y^2 dy + o(r^3)\\
%    &= (\Delta f)(x) \frac{\alpha(n-1)}{\alpha(n)} r^{-n} \int_0^r y^2(1-y^2)^{\frac{n-1}{2}} dy + o(r^3)\\
%    &= (\Delta f)(x) \frac{\alpha(n-1)}{\alpha(n)} r^2 \int_0^{\pi/2} [\cos^n(t) - \cos^{n+2}(t)] dt + o(r^3)\\
%    &= (\Delta f)(x) \frac{\alpha(n-1)}{\alpha(n)} r^2 (W_n/n) + o(r^3)
%\end{align*}
%
%So, up to a scalar factor depending on the dimension of the Laplacian, $(\Delta f)(x)$ measures the average difference in a small neighbourhood about the point.




\part{Nonlinear Partial Differential Equations}

\chapter{Ordinary Differential Equations}

We begin our discussion of nonlinear partial differential equations by building intuition for the phenomena that we will soon encounter with the general class of partial differential equations. In particular, many partial differential equations can be viewed as infinite dimensional variants of partial differential equations. An advantage that the theory of ordinary differential equations possesses is that we do not need to leave the domain of \emph{classical solutions}, i.e. functions differentiable enough times to satisfy the differential equation pointwise. Nonetheless, the theory of finite time blowup still occurs here, and as we consider systems of ordinary differential equations with larger and larger degrees of freedom, many of the same phenomena as partial differential equations begins to arise asymptotically.

In order to solve nonlinear partial differential equations, a general theme is the study of \emph{feedback}. One must study how the behaviour of a solution to an equation at a particular time influences later behaviour of the equation. In the theory of nonlinear equations, one therefore often needs to try to control solutions in terms of themselves.

Let's introduce notation. Let $D$ be a finite-dimensional vector space (we'll assume a real vector space, but complex works just as well), equipped with a norm $\| \cdot \|_D$. We call $D$ the state space, or data. In the most general form, an ordinary differential equation (of order $k$) on a time interval $I$ is a function $G: D^{k+1} \times I \to Y$, where $Y$ is another finite dimensional vector space (in particular, this is an \emph{order $k$ ordinary differential equation}. One then studies functions $u \in C^k(I,D)$ such that
%
\[ G(u,\partial_t u, \dots, \partial_t^k u,t) = 0. \]
%
Such solutions are known as \emph{classical solutions} to the partial differential equation.

The first simplification we can make is to assume our equation is \emph{autonomous}, i.e. so that the function $G$ does not depend on time. One can always convert a non-autonomous equation into an autonomous equation by incorporating $t$ into the state space $D$, i.e. letting $W = D \oplus \RR$, and defining $\tilde{G}: W^{k+1} \to Y \times \RR$ by setting
%
\[ \tilde{G}((u_0,s_0), (u_1,s_1), \dots, (u_k,s_k)) = ( G(u_0,\dots,u_k,s_0), s_1 - 1 ), \]
%
i.e. adding time as an extra variable $s$, and adding the equation $\partial s / \partial t = 1$ into the system. This is not necessarily `for free', e.g. it converts the linear equation $f(t) \partial_t u + g(t) u = 0$ into a nonlinear equation $f(s) \partial_t u + g(s) u = 0$. But it has several advantages; in particular, we have \emph{time symmetry}, i.e. if $u \in C^k(I,D)$ is any classical solution to a partial differential equation then for any $s \in I$, the function $\text{Trans}_s u \in C^k(I + s,D)$ given by setting $[\text{Trans}_s u](t) = u(t - s)$ is also a solution. In particular, we can always trade in this symmetry to assume that without loss of generality we are considering initial conditions starting at time $t = 0$.

Another simplification we make is that our ordinary differential equation $G(u, \partial_t u, \dots, \partial_t^k u, t) = 0$ can be rewritten in the form
%
\[ \partial_t^k u = F(u,\dots, \partial_t^{k-1} u, t). \]
% G solutions (if nondegenerate) forms a manifold of dimension (1 + k dim D) - dim Y
% F solutions (if nondegenerate) forms a manifold of dimension 1 + (k-1) dim D
% So dim D = dim Y
where $F: D^k \to D$. This is often possible if the equation has the right degrees of freedom relative to the constraints that the equation gives, i.e. if $\dim D = \dim Y$, so we have the same number of equations as parameters. If $\dim D < \dim Y$ (we have less parameters than equations), then we say the system is \emph{overdetermined}, i.e. there are less degrees of freedom than constraints, and one must likely assume certain constraints on initial conditions in order to guarantee the existence of solutions. If $\dim D < \dim Y$ (we have more parameters than equations), the system is \emph{underdetermined}, and one likely has multiple solutions given any particular initial data. This can be dealt with by studying parametric families of solutions, or to find a family of symmetries in the problem which, when removed by adding additional constraints, give us an equation of the form above. Another approach is to take an additional time derivative, and consider the equation
%
\[ 0 = \partial_t G(u,\dots,\partial_t^ku,t) = \sum_{i = 0}^k (\partial_i G)(u,\dots,\partial_t^k u, t) \cdot \partial_t^{i+1} u + \partial_{k+1} G(u,\dots,\partial_t^k u, t) \]
%
and then, provided $\partial_k G$ is non-vanishing, rewrite the equation as
%
\[ \partial_t^{k+1} u = - \frac{\sum_{i = 0}^{k-1} (\partial_i G)(u,\dots,\partial_t^k u, t) \cdot \partial_t^{i+1} u + \partial_{k+1} G(u,\dots,\partial_t^k u, t)}{\partial_k G(u,\dots,\partial_t^k u, t)}. \]
%
Regardless, we refer to equations of order $k$ that cannot be expressed in the form $\partial_t^k u = F(u,\dots,\partial_t^{k-1} u,t)$ as \emph{degenerate}. Such equations are quite difficult to analyze in full generality and we do not deal with them here. In the sequel, we restrict our attention to nondegenerate, autonomous equations.

Now suppose $F \in C^\infty(D^{k+1},Y)$. If $u \in C^k(I,D)$ satisfies this equation pointwise, then we actually have $u \in C^{k+1}(I,D)$, with
%
\[ \partial_t^{k+1} u = \sum_{i = 0}^{k-1} (\partial_i F)(u,\dots,\partial_t^{k-1}u) \cdot \partial_t^{i+1} u. \]
%
Continuing this by considering successive derivatives in $t$ gives that $u \in C^\infty(I,D)$. In particular, we note that the derivatives $\partial_t^i u$ for $i \geq k$ depend \emph{only} on the regularity of the derivatives $\partial_t^i u$ for $i < k$. For analytic functions, this approach also easily gives a uniqueness result.

\begin{lemma}
    Suppose $u_1,u_2 \in C^\omega(I,D)$ and for $\alpha \in \{ 1, 2 \}$,
    %
    \[ \partial_t^k u_\alpha = F(u_\alpha, \dots, \partial_t^{k-1} u_\alpha). \]
    %
    Suppose furthermore, that there is $t_0 \in I^\circ$ such that for each $i < k$,
    %
    \[ (\partial_t^i u_1)(t_0) = (\partial_t^i u_2)(t_0). \]
    %
    Then $u_1 = u_2$.
\end{lemma}
\begin{proof}
    It follows that $\partial_t^i u_1 = \partial_t^i u_2$ for \emph{all} $i > 0$, which implies that $u_1 = u_2$ in a neighbourhood of $t_0$. But this means that
    %
    \[ S = \{ t \in I: u_1(t) = u_2(t), \dots, \partial_t^{k-1} u_1(t) = \partial_t^{k-1} u_2(t) \} \]
    %
    is open and closed in $I$. Since $I$ is connected, this implies $S = I$, and this implies that $u_1 = u_2$.
\end{proof}

Of course, we can only really expect all solutions of an ordinary differential equations to be analytic when the function $F$ is also analytic. In this case, we also have a local existence statement.

\begin{lemma}[Cauchy-Kowalevski]
    Let $k \geq 1$. Suppose $F: D^k \to D$ is analytic, let $t_0 \in \RR$, and consider any $u_0,\dots,u_{k-1} \in D$. Then there exists an open interval $I$ containing $t_0$, and a unique analytic function $u: I \to D$ such that
    %
    \[ \partial_t^k u = F(u,\dots,\partial_t^{k-1} u) \]
    %
    and $\partial_t^i u(t_0) = u_i$ for each $0 \leq i < k$.
\end{lemma}
\begin{proof}
    Without loss of generality, assume $t_0 = 0$, $u_0 = 0$, and $D = \RR^d$. We can also reduce our discussion to the case $k = 1$ by letting $W = (\RR^d)^k$ and considering the function $\tilde{F}: W \to W$ by setting
    %
    \[ \tilde{F}(v_1,\dots,v_k) = (v_2,\dots,v_{k-1},F(v_1,\dots,v_k)) \]
    %
    which is analytic if $F$ is analytic. Thus we need only analyze the equation
    %
    \[ \partial_t u = F(u). \]
    %
    Consider an open subset $W$ of $D$ containing the origin such that there exists a family of constants $\{ c_\alpha \}$ ranging over multi-indices $\alpha$ such that for each $w \in W$,
    %
    \[ F(w) = \sum_\alpha c_\alpha w^\alpha \]
    %
    If we did have an analytic solution $u$ with $u(0) = 0$, then we would find that there exists a polynomial $Q_r$ with non-negative integer coefficients such that
    %
    \[ \partial_t^r u(0) = \partial_t^{r-1} [F \circ u](0) = Q_r(F(0),\dots,\nabla^{r-1} F(0),b_1,\dots,b_{r-1}). \]
    %
    Applying recursion shows that there actually exists a polynomial $R_r$ with non-negative integer coefficients such that
    %
    \[ \partial_t^r u(0) = R_r(F(0),\dots,\nabla^{r-1} F(0)) \]
    %
    In particular, this means that a solution $u$ to the differential equation exists in the sense of formal power series, and it suffices to show the formal power series converges in a neighbourhood of the origin. To do this, we apply the \emph{method of majorants}. We deduce by monotonicity that if we can find an analytic function $G: D \to D$ such that if $|D^\alpha F_i(0)| \leq D^\alpha G_i(0)$ for each $\alpha$ and $i$ (known as the \emph{majorant} of $F$), then we conclude that for any solution $v$ of $G$,
    %
    \[ |\partial_t^r u(0)| \leq |R_r(F(0),\dots,\nabla^{r-1} F(0))| \leq R_r(G(0),\dots,\nabla^{r-1}G(0)) = |\partial_t^r v(0)|. \]
    %
    The idea is that if we can choose an explicit function $G$ and an analytic solution $v$, we can obtain the convergence of the series defining $u$. Since $F$ converges absolutely in a neighbourhood of the origin, we can find a small $\delta > 0$ and $M > 0$ such that for each multi-index $\alpha$ and $i \in \{ 1, \dots, d \}$,
    %
    \[ |D^\alpha F_i(0)| \leq M \cdot |\alpha|! \cdot \delta^{-|\alpha|}. \]
    %
    Thus if we set
    %
    \[ G(w) = \left( \frac{M}{1 - z_1/\delta - \dots - z_d/\delta}, \dots, \frac{CM}{1 - z_1/\delta - \dots - z_d/\delta} \right) \]
    %
    then $G$ majorizes $F$ and has a convergent power series for $|z_i| \lesssim \delta$. But now we note that a solution is given by setting $v = (w,\dots,w)$, where
    %
    \[ w'(t) = \frac{M}{1 - dw/\delta} \]
    %
    a separable equation with solution
    %
    \[ w(t) = (\delta/d)(1 - (1 - 2Mt/\delta)^{1/2}). \]
    %
    This equation is clearly analytic in a neighbourhood of the origin, which completes the proof.
\end{proof}

\begin{remark}
    The local existence statement is necessary. For instance, the initial value problem
    %
    \[ \partial_t u = u^2 \]
    %
    with $u(0) = 1$ has a unique analytic solution of the form
    %
    \[ \frac{1}{1 - t} \]
    %
    which is only defined on the interval $(-\infty,1)$. Thus we have `finite time blowup'.
\end{remark}

It is often interesting to have other existence statements for ordinary differential equations which do not rely so much on the analyticity of the function $F$. Thus we continue analyzing the theory of the first order equation
%
\[ \partial_t u = F(u) \]
%
on an interval $I$ about the origin, subject to the initial conditions $u(0) = u_0$ and under the very weak assumption that $F \in C(\Omega,D)$, where $\Omega$ is a subset of $D$. There are three useful perspectives in differential equations which enable one to see a solution to this partial differential equation:
%
\begin{itemize}
    \item (Classical Solution): $u(0) = u_0$, and for each $t \in I$, $\partial_t u(t) = F(u(t))$.
    \item (Strong Solution): For each $t \in I$,
    %
    \[ u(t) = u_0 + \int_0^t F(u(s))\; ds. \]
    %
    This equation is obtained by applying the Fundamental theorem of calculus to a classical solution.

    \item (Weak Solution): For any $\psi \in C_c^\infty(I)$,
    %
    \[ \int_I u(t) \psi(t)\; dt = u_0 \int_I \psi(t)\; dt + \int_I \left( \int_0^t F(u(s))\; ds \right) \psi(t) dt. \]
    %
    One need only take the equation defining a strong solution and expand to obtain the equation for a weak solution.
\end{itemize}
%
We note that the equation defining a classical solution can be interpreted for any $u \in C^1(I,\Omega)$. On the other hand, the equation defining a strong solution can be interpreted for any $u \in C(I,\Omega)$, and the equation defining a weak solution can be interpreted for any $u \in L^\infty(I,\Omega)$. Fortunately in the case of ordinary differential equations with $F \in C(\Omega,D)$, the class of all such solutions are equivalent, and only serve to provide various different perspectives to the theory.

\begin{lemma}
    Let $F \in C(\Omega,D)$, and $u \in L^\infty(I,\Omega)$ is a weak solution to the equation $\partial_t u = F(u)$ with initial condition $u(0) = u_0$. Then there exists a classical solution $v \in C^1(I,\Omega)$ such that $u(t) = v(t)$ for almost every $t \in I$.
\end{lemma}
\begin{proof}
    Then $F \circ u \in L^\infty(I,D)$, and so the function $f: I \to D$ obtained by setting
    %
    \[ f(t) = u_0 + \int_0^t F(u(s))\; ds \]
    %
    is a Lipschitz continuous function with $f'(t) = F(u(t))$ for almost every $t \in I$. Our assumptions imply that for any $\psi \in C_c^\infty(I)$,
    %
    \[ \int_I f(t) \psi(t)\; dt = \int_I u(t) \psi(t)\; dt. \]
    %
    Thus $f(t) = u(t)$ for almost every $t \in I$. Thus without loss of generality we may assume $u$ is Lipschitz continuous. But then $F \circ u \in C(I,D)$, and so, by the fundamental theorem of calculus, $f \in C^1(I,\Omega)$ with $f'(t) = F(u(t))$ for all $t$. But $u(t) = f(t)$ for almost every $t \in I$, so without loss of generality we may assume $u \in C^1(I,\Omega)$. But then $u'(t) = F(u(t))$ for almost every $t \in I$, and by continuity this implies this is true for \emph{all} $t \in I$.
\end{proof}

\begin{remark}
    We will often find it convenient to work distributionally, i.e. identifying two functions if they agree almost everywhere. Thus we might write the conclusions of this theorem that if $u \in L^\infty(I,\Omega)$ is a weak solution to $\partial_t u = F(u)$, then $u \in C^1(I,\Omega)$ is a classical solution to the equation.
\end{remark}

The classical perspective is useful for obtaining conservation laws, monotonicity formulae, and symmetries. The strong perspective is useful for studying the regularity of solutions. Finally, the weak perspective is useful for using compactness methods, since one is able to take weak limits of equations. Let us now use the strong solution perspective to obtain an existence theorem for ordinary differential equations.

\begin{theorem}[Picard]
    Fix $u_0 \in D$ and $\varepsilon > 0$, and let $\Omega = B_D(u_0,\varepsilon)$. Suppose $F: \Omega \to D$ is a Lipschitz function with
    %
    \[ \| F(x) - F(y) \|_D \leq M \| x - y \|_D \]
    %
    for all $x,y \in \Omega$. Let $A = |F(u_0)|$. Then for $0 < T < 1/(M + A/\varepsilon)$, if we set $I = [-T,T]$, then there exists a unique strong solution $u \in C(I,\Omega)$ to the equation $\partial_t u = F(u)$ with the initial conditions $u(0) = u_0$.
\end{theorem}
\begin{proof}
    Consider the operator $L: C(I,\Omega) \to C(I,D)$ defined by setting
    %
    \[ (Lu)(t) = \int_0^t F(u(s))\; ds. \]
    %
    Let $D = \{ u \in C(I,\Omega) : u(0) = u_0 \}$. Then for any $u,v \in D$, and $t \in I$,
    %
    \begin{align*}
        |(Lu)(t) - (Lv)(t)| &= \left| \int_0^t F(u(s)) - F(v(s)) \right|\\
        &\leq M \int_0^t |u(s) - v(s)|\; ds\\
        &\leq |t| M \| u - v \|_{L^\infty(I,D)}\\
        &\leq T M \| u - v \|_{L^\infty(I,D)}.
    \end{align*}
    %
    Thus
    %
    \[ \| Lu - Lv \|_{L^\infty(I,D)} \leq TM \| u - v \|_{L^\infty(I,D)}. \]
    %
    In particular, noting that $(Lu_0)(t) = t F(u_0)$ for any $t \in I$, this implies that for any $u \in D$,
    %
    \begin{align*}
        \| Lu \|_{L^\infty(I,D)} &= \| Lu - Lu_0 \|_{L^\infty(I,D)} + \| Lu_0 \|_{L^\infty(I,D)}\\
        &\leq TM \| u - u_0 \|_{L^\infty(I,D)} + T |F(u_0)|\\
        &\leq T[M \varepsilon + A] < \varepsilon.
    \end{align*}
    %
    Since $TM < 1$, this means $L$ restricts to a contraction map from $D$ to $D$. Thus the Banach fixed point theorem implies that there exists a unique $u \in C(I,\Omega)$ such that $Lu = u$. But this means precisely that $u$ is a unique strong solution to the equation in question.
\end{proof}

\begin{remark}
    The Picard theorem actually gives a practical way to solve a given PDE. Set $u_0 \in D$ by letting $u_0(t) = u_0$ for all $t \in I$, and then define $u_n = L^n u_0$ for all $n > 0$. If $u$ is the unique strong solution to the differential equation, then
    %
    \[ \| u - u_n \|_{L^\infty(I,D)} \leq (TM)^n \| u - u_0 \|_{L^\infty(I,D)} \leq 2 \varepsilon \cdot (TM)^n. \]
    %
    Thus $u_n$ converges geometrically to $u$, uniformly for $t \in I$.
\end{remark}

We also have a \emph{local regularity result}, which shows that two solutions with close initial conditions remain close to one another over small time changes.

\begin{theorem}
    Fix a nonempty subset $\Omega$ of $D$, let $\varepsilon > 0$, and suppose $F: N_\varepsilon(\Omega) \to D$ is a Lipschitz function with
    %
    \[ |F(x) - F(y)| \leq M |x - y| \]
    %
    for some $M > 0$ and $x,y \in N_\varepsilon(\Omega)$. Let $A = \| F \|_{L^\infty(N_\varepsilon(\Omega))}$, and suppose $0 < T < \min(\varepsilon/A,1/M)$. If $I = [-T,T]$, then we have a map $S: \Omega \to C(I,N_\varepsilon(\Omega))$ such that for each $u_0 \in \Omega$, $Su_0$ is a strong solution to the equation $\partial_t u = F(u)$ with $Su_0(0) = u_0$. Then $S$ is a Lipschitz continuous map, with
    %
    \[ \| Su_0 - Sv_0 \|_{L^\infty(I,N_\varepsilon(\Omega))} \leq \frac{\| u_0 - u_1 \|_D}{1 - TM}. \]
\end{theorem}
\begin{proof}
    Let $u = Su_0$ and $v = Sv_0$. Then
    %
    \begin{align*}
        \| u - v \|_{L^\infty(I,D)} &\leq \| u_0 - v_0 \|_D + \max_{t \in I} \int_0^t \| F(u(s)) - F(v(s)) \|_D\\
        &\leq \| u_0 - v_0 \|_D + MT \| u - v \|_{L^\infty(I,N_\varepsilon(\Omega))}.
    \end{align*}
    %
    Rearranging this equation gives that
    %
    \[ \| u - v \|_{L^\infty(I,D)} \leq \frac{\| u_0 - v_0 \|_D}{1 - MT}. \]
\end{proof}

Note that uniqueness \emph{automatically} follows from this regularity statement. If we assume the function $F$ has more regularity, then the differential equation also has more regularity. PROOF: TODO.

If $F$ is \emph{globally Lipschitz continuous}, then in the above theorem we can set $\varepsilon = \infty$ and thus find a strong solution $u: (-1/M,1/M) \to D$ to the differential equation with $u(0) = u_0$. But now iterating this theorem with different initial conditions and then patching solutions together using the time invariance of the autonomous differential equation, we can find a unique global strong solution $u: \RR \to D$ to the differential equation with $u(0) = u_0$. More generally, if $F$ is \emph{locally Lipschitz continuous}, then for any $u_0$ we can find a \emph{maximal interval} $(T_-,T_+)$ and $u: (T_-,T_+) \to B_D(0,\varepsilon)$ satisfying the differential equation. If $T_+ < \infty$, then
%
\[ \limsup_{t \to T_+} \| u(t) \|_D = \varepsilon. \]
%
Otherwise, the fact that $F$ is locally Lipschitz implies that $u_{T_+} = \lim_{t \to T_+} u(t)$ exists, and one can continue $u$ at $T_+$ since $F$ is Lipschitz in a neighbourhood of $u_{T_+}$. Similarily, if $T_+ < \infty$, then
%
\[ \limsup_{t \to T_+} \| u(t) \|_D = \varepsilon. \]
%
Thus solutions to suitably smooth differential equations can only fail to exist due to a `finite time blowup' (if the differential equation is not suitably smooth, one might also get oscillatory singularities).

To guarantee the global existence of solutions to differential equations, it thus becomes of importance to understand the asymptotic behaviour of ordinary differential equations. In the next section we introduce Gronwall's inequality, which helps one bound quantities which satisfy a `linear differential inequality'. Given nonlinear growth one must rely on continuity methods.

\section{Gronwall's Inequality}

In order to understand the solutions of high dimensional systems of differential equations, it is often useful to find scalar quantities $u(t)$ associated with the system which give the qualitative features of the entire system, such as the size of the solution, or the center of mass, or the energy. Often these quantities are constant over time, satisfy a differential equation themselves, or in the most general form, satisfy a \emph{differential inequality}. One then hopes to use these inequalities to bound the behaviour of the function $u(t)$ in the future. Gronwall's inequality is useful for obtaining bounds for quantities involving \emph{linear} differential inequalities, i.e. an equation of the form
%
\[ u'(t) \leq B(t) u(t). \]
%
We prefer to work in integral form.

\begin{theorem}
    Let $u: [0,T] \to [0,\infty)$ be continuous, such that for all $0 \leq t \leq T$,
    %
    \[ u(t) \leq A + \int_0^t B(s) u(s)\; ds. \]
    %
    where $A \geq 0$ and $B: I \to [0,\infty)$ is continuous. Then
    %
    \[ u(t) \leq A \exp \left( \int_0^t B(s)|; ds \right). \]
\end{theorem}
\begin{proof}
    Without loss of generality assume $A > 0$. Now
    %
    \[ \frac{d}{dt} \left( A + \int_0^t B(s) u(s)\; ds \right) = B(t)u(t) \leq B(t) \left( A + \int_0^t B(s) u(s) B(t)\; dt \right). \]
    %
    Thus
    %
    \[ \frac{d}{dt} \log \left( A + \int_0^t B(s)u(s)\; ds \right) \leq B(t). \]
    %
    But now integrating this inequality gives
    %
    \[ \log \left( A + \int_0^t B(s)u(s)\; ds \right) \leq \log(A) + \int_0^t B(s)\; ds. \]
    %
    It now suffices to take exponentials of both sides.
\end{proof}

If we can differentiate $u$, then we can also allow $B$ to be negative.

\begin{theorem}
    Suppose $u: [0,T] \to [0,\infty)$ is absolutely continuous and $\partial_t u(t) \leq B(t) u(t)$ for almost every $t \in [0,T]$, where $B: [0,T] \to \RR$ is continuous. Then
    %
    \[ u(t) \leq u(0) \exp \left( \int_0^t B(s)\; ds \right). \]
\end{theorem}
\begin{proof}
    Set
    %
    \[ v(t) = u(t) \exp \left( - \int_0^t B(s)\; ds \right). \]
    %
    Then $v$ is absolutely continuous, and for almost every $t \in [0,T]$,
    %
    \[ v'(t) = [u'(t) - u(t)B(t)] \exp \left( - \int_0^t B(s)\; ds \right) \leq 0. \]
    %
    Thus $v$ is a decreasing function, which completes the proof.
\end{proof}

\section{Continuity Methods}

Most nonlinear differentiable equations are not explicitly solvable. In place of solving these equations, one must instead rely on the qualitative behaviour of these equations, or quantitative asymptotics. Since one does not have an explicit formula for solutions $u(t)$, one must instead rely on integral formulae. For instance, that
%
\[ u(t) = u_0 + \int_0^t F(u(s))\; ds. \]
%
Thus if we have some control on $u$, we might use integration to \emph{improve our control}. This is the bootstrapping method of understanding partial differential equations.

\begin{theorem}[Bootstrapping]
    Let $I$ be an interval, and for each $t \in I$ consider a hypothesis $H(t)$ and a conclusion $C(t)$. Suppose
    %
    \begin{itemize}
        \item If $H(t)$ is true, then $C(t)$ is true.
        \item If $C(t)$ is true, there is $\varepsilon_t$ such that $H(s)$ is true for all $|s - t| \leq \varepsilon_t$.
        \item If $C(t_k)$ is true for all $t_k$ in a sequence of times $\{ t_k \}$, and $t_k \to t$, then $C(t)$ is true.
        \item $H(t_0)$ is true for some $t_0$.
    \end{itemize}
    %
    Then $C(t)$ is true for all $t \in I$.
\end{theorem}

The first principle is often the most difficult assumption to obtain; the latter three often follow easily. Let us consider a simple example.

\begin{theorem}
    Let $H$ be a finite dimensional Hilbert space, and let $V \in C^2(H)$ be such that $V(0) = 0$, $\nabla V(0) = 0$, and $\nabla^2 V(0)$ is strictly positive definite. Then there exists $\delta > 0$ such that for each $\| u_0 \|_H, \| u_0' \|_H \leq \delta$, there exists a unique global solution $u \in C^2(\RR,H)$ such that
    %
    \[ \partial_t^2 u = - \nabla V(u) \]
    %
    and with $u(0) = u_0$ and $\partial_t u(0) = u_0'$. Moreover, $u \in L^\infty(\RR,H)$.
\end{theorem}
\begin{proof}
    By Picard's theorem, there exists a maximal interval $(T_-, T_+)$ and a function $u \in C^2((T_-,T_+),H)$. solving the equation. Define
    %
    \[ E(t) = \frac{\| \partial_t u(t) \|_H^2}{2} + V(u(t)). \]
    %
    Then
    %
    \[ \partial_t E(t) = \langle \partial_t u(t), \partial_t^2 u(t) \rangle + \langle \partial_t u(t), \nabla V(u(t)) \rangle = 0. \]
    %
    Thus $E$ is a constant function on $(T_-,T_+)$, i.e for all $t \in (T_-,T_+)$,
    %
    \[ E(t) = \frac{\| u_0' \|_H^2}{2} + V(u_0). \]
    %
    In particular, for any $\varepsilon > 0$, we can pick $\delta > 0$ in the hypothesis such that $|E(t)| \leq c \varepsilon$ for $c > 0$ to be chosen later. Now let $H(t)$ be true if
    %
    \[ \sqrt{\| u(t) \|_H^2 + \| \partial_t u(t) \|_H^2} \leq 2 \varepsilon, \]
    %
    and let $C(t)$ be true if
    %
    \[ \sqrt{\| u(t) \|_H^2 + \| \partial_t u(t) \|_H^2} \leq \varepsilon. \]
    %
    Since $H(0)$ is true, to bootstrap it suffices to show that $H(t)$ implies $C(t)$ for all $t \in (T_-,T_+)$, from which case we will have shown finite time blowup cannot occur. But if $H(t)$ holds, performing a Taylor expansion for $V$ around the origin and noticing positive definitiveness shows that
    %
    \[ V(u(t)) \gtrsim \| u(t) \|_H^2 - O(\varepsilon^3) \]
    %
    But this means that
    %
    \[ \| \partial_t u(t) \|_H^2 + \| u(t) \|_H^2 \lesssim \frac{\| \partial_t u(t) \|_H^2}{2} + V(u(t)) + O(\varepsilon^3) \leq c \varepsilon + O(\varepsilon^3). \]
    %
    If $c$ is chosen to be appropriately small relative to $\varepsilon$, then $C(t)$ is satisfied.
\end{proof}






























\chapter{The Schr\"{o}dinger Equation}

In this chapter, we study the \emph{Free Schr\"{o}dinger Equation}
%
\[ \partial_t u = (\hbar / 2m) i \Delta u = 0, \]
%
where $u: \RR \times \RR^d \to \mathbf{C}$ describes the wave function of a particule under no potential, where $\hslash > 0$ is \emph{Planck's constant}, and $m > 0$ is the mass of the particle. The Schr\"{o}dinger equation is an important example of a \emph{dispersive equation}, which preserve the `mass' of a function, but tend to distribute the solution evenly about physical space rather than to concentrate in a small region.

If $u$ lies in $C^1(\RR,\mathcal{S}(\RR^d))$ and $u_0 = f$, then taking the spatial Fourier transform, solving the resultant ODE, and then taking the inverse Fourier transform gives the representation formula
%
\[ u_t(x) = (S_t f)(x) = \int e^{2 \pi i (\xi \cdot x - (\hslash \pi / m) |\xi|^2 t)} \widehat{f}(\xi)\; d\xi. \]
%
Unless one is studying the asymptotic behaviour of solutions to the equation as $\hslash \to 0$ or $m \to 0$, it is useful to normalize $\hslash \pi / m = 1$, so that the integral formula is simplest. In other words, it is notationally simplest to study the equation
%
\[ \partial_t u = (i/2\pi) \Delta u, \]
%
where
%
\[ (S_t f)(x) = \int e^{2 \pi i (\xi \cdot x - |\xi|^2 t)} \widehat{f}(\xi)\; d\xi. \]
%
This is the \emph{Schr\"{o}dinger evolution operator}.

We immediate see from the formula defining $S_t$ that the operator is self adjoint, and we can extend the definition of $S_t$ to a widen range of functions distributionally (or by considering it in the general scheme of Fourier multipliers). Thus for each $s \geq 0$, $\| S_t f \|_{H^s(\RR^d)} = \| f \|_{H^s(\RR^d)}$. Another useful fact is the symmetry of the equation under \emph{parabolic rescaling}, i.e.
%
\[ S_t(\text{Dil}_\lambda f) = \text{Dil}_\lambda (S_{\lambda^2 t} f). \]
%
We also have translation invariance, i.e.
%
\[ S_t(\text{Trans}_{x_0} f) = \text{Trans}_{x_0} S_t f \]
%
and a \emph{modulation symmetry}
%
\[ S_t(\text{Mod}_{\xi_0} f)(x) = e^{-2 \pi i |\xi_0|^2 t} \text{Mod}_{\xi_0} \text{Trans}_{-2t \xi_0} (S_t f)(x). \]
%
Thus if we modulate the frequency of our initial conditions by $\xi_0$, then the group velocity of the solution to the Schr\"{o}dinger equation is affected by $-2\xi_0$, the modulation is preserved, and we also have an additional modulation in time with frequency $-|\xi_0|^2$. Finally, we note that we can also view $S_t$ as a convolution operator by the kernel
%
\[ K_t(x) = e^{- i \pi \text{sgn}(t) d/4} (2|t|)^{-d/2} e^{i \pi |x|^2/2t}, \]
%
the \emph{Schr\"{o}dinger kernel}. Thus $S_t f = K_t * f$. For simplicity, we write
%
\[ K_t(x) = c_d |t|^{-d/2} e^{i \pi |x|^2/2t}, \]
%
where we think of $c_d$ as a constant, whereas actually $c_d$ actually depends on $\text{sgn}(t)$.

The fact that $K_t$ is a Gaussian means we can compute $S_t f$ exactly if $f$ is a Gaussian. More precisely, if $f(x) = \delta^{-d/2} e^{- \pi |x|^2/\delta}$, then
%
\begin{align*}
    S_tf(x) &= c_d (\delta - 2ti)^{-d/2} e^{- \pi |x|^2 / (\delta - 2ti)}\\
    &= c_d (\delta - 2ti)^{-d/2} e^{-\pi |x|^2 /(\delta + 4t^2 / \delta)} e^{- 2\pi i (|x|^2 / (\delta^2/t + 4t))}
\end{align*}

Let us analyze the \emph{dispersive nature} of the Schr\"{o}dinger equation further from some basic estimates. The fact that $|K_t(x)| = c_d |t|^{-d/2}$, together with Young's inequality, implies that
%
\[ \| S_t f \|_{L^\infty(\RR^d)} = \| K_t * f \|_{L^\infty(\RR^d)} \leq \| K_t \|_{L^\infty(\RR^d)} \| f \|_{L^1(\RR^d)} \lesssim_d |t|^{-d/2} \| f \|_{L^1(\RR^d)}. \]
%
Thus if $f$ is integrable, then for any $t \neq 0$, $S_t f$ is bounded, and as $|t| \to \infty$, the height of the function $S_t f$ decays. One can interpolate between this bound and the bound $\| S_t f \|_{L^2(\RR^d)} = \| f \|_{L^2(\RR^d)}$ that we have similar decay from $L^p(\RR^d)$ to $L^{\smash{p^*}}(\RR^d)$ for any $1 \leq p < 2$. Thus the `height' of the function, in a rough sense, decays as $|t| \to \infty$. This is the \emph{dispersive nature} of the Schr\"{o}dinger equation; we have some control over the `width' of a function, often $L^2$ bounds, but the height decays over time.

Fourier extension theory gives up another manifestation of dispersion, in the form of \emph{Strichartz estimates}. The Tomas-Stein theorem immediately gives the following result.

\begin{theorem}[Strichartz Estimate]
    For any $f \in L^2(\RR^d)$, if $p = 2(d+2)/d$, then
    %
    \[ \left( \int_{-\infty}^\infty \int_{\RR^d} |(S_t f)(x)|^{p}\; dx\; dt \right)^{1/p} \lesssim \| f \|_{L^2(\RR^d)}. \]
\end{theorem}

Let us consider the dispersive nature of this theorem. If $f \in L^2(\RR^d)$, then $S_t f \in L^p(\RR^d)$ for \emph{almost all} $t$. Since $p > 2$, this means that the `height' of $S_t f$ is controlled to a greater qualitative extent for almost all $t$ than by the $L^2$ nature of $f$. moreover, the Strichartz estimate means that $\| S_t f \|_{L^p(\RR^d)}$ has some `decay' in a fuzzy, averaged sense. One way to exploit this decay more precisely is via the uncertainty principle. If $\| f \|_{L^2(\RR^d)} = 1$ and $\widehat{f}$ is concentrated in a neighbourhood of radius $O(R)$ of some $\xi \in \RR^d$ with $|\xi| = R$, then the same is true for $\widehat{S_t f}$ since $S_t f$ is a Fourier multiplier. The uncertainty principle implies that the majority of the mass of $S_t f$ cannot be concentrated on a region of radius smaller than $\Omega(1/R)$. Since $\| S_t f \|_{L^2(\RR^d)} = 1$, the solution has `height' at most $R^{d/2}$ on this concentrated region. But the Strichartz estimate shows that this can only happen on a set of times with `width' at most $O(1/R^2)$. Outside these times, we expect the solution to have height much less than $R^{d/2}$.

It is only slightly more difficult to obtain \emph{mixed norm} Strichartz estimates for solutions to the \emph{non-homogeneous} Schr\"{o}dinger equation
%
\[ \partial_t u - (i/2\pi) \Delta u = v. \]
%
If $u_0 = f$, where $f \in \mathcal{S}(\RR^d)$, $u \in C^1(\RR,\mathcal{S}(\RR^d))$ and $v \in C(\RR, \mathcal{S}(\RR^d))$. Then we can use Duhamel's formula, which gives the representation formula
%
\[ u_t = S_t f + \int_0^t S_{t-s} v_s\; ds. \]
%
To obtain mixed norm bounds on $u$, it will be useful to define a pair of exponents $(p_1,p_2)$ to be \emph{Strichartz admissable} if $2/p_1 + d/p_2 = d/2$. A bound of the form
%
\[ \| u \|_{L^{p_1}_t L^{p_2}_x} \lesssim \| f \|_{L^2_x} + \| v \|_{L^{q_1'}_t L^{q_2'}_x} \]
%
can \emph{only} hold when $(p_1,p_2)$ and $(q_1,q_2)$ are Strichartz admissable pairs, as can be seen by parabolically rescaling solutions to the equation.


\begin{theorem}
  If $2 < p_1,q_1 < \infty$, and $(p_1,p_2)$ and $(q_1,q_2)$ are Strichartz admissable pairs, then
  %
  \[ \smash{\| u \|_{\smash{L^{p_1}_t L^{p_2}_x}} \lesssim \| f \|_{L^2(\RR^{d-1})} + \| v \|_{\smash{L^{q_1'}_t L^{q_2'}_x}}.} \]
\end{theorem}

\begin{proof}
  It suffices by linearity to prove the theorem first when $f = 0$, then when $v = 0$. If $v = 0$, then it suffices to prove that $\| u \|_{\smash{L^{p_1}_t L^{p_2}_x}} \lesssim \| f \|_{\smash{L^2(\RR^{d-1})}}$. To prove this, we interpolate the dispersion bounds we obtained from the represention of $S_t$ to conclude that
  %
  \[ \| K_t * f \|_{\smash{L^p_x}} \lesssim |t|^{-(d/2)(1 - 1/p)} \| f \|_{\smash{L^{p'}_x}} \]
  %
  for $2 \leq p \leq \infty$. To prove the result, we rely on a $TT^*$ argument. We write $Sf(x,t) = S_tf(x)$. We calculate then that
  %
  \[ S^*g(x) = \int_{-\infty}^\infty (K_{-t} * g_t)(x)\; dt, \]
  %
  where $g_t(x) = g(t,x)$. Using the fact that $\{ K_t \}$ is a semigroup, we conclude that
  %
  \[ SS^*g(x,t) = (K_t * S^*g)(x) = \int (K_t * (K_{-s} * g_s))(x)\; ds = \int (K_{t-s} * g_s)(x)\; ds. \]
  %
  It suffices to prove that $\| SS^* g \|_{\smash{L^{p_1}_t L^{p_2}_x}} \lesssim \| g \|_{\smash{L^{p_1'}_t L^{p_2'}_x}}$.  Minkowski's inequality implies that
  %
  \begin{align*}
    \| SS^* g \|_{L^{p_2}_x} &\leq \left( \int \left| \int (K_{t-s} * g_s)(x)\; ds \right|^{p_2}\; dx \right)^{1/p_2}\\
    &\leq \int \int_{-\infty}^\infty \| K_{t-s} * g_s \|_{L^{p_2}_x}\; ds\\
    &\lesssim \int_{-\infty}^\infty |t - s|^{-(d/2)(1 - 1/p_2)} \| g_s \|_{L^{p_2'}_x}\; ds.
  \end{align*}
  %
  Applying the Hardy-Littlewood-Sobolev inequality, we conclude that
  %
  \[ \| SS^* g \|_{\smash{L^{p_1}_t L^{p_2}_x}} \lesssim \| g \|_{\smash{L^{p_1'}_t L^{p_2'}_x}}, \]
  %
  which completes the proof.

  Now we consider the case $f = 0$. Then
  %
  \[ u_t = \int_0^t K_{t-s} v_s\; ds, \]
  %
  and our goal is to show that $\| u \|_{\smash{L^{p_1}_t L^{p_2}_x}} \lesssim \| v \|_{\smash{L^{q_1'}_t L^{q_2'}_x}}$. Minkowski's inequality implies that
  %
  \[ \| u(t) \|_{\smash{L^{p_2}_x}} \leq \int_{-\infty}^\infty \chi_{[0,t]}(s) \| K_{t-s} v_s \|_{\smash{L^{p_2}_x}}\; ds \leq \int_{-\infty}^\infty \| K_{t-s} v_s \|_{\smash{L^{p_2}_x}}\; ds. \]
  %
  Thus
  %
  \[ \| u(t) \|_{L^{p_1}_t L^{p_2}_x} \leq \int_{-\infty}^\infty \| K_{t-s} v_s \|_{L^{p_1}_t L^{p_2}_x}\; ds. \]
  %
  In the last paragraph we proved that
  %
  \[ \| K_{t-s} v_s \|_{L^{p_1}_t L^{p_2}_x} \lesssim \| v_s \|_{L^2_x}, \]
  %
  and so
  %
  \[ \| u \|_{L^{p_1}_t L^{p_2}_x} \lesssim \int_{-\infty}^\infty \| v_s \|_{L^2_x}\; ds = \| v \|_{L^1_t L^2_x}. \]
  %
  But now we can interpolate between the bound
  %
  \[ \| u \|_{L^{p_1}_t L^{p_2}_x} \lesssim \| v \|_{L^{p_1'}_t L^{p_2'}_x}, \]
  %
  which was essentially proved in the last paragraph, to complete the proof.
\end{proof}

\section{Nonlinear Schr\"{o}dinger}

Let us use the Strichartz estimates we have developed to obtain estimates for the nonlinear Schr\"{o}dinger equation
%
\[ \partial_t u = (i/2\pi)(\Delta u + \lambda |u|^{4/d} u). \]
%
The parameter $\lambda$ gives a measure of the `strength' of the nonlinearity of the equation. A useful way to analyze the existence and uniqueness problem for this equation is to apply Duhamel's formula, viewing $(i/2\pi) \lambda |u|^{4/d} u$ as a forcing term and assuming regularity of $u$ to conclude that if $u_0 = f$ lies in $\mathcal{S}(\RR^d)$, then
%
\[ u_t = S_t f + (i\lambda/2\pi) \int_0^t S_{t-s}(|u_s|^{4/d} u_s)\; ds. \]
%
A \emph{weak} solution to the nonlinear Schr\"{o}dinger is a function satisfying this equation. More precisely, we work with weak solutions $u$ in the Banach space
%
\[ D = L^{p_0}(\RR, L^{2p_0}(\RR^d)) \cap L^\infty(\RR, L^2(\RR^d)), \]
%
where $p_0 = 1 + 4/d$. The reason for this exponent is that if $u \in D$, then $|u|^{4/d} u \in L^1(\RR,L^2(\RR^d))$, which means that $|u_s|^{4/d} u_s \in L^2(\RR^d)$ for almost every $s$. This means that $S_{t-s}(|u_s|^{4/d} u_s) \in L^2(\RR^d)$ for almost every $s$, and moreover
%
\begin{align*}
    \left\| (i\lambda/2\pi) \int_0^t S_{t-s}(|u_s|^{4/d} u_s)\; ds \right\|_{L^\infty_t L^2_x} &\lesssim \left\| S_{t-s}(|u_s|^{4/d} u_s) \right\|_{L^1_t L^2_x}\\
    &= \left\| |u|^{4/d} u \right\|_{L^1_t L^2_x}\\
    &= \left\| u \right\|_{L^1_t L^{2p_0}_x}^{p_0}\\
    &\leq \| u \|_{L^{p_0}_t L^{2p_0}_x}^{p_0} < \infty.
\end{align*}
%
Similarily, we calculate that $\| S_t f \|_{L^\infty_t L^2_x} = \| f \|_{L^2(\RR^d)}$. Thus, if for $u \in D$ we set
%
\[ Au = S_t f + (i\lambda/2\pi) \int_0^t S_{t-s}(|u_s|^{4/d} u_s)\; ds, \]
%
then $Au \in L^{p_0}(\RR,L^{2p_0}(\RR^d))$. Since $D \subset L^{p_0}(\RR,L^{2p_0}(\RR^d))$, we can formally say $u$ is a weak solution to the nonlinear Schr\"{o}dinger equation if $Au = u$.

\begin{lemma}
    There exists $\varepsilon$ depending on $\lambda$ and $d$ such that if $\| f \|_{L^2(\RR^d)} \leq \varepsilon$, then there exists a unique element $u$ of $D$ which is a weak solution to the nonlinear Schr\"{o}dinger equation with initial condition $f$.
\end{lemma}
\begin{proof}
  Let us for simplicity assume that $d = 2$, so $1 + 4/d = 3$. Given $\phi,\psi \in D$ solving the equation, we conclude that
  %
  \[ \phi_t - \psi_t = \lambda \int_0^t e^{2 \pi i \Delta (t - s)} \left( |\phi_s|^2 \phi_s - |\psi_s|^2 \psi_s \right)\; ds. \]
  %
  Fix an interval $I = [0,T]$. Our Strichartz estimates for the nonhomogeneous Schr\"{o}dinger equation implies that
  %
  \[ \| \phi - \psi \|_{L^3_t(I) L^6_x} \lesssim |\lambda| \| |\phi|^2 \phi - |\psi|^2 \psi\|_{L^1_t(I) L^2_x}. \]
  %
  Now
  %
  \[ |\phi|^2 \phi - |\psi|^2 \psi = \overline{\psi - \phi} \phi^2 + \overline{\psi} (\phi + \psi)(\phi - \psi). \]
  %
  Thus we have a pointwise bound
  %
  \[ ||\phi|^2 \phi - |\psi|^2 \psi| \lesssim |\psi - \phi| (|\psi|^2 + |\phi|^2) \]
  %
  H\"{o}lder's inequality thus implies that
  %
  \[ \| |\phi|^2 \phi - |\psi|^2 \psi\|_{L^1_t(I) L^2_x} \lesssim \| \psi - \phi \|_{L^3_t(I) L^6_x} \left( \| \psi \|_{L^3_t(I) L^6_x}^2 + \| \phi \|_{L^3_t(I) L^6_x}^2 \right). \]
  %
  Putting this together with the calculation at the beginning of the paragraph, we conclude that if $\psi \neq \phi$ on $I$, then
  %
  \[ 1 \lesssim |\lambda| \left( \| \psi \|_{L^3_t(I) L^6_x}^2 + \| \phi \|_{L^3_t(I) L^6_x}^2 \right). \]
  %
  As $T \to 0$, $\| \psi \|_{L^3_t(I) L^6_x}, \| \phi \|_{L^3_t(I) L^6_x} \to 0$. Thus if $T$ is suitably small, this equation is impossible, which implies that $\psi = \phi$ on $I$. Applying this technique repeatedly glives global uniqueness.

  To prove existence, we apply a contraction argument. Consider the operator
  %
  \[ (A \psi)(x,t) = e^{2 \pi i \Delta t} f + \lambda \int_0^t e^{2 \pi i \Delta (t - s)} ( |\psi_s|^2 \psi_s )\; ds. \]
  %
  Then Strichartz estimates implies that there exists a constant $C > 0$ such that
  %
  \[ \| A \psi - A \phi \|_{L^3_t L^6_x} \leq C |\lambda| \| \psi - \phi \|_{L^3_t L^6_x}^3 \left( \| \psi \|_{L^3_t L^6_x}^2 + \| \phi \|_{L^3_t L^6_x}^2 \right). \]
  %
  Fix $R > 0$. Then if $\| \psi \|_{L^3_t L^6_x}, \| \phi \|_{L^3_t L^6_x} \leq R$, then
  %
  \[ |\lambda| \| \psi - \phi \|_{L^3_t L^6_x}^3 \left( \| \psi \|_{L^3_t L^6_x}^2 + \| \phi \|_{L^3_t L^6_x}^2 \right) \leq 8|\lambda| R^4 \| \psi - \phi \|_{L^3_t L^6_x}. \]
  %
  Thus if $C |\lambda| R^4 < 1$, we get a contraction map on $\overline{B_R}$, which implies there exists a unique element $\psi$ of $\overline{B_R}$ such that $A\psi = \psi$. But this means $\psi$ is a weak solution to the equation. TODO COMPLETE THIS ARGUMENT WHY IS IT ONLY $A\psi$ THAT NEEDS TO BE BOUNDED IN SERGEY'S NOTES?
\end{proof}








\begin{thebibliography}{9}

\bibitem{evans}
Lawrence C. Evans
\textit{Partial Differential Equations}

\end{thebibliography}

 \end{document}