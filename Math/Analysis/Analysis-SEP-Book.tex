\documentclass[answers]{exam}

\usepackage[utf8]{inputenc}

\usepackage{comment}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage[shortlabels]{enumitem}
\usepackage{esint}
\usepackage{mathabx}
\usepackage{tikz}

\DeclareMathOperator{\CC}{\mathbb{C}}
\DeclareMathOperator{\RR}{\mathbb{R}}
\DeclareMathOperator{\ZZ}{\mathbb{Z}}
\DeclareMathOperator{\TT}{\mathbb{T}}
\DeclareMathOperator{\PP}{\mathbb{P}}
\DeclareMathOperator{\NN}{\mathbb{N}}
\DeclareMathOperator{\EE}{\mathbb{E}}

% Define a custom "problem" environment
\newtheoremstyle{problemstyle}  % <name>
        {3pt}                                               % <space above>
        {3pt}                                               % <space below>
        {\normalfont}                               % <body font>
        {}                                                  % <indent amount}
        {\bfseries\itshape}                 % <theorem head font>
        {\normalfont\bfseries:}         % <punctuation after theorem head>
        {.5em}                                          % <space after theorem head>
        {}                                                  % <theorem head spec (can be left empty, meaning `normal')>
\theoremstyle{problemstyle}
\newtheorem{problem}{Problem}%[section] % Comment out [section] to remove section number dependence


%Custom Math Commands
\newtheorem{theorem}{Theorem}
\newcommand{\vt}{\vskip 5mm} % vertical space
\newcommand{\fl}{\noindent\textbf} % first line
\newcommand{\Fl}{\vt\noindent\textbf} % first line with space above
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % norm
\newcommand{\pnorm}[1]{\left\lVert#1\right\rVert_p} % p-norm
\newcommand{\qnorm}[1]{\left\lVert#1\right\rVert_q} % q-norm
\newcommand{\1}[1]{\textbf{1}_{\left[#1\right]}} % indicator function
\def\limn{\lim_{n\to\infty}} % shortcut for lim as n-> infinity
\def\sumn{\sum_{n=1}^{\infty}} % shortcut for sum from n=1 to infinity
\def\sumkn{\sum_{k=1}^{n}} % shortcut for sum from k=1 to n
\def\sumin{\sum_{i=1}^{n}} % shortcut for sum from i=1 to n
\def\SAs{\sigma\text{-algebras}} % shortcut for $\sigma$-algebras
\def\SA{\sigma\text{-algebra}} % shortcut for $\sigma$-algebra
\def\Ft{\mathcal{F}_t} % time-indexed sigma-algebra (t)
\def\Fs{\mathcal{F}_s} % time-indexed sigma-algebra (s)
\def\F{\mathcal{F}} % sigma-algebra
\def\G{\mathcal{G}} % sigma-algebra
\def\R{\mathbb{R}} % Real numbers
\def\Z{\mathbb{Z}} % Integers
\def\E{\mathbb{E}} % Expectation
\def\P{\mathbb{P}} % Probability
\def\Q{\mathbb{Q}} % Q probability
\def\dist{\text{dist}} %Text 'dist' for things like 'dist(x,y)'

% Brackets and Parentheses
%\def\[{\left [}
%\def\]{\right ]}
\def\({\left (}
\def\){\right )}

\usepackage{titling}


% Colorful Notes
\usepackage{color}
\definecolor{Red}{rgb}{1,0,0}
\definecolor{Blue}{rgb}{0,0,1}
\def\red{\color{Red}}
\def\blue{\color{Blue}}
\newcommand{\rnote}[1]{\red#1} % \rnote{foo} then 'foo' is red
\newcommand{\bnote}[1]{{\blue#1}} % \bnote{foo} then 'foo' is blue



\pagestyle{headandfoot}
\runningheadrule
\runningheader{Analysis SEP}{}{}
\firstpagefooter{}{}{}
\runningfooter{}{}{} %Page \thepage}{}




\title{Analysis SEP Problems \& Solutions}
\date{July-August 2021\\\text{and}\\July-August 2022\\\text{and}\\July-August 2025}
\author{Max Bacharach and Jacob Denson}

\begin{document}



\maketitle

\tableofcontents

\newpage

\section{Basic Analysis Notes}

Let's begin by reviewing some fundamental techniques in analysis. These include techniques from inequalities, calculus, sequences and series, and integration techniques. In this section we give a brief summary of the techniques that we feel can often be applied to the basic analysis questions that occur on qualifying exams.

\begin{itemize}
	\item The use of asymptotic notation can often simplify calculations: in these notes we use both Vinogradov and Bachmann-Landau notation. For some quantities $x$ and $y$ depending on some parameters, we write $x \lesssim y$, or $x = O(y)$, if there exists a constant $c > 0$ so that $x \leq c y$ holds. If the constant $c$ depends on some parameter $z$, we write $x \lesssim_z y$ or $x = O_z(y)$. We also write $x \sim y$ if $x \lesssim y$ and $y \lesssim x$.

	\item Swapping Limits with it's Limit Inferior and Superior: To show a sequence $\{ c_n \}$ converges to $c$ as $n \to \infty$, it suffices to show
	%
	\[ \limsup_{n \to \infty} c_n \leq c \quad\text{and}\quad \liminf_{n \to \infty} c_n \geq c. \]
	%
	This strategy is closely related to the method of ``giving yourself a $\varepsilon$ of room''. To show an inequality $a \leq b$, it suffices to show that $a \leq b + \varepsilon$ is true for any $\varepsilon > 0$. And to show an identity $a = b$, it suffices to show $a \leq b + \varepsilon$ and $b \leq a + \varepsilon$ for any $\varepsilon > 0$.

	\item A \emph{dyadic decomposition} is often useful to obtain rough bounds for quantities, i.e. breaking up regions of integration and regions of summation which have total width given by a power of two. The exponential increase in the size of these regions means we can often apply rather crude estimates for the behaviour of a sum or integral on these regions in order to obtain a bound on the overall sum.

	\item The \emph{Stone-Weirstrass theorem} tells us that in $\RR^n$, the family of multi-variate polynomials on $\RR^n$ form a dense subspace of $C(K)$, for any compact set $K \subset \RR^n$, where $C(K)$ is the Banach space given by the $L^\infty$ norm. It follows that this class is also dense in most other function spaces encountered in analysis, e.g. for the spaces $L^p(\RR^n)$ for $1 \leq p < \infty$.

	\item Inequalities involving the trigonometric functions often come up on the qualifying exam, and are useful to remember. For instance, the inequality $x/2 \leq \sin x \leq x$ holds for $x \in [0,\pi/2]$, and the inequality $x - x^2/2 \leq \sin x \leq x$ holds for all $x \in \RR$. Similarily, $1 - x^2/2 \leq \cos x \leq 1 - x^2/6$ for all $x \in \RR$.

	\item The \emph{mean value theorem} implies that for $a < b$, and for $f \in C^1[a,b]$, there exists $c \in [a,b]$ such that
	%
	\[ f(c) = f(a) + b f'(c). \]
	%
	More generally, in $\RR^d$ we have \emph{Taylor's formula}, which allows us to write a function $f \in C^{k+1}(\RR^n)$ as
	%
	\[ f(x) = \sum\nolimits_{|\alpha| \leq k} D^\alpha f(x_0) (x - x_0)^\alpha + \sum\nolimits_{|\alpha| = k+1} R_\alpha(x) (x - x_0)^\alpha, \]
	%
	where $\alpha$ ranges over multi-indices, and
	%
	\[ R_\alpha(x) = \frac{|\alpha|}{\alpha!} \int_0^1 (1 - t)^{|\alpha|-1} (D^\alpha f)(x_0 + t(x - x_0)) = o(|x - x_0|^{k+1}). \]

	\item The Cauchy-Schwarz inequality implies that
	%
	\[ \left| \sum a_n b_n \right| \leq \left( \sum |a_n|^2 \right)^{1/2} \left( |b_n|^2 \right)^{1/2} \]
	%
	and more generally, we have \emph{H\"{o}lder's inequality}
	%
	\[ | \sum a_n b_n | \leq \left( \sum |a_n|^p \right)^{1/p} \left( |b_n|^q \right)^{1/q} \]
	%
	where $1 \leq p,q \leq \infty$ and $1/p + 1/q = 1$. The same inequalities also hold when we swap sums with integrals.

	\item The \emph{Arzela-Ascoli theorem}, which says that a set $K$ of functions in $C[0,1]$ is compact in the $L^\infty$ norm if $K$ is \emph{uniformly bounded}, i.e. there is $M > 0$ such that $\| f \|_{L^\infty} \leq M$ for all $f \in K$, and \emph{uniformly equicontinuous}, i.e. for any $\varepsilon > 0$, there is $\delta > 0$ such that if $|x - y| \leq \delta$ then for any $f \in K$, $|f(x) - f(y)| \leq \varepsilon$.
\end{itemize}

\subsection*{Sequences, Series, and Integrals}

There are many questions on analysis qualifying exams asking to determine whether a given infinite series
%
\[ \sum_{n = 1}^\infty a_n \]
%
converges. Depending on the sequence, one of various techniques may apply:
%
\begin{itemize}
	\item \emph{Breaking a sum into layers}: If $a_n$ is a non-negative integer for each $n$, then
	%
	\[ \sum\nolimits_n a_n = \sum\nolimits_{k \geq 1} \# \{ n : a_n = k \} \cdot k. \]
	%
	More generally, for any positive sequence $\{ a_n \}$ we may apply a dyadic decomposition, so that the convergence of $\sum a_n$ is equivalent to the convergence of
	%
	\[ \sum\nolimits_{k \in \ZZ} \# \{ n : 2^k \leq a_n < 2^{k+1} \} \cdot 2^k. \]
	%
	Similarily, for $a: X \to [0,\infty)$ on a measure space $X$, the integral $\int_X a(x)\; dx$ is finite if and only if
	%
	\[ \sum\nolimits_{k \in \ZZ} |\{ x \in X: 2^k \leq a(x) < 2^{k+1} \}| \cdot 2^k < \infty. \]

	\item If the sequence $\{ a_n \}$ is positive and non-increasing, then one can apply \emph{Cauchy's condensation theorem}, which says that the convergence of the series is equivalent to convergence of the sequence
	%
	\[ \sum_{k = 1}^\infty 2^k \cdot a_{2^k}. \]
	%
	This is often useful if the sequence $\{ a_n \}$ grows somewhat logarithmically, since logarithmic growth in $\{ a_n \}$ will be turned into linear growth in the sequence $\{ a_{2^k} \}$.

	\item One can often convert sums into integrals, and vice versa. If $a_n = a(n)$ for some function $a: [1,\infty) \to \RR$, then it is often true that the convergence of $\sum a_n$ is equivalent to the convergence of the integral
	%
	\[ \int_1^\infty a(x)\; dx. \]
	%
	This is true, for instance, if $\{ a_n \}$ and the function $a$ are both positive and non-increasing,  or if $\sum_{k = 1}^\infty |\Delta a(k)| < \infty$, where $\Delta a(k) = \int_0^1 |a(k+x) - a(k)|\; dx$.

	\item If a function $f$ is smooth (the derivative of $f$ is well behaved), and a function $g$ is oscillating very fast, then one can often control an integral via an integration by parts, e.g. writing
	%
	\[ \int_a^b f(x) g(x)\; dx = f(b) G(b) - \int_a^b f'(x) G(x)\; dx \]
	%
	where $G(x) = \int_a^x g(x)\; dx$ is likely small since $g$ is oscillating fast. A similar method in the theory of series is using \emph{summation by parts}, e.g. writing
	%
	\[ \sum_{k = 1}^n a_k b_k = a_k B_n + \sum_{k = 1}^{n-1} (a_k - a_{k+1}) B_k, \]
	%
	where $B_k = \sum_{i \leq k} b_k$, which might help understand the sum if the discrete derivative $a_k - a_{k+1}$ is well behaved, and the sequence $\{ b_k \}$ is oscillating fast.

	\item If a sequence $\{ a_n \}$ \emph{converges absolutely}, then any rearrangement of $\{ a_n \}$ converges, and converges to the same value, i.e. for any bijection $\pi: \{ 1, 2, \dots \} \to \{ 1, 2, \dots \}$, $\sum a_n = \sum a_{\pi(n)}$.

	\item The \emph{Leibnitz test} says if $\{ a_n \}$ is positive, non-increasing, and $\lim_{n \to \infty} a_n = 0$ then $\sum (-1)^n a_n$ converges.

	\item A \emph{power series} $\sum a_n z^n$ converges for any $z \in \CC$ absolutely when $|z| \leq \limsup_{n \to \infty} |a_n|^{1/n}$.
\end{itemize}

\newpage

\section{Day 1: Warm Up Question}

\begin{questions}

\question (Fall 2016)
  For $n \geq 2$ an integer, define
  %
  \[ F(n)= \max \left\{ k\in \Z: 2^{k}/k\leq n \right\}. \]
  %
  Does the infinite series
  %
  \[ \sum_{n=2}^{\infty}2^{-F(n)} \]
  %
  converge or diverge?
\begin{solution}
	The series diverges.

	One solution follows by breaking the sum into layers, i.e. writing
	%
	\[ \sum_{n = 2}^\infty 2^{-F(n)} = \sum_{k = 1}^\infty 2^{-k} \# \left\{ n \geq 1 : F(n) = k \right\}. \]
	%
	But $F(n) = k$ holds if and only if
	%
	\[ 2^k/k \leq n < 2^{k+1} / (k+1), \]
	%
	and there are at most $2^{k+1} / (k+1) - 2^k / k \sim 2^k / k$ integers between $2^k / k$ and $2^{k+1} / (k+1)$, which implies that
	%
	\begin{align*}
		\sum_{k = 1}^\infty \# \left\{ n \geq 1: F(n) = k \right\} &\geq \sum_{k=1}^{\infty} 2^{-k} \cdot \left( \frac{2^{k+1}}{k+1}-\frac{2^{k}}{k} \right)\\
    &\sim \sum_{k=1}^{\infty} 2^{-k} \cdot (2^k / k)\\
    &=\sum_{k=1}^{\infty} 1/k = \infty.
  \end{align*}
  %
  Thus we have proved the series diverges.

  Another method to prove this result is to use the \emph{Cauchy condensation theorem}, which states that for any non-increasing sequence of non-negative real numbers $\{ a(n) \}$, the series
	%
	\[ \sum_n a(n) \]
	%
	converges if and only if the sum
	%
	\[ \sum_n 2^n a(2^n) \]
	%
	converges. The advantage of the theorem is that in many series, especially those involving \emph{logarithms}, the values $\{ a(2^n) \}$ will be easier to study than the sequence $\{ a(n) \}$. Though $F(n)$ is not explicitly a logarithm, it behaves very similarly to a logarithm, i.e. because $2^x / x \leq n$ when $x \leq \log_2(n) + \log_2 \log_2(n)$, but there exists a sufficiently large $C > 0$ so that for all $n \geq 1$, $2^x / x > n$ when $\log_2(n) + \log_2 \log_2(n) + k$, we have
	%
	\[ \log_2(n) + \log_2 \log_2(n) \leq F(n) \leq \log_2(n) + \log_2 \log_2(n) + C. \]
	%
	This means that the convergence of $\sum 2^{-F(n)}$ is equivalent to the convergence of
	%
	\[ \sum_n 2^n 2^{-F(2^n)} \geq \sum_n 2^n 2^{- n - \log_2 \log_2(n) - C } \gtrsim \sum_n \frac{1}{\log_2(n)}, \]
	%
	and the later sum diverges.

	Alternatively, for $r \geq 1$ we can define $G(r)$ so that $2^{G(r)} / G(r) = r$. Then $F(r) \leq G(r) \leq F(r) + 1$, so that $\sum 2^{-F(n)}$ converges if and only if the integral
	%
	\[ \int_2^\infty 2^{-G(r)} \]
	%
	converges. If we use the bound $G(r) \leq \log_2(r) + \log_2 \log_2(r) + C$ we obtain that
	%
	\[ \int_2^\infty 2^{-G(r)} \gtrsim \int_1^\infty \frac{1}{r \log_2(r)} \]
	%
	and we can see this integral diverges by changing variables, letting $x = \log_2(r)$, to convert the integral into $\int_1^\infty 1/x$, which diverges because the antiderivative of $1/x$ is $\log(x)$, which tends to infinity as $x \to \infty$.
\end{solution}

\newpage

\section{Day 1: Basic Analysis}

\question (Fall 2017) Let $\{ a_n \}$ be a sequence of complex numbers and let
%
\[ c_n = n^{-5} \sum_{k = 1}^n k^4 a_k. \]
\begin{parts}
	\part Prove or Disprove: If $\lim_{n \to \infty} a_n = a$ exists, then $\lim_{n \to \infty} c_n = c$ exists.
	\begin{solution}
		The result is \emph{true}, so let us prove it. Roughly speaking, the value of $c_n$ is the weighted average of the sequence $\{ a_1, \dots, a_n \}$, heavily weighted in the favor of the later elements of the sequence. Thus it is intuitive that our proof should decompose the sum defining $c_n$ into earlier terms, which must be shown to be neglible, and later terms, which are more well behaved because they are close to the value $a$. Indeed, fix $\varepsilon > 0$. Then there is $k_0$ such that $|a_k - a| < \varepsilon$ for $k > k_0$. But this means that
		%
		\begin{align*}
			c_n &= n^{-5} \sum_{k = 1}^n k^4 a_k\\
			&= n^{-5} \sum_{k = 1}^{k_0} k^4 a_k + \sum_{k = k_0 = 1}^n k^4 a_k\\
			&= O_{k_0}(n^{-5}) + n^{-5} \sum_{k = k_0 + 1}^n k^4 a_k\\
			&= O_{k_0}(n^{-5}) + n^{-5} \sum_{k = k_0 + 1}^n k^4 a + n^{-5} \sum_{k = k_0 + 1}^n k^4 (a_k - a)\\
			&= O_{k_0}(n^{-5}) + a \cdot \left( \frac{1}{n^5} \sum_{k = k_0+1}^n k^4 \right) + O(\varepsilon n^{-5} \sum_{k = k_0+1}^n k^4)\\
			&= O_{k_0}(n^{-5}) + a(1/5 + O_{k_0}(1/n)) + O(\varepsilon).
		\end{align*}
		%
		But this calculation implies that there exists a positive integer $n_0$ and a constant $C > 0$ such that for $n \geq n_0$,
		%
		\[ |c_n - a/5| \leq C \varepsilon. \]
		%
		Since $\varepsilon$ was arbitrary, we have shown that $c_n \to a/5$.

		A key step in the calculation of the limit above was that
		%
		\[ \frac{1}{n^5} \sum_{k = k_0 + 1}^n k^4 = 1/5 + O_{k_0}(1/n). \]
		%
		One can prove this using the technique of \emph{replacing sums with integrals}, which we are often more used to manipulating. Observe that
		%
		\begin{align*}
			\frac{1}{n^5} \sum_{k = k_0}^n k^4 = \frac{1}{n^5} \sum_{k = k_0}^n \int_k^{k+1} k^4\; dx \leq \frac{1}{n^5} \int_{k_0}^{n+1} x^4\; dx = \frac{(n+1)^5 - k_0^5}{5 n^5} = \frac{1}{5} + O_{k_0}(1/n).
		\end{align*}
		%
		A similar argument shows the opposite inequality, namely, that
		%
		\[ \frac{1}{n^5} \sum_{k = k_0}^n k^4 \geq \frac{1}{5} + O_{k_0}(1/n), \]
		%
		which gives this result.

		One can perform a similar calculation using \emph{limsup} and \emph{liminf} arguments, which yields a slightly different method and avoids some of the more quantitative calculations as above. It is intuitive that for large $n$ we should have
		%
		\[ c_n \approx \frac{a}{n^5} \sum_{k = 1}^n k^4. \]
		%
		Indeed, we will show that
		%
		\[ \limsup_{n \to \infty} \left| c_n - \frac{a}{n^5} \sum_{k = 1}^n k^4 \right| = 0. \]
		%
		To do this, we utilize a \emph{$\varepsilon$ of room argument}, i.e. showing that for any $\varepsilon > 0$,
		%
		\[ \limsup_{n \to \infty} \left| c_n - \frac{a}{n^5} \sum_{k = 1}^n k^4 \right| \leq \varepsilon. \]
		%
		One can pick $k_0$ such that for $k \geq k_0$, $|a_k - a| \leq \varepsilon$. Thus
		%
		\begin{align*}
			\limsup_{n \to \infty}  \left| c_n - \frac{a}{n^5} \sum_{k = 1}^n k^4 \right| &= \limsup_{n \to \infty} \left| \frac{1}{n^5} \sum_{k = 1}^n k^4 (a_k - a) \right|\\
			&= \limsup_{n \to \infty} \left| \frac{1}{n^5} \sum_{k = k_0}^n k^4 (a_k - a) \right|\\
			&= \limsup_{n \to \infty} \frac{\varepsilon}{n^5} \sum_{k = k_0}^n k^4\\
			&= \varepsilon \cdot (1/5) \leq \varepsilon.
		\end{align*}
		%
		But now we find that
		%
		\[ \lim_{n \to \infty} c_n = \lim_{n \to \infty} \frac{a}{n^5} \sum_{k = 1}^n k^4 = a \lim_{n \to \infty} (1/5 + O(1/n)) = a/5. \]
  \end{solution}


	\part Prove or Disprove: If $\lim_{n \to \infty} c_n = c$ exists, then $\lim_{n \to \infty} a_n = a$ exists.
	\begin{solution}
		This is false, as we might expect since averages like those defining the sequence $\{ c_n \}$ are often much more well behaved than the original sequence $\{ a_n \}$ upon which they are defined. In particular, if the sequence $\{ a_n \}$ has a lot of \emph{oscillation}, then we might expect cancellation in the average to yield a much smaller sequence $\{ c_n \}$. Thus we are lead to consider the sequence $a_n = (-1)^n$ to obtain a counterexample. The sequence $\{ a_n \}$ clearly does not converge. But we claim that the sequence $\{ c_n \}$ \emph{does} converge. We have
		%
		\[ c_n = \frac{1}{n^5} \sum_{k = 1}^n (-1)^k k^4 = \frac{S_n}{n^5}, \]
		%
		where $S_n = \sum (-1)^k k^4$. We will prove by induction that for any $n \geq 1$, $|S_n| \leq n^4$, and $\text{Sign}(S_n) = (-1)^n$. The case $n = 1$ is obvious. Assuming the result for $n$, we have
		%
		\[ S_{n+1} = S_n + (-1)^{n+1} (n+1)^4. \]
		%
		Now $S_n$ and $(-1)^{n+1}(n+1)^4$ have opposite signs, $|S_n| \leq n^4$, and $(n+1)^4 > n^4$, so we conclude that $S_n + (-1)^{n+1} (n+1)^4$ has sign $(-1)^{n+1}$, and
		%
		\[ |S_{n+1}| \leq (n+1)^4. \]
		%
		But this implies that $c_n = S_n/n^5 = O(1/n)$, so $c_n \to 0$.

		Another counterexample follows if we let $\{ a_n \}$ be a \emph{sparse sequence}, i.e. most elements are zero, in which case the averaged sequence $\{ c_n \}$ will be small. Let
		%
		\[ a_k = \begin{cases} 1 &: k = 2^l\ \text{for some $l = 0,1,2,\dots$,} \\ 0 &: \text{otherwise.} \end{cases} \]
		%
		Then we find that for any $n \geq 1$, and any $m < 2^n$, $c_{2^n + m} < c_{2^n}$, and
		%
		\[ c_{2^n} = \frac{1}{2^{5n}} \sum_{k = 0}^n 2^{4k} = \frac{1}{2^{5n}} \frac{2^{4n} - 1}{2^4 - 1} \leq 2^{-n} \to 0. \]
		%
		Thus $\{ c_n \} \to 0$.
	\end{solution}
\end{parts}

\question (Fall 2018)
  For $c_k\in \R$, say that $\Pi c_k$ converges if $\lim_{K\to\infty} \prod_{k=1}^{K} c_k = C$ exists with $C\neq 0, \infty$.
\begin{parts}
  \part Prove that if $0<a_{k}<1$ for all $k$, or if $-1<a_{k}<0$, for all $k$, then $\prod (1+a_{k})$ converges if and only if $\sum_{k}a_{k}$ converges.
  
  \begin{solution}
  	Let
  	%
  	\[ P_n = \prod_{k = 1}^n (1 + a_k) \quad\text{and}\quad S_n = \sum_{k = 1}^n a_k. \]
  	%
  	Then we have
  	%
  	\[ P_n = \exp \left( \sum_{k = 1}^n \log(1 + a_k) \right) = \exp \left( Q_n \right). \]
  	%
  	Thus $\{ P_n \}$ converges to a nonzero value if and only if $\{ Q_n \}$ converges to a finite value, and it suffices to show that the convergence of $\{ Q_n \}$ is equivalent to the convergence of $\{ S_n \}$. To do this, we apply the \emph{limit comparison test} for series, which states that for any two non-negative sequences $\{ a_n \}$ and $\{ b_n \}$, if $\lim_{n \to \infty} (a_n/b_n) = c$, where $0 < c < \infty$, then either both of the series $\sum a_n$ and $\sum b_n$ converge, or they both diverge. We employ the inequality
  	%
  	\[ \frac{x}{1 + x} \leq \log(1 + x) \leq x, \]
  	%
  	which holds for any $x \geq 1$. Thus
  	%
  	\[ \frac{\log(1 + a_k)}{a_k} \leq \frac{a_k}{a_k} = 1 \]
  	%
  	and
  	%
  	\[ \frac{\log(1 + a_k)}{a_k} \geq \frac{1}{1 + a_k} \geq 1 - a_k. \]
  	%
  	Thus if $\{ a_k \}$ converges to zero, then the convergence of $\{ Q_n \}$ and $\{ S_n \}$ are equivalent. But if $\{ S_n \}$ converges, it is obvious that $\{ a_k \}$ converges to zero, and if $\{ Q_n \}$ converges, then $\log(1 + a_k) \to 0$, which implies $a_k \to 0$. Thus we see that the convergence of $\{ Q_n \}$ and $\{ S_n \}$ are always equivalent.

  	Here is another approach, avoiding the limit comparison test, but at the cost of requiring some more quantitative calculations:

  \textbf{Case 1:} Assume $a_k \in (0,1)$ for all $k$.

  First assume $\lim_{n\to\infty} S_n = L <\infty$. We wish to show that $\lim_{n\to\infty} P_n$ converges. Since $\{ P_n \}$ is an increasing sequence, it suffices to show that $\{ P_n \}$ is bounded above. Since $e^x\geq 1+x$ for all $x \in \RR$, we have
  \begin{equation*}
  P_n \leq \prod_{k=1}^n e^{a_k} = e^{S_n} \leq e^L
  \end{equation*}
  %
  where we have used the fact that $S_n$ is increasing and convergent, so that $S_n \leq L$.
  
  Next, assume that $\lim_{n\to\infty} P_n = M$ where $M \neq 0,\infty$. We wish to show that $\lim_{n\to\infty} S_n$ exists. Since $\{ S_n \}$ is increasing, it again suffices to show that $\{ S_n \}$ is bounded above. This will follow from an argument showing $S_n \leq P_n - 1$, since we then have $S_n \leq M - 1$ for all $n$. We prove this using induction. The case $n=1$ is trivial since $S_1 = P_1 - 1$. For $n>1$, observe that
  \begin{align*}
  P_n 
  &= (1+a_n) P_{n-1}\\
  &\geq (1+a_n)(1+S_{n-1}) &&\text{by induction hypothesis}\\
  &= 1+ S_{n} + a_nS_{n-1}\\
  &\geq 1+S_n
  \end{align*}
  This proves the claim. Alternatively, using the inequality
  %
  \[ \log(1+x)\geq \frac{x}{1+x}, \]
  %
  which holds for all $x>-1$, we conclude that
  \begin{align*}
  P_n = \exp \(\sum_{k=1}^n \log(1+a_k)\) \geq \exp\(\sum_{k=1}^{n} \frac{a_k}{1+a_k} \)\geq \exp\(\frac{1}{2}\sum_{k=1}^n a_k\) = e^{S_n/2},
  \end{align*}
  %
  which implies $\{ S_n \}$ is bounded from above if $\{ P_n \}$ is bounded from above.
  
  \textbf{Case 2:} Assume that $a_k\in (-1,0)$ for all $k$.

  Suppose $\lim_{n\to\infty} P_n = L$, with $L \neq 0$. We wish to show that $S_n$ converges as well. Since $\{ S_n \}$ is now decreasing, it suffices to show that $\{ S_n \}$ is bounded from below. But this follows because $S_n \geq \log P_n$, so that $S_n \geq \log L$. Indeed, using the inequality $\log(1+x)\leq x$, which holds for $x>-1$, we have
  \begin{align*}
  	P_n = \exp \left( \sum_{k=1}^n \log(1+a_k) \right) \leq e^{S_n}.
  \end{align*}
  %
  Taking logarithms on both sides verifies the inequality.
  
  Next, assume that $\{ S_n \}$ converges. We will show that $\{ P_n \}$ converges. It suffices to show that $\{ P_n \}$ is bounded from below, since it is decreasing. But this follows from the inequality
  %
  \[ \log(1 + x) \geq \frac{x}{1 + x}, \]
  %
  which holds for $x > -1$, so that
  %
  \[ P_n = \exp \left( \sum_{k = 1}^n \log(1 + a_k) \right) \geq \exp \left( \sum_{k = 1}^n \frac{a_k}{1 + a_k} \right) \geq \exp \left( \sum_{k = 1}^n a_k (1 - a_k) \right), \]
  %
  The fact that $\{ S_k \}$ tends to zero implies that $\{ a_k \}$ tends to zero, so that there is $c > 0$ such that $1 - a_k \geq c$, and thus
  %
  \[ P_n \geq \exp \left( c \cdot S_n \right). \]
  %
  Thus $\{ P_n \}$ is lower bounded.
  \end{solution}
  
  \part However, prove that $\prod_{k>1} \left( 1+ \frac{(-1)^{k}}{\sqrt{k}} \right)$ diverges.
  
  \begin{solution}
  \noindent\textbf{Proof of Part (b):} Let $P_n = \prod_{k=2}^n \(1+ \frac{(-1)^k}{\sqrt{k}}\)$. Our goal is to show the sequence $\{ P_n \}$ diverges. Though one cannot apply the result above directly to this product, since the terms are not all positive or negative, we can apply the result to a subsequence of the products $\{ P_n \}$ above by noting that \emph{consecutive products} are negative. Thus
  \begin{align*}
  P_{2n-1} 
  &= \prod_{k=2}^{2n+1} \(1+\frac{(-1)^k}{\sqrt{k}}\) \\
  &= \prod_{k=1}^n \(1-\frac{1}{\sqrt{2k+1}}\)\(1+\frac{1}{\sqrt{2k}}\) \\
  &= \prod_{k=1}^n \left( 1 - \frac{1 - (\sqrt{2k+1} - \sqrt{2k})}{\sqrt{2k} \sqrt{2k+1}} \right). \\
  \end{align*}
  %
  Using the fact that
  %
  \[ \sqrt{x} - \sqrt{y} = (\sqrt{x} - \sqrt{y}) \frac{\sqrt{x} + \sqrt{y}}{\sqrt{x} + \sqrt{y}} = \frac{x - y}{\sqrt{x} + \sqrt{y}}, \]
  %
  we find that
  %
  \[ 0 < \frac{1 - (\sqrt{2k + 1} - \sqrt{2k})}{\sqrt{2k} \sqrt{2k+1}} \leq \frac{1 - 1/(2k+1)}{2k} = \frac{1}{2k+1} < 1. \]
  %
  Thus the previous result implies $\{ P_{2n-1} \}$ converges to zero. Since
  %
  \[ \sum_{k = 1}^n \frac{1 - (\sqrt{2k+1} - \sqrt{2k})}{\sqrt{2k} \sqrt{2k+1}} \sim \sum_{k = 1}^n \frac{1}{k} \]
  %
  diverges, it follows that the product diverges.
 \end{solution}
\end{parts}
  
\question (Fall 2019) Let $f$ be a continuous function on $\RR$ satisfying
%
\[ |f(x)| \leq \frac{1}{1 + x^2}. \]
%
Define a function $F$ on $\RR$ by
%
\[ F(x) = \sum_{n = -\infty}^\infty f(x + n). \]
\begin{parts}
	\part Prove that $F$ is continuous and periodic with period 1.
	\begin{solution}
		For each $x \in \RR$, the sum
		%
		\[ \sum_{n = -\infty}^\infty f(x + n) \]
		%
		converges \emph{absolutely}. This justifies that the limit of the infinite series remains the same if we rearrange the series. Thus
		%
		\[ F(x + m) = \sum_{n = -\infty}^\infty f(x +n + m) = \sum_{n = -\infty}^\infty f(x+n) = F(x). \]
		%
		Thus $F$ is periodic. Since the finite sum of continuous functions is continuous, each of the partial sums
		%
		\[ F_K(x) = \sum_{n = -K}^K f(x+n) = \sum_{n = -K}^K f_n(x) \]
		%
		is continuous. Fix $N > 0$ and define $I_N = [-N,N]$. That $F$ is continuous will follow if we can show that $\{ F_N \}$ converges to $F$ uniformly on $I_N$ for each $N$, since the uniform limit of continuous functions is continuous. Thus we find $F$ is continuous on $I_N$ for each $N$, and taking $N \to \infty$ gives that $F$ is continuous everywhere.

		So let us show that $\{ F_K \}$ converges to $F$ uniformly in $L^\infty(I_N)$ for each $N$. If $|n| \geq 2N$, if $x \in I_N$, then
		%
		\[ |x+n| \geq |n| - |x| \geq n/2. \]
		%
		Thus
		%
		\[ |f_n(x)| \leq \frac{1}{1 + (n/2)^2} \leq \frac{1}{1 + n^2/4} \lesssim 1/n^2. \]
		%
		Thus for $K_2 \geq K_1 \geq T \geq 2N$, and $x \in I_N$,
		%
		\[ \| F_{K_2} - F_{K_1} \|_{L^\infty(I_N)} = \| \sum_{K_1 < |n| \leq K_2} f_n \|_{L^\infty(I_N)} \lesssim \sum_{K_1 < n \leq K_2} 1/n^2 \lesssim 1/K_1 \leq 1/T. \]
		%
		Taking $T \to \infty$ shows that the sequence $\{ F_K \}$ is Cauchy in $L^\infty(I_N)$, and thus converges uniformly to $F$.
	\end{solution}

	\part Prove that if $G$ is continuous and periodic with period one, then
	%
	\[ \int_0^1 F(x) G(x) = \int_{-\infty}^\infty f(x) G(x)\; dx. \]
	\begin{solution}
		Since $F$ is the locally uniform limit of the functions $\{ F_N \}$ defined above, we can interchange integrals and limits, writing
		%
		\begin{align*}
			\int_0^1 F(x) G(x)\; dx &= \lim_{N \to \infty} \int_0^1 F_N(x) G(x)\; dx\\
			&= \lim_{N \to \infty} \sum_{n = -N}^N \int_0^1 f_n(x) G(x)\; dx\\
			&= \lim_{N \to \infty} \int_{-N}^{N+1} f(x) G(x)\; dx\\
			&= \int_{-\infty}^\infty f(x) G(x)\; dx,
		\end{align*}
		%
		where the last result follows because $G$ is continous and periodic, and thus bounded, and so
		%
		\[ |f(x) G(x)| \lesssim |f(x)| \lesssim \frac{1}{1 + x^2}. \]
	\end{solution}
\end{parts}

\question (Fall 2015) Let $a_1, a_2, \dots$ be a sequence of positive real numbers and assume that
%
\[ \lim_{n \to \infty} \frac{a_1 + \dots + a_n}{n} = 1. \]
\begin{parts}
	\part Show that $\lim_{n \to \infty} a_n n^{-1} = 0$.
	\begin{solution}
		We can extract the value $a_n/n$ from the sequence $\{ S_n/n \}$, since
		%
		\[ \frac{a_n}{n} = \frac{S_n}{n} - \frac{n-1}{n} \frac{S_{n-1}}{n-1}. \]
		%
		Thus
		%
		\[ \lim_{n \to \infty} \frac{a_n}{n} = \lim_{n \to \infty} \frac{S_n}{n} - \lim_{n \to \infty} \frac{n-1}{n} \frac{S_{n-1}}{n-1} = 1 - 1 = 0. \]
	\end{solution}

	\part If $b_n = \max(a_1,\dots,a_n)$, show that $\lim_{n \to \infty} b_n n^{-1} = 0$.
	\begin{solution}
		We have
		%
		\[ b_n/n = \max(1/n \cdot a_1/1, 2/n \cdot a_2/2, \dots, (n/n) \cdot (a_n/n)). \]
		%
		Fix $\varepsilon > 0$. There exists $N > 0$ such that for $n \geq N$, $|a_n/n| \leq \varepsilon$. Thus if $A = \max(a_1/1, \dots, a_N/N)$, we conclude that for $M \geq N$,
		%
		\[ b_M/M \leq \max(A (N/M), \varepsilon). \]
		%
		Thus if $M \geq AN/\varepsilon$, $b_M/M \leq \varepsilon$. Taking $\varepsilon \to 0$ completes the proof.
	\end{solution}

	\part Show that
	%
	\[ \lim_{n \to \infty} \frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} = \begin{cases} 0 &: \beta > 1 \\ \infty &: \beta < 1. \end{cases}. \]
	\begin{solution}
		For $\beta > 1$, we have
		%
		\begin{align*}
			\frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} &= \frac{a_1 a_1^{\beta - 1} + \dots + a_n a_n^{\beta - 1}}{n^\beta}\\
			&\leq \frac{a_1 + \dots + a_n}{n} \max(a_1/n, \dots, a_n/n)^{\beta - 1}\\
			&\leq (S_n/n) \cdot (b_n/n)^{\beta-1}.
		\end{align*}
		%
		Thus
		%
		\[ \lim_{n \to \infty} \frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} = \lim_{n \to \infty} (S_n / n) \cdot ( \lim_{n \to \infty} b_n/n )^{\beta-1} = 1 \cdot 0 = 0. \]

		To prove the result for $\beta < 1$, let us begin with an intuitive argument. For $\beta < 1$, if $x \ll 1$, then $x^\beta \gg x$. Because $b_n / n \to 0$, for suitably large $n$ we have $a_k/n \ll 1$ for $1 \leq k \leq n$. But this means that $(a_k/n)^\beta \gg a_k/n$, which implies
		%
		\[ \frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} \gg \frac{a_1 + \dots + a_n}{n} \approx 1. \]
		%
		More precisely, if $\delta > 0$, then there exists $n_0$ such that for $n \geq n_0$, and $1 \leq k \leq n$,
		%
		\[ a_k/n \leq \delta \quad \text{and} \quad S_n/n \geq 1/2. \]
		%
		But this implies that
		%
		\[ (a_k/n)^\beta = \frac{(a_k/n)}{(a_k/n)^{1 - \beta}} \geq \frac{(a_k/n)}{\delta^{1-\beta}}. \]
		%
		Thus
		%
		\[ \frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} = (a_1/n)^\beta + \dots + (a_n/n)^\beta \geq \frac{1}{\delta^{1 - \beta}} \frac{a_1 + \dots + a_n}{n} \geq (1/2) \frac{1}{\delta^{1-\beta}}. \]
		%
		Taking $\delta \to 0$ completes the proof.

		Another solution is to use an \emph{interpolation argument}. We have
		%
		\[ \left( \frac{a_1^\beta + \dots + a_n^\beta}{n^\beta} \right) = \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^\beta}^\beta. \]
		%
		It thus suffices to show that for $\beta > 1$,
		%
		\[ \limsup_{n \to \infty} \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^\beta} = 0, \]
		%
		and for $\beta < 1$,
		%
		\[ \liminf_{n \to \infty} \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^\beta} = \infty, \]
		%
		For each $1 \leq \beta \leq \infty$, let
		%
		\[ A_n(\beta) = \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^\beta}. \]
		%
		Then
		%
		\[ \limsup_{n \to \infty} A_n(1) = \limsup_{n \to \infty} \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^1} = 1 \]
		%
		and
		%
		\[ \limsup_{n \to \infty} A_n(\infty) = \limsup_{n \to \infty} \left\| \frac{a_1e_1 + \dots + a_ne_n}{n} \right\|_{l^\infty} = \max_{1 \leq i \leq n}(a_i/n) = 0. \]
		%
		H\"{o}lder's inequality implies that
		%
		\[ A_n(\beta) \leq A_n(1)^{1/\beta} A_n(\infty)^{1-1/\beta}. \]
		%
		Thus
		%
		\[ \limsup_{n \to \infty} A_n(\beta) \leq (\limsup_n A_n(1))^{1/\beta} (\limsup_n A_n(\infty))^{1 - 1/\beta} = 1^{1/\beta} \cdot 0^{1 - 1/\beta} = 0. \]
		%
		This is the interpolation argument (upper bounding a quantity for two parameters $\beta_0$ and $\beta_1$, and then using those results to upper bound a quantity for a parameter $\beta$ lying in between $\beta_0$ and $\beta_1$). For $0 < \beta < 1$, another application of H\"{o}lder's inequality shows that if
		%
		\[ \lambda = \frac{1/\beta - 1}{1/\beta - 1/2} \]
		%
		then $0 < \lambda < 1$, and
		%
		\[ A_n(1) \leq A_n(\beta)^{1-\lambda} A_n(2)^\lambda. \]
		%
		Thus using the fact that $A_n(2) \to 0$ as $n \to \infty$ (we proved this above), we conclude that
		%
		\[ \liminf_{n \to \infty} A_n(\beta) \geq \frac{\left( \lim_{n \to \infty} A_n(1) \right)^{\frac{1}{1 - \lambda}}}{\left( \lim_{n \to \infty} A_n(2) \right)^{\frac{\lambda}{1 - \lambda}}} = \frac{1}{0} = \infty. \]
	\end{solution}
\end{parts}


\question (Fall 2021) Let $f \in C^1[0,1]$. Show that for every $\varepsilon > 0$, there exists a polynomial $p$ such that
%
\[ \| f - p \|_{L^\infty[0,1]} + \| f' - p' \|_{L^\infty[0,1]} \leq \varepsilon. \]
\begin{solution}
	Since $f' \in C[0,1]$, applying the \emph{Stone-Weirstrass theorem}, for any $\varepsilon > 0$, we can find a polynomial $q$ such that $\| f' - q \|_{L^\infty[0,1]} \leq \varepsilon / 2$. But then if we consider the polynomial $p$ such that $p(0) = f(0)$, and $p' = q$, then the fundamental theorem of calculus shows that for $x \in [0,1]$,
	%
	\[ |f(x) - p(x)| = \left| \int_0^x f'(x) - p'(x) \right| \leq \int_0^x |f'(x) - p'(x)| \leq (\varepsilon / 2) \cdot x \leq (\varepsilon/2). \]
	%
	Thus we have proved that
	%
	\[ \| f - p \|_{L^\infty[0,1]} + \| f' - p' \|_{L^\infty[0,1]} \leq \varepsilon. \]
\end{solution}
   
\newpage
\section{Day 2: Warm Up Question}

\question (Fall 2023) Let $I$ be a compact interval. Prove that the series
\[ \sum_{n = 1}^\infty (-1)^n n^{-1/2} \cos(x/n) \]
converges uniformly on $I$.
\begin{solution}
The terms of the sum are $O(n^{-1/2})$, and so do not converge absolutely. The sum is also not decreasing, so we cannot apply Leibnitz's test directly. However, as $n \to \infty$ we have $\cos(x/n) \approx 1 - x^2/n^2$, i.e. we have $|1 - \cos(x/n)| \lesssim x^2/n^2$, and this implies that the series
\[ \sum_{n = 1}^\infty (-1)^n n^{-1/2} ( \cos(x/n) - 1 ) \]
\emph{does} converge absolutely. In particular, since
\[ \sum_{n = N}^\infty |n^{-1/2} ( \cos(x/n) - 1 )| \lesssim |x|^2 \sum_{n = N}^\infty \frac{1}{n^{5/2}} \lesssim \frac{|x|^2}{|N|^{3/2}}, \]
this sum converges uniformly for $x$ in a compact interval. This series differs from the original series by the sum
\[ \sum_{n = 1}^\infty (-1)^n n^{-1/2}, \]
which \emph{does} converge by Leibnitz's test, and uniformly in $x$ since the series does not depend on $x$, which completes the proof.
\end{solution}

\newpage
\section{Day 2: Basic Analysis}

\question (Spring 2017, Spring 2011, and Spring 2007)
Show that the sequence of functions
  \begin{equation*}
    S_n(x)= \sum_{k=1}^{n}\frac{\sin(kx)}{k}, \quad \quad n=1,2,3,\ldots,
  \end{equation*}
  is uniformly bounded in $\R$.

%\bnote{Hint: Break the sum into two parts, one summing over values of $k$ with $k \leq 1/x$ the other $k > 1/x$.}
\begin{solution}
One proof idea uses \emph{Summation by Parts}, also known as \emph{Abel's lemma}, which says the following: Let $\{ a_k \}$ and $\{ b_k \}$ be two sequences of real numbers, and let
%
\[ T_n = \sum_{k = 1}^n a_k \]
%
be the partial sum of the terms $\{ a_k \}$. Then
%
\[ \sum_{k = m}^n a_k b_k = T_n b_n - T_{m-1} b_m - \sum_{k = m}^{n-1} T_k (b_{k+1} - b_k). \]
%
%we have:
%\begin{equation*}
%  \sum_{k=m}^{n}a_{k}b_{k} = \sum_{k=m}^{n}\left( T_{k}-T_{k-1} \right)b_{k} = \sum_{k=m}^{n}T_{k}b_{k}-\sum_{k=m}^{n}T_{k-1}b_k = T_{n}b_{n} - T_{m-1}b_{m} + \sum_{k=m}^{n-1}T_{k}\left( b_{k}-b_{k+1} \right).
%\end{equation*}
Summation by parts can be useful in many different scenarios, but the most useful scenario occurs when the sequence $\{ a_k \}$ is oscillating, and the sequence $\{ b_k \}$ is `smooth' (i.e. changing gradually). The oscillation of $\{ a_k \}$ causes the sequence $\{ T_k \}$ to be small overall because of cancellation, and the smoothness of $\{ b_k \}$ causes the terms $b_{k+1} - b_k$ to be small.

Let us use summation by parts to prove this result. It suffices to prove a uniform bound for $x \in [-\pi/2,\pi/2]$, because the absolute value of each of the functions $S_n$ is periodic with degree $\pi$. First, we break the sum into terms will little oscillation, but whose terms are overall relatively small, and terms with large oscillation to which we can apply summation by parts.. Thus for each $x \in (-\pi/2,\pi/2)$, find a non-negative integer such that $N < 1/|x| < N+1$. Write
%
\[ S_n(x) = L_n(x) + H_n(x), \]
%
where
%
\[ L_n(x) = \sum_{k = 1}^N \frac{\sin(kx)}{k} \quad\text{and}\quad H_n(x) = \sum_{k = N+1}^n \frac{\sin(kx)}{k}. \]
%
Utilizing the inequality $|\sin(a)| \leq |a|$ and the triangle inequality, we find that
%
\begin{equation*}
  |L_n(x)| = \left| \sum_{k=1}^{N}\frac{\sin(kx)}{k} \right|\leq \sum_{k=1}^{N}\frac{|kx|}{k}\leq N|x|<1.
\end{equation*}
%
To estimate $H_n$, we apply summation by parts. Let
%
\[ T_k(x) = \sum_{j = 1}^k \sin(jx). \]
%
The triangle inequality gives the trivial estimate $|T_k(x)| \leq k$ for each $k > 0$ and all $x \in \RR$. A more robust estimate follows via the calculation that for $x \in (-\pi/2,\pi/2)$,
%
\begin{equation*}
  |T_k(x)| = \left| \sum_{j=1}^{k}\sin(jx) \right| = \left| \text{Im} \left( \sum_{j=1}^{k}e^{ijx} \right) \right|\leq \left| e^{ix} \frac{ e^{kix} - 1}{e^{ix} - 1}\right| \leq \frac{2}{|\sin (x/2)|} \lesssim \frac{1}{|x|},
\end{equation*}
%
where in the second last inequality, we have used
%
\begin{equation*}
  \left| e^{ix}\frac{e^{kix}-1}{e^{ix}-1} \right|
  \leq \frac{2}{\left| e^{ix}-1 \right|}
  = \frac{2}{\sqrt{( \cos(x)-1 )^{2}+ \sin^{2}(x)}}
  = \sqrt{\frac{2}{1-\cos x}}
  = \frac{1}{|\sin (x/2)|}.
\end{equation*}
%
Thus
%
\begin{align*}
  |H_n(x)| &= \left| \sum_{k=N+1}^{n}\frac{\sin(kx)}{k} \right|\\
          &= \left| \frac{T_n(x)}{n}-\frac{T_N(x)}{N+1} + \sum_{k=N+1}^{n-1}T_k(x) \left(\frac{1}{k}-\frac{1}{k+1}  \right) \right|\\
         &\leq 2 + \left| \sum_{k=N+1}^{n-1}\frac{T_k(x)}{k(k+1)} \right|\\
         &\lesssim 1 + \frac{1}{|x|} \sum_{k = N+1}^{n-1} \frac{1}{k^2}\\
         &\lesssim 1 + \frac{1}{N |x|} \lesssim 1.
\end{align*}
%
This shows that both $L_n$ and $H_n$ are uniformly bounded in $n$, and thus the same is true of $S_n$.

An alternate approach is to employ \emph{Fourier analysis} together with a \emph{Tauberian theorem}. Tauberian theorems use tools that  are beyond the scope of the prerequisites of the qualifying exam, though the proof may be of interest for those who know some basic Fourier analysis. The sums here are the partial Fourier series corresponding to the Fourier coefficients of a bounded, $2 \pi$-periodic function $f \in L^\infty[-\pi,\pi]$, i.e. the function $f$ such that
%
\[ f \sim \sum_{k = 1}^\infty \frac{\sin(kx)}{k}. \]
%
It is not necessary to know the function $f$, but it is a constant multiple of the function
%
\[ x \mapsto \begin{cases} -\pi/2 - x/2 &: -\pi < x < 0 \\ +\pi/2 - x/2 &: 0 < x < \pi \end{cases} \]
%
It follows that $S_n = D_n * f$, where $D_n$ is the \emph{Dirichlet kernel}. If $\| D_n \|_{L^1([0,2\pi])}$ was uniformly bounded in $n$, we could apply Young's convolution inequality, implying
%
\[ \| S_n \|_{L^\infty([0,2\pi])} \leq \| D_n \|_{L^1([0,2\pi])} \| f \|_{L^\infty[0,2\pi])} \lesssim 1. \]
%
Unfortunately, we have $\| D_n \|_{L^1([0,2\pi])} \sim \log n$, so this approach doesn't work completely. But there is a general result for functions $f \in L^1([0,2\pi])$ such that
%
\[ |\widehat{f}(n)| \lesssim 1/n, \]
%
known as a \emph{Tauberian theorem}, which shows that $D_n * f$ is uniformly bounded in $n$ if and only if $P_r * f$ is bounded as $r \to 1$, where $\{ P_r \}$ is the \emph{Poisson kernel} given by
%
\[ P_r(x) = \frac{1 - r^2}{1 - 2 r \cos(\theta) + r^2}. \]
%
Since $\| P_r \|_{L^1([0,2\pi])} \lesssim 1$ for $0 < r < 1$, we can use Young's convolution inequality to conclude that $\| P_r * f \|_{L^\infty((0,2\pi))} \lesssim 1$. Thus $P_r * f$ is uniformly bounded in $r$, and thus $D_n * f$ is uniformly bounded in $n$, completing the proof.

\end{solution}

\question (Spring 2018 and Spring 2021) Determine if
%
\[ \sum_{k = 1}^\infty \frac{\cos(\sqrt{k})}{k} \]
%
converges.
\begin{solution}
    Since the sum of $1/k$ diverges, this convergence could only possibly happen if the oscillation present in the $\cos(\sqrt{k})$ causes enough cancellation. Thus trying to control the oscillation is key to understanding this series. In particular, let us try and understand how often $\cos(\sqrt{k})$ changes sign. We can use conjugation, an often useful trick to understand square roots, to write
    %
    \begin{align*}
        \sqrt{k+n} - \sqrt{k} &= (\sqrt{k+n} - \sqrt{k}) \left( \frac{\sqrt{k+n} + \sqrt{k}}{\sqrt{k+n} + \sqrt{k}} \right)\\
        &= \frac{n}{\sqrt{k+n} + \sqrt{k}}\\
        &= \frac{n}{2\sqrt{k} + O(n/\sqrt{k})}\\
        &= \frac{n}{2\sqrt{k}} + O \left( \frac{n^2}{k^{3/2}} \right).
    \end{align*}
    %
    In particular,
    %
    \[ \sqrt{k + 2 \pi \sqrt{k}} + \sqrt{k} = \pi + O(1/\sqrt{k}). \]
    %
    Thus for each $n$, the sign of $\cos(\sqrt{k})$ stays roughly the same for $k = n \pm \sqrt{n}$. More precisely, let $\{ m_1, m_2, \dots \}$ be an increasing family of integers with $m_1 = 1$, and such that for each $i$, and $k \in \{ m_i, m_i + 1, \dots, m_{i+1} - 1 \}$ , $\cos(\sqrt{k})$ has the same sign. Then $m_{i+1} - m_i \sim m_i^{1/2}$, and it follows from this that $m_i \sim i^{3/2}$. For more than half of the values in $\{ m_i, \dots, m_{i+1} - 1 \}$, $|\cos(\sqrt{k})| \geq 1/2$, and so it follows that
    %
    \begin{align*}
        \left| \sum_{k = m_i}^{m_{i+1} - 1} \frac{\cos(\sqrt{k})}{k} \right| \sim \frac{1}{\sqrt{m_i}} \sim i^{-3/4}.
    \end{align*}
    %
    Roughly speaking, an application of Leibnitz's alternating series test then implies that this series converges. More precisely, we note that because $i^{-3/4}$ is decreasing in $i$, for suitably large $i$ the terms
    %
    \[ a_i = \left| \sum_{k = m_i}^{m_{i+1} - 1} \frac{\cos(\sqrt{k})}{k} \right| \]
    %
    are decreasing and positive. Thus by the alternating series test, the series $\sum (-1)^{i-1} a_i$ converges. But
    %
    \[ \sum_{i = 1}^k (-1)^{i-1} a_i = \sum_{j = 1}^{m_{k+1} - 1} \frac{\cos(\sqrt{k})}{k}. \]
    %
    Since, for $k \leq m_{i+1}$, we have
    %
    \[ \left| \sum_{j = m_i}^k \frac{\cos(\sqrt{k})}{k} \sqrt m_i^{-1/2} \right| \sim i^{-3/4}, \]
    %
    which tends to zero as $i \to \infty$, it follows from this that the original series also converges (this second calculation justifies that the terms missed by the series $\sum (-1)^i a_i$ do not deviate enough to fail to converge).
\end{solution}


\question (Fall 2015) Consider the series
%
$$ \sum_{n = 1}^\infty \frac{1}{\sqrt{n}} \sin(x/n). $$
%
\begin{parts}
	\part Show that the series converges pointwise to some function $f$ on $\RR$.
	\begin{solution}
		The main idea to understanding the sum is to use the bound $|\sin(y)| \leq |y|$, which is tight when $|y| \leq 1$. Thus little is lost in our estimates if we apply this bound in the sum above for $|n| \geq |x|$, since then $|x/n| \leq 1$. We will actually find this sum converges absolutely, locally uniformly in $x$. Indeed, plugging in the bound $|\sin(x)| \leq |x|$, we find that
		%
		\[ \sum_{n = 1}^\infty \left| \frac{1}{\sqrt{n}} \sin(x/n) \right| \leq \sum_{n = 1}^\infty \frac{1}{\sqrt{n}} \frac{|x|}{n} = |x| \sum_{n = 1}^\infty \frac{1}{n^{3/2}} \lesssim |x|. \]
		%
		Since the sum converges pointwise absolutely, it must converge pointwise.
	\end{solution}

	\part Is $f$ continuous on $\RR$? Does $f'(x)$ exist for all $x \in \RR$?
	\begin{solution}
		To see that $f$ is continuous, it suffices to show that the partial sums
		%
		\[ f_N(x) = \sum_{n = 1}^N \frac{1}{\sqrt{n}} \sin(x/n) \]
		%
		converge \emph{locally uniformly} in $x$ to $f$, since the limit of a sequence of continuous functions converging locally uniformly is continuous. But employing the bound $|\sin(x)| \leq |x|$ as above, we find that for $M > N$,
		%
		\[ |f_M(x) - f_N(x)| \leq \sum_{n = N+1}^M \left| \frac{1}{\sqrt{n}} \sin(x/n) \right| \leq |x| \sum_{n = N+1}^M 1/n^{3/2} \lesssim |x| / N^{1/2}. \]
		%
		But this equation implies locally uniform convergence, i.e. for any interval $I = [-K,K]$, we have proved that
		%
		\[ \| f_M - f_N \|_{L^\infty(I)} \lesssim K/N^{1/2}. \]
		%
		Thus $f$ is a continuous function.

		Now to understand the differentiability of $f$, it makes sense to look at the derivatives
		%
		\[ f_N'(x) = - \sum_{n = 1}^N (1/n^{3/2}) \cos(x/n). \]
		%
		Since the cosine term is uniformly bounded in $x$ and $n$, it is fairly negligible to studying the limit of this quantity as $N \to \infty$. Indeed, we find that for $M > N$, we can plug in the bound $|\cos(y)| \leq 1$ to conclude that
		%
		\[ |f_M'(x) - f_N'(x)| \leq \sum_{n = N+1}^M 1/n^{3/2} \lesssim N^{-1/2}. \]
		%
		Thus $\| f_M - f_N \|_{L^\infty(\RR)} \lesssim N^{-1/2}$, which implies the sequence is uniformly Cauchy, and thus converges to some function $g$. But it is a general rule that if $\{ f_N \}$ converges locally uniformly to some function $f$, and $\{ f_N' \}$ converges locally uniformly to some function $g$, then $f$ is differentiable, and $f' = g$. Thus $f$ is differentiable.

		An alternative, but less clean way to show $f$ is differentiable is using the definition of the derivative, i.e. by proving that for each $x \in \RR$, the limit
		%
		\[ \frac{f(x+h)-f(x)}{h} \]
		%
		exists. To see this, write
    %
    \begin{align}
          \label{eq:fall2015-0}
          \frac{f(x+h)-f(x)}{h} 
          &= \underbrace{\frac{1}{h} \sum_{n=1}^{N}
            \frac{1}{\sqrt{n}}\left[ \sin \left( \frac{x+h}{n} \right)
            - \sin \left( \frac{x}{n} \right)   \right]}_{\text{Call it }A_{N,h}(x)} \nonumber\\
            \quad&+ \underbrace{\frac{1}{h} \sum_{n=N+1}^{\infty}
            \frac{1}{\sqrt{n}}\left[ \sin \left( \frac{x+h}{n} \right)
            - \sin \left( \frac{x}{n} \right)   \right]}_{\text{Call it }B_{N}(x)}.
        \end{align}
        We consider the two terms on the right hand side individually. By the mean value theorem, $ \left|\sin \left( \frac{x+h}{n} \right)
        - \sin \left( \frac{x}{n} \right)\right|\leq \frac{|h|}{n}$, and hence
        \begin{equation}\label{eq:fall2015-1}
          |B_{N}(x)| \leq \sum_{n=N+1}^{\infty}\frac{1}{n^{3/2}} \leq \frac{C}{\sqrt{N}}
        \end{equation}
        for some constant $C>0$ not depending on $x$.
        
        Next we consider $A_{N,h}$. Let
				%
				\[ g_{N}(x) = \sum_{n=1}^{N}\frac{1}{n^{3/2}}\cos(x/n) \]
				%
				and let
				%
				\[ g_{\infty}(x) = \sum_{n=1}^{\infty}\frac{1}{n^{3/2}}\cos(x/n). \]
				%
				Then (it is easy to check that)
        \begin{equation}\label{eq:fall2015-3}
          \lim_{h\to \infty} A_{N,h}(x)  = g_{N}(x) \text{ for each $x$}
        \end{equation}
        and
        \begin{equation}\label{eq:fall2015-2}
          \lim_{N \to \infty} g_{N}(x) = g_{\infty}(x) \text{ uniformly in $x$.}
        \end{equation}
        Therefore by Eq. \eqref{eq:fall2015-0},
        \begin{align*}
           \frac{f(x+h)-f(x)}{h}-g_{\infty}(x)
          = g_{N}(x)-g_{\infty}(x) +A_{N,h}(x)-g_{N}(x) +B_{N}
        \end{align*}
        so that
        \begin{equation*} 
          \left| \frac{f(x+h)-f(x)}{h}-g_{\infty}(x) \right| 
          \leq \sup_{x\in \mathbb{R}}\left|g_{N}(x)-g_{\infty}(x)\right| +\left|A_{N,h}(x)-g_{N}(x)\right| + \frac{C}{\sqrt{N}}
        \end{equation*}
        Let $\epsilon>0$ be arbitrary. By \eqref{eq:fall2015-1} and \eqref{eq:fall2015-2}, we may choose $N$ sufficiently
        large that $C/\sqrt{N} <\epsilon/2$ and $\sup_{x\in \mathbb{R}}\left|g_{N}(x)-g_{\infty}(x)\right|
        <\epsilon/2$, which implies
        \begin{align*} 
          \left| \frac{f(x+h)-f(x)}{h}-g_{\infty}(x) \right|
          \leq \left|A_{N,h}(x)-g_{N}(x)\right| + \epsilon.
        \end{align*}
        Taking the limit suprememum of both sides as $h\to 0$ and using \eqref{eq:fall2015-3}, we obtain
        \begin{equation*}
          \limsup_{h\to 0} \left| \frac{f(x+h)-f(x)}{h}-g_{\infty}(x) \right| \leq \epsilon
        \end{equation*}
        Since $\epsilon>0$ was arbitrary, it holds that
        %
        \[ \limsup_{h\to 0} \left|
          \frac{f(x+h)-f(x)}{h}-g_{\infty}(x) \right| =0. \]
          %
          Therefore
          %
          \[ f'(x) = \lim_{h \to 0}
        \frac{f(x+h)-f(x)}{h}=g_{\infty}(x)\in \mathbb{R}. \]

		%An alternative, but less clean way to show $f$ is differentiable is using the definition of the derivative, i.e. by proving that for each $x \in \RR$, the limit
		%
		%\[ \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} = \lim_{h \to 0} \sum_{n = 1}^\infty \frac{1}{\sqrt{n}} \frac{\sin((x + h)/n) - \sin(x/n)}{h}. \]
		%
		%exists. One way to do this is to write
		%
		%\[ \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} = \lim_{h \to 0} \frac{f_N(x + h) - f_N(x)}{h} + \lim_{h \to 0} \sum_{n = N+1}^\infty \frac{1}{\sqrt{n}} \frac{\sin((x + h)/n) - \sin(x/n)}{h}. \]
		%
		%Since $f_N$ is differentiable, the first limit is easily shown to exist. One might understand the second sum by using the mean value theorem to conclude that $|\sin((x + h)/n) - \sin(x/n)| \lesssim |h| / n$, thus obtaining that
		%
		%\[ \limsup_{h \to 0} \left| \sum_{n = N+1}^\infty \frac{1}{\sqrt{n}} \frac{\sin((x + h)/n) - \sin(x/n)}{h} \right| \lesssim 1/N^{1/2}. \]
		%
		%This means that
		%
		%\[ \limsup_{h \to 0} \frac{f(x + h) - f(x)}{h} - \liminf_{h \to 0} \frac{f(x + h) - f(x)}{h} \lesssim 1/N^{1/2}. \]
		%
		%Taking $N \to \infty$ shows that $\liminf = \limsup$, so that the limit exists, and thus $f$ is differentiable.
	\end{solution}

	\part Does the series converge uniformly on $\RR$?
	\begin{solution}
		The bounds in part (a) of the problem, which depend on $x$, should hint to you that the series does \emph{not} converge uniformly in $x$, in particular, as we take $x$ to be very large. To show that the sum does not converge uniformly, we need lower bounds, and to do this we again employ the heuristic that $|\sin(y)| \leq |y|$ is a good estimate for $|y| \leq 1$, so we should get a similar estimate which lower bounds $\sin(y)$ in this range. In fact, for $0 \leq y \leq 1$ we actually have $\sin(y) \geq y/2$. Concentrating on the case where $x$ is a positive integer for simplicity, we find that
		%
		\[ \sum_{n \geq x} \frac{1}{\sqrt{n}} \sin(x/n) \gtrsim \sum_{n \geq x} \frac{x}{n^{3/2}} \gtrsim \sqrt{x}. \]
		%
		Thus if $N = x$, we can reword this inequality as saying that
		%
		\[ \lim_{M \to \infty} f_M(x) - f_N(x) \gtrsim \sqrt{x}. \]
		%
		Thus
		%
		\[ \limsup_{M \to \infty} \| f_M - f_N \|_{L^\infty(\RR)} \gtrsim \sqrt{N}. \]
		%
		In particular, this implies $\{ f_N \}$ is not a Cauchy sequence in $L^\infty(\RR)$, and thus does not converge uniformly.

		Alternatively, we can use the fact that if $\pi/3 \leq y \leq 2\pi/3$, then $\sin(y) \geq 1/2$. Thus, for any $N$, if we pick $x = (\pi/3)N$, then for $1 \leq n \leq N$, $\sin(x/n)$ is non-negative, and for $N/2 \leq n \leq N$, $\sin(x/n) \geq 1/2$, so
		%
		\[ |f_N(x)| \gtrsim \sum_{N/2 \leq n \leq N} 1/\sqrt{n} \gtrsim \sqrt{N}. \]
		%
		Thus $\| f_N \|_{L^\infty(\RR)} \gtrsim \sqrt{N}$, which implies $\{ f_N \}$ is not bounded in $L^\infty(\RR)$, and so $\{ f_N \}$ is not a Cauchy suquence in $L^\infty(\RR)$.
	\end{solution}
\end{parts}

\question (Fall 2019) Show that if $K \subset \RR^n$, and every continuous function on $K$ is bounded, then $K$ is compact.
\begin{solution}
	The \emph{Heine-Borel theorem} implies that it suffices to show that $K$ is bounded and closed.

	If $K$ was not closed, then we could find $y \in \overline{K} - K$. The function $f(x) = 1/|x-y|$ would then be continuous, but not bounded, on $K$. Thus $\overline{K} - K$ is empty, so $\overline{K} = K$.

	If $K$ was not bounded, then $f(x) = |x|$ would be a continuous, unbounded function. Thus $K$ is bounded.

	Thus $K$ is compact.
\end{solution}

\question (Spring 2015) Prove that the integral
%
\[ f(a) = \int_0^\infty \frac{\sin(x^2 + ax)}{x}\; dx \]
%
converges for $a \geq 0$, and $f$ is continuous on $[0,\infty)$.
\begin{solution}
	We perform a \emph{dyadic decomposition}, writing, for $n \in \ZZ$,
	%
	\[ f_n(a) = \int_{2^n}^{2^{n+1}} \frac{\sin(x^2 + ax)}{x}\; dx. \]
	%
	Then $f = \sum_{n = -\infty}^\infty f_n$. Since the integrand is continuous, and each integral is on a finite interval, all of the functions $\{ f_n \}$ are continuous.

	We claim this sum converges locally uniformly. To do this we \emph{must} exploit the oscillation in the integrand, for $1/x$ is not summable. To do this, we perform an \emph{integration by parts}. Indeed, for $a \geq 0$, and $n \geq 0$ we have
	%
	\begin{align*}
		\int_{2^n}^{2^{n+1}} \frac{\sin(x^2 + ax)}{x} &= \int_{2^n}^{2^{n+1}} \frac{d}{dx} \left( \cos(x^2 + ax) \right) \cdot \frac{-1}{x(2x + a)}\\
		&= \left( \int_{2^n}^{2^{n+1}} \frac{\cos(x^2 + ax)}{x(2x + a)} \right) + O(1/2^n)\\
		&\leq \int_{2^n}^{2^{n+1}} \frac{1}{x(2x + a)} + O(1/2^n)\\
		&\lesssim \frac{1}{a} \frac{1}{2^n}.
	\end{align*}
	%
	Thus $\| f_n \|_{L^\infty[0,\infty)} \lesssim 2^{-n}/a$. For $n \leq 0$ we employ the bound $\sin(x^2 + ax) \leq x^2 + ax$, to conclude that
	%
	\[ \int_{2^n}^{2^{n+1}} \frac{\sin(x^2 + ax)}{x} \lesssim 2^{2n} + a 2^n \leq 4^n + 2^n a. \]
	%
	Thus $\| f_n \|_{L^\infty[0,\infty)} \lesssim 2^n (a + 2^n)$. Summing over all $n \in \ZZ$, we obtain uniform convergence to a continuous function $f = \sum f_n$ with $\| f \|_{L^\infty} \leq 1/a + a + 1$.

	One can also work without a dyadic decomposition, i.e. using integration by parts to write
	%
  \begin{align*}
      &= \int_{A}^{B}\frac{\sin(x^{2}+ax)}{x}dx\\ 
      &= \int_{A}^{B}\frac{\sin(x^{2}+ax)(2x+a)}{x(2x+a)}dx\\
      &= \int_{A}^{B} \frac{d}{dx}\left[ -\cos(x^{2}+ax) \right] \cdot \frac{1}{x(2x+a)}dx\\
      &= \left.\frac{-\cos(x^{2}+ax)}{x(2x+a)}\right|^{x=B}_{x=A} + \int_{A}^{B}\cos(x^{2}+ax)\left( \frac{-1}{x^{2}(2x+a)}-\frac{2}{x(2x+a)} \right)  dx
    \end{align*}
    Therefore if
    %
    \[ f_C(a) = \int_0^C \frac{\sin(x^2 + ax)}{x}\; dx, \]
    %
    then we have shown that
    %
    \begin{equation*}
      \left|   f_{B}(a)-f_{A}(a) \right|
      \leq \frac{1}{2A^{2}}+ \frac{1}{2B^{2}} + \int_{A}^{B}\frac{1}{2x^{3}}+ \frac{2}{x(2x)^{2}} dx
      \leq 10 \left( \frac{1}{A^{2}}+\frac{1}{B^{2}}  \right).
    \end{equation*}
    %
    Thus
    %
    \[ \| f_B - f_A \|_{L^\infty[0,\infty)} \leq 10 (1/A^2 + 1/B^2) \to 0, \]
    %
    which shows $\{ f_A \}$ converges uniformly to a continuous function as $A \to \infty$.
\end{solution}

\question (Fall 2017) Consider the sequence of functions $f_n: \RR \to \RR$ defined by
%
\[ f_n(x) = \int_0^n \frac{\sin(sx)}{\sqrt{s}}\; ds. \]
%
\begin{parts}
	\part Show that $\{ f_n \}$ converges locally uniformly on $(0,\infty)$.
	\begin{solution}
		This integral can only converge because of the oscillation of $\sin(sx)$ for large $x$. Thus we \emph{integrate by parts}, writing
		%
		\begin{align*}
			\int_{n_0}^{n_1} \frac{\sin(sx)}{\sqrt{s}}\; ds &= \int_{n_0}^{n_1} \frac{- d/ds (\cos(sx))}{x \sqrt{s}}\; ds\\
			&= \left. \frac{- \cos(sx)}{x s^{1/2}} \right|^{n_1}_{n_0} + \int_{n_0}^{n_1} \frac{\cos(sx)}{x s^{3/2}} (-1/2)\; ds\\
			&= O(1/xn_0^{1/2}) + O(1/x \int_{n_0}^{n_1} 1/s^{3/2}\; ds)\\
			&= O(1/xn_0^{1/2}).
		\end{align*}
		%
		Thus for any fixed $\alpha > 0$, we get uniform convergence for $\alpha < x < \infty$.
	\end{solution}

    
	\part Show that $\{ f_n \}$ does \emph{not} converge uniformly on $(0,1]$.
	\begin{solution}
		If the convergence was uniform, there would be $n_0$ such that for any $n_1 \geq n_0$, and $x \in [0,1)$,
		%
		\[ \left| \int_{n_0}^{n_1} \frac{\sin(sx)}{\sqrt{s}}\; ds \right| \leq 1. \]
		%
		Now pick $x \in [0,1)$ with $x \sim 1/n_1$, such that for $s \in [n_0,n_1]$, $\sin(sx) \geq sx/2$. Then
		%
		\[ 1 \geq \int_{n_0}^{n_1} \frac{\sin(sx)}{\sqrt{s}}\; ds \geq \frac{x}{2} \int_{n_0}^{n_1} \sqrt{s}\; ds \gtrsim n_1^{3/2} x \gtrsim n_1^{1/2}. \]
		%
		This gives a contradiction for sufficiently large $n_1$.
	\end{solution}

	\part Does the sequence $\{ f_n \}$ converge uniformly on $[1,\infty)$ as $n \to \infty$?
	\begin{solution}
		Yes, as implied by the calculations in part (a).
	\end{solution}
\end{parts}

\question (Fall 2021) Does the improper integral
%
\[ \int_2^\infty \frac{x \sin(e^x)}{x + \sin(e^x)}\; dx \]
%
converge?
\begin{solution}
	Here's one solution. Our first intuition is to break the integrand into an oscillatory part, and a fairly well behaved part. A good option seems to be breaking the sum into the product
	%
	\[ \frac{x}{x + \sin(e^x)} \cdot \sin(e^x) = A(x) \cdot B(x). \]
	%
	For large $x$, the well behaved part of the integrand is close to one. So let's do a Taylor expansion. A first order expansion yields $A(x) = 1 + O(1/x)$, but the error term here is not good enough (it's not integrable in $x$). Thus we perform a second order expansion, i.e. writing $A(x) = 1 - \sin(e^x) / x + O(1/x^2)$. Now the error term is integrable, so we conclude that the convergence of the improper integral above is equivalent to the convergence of the integral
	%
	\[ \int_2^\infty \left( 1 - \frac{\sin(e^x)}{x} \right) \sin(e^x)\; dx. \]
	%
	A change of variables might help us out here. We write $y = e^x$. Then $dy = y dx$, and so this integral is equal to
	%
	\[ \int_c^\infty \frac{\sin(y)}{y} \left( 1 - \frac{\sin(y)}{\ln y} \right). \]
	%
	A simple integration by parts (antidifferentiate the oscillating term $\sin(y)$) yields
	%
	\[ \int_c^\infty \frac{\sin(y)}{y} = C + \int_{O(1)}^\infty \frac{\cos(y)}{y^2}. \]
	%
	The integral on the right hand side is summable, and thus the integral converges. Thus the convergence of the overall integral is equivalent to
	%
	\[ \int_c^\infty \frac{\sin(y)^2}{y \ln(y)}. \]
	%
	This integral is positive, and \emph{not integrable}. We can see this by performing a dyadic decomposition on the region of integration (which helps with the logarithmic terms). We thus obtain that
	%
	\[ \int_c^\infty \frac{\sin(y)^2}{y \ln(y)} \gtrsim \sum_{k = 1}^\infty \int_{2^k}^{2^{k+1}} \frac{\sin(y)^2}{y \ln(y)} \gtrsim \sum_{k = 1}^\infty 2^k \frac{1}{k 2^k} = \sum_{k = 1}^\infty \frac{1}{k} = \infty. \]
	%
	Thus we conclude that the integral diverges, and thus the same must be true for the overall integral.

	Here is an alternate solution. If $y = e^x$, then $dy = e^x dx = y dx$, and so
	%
	\[ \int_2^\infty \frac{x \sin(e^x)}{x + \sin(e^x)}\; dx = \int_{\log 2}^\infty \frac{\sin y}{y} \frac{\log(y)}{\log(y) + \sin(y)} \]
	%
	The integrand oscillates between positive and negative values, so it suffices to decompose the integral onto the intervals where the integrand is positive or negative. For suitably large $n$, write
	%
	\[ C_n = \left| \int_{\pi n}^{\pi (n + 1)} \frac{\sin y}{y} \frac{\log y}{\log y + (-1)^n \sin y} \right| \]
	%
	The integral will then converge if and only if the alternating series
	%
	\[ \sum_{n = 1}^\infty (-1)^n C_n \]
	%
	converges. For $\pi n \leq y \leq \pi(n+1)$, a Taylor series expansion shows that
	%
	\begin{align*}
		\frac{1}{y} \frac{\log y}{\log y + (-1)^n \sin y} &= \frac{1}{y} \frac{1}{1 + (-1)^n \sin y / \log y}\\
		&= \left( \frac{1}{\pi n} + O(1/n^2) \right) \left(1 - (-1)^n \frac{\sin y}{\log y} + O \left( \frac{1}{(\log n)^2} \right) \right)\\
		&= \left( \frac{1}{\pi n} + O(1/n^2) \right) \left(1 - (-1)^n \frac{\sin y}{\log(\pi n)} + O \left( \frac{1}{(\log n)^2} \right) \right)\\
		&= \left( \frac{1}{\pi n} - (-1)^n \frac{\sin y}{\pi n \log \pi n} + O \left( \frac{1}{n (\log n)^2} \right) \right).
	\end{align*}
	%
	Thus
	%
	\begin{align*}
		C_n &= \left| \int_0^\pi \frac{\sin y}{\pi n} - (-1)^n \frac{(\sin y)^2}{\pi n \log \pi n}\; dy \right| + O \left( \frac{1}{n (\log n)^2} \right)\\
		&= \left| \frac{2}{\pi n} - (-1)^n \frac{1}{2 n \log(\pi n)} \right| + O \left( \frac{1}{n (\log n)^2} \right)\\
		&= C_n' + O \left( \frac{1}{n (\log n)^2} \right).
	\end{align*}
	%
	For large $n$, we have
	%
	\[ C_n' = \frac{2}{\pi n} - (-1)^n \frac{1}{2 n \log(\pi n)}. \]
	%
	Thus
	%
	\[ \sum (-1)^n C_n' = \sum (-1)^n (2 / \pi n) - \frac{1}{2 n \log(\pi n)}. \]
	%
	The sum $\sum (-1)^n (2 / \pi n)$ converges by the \emph{Leibnitz test}. But the other part of the series diverges, i.e.
	%
	\[ \sum_n \frac{1}{2 n \log(\pi n)} = \infty. \]
	%
	One can prove this using the Cauchy condensation theorem. Thus the alternating series \emph{diverges}, and so the entire integral diverges.
\end{solution}

\newpage
\section{Day 3: Warm Up Question}

\question (Spring 2018 and Spring 2021)
	Let $u: \RR \to \RR$ be a differentiable function with $|u'(x)| \leq a < 1/2$ everywhere. Show that $g(x,y)	= (x + u(2y), y + u(x))$ is surjective.
\begin{solution}
	Here is one solution involving the use of the inverse function theorem. We calculate that
	%
	\[ Dg(x,y) = \begin{bmatrix} 1 & 2u'(2y) \\ u'(x) & 1 \end{bmatrix}. \]
	%
	The determinant of $Dg$ is $1 - 2 u'(x) u'(2y) \geq 1 - 2 a^2 \geq 1/2$. The inverse function theorem implies that the map $g$ is \emph{open}, i.e. $g(U)$ is an open set whenever $U$ is an open set. It follows that $g(\RR^2)$ is an open set.

	It is also a \emph{closed set}. To see this, let $\{ z_n = g(x_n,y_n) \}$ be a convergent sequence in $g(\RR^2)$, converging to some point $(z,w) \in \RR^2$. Then we claim that $\{ (x_n,y_n) \}$ has a convergent subsequence, converging to some pair $(x,y)$, and then by continuity of $g$ it follows that $g(x,y) = (z,w)$, proving that $(z,w) \in g(\RR^2)$, and thus proving that $g(\RR^2)$ is closed. To prove that $\{ (x_n,y_n) \}$ has a convergent subsequence, we prove that the sequence is bounded, so we may apply the Heine-Borel theorem. We first pick $M > 0$ so that
	%
	\[ |x_n + u(2y_n)| \leq M \quad\text{and}\quad |y_n + u(x_n)| \leq M. \]
	%
	Then
	%
	\[ |x_n| \leq M + |u(2y_n)| \leq M + 2a |y_n| \]
	%
	and
	%
	\[ |y_n| \leq M + |u(x_n)| \leq M + a |x_n|. \]
	%
	Combining these inequalities gives that
	%
	\[ |x_n| \leq M + 2 a | y_n| \leq M + 2a ( M + a |x_n| ) \leq (1 + 2a) M + 2a^2 |x_n|, \]
	%
	which we rearrange to read that
	%
	\[ |x_n| \leq \frac{1 + 2a}{1 - 2a^2} M. \]
	%
	Similarily,
	%
	\[ |y_n| \leq M + a |x_n| \leq M + a ( M + 2 a |y_n| ) \leq (1 + a) M + 2a^2 |y_n|, \]
	%
	so
	%
	\[ |y_n| \leq \frac{1 + a}{1 - 2a^2} M. \]
	%
	Thus we have proved the sequences $\{ x_n \}$ and $\{ y_n \}$ are bounded, which completes the proof that $g(\RR^2)$ is closed.

	We have proved $g(\RR^2)$ is open and closed, and it is clearly non-empty. Since $\RR^2$ is a connected set, and $g(\RR^2) \subset \RR^2$, it follows that $g(\RR^2) = \RR^2$, completing the proof of surjectivity.

	An alternate proof follows from the Banach fixed point theorem. For any $z$ and $w$, we note that $g(x,y) = (z,w)$ holds if and only if $x = z - u(2y)$ and $y = w - u(x)$, i.e. $(x,y)$ is a fixed point of the map $h(x,y) = (z - u(2y), w - u(x))$. Now
	%
	\[ |h(x,y) - h(x',y')| \leq \| ( u(2y), u(x) ) \| \leq ( 4 a^2 y^2 + a^2 x^2 )^{1/2} \leq 2a |(x,y)|. \]
	%
	Since $a < 1/2$, it follows that $h$ is a \emph{contraction map}, and so it follows that $h$ has a unique fixed point. Thus we have proved that $g$ is both injective and surjective.

	(Thanks to Seong Kyun Jung for this solution) For any $z$ and $w$, define the error function
	%
	\[ E(x,y) = ( x + u(2y) - z )^2 + ( y + u(x) - w )^2. \]
	%
	It suffices to show that there exists $(x,y)$ such that $E(x,y) = 0$. Such a pair must be a \emph{minimum} for $E$, which leads us to analyze the minima of $E$. For suitably large $x$ and $y$ we have $|x + u(2y) - z| \geq |x| - 2a |y| - |z|$ and $|y + u(x) - w| \geq |y| - a |x| - |w|$, so that
	%
	\[ |E(x,y)| \geq \max( |x| - 2a |y| - |z|, |y| - a |x| - |w| ), \]
	%
	and so if $\| (x,y) \| \geq 10 |z|$ then $E(x,y) \gtrsim \max( |x|, |y| )$, proving that $E(x,y) \to \infty$ as $(x,y) \to \infty$, and proving that minima exist for $E$. Let $(x_0,y_0)$ be such a minimum. Then $(x_0,y_0)$ is a critical point of $E$, i.e.
	%
	\[ (x_0 + u(2y_0) - z) + (y_0 + u(x_0) - w ) u'(x) = 0 \quad\text{and}\quad (x_0 + u(2y_0) - z) 2 u'(2y_0) + (y_0 + u(x_0) - w) = 0. \]
	%
	Since $E(x_0,y_0) > 0$, either $x_0 + u(2y_0) - z \neq 0$ or $y_0 + u(x_0) - w \neq 0$. But then the fact that both equations hold above implies that $x_0 + u(2y_0) - z \neq 0$ and $y_0 + u(x_0) - w \neq 0$. We then find that
	%
	\[ 2u'(2y_0) = - \frac{y_0 + u(x_0) - w}{x_0 + u(2y_0) - z} = \frac{1}{u'(x)}. \]
	%
	But $2u'(2y_0) = 1/u'(x)$ gives a contradiction, since $|2u'(2y_0)| \leq 2a < 1$ whereas $1/|u'(x)| \geq 1/a > 2$.
\end{solution}




\newpage
\section{Day 3: Basic Analysis}

\question (Fall 2018) 
Prove that for $1\leq p \leq 2$ and $0<b<a$, $$(a+b)^p + (a-b)^p \geq 2a^p + p(p-1)a^{p-2}b^2.$$

\begin{solution}
  By subtracting the right-hand side, factoring out $a^p$, and noting that $0<b/a<1$, it suffices to show that the function
  \begin{equation*}
    f(x) = \left( 1+x \right)^{p} + \left( 1-x \right)^{p} - 2 - p(p-1)x ^{2}
  \end{equation*}
  is nonnegative for all $x\in (0,1)$. Let $g(x) = (1+x)^p$, so that
  \begin{equation*}
    f(x) = g(x)+ g(-x) -2 - p(p-1)x^{2}.
  \end{equation*}
  We next Taylor expand $g$ about the point $0$.
  For any $x>-1$, we have
  \begin{equation*}
    g(x) = 1 + px + \frac{1}{2}p(p-1)x^{2} + R_{2}(x)
  \end{equation*}
  where
  \begin{equation*}
    R_{2}(x)= \frac{p(p-1)(p-2)(1+\xi)^{p-3}x^{3}}{6}
  \end{equation*}
  for some number $\xi$ between $0$ and $x$. Substituting the Taylor expansions for $g(x)$ and $g(-x)$ into the formula for $f$ gives
  \begin{equation*}
    f(x) = R_{2}(x) + R_{2}(-x)
  \end{equation*}
  or equivalently
  \begin{equation*}
    f(x) = \frac{p(p-1)(p-2)}{6} \left[(1+\xi_{+})^{p-3}-(1+\xi_{-})^{p-3}\right]x^{3}
  \end{equation*}
  for some $\xi_{+}\in(0,x)$ and $\xi_{-}\in(-x,0)$. Since $\xi_{-}<\xi_{+}$ and $p<3$, the part in brackets is negative, and since $0\leq p \leq 2$, it follows that $f(x)$ is nonnegative, as required.
\end{solution}

\question (Spring 2015) Let $g$ be a non-constant differentiable real function on a finite interval $[a,b]$, with $g(a) = g(b) = 0$. Show that there exists $c \in (a,b)$ such that
%
\[ |g'(c)| > \frac{4}{(b - a)^2} \int_a^b |g(t)|\; dt. \]
\begin{solution}
	My main intuition is that the fundamental theorem of calculus enables us to relate $g'$ and $g$. If we are clever about using the fundamental theorem, we obtain the bound, namely
	%
	\begin{align*}
		\int_a^b |g(t)|\; dt &= \int_a^b \min \left( \left| \int_a^t g'(s)\; ds \right|, \left| \int_t^b g'(s) \right| \right)\; dt\\
		&\leq \max_{a < c < b} |g'(c)| \cdot \int_a^b \min(|t - a|, |t - b|)\; dt\\
		&= \max_{a < c < b} |g'(c)| \cdot \left( \int_a^{(a + b)/2} (t - a)\; dt + \int_{(a + b)/2}^b (b - t)\; dt \right)\\
		&= \max_{a < c < b} |g'(c)| \cdot \left( (b - a)^2/8 + (b - a)^2/8 \right)\\
		&= \max_{a < c < b} |g'(c)| \cdot (b-a)^2/4.
	\end{align*}
	%
	% \int_a^b |g(t)|\; dt <= int_a^{(a+b)/2} g'(s) (s - a) + int_{(a+b)/2}^b g'(s) (b - s) \]
	%
	% 
	%
	%
	% There exists c so that
	%
	% (1/(a-b)) int_a^b |g(t)|\; dt <= (c - a) g'(c) dt
	%
	% A/(a-b)((a + b)/2 - a) <= g'(c)
	% Theorem is proved if
	%
	% 1/(c-a) <= 2/(b-a)
	%
	%
	Rearranging this inequality completes the proof, except in the case that all the inequalities in this calculation are equalities (so we cannot get the strict inequality required). But this only holds if $g'$ is a constant function, hence $g$ is linear, which is impossible since $g$ is non constant, and $g(0) = g(1) = 0$.

	(Thanks to Karthik Ravishankar for this proof) There is a geometric argument for this result as well, though it is difficult to make it rigorous. If $g$ is a function which maximizes
	%
	\[ \int_a^b |g(t)|\; dt \]
	%
	subject to the constraint that $\| g' \|_{L^\infty[a,b]}$ and that $g(a) = g(b) = 0$, then we must have $|g'(x)| = 1$ for all $x \in (a,b)$ wherever $g$ is differentiable, since otherwise we could modify $g$ near this point so that $|g(t)|$ is larger at every point in this interval. In this case it is also true that $g$ must be increasing on $[0,1/2]$, and decreasing on $[1/2,1]$, since if $g'$ is negative at some point in $[0,1/2]$, then we can flip the sign of the derivative near this point, and flip the sign later on, and this will increase the value of $\int_a^b |g(t)|\; dt$. But then $g(t)$ is a triangle function, i.e. obtained by linearly interpolating the points $g(a) = 0$, $g(b) = 0$, and $g((a + b)/2) = (b - a)/2$, and then
	%
	\[ \int_a^b |g(t)|\; dt = \frac{(b - a)^2}{4}. \]
	%
	If $g$ is differentiable at every point, it can't be the function $g$ above which maximizes the integral, so we must have
	%
	\[ \int_a^b |g(t)|\; dt < \frac{(b - a)^2}{4}, \]
	%
	which completes the proof.
\end{solution}

\question (Fall 2019) If $f: \RR^n \to \RR$ is differentiable on $\RR^n - \{ 0 \}$, continuous at 0, and
%
\[ \lim_{x \to 0} \frac{\partial f}{\partial x^i}(x) = 0, \]
%
for $1 \leq i \leq n$, then $f$ is differentiable at 0.
\begin{solution}
	We claim that $\nabla f(0) = 0$, i.e. that $|f(x) - f(0)| = o(|x|)$ as $x \to 0$. To see this, we apply the mean value theorem. Assume without loss of generality that $f(0) = 0$. Fix $\varepsilon > 0$. For any $x \in \RR^n - \{ 0 \}$, since $f$ is continuous at $0$, there is $\lambda_0 \leq 1/2$ depending only on $|x|$ such that
	%
	\[ |f(\lambda_0 x)| \leq |x|^2. \]
	%
	Now we apply the mean value theorem, writing $g(\lambda) = f(\lambda x)$. The mean value theorem implies that there is $\lambda \in (\lambda_0, 1)$ such that
	%
	\[ g'(\lambda) = \frac{f(x) - f(\lambda_0 x)}{1 - \lambda_0}. \]
	%
	But
	%
	\[ g'(\lambda) = \sum_{i = 1}^n \frac{\partial f}{\partial x^i}(\lambda x) \cdot x_i. \]
	%
	If $|x|$ is suitably small, then $|\partial_i f(\lambda x)| \leq \varepsilon$, which implies that
	%
	\[ |f(x) - f(\lambda_0 x)| \leq 2 \left| \frac{f(x) - f(\lambda_0 x)}{1 - \lambda_0} \right| \lesssim \varepsilon |x|. \]
	%
	Thus
	%
	\[ |f(x)| \leq |f(\lambda_0 x)| + 2 \varepsilon |x| \leq |x|^2 + \varepsilon |x|. \]
	%
	Thus if $|x|$ is suitably small, we conclude that $|f(x)| \lesssim \varepsilon |x|$. Since $\varepsilon$ was arbitrary, this shows that $f$ is differentiable at zero, and $\nabla f(0) = 0$.
\end{solution}




\question (Spring 2017) Show there exists a constant $C > 0$ such that for any pair of sequences $\{ a_k \}$ and $\{ b_n \}$,
%
\[ \sum_{n = 1}^\infty \sum_{m = 1}^\infty \frac{a_n b_m}{n + m} \lesssim \left( \sum_{n = 1}^\infty a_n^2 \right)^{1/2} \left( \sum_{m = 1}^\infty b_m^2 \right)^{1/2}. \]
\begin{solution}
	This is a hard problem, with the simplest proof relying on some clever applications of Cauchy-Schwarz. We would naturally want to apply Cauchy Schwatz to write
	%
	\[ \left| \sum_{n,m} \frac{a_n b_m}{n + m} \right| \lesssim \left| \sum_{n = 1}^\infty \sum_{m = 1}^\infty \frac{a_n^2}{n + m} \right|^{1/2} \left| \sum_{n = 1}^\infty \sum_{m = 1}^\infty \frac{b_m^2}{n + m} \right|^{1/2}. \]
	%
	But the inner sums do not converge. The trick is to realize that $\sum_{m = 1}^\infty 1/(n+m)$ does not converge because of a lack of decay in $m$, and $\sum_{n = 1}^\infty 1/(n+m)$ does not converge because of the lack of decay in $n$. Thus we can trade off this decay before we apply Cauchy-Schwarz to obtain the bound. For some $\lambda > 0$, write
	%
	\[ a_{nm} = \frac{a_n}{\sqrt{m + n}} (n/m)^\lambda \quad\text{and}\quad b_{nm} = \frac{b_m}{\sqrt{m + n}} (m/n)^\lambda. \]
	%
	Then Cauchy-Schwarz implies that
	%
	\[ \sum_{n,m} \frac{a_n b_m}{n + m} = \sum_{n,m} a_{nm} b_{nm} \leq \left( \sum_{n,m} a_{nm}^2 \right)^{1/2} \left( \sum_{n,m} b_{nm}^2 \right)^{1/2}. \]
	%
	Now
	%
	\begin{align*}
		\sum_{n,m} a_{nm}^2 &= \sum_{n = 1}^\infty a_n^2 \sum_{m = 1}^\infty \frac{(n/m)^{2\lambda}}{n + m}\\
		&= \sum_{n = 1}^\infty a_n^2 n^{2\lambda} \sum_{m = 1}^\infty \frac{1}{m^{2\lambda} (n + m)}. 
	\end{align*}
	%
	Now
	%
	\begin{align*}
		\sum_{m = 1}^\infty \frac{1}{m^{2\lambda} (n + m)} &\lesssim \frac{1}{n} \sum_{m = 1}^n \frac{1}{m^{2\lambda}} + \sum_{m = n+1}^\infty \frac{1}{m^{1 + 2\lambda}}\\
		&\lesssim \frac{1}{n^{2\lambda}}
	\end{align*}
	%
	Thus
	%
	\[ \left( \sum_{n,m} a_{nm}^2 \right) \lesssim \sum_{n = 1}^\infty a_n^2. \]
	%
	Similarily, one can show
	%
	\[ \left( \sum_{n,m} b_{nm}^2 \right) \lesssim \sum_{n = 1}^\infty b_n^2. \]
	%
	Combining these estimates completes the proof.

	If one is less clever, but knows more analysis, there is an alternate, much more technical proof (using techniques that do not appear often in basic analysis problems). Consider the operator $T$, mapping a sequence $a(n)$ to a sequence $(Ta)(m)$ such that
	%
	\[ (Ta)(m) = \sum_{n = 1}^\infty \frac{a_n}{n+m}, \]
	%
	The Marcinkiewicz interpolation theorem (i.e. see Theorem 1.4.19 of  Grafakos' \emph{Classical Fourier Analysis}) implies that to show $T$ is bounded from $l^2(\mathbf{N})$ to $l^2(\mathbf{N})$, it suffices to show that the operator is of \emph{restricted weak type} from $l^p(\mathbf{N})$ to $l^p(\mathbf{N})$ for \emph{some value} $p < 2$, and \emph{some} value $p > 2$. By restricted weak type, we mean that $T$ satisfies a bound of the form
	%
	\[ \| T \chi_E \|_{l^{p,\infty}(\mathbf{N})} \lesssim_p N^{1/p} \]
	%
	where $\chi_E$ is the indicator function of some finite set $E \subset \{ 1,\dots, \infty \}$ of cardinality $N$, where the bound holds uniformly in $N$. And by the $l^{p,\infty}$ norm, we mean that for any $\alpha > 0$, we have a bound of the form
	%
	\[ \# \{ m : T \chi_E (m) \geq \alpha \} \lesssim_p N / \alpha^p. \]
	%
	We will actually find that $T$ is of restricted weak type for any $p \in [1,\infty)$. To obtain this result, it is simple to see that $T\chi_E \leq T\chi_N$, where $\chi_N$ is the indicator function of $\{ 1, \dots, N \}$, and so it suffices to prove that for any $\alpha > 0$, we have a bound
	%
	\[ \# \{ m : T \chi_N(m) \geq \alpha \} \lesssim_p N / \alpha^p. \]
	%
	We first claim that for all $\alpha > 0$,
	%
	\[ \# \{ m \geq N : T \chi_N(m) \geq \alpha \} \leq N / \alpha^p. \]
	%
	To get this result, we rely on the crude estimate
	%
	\[ T \chi_N(m) = \sum_{n = 1}^N \frac{1}{n + m} \leq N/m. \]
	%
	If $\alpha > 1$, then if $m \geq N$, $T \chi_N(m) \leq 1 \leq \alpha$, and thus
	%
	\[ \# \{ m \geq N : T \chi_N(m) \geq \alpha \} = 0 \leq N / \alpha^p \]
	%
	For $\alpha \leq 1$,
	%
	\[ \# \{ m \geq N: T \chi_N(m) \geq \alpha \} \leq \# \{ m : N \leq m \leq N/\alpha \} \leq N / \alpha \leq N / \alpha^p. \]
	%
	Thus it suffices to show that for all $\alpha > 0$,
	%
	\[ \# \{ m < N : T \chi_N(m) \geq \alpha \} \lesssim_p N / \alpha^p. \]
	%
	But now we use the more tight estimates, that for $m < N$,
	%
	\[ T \chi_N(m) \lesssim 1 + \ln(N/m). \]
	%
	Thus if $m < N$, and $T \chi_N(m) \geq \alpha$, then $1 + \ln(N/m) \geq c \alpha$ for some small universal constant $c > 0$, which implies that $m \leq N e^{1 - c \alpha}$, which gives that for $\alpha \geq 2/c$,
	%
	\[ \# \{ m < N : T\chi_N(m) \geq \alpha \} \leq N e^{1 - c \alpha} \leq N e^{- \alpha / 2} \lesssim N / \alpha^p. \]
	%
	For $\alpha \leq 2/c$,
	%
	\[ \# \{ m < N : T\chi_N(m) \geq \alpha \} \leq N \lesssim N / \alpha^p. \]
	%
	This completes the proof that $T$ has a restricted weak type bound for any $1 \leq p < \infty$.
\end{solution}

\end{questions}





\newpage
\section{Measure Theory Notes}

It is useful to keep in mind \emph{Littlewood's Three Principles}:
%
\begin{itemize}
	\item Every finite measure set is \emph{nearly} the union of a finite collection of disjoint sets. One instance of this principle is that if $E \subset \RR^d$ is measurable, then for any $\varepsilon > 0$, there is a disjoint family of cubes $\{ Q_i \}$ such that $|E \Delta Q_i| \leq \varepsilon$.

	\item Every measurable function is \emph{nearly} continuous. One instance of this principle is that $C_c(\RR^d)$ is dense in $L^1(\RR^d)$. A more technical instance is \emph{Lusin's theorem}, that for every measurable function $f: E_0 \to \CC$, where $|E| < \infty$, and for any $\varepsilon > 0$, there is $E \subset E_0$ with $|E \Delta E_0| \leq \varepsilon$, such that $f|_E$ is a continuous function.

	\item Every almost everywhere convergent sequence of measurable functions is \emph{nearly} uniformly convergent. One instance of this principle is \emph{Egorov's theorem}, that if $\{ f_n \}$ is a sequence of measurable functions on a finite measure set $E_0 \subset \RR^d$ converging pointwise to some function $f: E_0 \to \CC$, then for any $\varepsilon > 0$, we can find $E \subset E_0$ with $|E \Delta E_0| < \varepsilon$ such that $\{ f_n \}$ converges uniformly to $f$ on $E$.
\end{itemize}
%
Here are some other results which are often useful:
%
\begin{itemize}
	\item The \emph{monotone convergence theorem}: If $\{ f_n \}$ is a monotone sequence of non-negative measurable functions converging to some function $f$ pointwise almost everywhere as $n \to \infty$, then
	%
	\[ \lim_n \int f_n(x)\; dx = \int f(x)\; dx. \]

	\item The \emph{dominated convergence theorem}: If $\{ f_n \}$ is a sequence of measurable functions converging pointwise almost everywhere to some function $f$, and $|f_n| \leq g$ for some integrable function $g$, then
	%
	\[ \lim_n \int f_n(x)\; dx = \int f(x)\; dx. \]

	\item \emph{Fatou's lemma}, which says that if $\{ f_n \}$ are a family of non-negative, measurable functions, then
	%
	\[ \int \left( \liminf_{n \to \infty} f_n(x) \right)\; dx \leq \liminf_{n \to \infty} \left( \int f_n(x)\; dx \right). \]

	\item The \emph{Borel-Cantelli Lemma}: If $\{ E_n \}$ is a sequence of measurable sets such that
	%
	\[ \sum_{n = 1}^\infty |E_n| < \infty, \]
	%
	then $\limsup_n E_n = \bigcap_{n = 1}^\infty \bigcup_{m \geq n} E_m$ is a set of measure zero.
\end{itemize}
%
One should also have an aptitude for manipulating sets to calculate their measure, but it is difficult to summarize these techniques here: they come from practice in various problems.



\newpage
\section{Convergence of Measurable Functions Notes}

Let's discuss the notions of convergence one can have for random variables, and for measurable functions. Random variables are just measurable functions on a measure space with total measure one, so for every notion of convergence we obtain for measurable functions, we will obtain an analogous definition for random variables, which is the same definition, but written in a more probabilistic language. Let's begin by listing out the main types of convergence one can have on a measure space $\Omega$, equipped with a measure $\mu$:
%
\begin{itemize}
    \item A sequence of measurable functions $\{ f_n: \Omega \to \RR \}$ \emph{converges in measure} to a function $f: \Omega \to \RR$ if, for any $\varepsilon > 0$,
    %
    \[ \limsup_{n \to \infty} \mu \Big( \{ x \in \Omega : |f_n(x) - f(x)| \geq \varepsilon \} \Big) = 0. \]
    %
    Analogously, for a sequence of random variables $\{ X_n : \Omega \to \RR \}$, we say these random variables \emph{converge in probability} to a random variable $X: \Omega \to \RR$ if, for any $\varepsilon > 0$,
    %
    \[ \limsup_{n \to \infty} \PP \Big( |X_n - X| \geq \varepsilon \Big) = 0. \]

    \item A sequence of measurable functions $\{ f_n : \Omega \to \RR \}$ \emph{converges almost everywhere} to a measurable function $f: \Omega \to \RR$ if, there exists a measurable set $\Omega_0 \subset \Omega$ with $\mu(\Omega_0^c) = 0$, such that for any $x \in \Omega_0$, $\lim_{n \to \infty} f_n(x) = f(x)$. Equivalently,
    %
    \[ \mu \Big( \{ x \in \Omega : \limsup_{n \to \infty} |f_n(x) - f(x)| \neq 0 \} \Big) = 0. \]

    Analogously, a sequence of random variables $\{ X_n: \Omega \to \RR \}$ \emph{converges almost surely} to a random variable $X: \Omega \to \RR$ if there exists a measurable set $\Omega_0 \subset \Omega$ with $\PP(\Omega_0) = 1$ such that for any $x \in \Omega_0$, $\lim_{n \to \infty} f_n(x) = f(x)$. Equivalently,
    %
    \[ \PP \Big( \limsup_{n \to \infty} |X_n - X| \neq 0 \Big) = 0. \]

    \item A sequence of measurable functions $\{ f_n : \Omega \to \RR \}$ \emph{converges in $L^p$} to a function $f: \Omega \to \RR$ if
    %
    \[ \limsup_{n \to \infty} \int |f_n(x) - f(x)|^p\; dx = 0. \]
    %
    In order for this definition to makes sense, one normally assumes that each function in the family $\{ f_n \}$, and the limiting function $f$, lies in $L^p(\Omega)$, the space of all functions $g: \Omega \to \RR$ such that
    %
    \[ \int |g(x)|^p\; dx < \infty. \]

    A sequence of random variables $\{ X_n : \Omega \to \RR \}$ \emph{converges in $L^p$} to a random variable $X: \Omega \to \RR$ if
    %
    \[ \limsup_{n \to \infty} \EE |X_n - X|^p = 0. \]
    %
    In order for this definition to make sense, one normally assumes that each random variable in the family $\{ X_n \}$, and the limiting variable $X$, lies in $L^p(\Omega)$, the space of all random variables $Y$ such that $\EE |Y|^p < \infty$.
\end{itemize}

The notions of convergence in measure and convergence almost everywhere are very closely related, by virtue of the fact that they look at quantities associated with how fast a function is converging pointwise. Convergence in measure tells us that for any $\varepsilon > 0$ and $\delta > 0$, if $n$ is suitably large, then all points $x \in \Omega$ outside a set of measure $\delta$ will satisfy $|f_n(x) - f(x)| \leq \varepsilon$. Convergence almost everywhere tells us that for any point $x$ outside a set of measure zero, for any $\varepsilon > 0$, if $n$ is taken suitably large, then $|f_n(x) - f(x)| \leq \varepsilon$.

These definitions are \emph{almost} exactly the same, but the problem is in the order of the quantifiers, which means that, in general, neither condition implies the other condition. In other words, there exists sequences of functions converging in measure, but not converging almost surely, and there also exists sequences of functions converging almost surely, but not converging in measure. For convergence in measure, the value $n$ selected is allowed to depend on $\varepsilon$ and $\delta$, but not the point $x$ (except that we may throw away a set of `bad points' that has measure at most $\delta$). For convergence in measure, the value $n$ is allowed to depend on $\varepsilon$ and $x$ (but we cannot throw away a set of `bad points' with measure $\delta$ as in convergence in measure).

\subsection*{Convergence in Measure Implies Convergence Almost Everywhere}

One way convergence in measure can imply convergence almost everywhere is if one has a more quantified estimate on the rate of convergence result and applying the \emph{Borel-Cantelli Lemma}. For instance, suppose we can justify that, for some sequence $\{ f_n \}$,
%
\[ \mu \Big( \{ x \in \Omega : |f_n(x) - f(x)| \geq 1/n \} \Big) \leq 1/2^n. \]
%
If we define $E_n = \{ x \in \Omega : |f_n(x) - f(x)| \geq 1/n \}$, then we find that
%
\[ \sum_{n = 1}^\infty \mu(E_n) \leq \sum_{n = 1}^\infty 1/2^n < \infty. \]
%
Thus the Borel Cantelli Lemma implies that the complement of the set
%
\[ E^* = \bigcup_{n_0 = 1}^\infty \bigcap_{n = n_0}^\infty E_n = \limsup_{n \to \infty} E_n \]
%
is a set of measure zero. But
%
\[ E^* = \{ x \in \Omega : \limsup_{n \to \infty} n \cdot |f_n(x) - f(x)| \leq 1 \} \]
%
and so if $x \in E^*$, then $f_n(x)$ converges to $f(x)$. More explicitly, if $x \in E^*$, then there exists $n_0$ such that $x \in E_n$ for all $n \geq n_0$, which means that
%
\[ |f_n(x) - f(x)| \leq 1/n. \]
%
for all such $n$. This implies that $f_n(x) \to f(x)$ for all $x \in E^*$, and thus the sequence $\{ f_n \}$ converges to $f$ almost everywhere.

We now use another secret trick: \emph{For any sequence converging qualitatively, by taking a clever subsequence we can often introduce a more quantitative convergence}. Thus if $\{ f_n \}$ is a sequence converging in measure to a function $f$, then for any $n$, we take $\varepsilon = 1/n$. We can then choose an integer $k_n$ such that
%
\[ \mu \Big( \{ x \in \Omega: |f_{k_n}(x) - f(x)| \geq 1/n \} \Big) \leq 1/2^n. \]
%
The Borel-Cantelli method in the last paragraph can then by applied to the subsequence $\{ f_{k_n} \}$. Thus we conclude every sequence converging in measure has a \emph{subsequence} converging almost everywhere.

On the other hand, there are sequences converging in probability but not almost everywhere. The standard example is the \emph{typewriter sequence}, given for $f_n: [0,1] \to \{ 0, 1 \}$ defined to be the indicator function of the set
%
\[ E_n = \{ [x] : \log n \leq x \leq \log(n+1) \}, \]
%
where $[x]$ denotes the decimal part of $x$. Then $E_n$ has Lebesgue measure at most $\log(n+1) - \log(n) \lesssim 1/n$. This implies $\{ f_n \}$ converges to zero in measure since for $\varepsilon < 1$,
%
\[ \mu \Big( \{ x \in [0,1]: |f_n(x)| \geq \varepsilon \} \Big) = \mu(E_n) \leq 1/n, \]
%
and so
%
\[ \lim_{n \to \infty} \mu \Big( \{ x \in [0,1]: |f_n(x)| \geq \varepsilon \} \Big) = \lim_{n \to \infty} \mu(E_n) \lesssim \lim_{n \to \infty} 1/n = 0. \]
%
On the other hand, for any $x \in [0,1]$, $x$ is contained in infinitely many of the sets $E_n$ and so
%
\[ \limsup |f_n(x)| = 1. \]
%
Thus $f_n$ does not converge pointwise to zero for any particular value.

\subsection*{Convergence Almost Everywhere Implies Convergence in Measure}

What does convergence almost everywhere imply about convergence in $L^p$? Let $\{ f_n \}$ be a sequence of measurable functions converging almost surely to a function $f$, i.e. that if
%
\[ E^* = \{ x \in \Omega : \limsup_{n \to \infty} |f_n(x) - f(x)| = 0 \}, \]
%
then $\mu((E^*)^c) = 0$. We claim that \emph{in a finite measure space}, convergence almost everywhere implies convergence in measure. We must show that for any $m > 0$,
%
\[ \lim_{n \to \infty} \mu \Big ( \{ x \in \Omega : |f_n(x) - f(x)| \geq 1/m \} \Big) = 0. \]
%
The key trick here is to introduce \emph{monotonicity} into the problem. Let
%
\[ E_{n,m} = \{ x \in \Omega : |f_n(x) - f(x)| \geq 1/m \}. \]
%
If it was true that $E_{1,m} \supset E_{2,m} \supset \dots$, then the monotone convergence theorem would imply that (provided that we are in a \emph{finite} measure space) that
%
\[ \limsup_{n \to \infty} \mu(E_{n,m}) = \mu \left( \bigcap_{n = 1}^\infty E_{n,m} \right). \]
%
We have
%
\[ \bigcap_{n = 1}^\infty E_{n,m} = \left\{ x \in \Omega : \sup_{n \to \infty} |f_n(x) - f(x)| \geq 1/m \right\} \subset (E^*)^c, \]
%
and so we would conclude
%
\[ \limsup_{n \to \infty} \mu(E_{n,m}) = \mu \left( \bigcap_{n = 1}^\infty E_{n,m} \right) = 0, \]
%
which would complete the argument. Unfortunately, $\{ E_{n,m} \}$ are not monotone, but we can \emph{replace} them with bigger sets that \emph{are} monotone. Define
%
\[ F_{n,m} = \left\{ x \in \Omega: \sup_{n' \geq n} |f_{n'}(x) - f(x)| \geq 1/m \right\}, \]
%
then $E_{n,m} \subset F_{n,m}$ for all $n$ and $m$, and so
%
\[ \limsup_{n \to \infty} \mu(E_{n,m}) \leq \limsup_{n \to \infty} \mu(F_{n,m}). \]
%
It thus suffices to show the right hand side is zero for all $m$. But the sets $\{ F_{n,m} \}$ \emph{are} monotone decreasing, and so
%
\[ \limsup_{n \to \infty} \mu(F_{n,m}) = \mu \left( \bigcap_{n = 1}^\infty F_{n,m} \right). \]
%
We still have
%
\[ \bigcap_{n = 1}^\infty F_{n,m} \subset (E^*)^c, \]
%
and so the rest of the proof from before still gives us that
%
\[ \limsup_{n \to \infty} \mu(F_{n,m}) = \mu \left( \bigcap_{n = 1}^\infty F_{n,m} \right) = 0. \]
%
Thus we have shown that in a finite measure space, convergence almost everywhere implies convergence in measure. In particular, convergence almost surely implies convergence in probability. One way to remember this is through the \emph{weak} and \emph{strong} law of large numbers, since the \emph{strong} law implies the \emph{weak} law, the weak law is about convergence in probability, and the strong law is about almost everywhere convergence (so convergence almost everywhere implies convergence in probability).

On an infinite measure space, however, convergence almost surely does not imply convergence almost everywhere. For instance, on the measure space $\Omega = \{ 0, 1, \dots \}$ equipped with the counting measure, we can define
%
\[ f_n(x) = \mathbf{I}(x = n). \]
%
Then $f_n(x) \to 0$ for any $x \in \Omega$, but for $\varepsilon < 1$,
%
\[ \mu \Big( \{ x \in \Omega: |f_n(x)| \geq \varepsilon \} \Big) = \mu \Big( \{ n \} \Big) = 1. \]
%
Thus $\{ f_n \}$ does not converge to zero in measure. Similar examples exist for any infinite measure space, by taking a sequence of functions which are the indicator functions of disjoint sets of measure one.

\subsection*{Convergence in $L^p$}

Note that convergence almost everywhere and convergence in measure tell us most points are close to converging when $n$ is large, \emph{but they tell us nothing about the points that are not close to converging, except that this set is small}. We can get more control over these non-converging points by introducing the $L^p$ norms, which measure convergence `on average'. For $1 \leq p < \infty$, convergence in $L^p$ is equivalent to
%
\[ \limsup_{n \to \infty} \int |f_n(x) - f(x)|^p = 0. \]
%
This implies \emph{most points} $x$ have $|f_n(x) - f(x)|$ small, i.e. Chebyshev's inequality implies that for any $\varepsilon > 0$, if $n$ is suitably large, for all $t \geq 0$,
%
\[ \mu \Big( \{ x \in \Omega : |f_n(x) - f(x)| \geq t \} \Big) \leq \varepsilon / t^p. \]
%
Again the order of quantifiers has changed. But in this case the order of quantifiers here is stronger than convergence in measure, so that $L^p$ convergence implies convergence in measure. But it does not imply convergence almost surely for $p < \infty$ (the typewriter sequence converges in $L^p$ to zero for all $1 \leq p < \infty$). Convergence in $L^\infty$, on the other hand, \emph{does} imply convergence almost surely, again by looking at quantifiers.

Finally, we discuss the relation between convergence in various different $L^p$ spaces. On a finite measure space, $L^p$ convergence for larger $p$ implies $L^p$ convergence for lower $L^p$ (by \emph{H\"{o}lder's inequality}). The situation is reversed for `discrete' measure spaces like the integers, convergence in $L^p$ here for lower $p$ implies convergence in $L^p$ for higher $L^p$. For general measure spaces, $L^p$ convergences are disjoint from one another. Examples to show this follow by taking
%
\[ f_n(x) = H_n \mathbf{I}(x \in E_n) \]
%
for some measurable set $E_n$ with $|E_n| = W_n$ for some number $W_n$. One can check that
%
\[ \int |f_n(x)|^p = H_n^p W_n. \]
%
To show $L^{p_1}$ convergence does not imply $L^{p_2}$ convergence, it suffices to choose sequences $\{ H_n \}$ and $\{ W_n \}$ such that $H_n^{p_1} W_n \to 0$, but $H_n^{p_2} W_n$ does not converge to zero. If $p_1 < p_2$, then one will have to choose $W_n \to 0$ (which is why this argument doesn't work for `discrete spaces' like the integers), and if $p_1 > p_2$, then one will have to choose $W_n \to \infty$ (which is why this argument doesn't work for finite measure spaces).



\begin{questions}    

\newpage
\section{Day 4: Warm Up Problems}

\question  (Spring 2021 and Spring 2016)
  Let $E\subset \R$ be a Lebesgue measurable set with $|E|<\infty$. Prove that the function $f:\R\to \R$ defined by $f(r)= |E\cap (E+r)|$ is continuous.
\begin{solution}
	Here is a proof using Young's convolution inequality. One component of this inequality says that if $1 = 1/p + 1/q$, if $f \in L^p(\RR)$ and $g \in L^q(\RR)$, then the function
	%
	\[ (f * g)(x) = \int f(y) g(x - y)\; dy \]
	%
	is continuous. But we can write $|E \cap (E + r)| = (\mathbb{I}_E * \mathbb{I}_{-E})(r)$, and we can apply Young's convolution inequality with $\mathbb{I}_E \in L^1(\RR)$ and $\mathbb{I}_{-E} \in L^\infty(\RR)$.

    Here is a proof using methods of functional analysis. It suffices to prove that for all $s \in \RR$,
    %
    \[ \lim_{r \to s} f(r) = f(s). \]
    %
    We write
    %
    \[ f(r) = \int \mathbb{I}_E(x) \mathbb{I}_E(x + r). \]
    %
    Now
    %
    \[ f(r) - f(s) = \int \mathbb{I}_E(x) [\mathbb{I}_E(x + r) - \mathbb{I}_E(x + s)]. \]
    %
    The result then follows from the fact that
    %
    \[ \lim_{r \to s} \int | \mathbb{I}_E(x + r) - \mathbb{I}_E(x + s)|\; dx \to 0, \]
    %
    because for any $f \in L^1(\RR)$,
    %
    \[ \lim_{r \to 0} \int |f(x + r) - f(x)|\; dx = 0. \]
    %
    To see that this equation is true, we note it is certainly true for any $f \in C_c^\infty(\RR)$, which is a dense subfamily of $L^1(\RR)$. But the operators $A_r f(x) = f(x + r) - f(x)$ are uniformly bounded in $L^1(\RR)$, i.e. $\| A_r f \|_{L^1(\RR)} \leq 2 \| f \|_{L^1(\RR)}$. And thus the result follows by the uniform boundedness theorem.

    We can also obtain a proof using Littlewood's first principle, that every measurable set is \emph{almost} a union of intervals. For any $\varepsilon > 0$, we can find disjoint intervals $I_1,\dots,I_N$ such that if $E_0 = I_1 \cup \cdots \cup I_N$, then $|E \Delta E_0| \leq \varepsilon$. This implies that for any $t \in \RR$,
    %
    \[ \big| |E \cap (E + t)| - |E_0 \cap (E_0 + t)| \big| \leq 10 \varepsilon.  \]
    %
    Now 
    %
    \[ |E_0 \cap (E_0 + r + s)| = \int_{E_0} \mathbb{I}_{E_0 + r}(x - s)\; dx, \]
    %
    Aside from points $x$ on the boundary of the intervals defining $E_0$ and $E_0 + r$ (of which there are finitely many), $\mathbb{I}_{E_0 + r}(x - s) \to \mathbb{I}_{E_0 + r}(x)$ as $s \to 0$. Since $E_0$ is  finite set, and $\mathbb{I}_{E_0 + r} \leq 1$, where $1$ is an integrable function on $E_0$ (since $E_0$ is a finite measure set), it follows that as $s \to 0$,
    %
    \[ \lim_{E_0} \mathbb{I}_{E_0 + r}(x - s)\; dx \to \int_{E_0} \mathbb{I}_{E_0 + r}(x)\; dx. \]
    %
    Thus it follows that we can find $\delta > 0$ so that for $|s| \leq \delta$,
    %
    \[ \big| |E_0 \cap (E_0 + r + s)| - |E_0 \cap (E_0 + r)| \big| \leq \varepsilon, \]
    %
    Since we then conclude from these inequalities that
    %
    \[ \limsup_{s \to 0}\big| |E \cap (E + r + s)| - |E \cap (E + r)| \big| \leq 100 \varepsilon,  \]
    %
    and taking $\varepsilon \to 0$ gives that
    %
    \[ \limsup_{s \to 0}\big| |E \cap (E + r + s)| - |E \cap (E + r)| \big| = 0,  \]
    %
    proving continuity of $f(r)$ at $r$.

    We can also obtain a proof using Littlewood's third principle, that every measurable function is \emph{almost} continuous. For any $\varepsilon > 0$, we can find $E_0 \subset E$ so that $\mathbb{I}_E|_{E_0}$ is continuous on $E_0$, and $|E \Delta E_0| \leq \varepsilon$. So
    %
    \[ |f(t) - \int_{E_0} \mathbb{I}_{E_0}(x - t) | \leq 10 \varepsilon. \]
    %
    Since $\mathbb{I}_{E_0}$ is continuous on $E_0$, $\mathbb{I}_{E_0}(x - r - s)$ converges pointwise to $\mathbb{I}_{E_0}(x - r)$ on $E_0$ as $s \to 0$, and combined with the dominated convergence theorem (and the finite measure of $E_0$, this implies that
    %
    \[ \lim_{s \to 0} \int_{E_0} \mathbb{I}_{E_0}(x - r - s) = \int_{E_0} \mathbb{I}_{E_0}(x - r)\; dx. \]
    %
    Thus
    %
    \[ |\limsup_{s \to 0} f(r + s) - f(r)| \leq 20\varepsilon \]
    %
    and taking $\varepsilon \to 0$ yields the claim.
    \begin{comment}
    As $r \to s$, the integrand tends to zero pointwise. And moreover, the integrands are uniformly dominated by $2\mathbf{I}_E$, which is an integrable function. Thus the dominated convergence theorem implies that
    %
    \[ \lim_{r \to s} f(r) - f(s) = 0. \]
    %
    Thus $f$ is a continuous function.

  Let $\epsilon>0$. Since $E$ is measurable and $|E|<\infty$, $\chi_{E}$ is integrable. Since $C_{c}(\R)$ is dense in $L^{1}(\R)$, there exists $g\in C_{c}(\R)$ such that $\norm{g-\chi_{E}}_{L^{1}(\R)}<\epsilon/3$. Since $g$ is continuous and compactly supported, it is unformly continuous. Therefore (again using the fact that $|E|<\infty$) we may choose $\delta>0$ such
  \begin{equation}
    |g(x)-g(y)|< \frac{\epsilon}{3|E|}\label{eq:uniform-continuity}
  \end{equation}
  whenever $|x-y|<\delta$.
Let $r,s\in \R$ such that $|s-r|<\delta$. Writing $f(r)= \int_{\R}\chi_{E}(x)\chi_{E}(x-r)dx$, we have
\begin{align*}
  f(r)-f(s) & = \int_{\R}\chi_{E}(x)\left[\chi_{E}(x-r)-\chi_{E}(x-s)\right]dx\\
            & = \int_{\R}\chi_{E}(x)\left[\chi_{E}(x-r)-g(x-r)+g(x-r)-g(x-s)+g(x-s)-\chi_{E}(x-s)\right]dx.
\end{align*}
Therefore by the triangle inequality,
\begin{align*}
  |f(r)-f(s)| &\leq \int_{\R}\chi_{E}(x)|\chi_{E}(x-r)-g(x-r)|dx + \int_{\R}\chi_{E}(x)|g(x-r)-g(x-s)|dx \\ 
  &\quad +\int_{\R}\chi_{E}|g(x-s)-\chi_{E}(x-s)|dx.\\
              &\leq \int_{\R}|\chi_{E}(x-r)-g(x-r)|dx + \int_{E}|g(x-r)-g(x-s)|dx \\\quad &+\int_{\R}|g(x-s)-\chi_{E}(x-s)|dx.
\end{align*}
By translation invariance (i.e. do a u-substitution), the first and third integrals both equal  $\int_{\R}|\chi_{E}(x)-g(x)|dx < \epsilon/3$. By \eqref{eq:uniform-continuity}, the second integral is less than $\epsilon/3$. Therefore $|f(r)-f(s)|<\epsilon$. We have shown that $\delta$ responds to the $\epsilon$-challenge in the definition of continuity. Therefore $f$ is continuous.
\end{comment}
\end{solution}

\newpage
\section{Day 4: Measure Theory}


\question (Spring 2015) Does there exists a Borel measurable function $f: \RR \to [0,\infty)$ such that
%
\[ \int_a^b f(x)\; dx = \infty \]
%
for all real numbers $a < b$. Find an example or show that no such function exists.
\begin{solution}
    I construct such a function. There are many ways of doing this. We note that it suffices to construct such a function $f: [0,1] \to [0,\infty)$, since one can piece together a Borel measurable function on $\RR$ by patching together translates of this function.
    
    For each rational number $x \in [0,1]$ expressed in the simplest form as $p/q$, let
    %
    \[ E_x = \{ y \in \RR: |y - p/q| \leq 1/q^3 \}. \]
    %
    Then
    %
    \[ \sum_{x \in \mathbf{Q} \cap [0,1]} |E_x| \leq \sum_{q = 1}^\infty \sum_{p = 1}^q 2/q^3 = \sum_{q = 1}^\infty 2/q^2 < \infty. \]
    %
    The Borel-Cantelli lemma implies that $E = \limsup_x E_x$ is a set of measure zero. We set
    %
    \[ f(x) = \mathbf{I}(x \not \in E) \sum_{p/q \in \mathbf{Q} \cap [0,1]} q^4 \cdot \mathbf{I}(x \in (p/q - 1/q^3, p/q + 1/q^3)). \]
    %
    If $x \not \in E$, then $x$ lies in at most finitely many of the intervals $(p/q - 1/q^3, p/q + 1/q^3)$, so $f(x)$ is finite. Moreover, $f$ is a limit of simple functions, and is therefore measurable. And if $a < b$, there exists arbitrarily large denominators $q$ for which there is a simple fraction $p/q$ with
    %
    \[ a < p/q - 1/q^3 < p/q + 1/q^3 < b. \]
    %
    On $(p/q - 1/q^3, p/q + 1/q^3) \cap E^c$, we have $f(x) \geq q^4$, and thus
    %
    \[ \int_a^b f(x)\; dx \geq \int_{p/q - 1/q^3}^{p/q + 1/q^3} q^4 = 2q. \]
    %
    Taking $q \to \infty$ shows that
    %
    \[ \int_a^b f(x)\; dx = \infty, \]
    %
    which yields the claim.

    I would be interested in whether one can obtain the existence of an example using the Baire category theorem applied to a suitably complete metric space $X$. Let me know if you come up with one.
\end{solution}

\question (Fall 2018) Two parts:

    \begin{parts}
    \part Give an example, with explanation, of each of the following:
        \begin{itemize}
        \item A sequence of functions on $\R$ that converges to zero in $L^1(\R)$, but it does not converge almost anywhere on $\R$ to any function. 
        \item A sequence of functions in $L^1(\R)$ that converges almost everywhere to zero, but it does not converge in measure to any function. 
        \end{itemize}
  \begin{solution} 
  	For the first part of (a), It suffices to construct such a function on $L^1([0,1))$, since one can patch together functions onto all of $\RR$. Given $a \in \RR$, let $[a] \in [0,1)$ be the unique value such that $a - [a] \in \ZZ$. Define
  	%
  	\[ H_n = \sum_{k = 1}^n \frac{1}{k}. \]
  	%
  	Define $I_n = \{ [x] \in [0,1): x \in [H_n,H_{n+1}] \}$ and then let $f_n = \chi_{I_n}$. These functions are known as the \emph{typewriter sequence}, for some reason. Then
  	%
  	\[ \| f_n \|_{L^1(\RR)} = H_{n+1} - H_n \lesssim 1/n. \]
  	%
  	Thus the sequence $\{ f_n \}$ converges to zero in $L^1(\RR)$. However, since $H_n \to \infty$, $\limsup I_n = [0,1)$, which means that for any $x \in [0,1)$,
  	%
  	\[ \liminf_{n \to \infty} f_n(x) = 1. \]
  	%
  	Thus for any $x \in [0,1]$, $\{ f_n(x) \}$ does not converge as $n \to \infty$. Thus the sequence of functions $\{ f_n \}$ does not converge pointwise to zero at any point in $[0,1)$.

  	For the second part, let $I_n = [n,n+1]$, and let $f_n = \chi_{I_n}$. The sequence $\{ f_n \}$ is then the required example. This is the classic ``moving bump'' counterexample.
  \end{solution}    
        
    \part Prove that a sequence of functions on $\R$ that converges to zero in measure must have a subsequence that converges to zero almost everywhere. Do not quote any theorems that trivialize the problem. 
    
    \begin{solution}
    By definition of convergence in measure, there exists a subsequence $(n_k)$ such that 
    $$|\{ x: f_{n_k}(x)>2^{-k} \}|< 2^{-k}$$
    for every $k\geq1$. Therefore by the Borel-Cantelli lemma, it follows that 
    $$|\{x: f_{n_k}(x) > 2^{-k} \mathrm{\ i.o.}\}|=0.$$ 
    In other words, for a.e. $x\in\R$, there exists an integer $k_0$ (possibly depending on $x$) such that $f_{n_k}(x)<2^{-k}$ whenever $k\geq k_0$. Therefore $\lim_{k\to\infty} f_{n_k}(x)= 0$ a.e.
    \end{solution}
    \end{parts}
    
    
\question (Spring 2017)
    Let $f: [0,\infty) \to \RR$ be a continuously differentiable function for which $\| f' \|_\infty < \infty$. Define, for $x > 0$,
    %
    \[ F(x) = \int_0^\infty f(x + yx) \psi(y)\; dy, \]
    %
    where $\psi$ satisfies
    %
    \[ \int_0^\infty |\psi(y)|\; dy \quad\text{and}\quad \int_0^\infty y \cdot |\psi(y)|\; dy < \infty. \]
    %
    Show that $F(x)$ is well defined for all $x \geq 0$, and that $F$ is continuously differentiable.
\begin{solution}
    The fundamental theorem of calculus implies that
    %
    \[ |f(x + yx)| = \left| f(x) + \int_x^{x+yx} f'(t)\; dt \right| = f(x) + O(yx). \]
    %
    Thus combined with the fact that $\int_0^\infty |\psi(y)|\; dy < \infty$ and $\int_0^\infty y \cdot |\psi(y)|\; dy < \infty$, this implies that
    %
    \[ \int_0^\infty |f(x + yx)| |\psi(y)|\; dy \lesssim |f(x)| + |x| < \infty, \]
    %
    so $F$ is well defined.
    
    To show that $F$ is continuously differentiable, we note that for a fixed $x \in [0,\infty)$, we calculate that, for $h$ with $0 \leq x + h$,
    %
    \begin{align*}
        \frac{F(x+h) - F(x)}{h} &= \int_0^\infty \frac{|f(x+yx + h(1 + y)) - f(x + yx)|}{h} \psi(y)\; dy.
    \end{align*}
    %
    Now
    %
    \[ \left| \frac{|f(x+yx + h(1 + y)) - f(x + yx)|}{h} \psi(y) \right| \lesssim (1 + y) |\psi(y)|. \]
    %
    Since $(1 + y) \psi(y)$ is integrable, the dominated convergence theorem implies that $F$ is differentiable at $x$, and
    %
    \[ F'(x) = \int_0^\infty f'(x + yx) (1 + y) \psi(y)\; dy. \]
    %
    Finally, we show $F'$ is continuous. We note that for any $\varepsilon > 0$, there exists $R > 0$ such that
    %
    \[ \int_R^\infty (1 + y) |\psi(y)|\; dy \leq \varepsilon. \]
    %
    Since $f'$ is continuous, it is uniformly continuous on $[0,2R(1 + x)]$. Thus there exists $\delta > 0$ such that for $|h| \leq \delta$, and $0 \leq y \leq R$, $|f'((x + yx) + h(1 + y)) - f(x + yx)| \leq \varepsilon$, and so
    %
    \begin{align*}
        F'(x+h) - F(x) &\lesssim \varepsilon + \int_0^R [f'((x + yx) + h(1 + y)) - f'(x + yx)] (1 + y) \psi(y)\; dy\\
        &\lesssim \varepsilon + \int_0^R \varepsilon (1 + y) \psi(y)\; dy \lesssim \varepsilon.
    \end{align*}
\end{solution}

\question (Fall 2016) Let $f:[0,1]\to \R$ be continuous with $\min_{0\leq x\leq 1} f(x) = 0$. Assume that for any $0\leq a\leq b\leq 1$ we have
%
\[ \int_{a}^{b}[f(x)-\min_{a\leq y\leq b}f(y)]dx \leq \frac{|b-a|}{2}. \]
%
Prove that for any $\lambda\geq 0$, we have
\begin{equation*}
\left| \left\{ x:f(x)>\lambda+1 \right\} \right|\leq \frac{1}{2} \left| \left\{ f(x)>\lambda) \right\} \right|.
\end{equation*}

\begin{solution}
  Since $f$ is continuous, $\left\{x: f(x)>\lambda \right\}$ is open. Therefore we may write $\left\{x: f(x)>\lambda \right\} = \bigcup_{i=1}^{\infty}I_{i}$ where $(I_{i})_{i=1}^{\infty}$ is a countable pairwise disjoint sequence of intervals $I_{i}=(a_{i},b_{i})$ with $a_{i}\leq b_{i}$.

  We claim that for each $i$,
  %
  \[ \inf_{t \in \overline{I}_i} f(t) \leq \lambda. \]
  %
  If $I_i = \emptyset$, there is nothing to show. If $a_i = 0$ and $b_i = 1$, then by hypothesis, $\min_{[a_{i},b_{i}]}f =\min_{[0,1]}f = 0\leq \lambda$. So assume that is not the case; i.e., that at least one of $a_{i},b_{i}$ is in $(0,1)$. Without loss of generality, assume that $a_i \in (0,1)$. Then $a_i \in \partial \left\{ x: f(x)\leq \lambda \right\}$. Hence by continuity of $f$, we have $f(a_{i})\leq \lambda$.

  Now that this has been proved, we have:
  \begin{align*}
    \frac{1}{2}|I_{i}| &\geq \int_{I_{i}} \left\{ f(x)-\min_{t\in {\bar{I}_{i}}}f(t) \right\} dx
    &&\text{by assumption}\\
                       &\geq\int_{I_{i}\cap \left\{ f > \lambda+1 \right\}} \left\{ f(x)-\min_{t\in{\bar{I}_{i}}}f(t) \right\} dx
    &&\text{by nonnegative of the integrand}\\
                       &\geq \int_{I_{i}\cap \left\{ f > \lambda+1 \right\}} (f(x)-\lambda) dx
    &&\text{by the claim}\\
                       &\geq \int_{I_{i}\cap \left\{ f > \lambda+1 \right\}} ((\lambda+1)-\lambda) dx
    &&\text{by domain of integration}\\
                       &= \left|I_{i} \cap \left\{  f> \lambda+1\right\} \right|.
  \end{align*}
  By countable additivity for disjoint sets, summing over $i$ and using $\left\{f>\lambda+1 \right\}\subseteq
 \left\{f>\lambda \right\}$ gives
  \begin{equation*}
    \frac{1}{2}\left| \left\{f >\lambda \right\} \right|  \geq \left| \left\{ f>\lambda \right\}\cap \left\{ f>\lambda+1 \right\} \right| = \left| \left\{ f>\lambda+1 \right\} \right|.
  \end{equation*}
\end{solution}




\newpage
\section{Functional Analysis Notes}

Regarding the functional analysis problems, the best advice I can give is to be very familiar with applying the major results and theorems listed on the syllabus, especially in their simpler cases. Rarely do questions require knowledge of the most general statements of these theorems. Here are theorems stated in the forms most likely to be useful (based on my judgement of past qualifying problems). You may find it useful to compile a similar list for other major functional analysis results. Here we state the four main theorems of basic functional analysis:
%
\begin{itemize}
	\item The \emph{Closed Graph Theorem} says that if $T: X \to Y$ is a linear map between Banach spaces, then $T$ is bounded / continuous if and only if the \emph{graph}
	%
	\[ G(T) = \{ (x,y) \in X \times Y: Tx = y \} \]
	%
	is closed. In practice, the closed graph theorem is most useful when attempting to show that a linear operator $T: X \to Y$ is bounded. Working directly from the definitions, to show that a linear operator $T$ is bounded, it suffices to show that for any sequence $\{ x_n \}$ in $X$ which converges to zero, the seqeuence $\{ Tx_n \}$ converges to zero. The closed graph theorem is useful because it allows us to assume the \emph{additional constraint} on our sequence $\{ x_n \}$, i.e. that $\{ Tx_n \}$ converges to some $y \in Y$, and the problem is then converted into showing that $y = 0$.

	\item The \emph{Open Mapping Theorem} is mostly used on the exam in the following form: If $T: X \to Y$ is a bounded linear bijection between Banach spaces, then $T^{-1}: Y \to X$ is a bounded linear operator. Quantitatively, the open mapping theorem says that if $T$ is a bounded, linear bijection, then there exists $C > 0$ such that for any $x \in X$,
	%
	\[ C^{-1} \| x \|_X \leq \| Tx \|_Y \leq C \| x \|_X, \]
	%
	i.e. $T$ roughly preserves the magnitude of vectors.

	\item The \emph{Hahn-Banach Theorem} says that if $Y$ is a subspace of a norm space $X$, and if $\phi: Y \to \RR$ is a bounded linear functional, then there is an extension of $\phi$ to a map $\tilde{\phi}: X \to \RR$ with $\| \phi \| = \| \tilde{\phi} \|$. The most common use of the Hahn-Banach theorem is that to show two vectors $x_1,x_2$ in a Banach space $X$ are equal to one another, then it suffices to show that $\phi(x_1) = \phi(x_2)$ for all $\phi \in X^*$.

	\item The most commonly used of the four main theorems on qual problems is the \emph{uniform boundedness principle}, which states that for a collection of linear operators $\{ T_\alpha: X \to Y \}$ on a Banach space $X$ satisfy a \emph{uniform bound}
	%
	\[ \sup_\alpha \| T_\alpha x \| \lesssim \| x \| \]
	%
	if and only if they satisfy a \emph{pointwise bound}
	%
	\[ \sup_\alpha \| T_\alpha x \| < \infty \]
	%
	for all $x \in X$.
\end{itemize}

Here are some useful results about finite dimensional norm spaces that are useful to keep in mind:
%
\begin{itemize}
	\item If $\dim(X) < \infty$, then \emph{all norms on $X$ are equivalent}. In other words, for any two norms $\| \cdot \|_1$ and $\| \cdot \|_2$ on $X$, there exists $C > 0$ such that
	%
	\[ (1/C) \| x \|_1 \leq \| x \|_2 \leq C \| x \|_1. \]

	\item If $X$ is a finite dimensional subspace of a norm space $Y$, then $X$ is closed in $Y$.

	\item If $X$ is a finite dimensional subspace of a norm space $Y$, then there exists a closed subspace $X'$ of $Y$ such that $Y = X \oplus X'$.
\end{itemize}




\newpage
\section{Day 5: Warm Up Problems}

\question (Spring 2015) Let $f \in L^2[0,1]$ satisfy $\int_0^1 t^n f(t)\; dt = (n+2)^{-1}$ for $n = 0, 1,\dots$. Must then $f(t) = t$ for almost every $t \in [0,1]$?
\begin{solution}
    The identity $\int_0^1 t^n f(t)\; dt = (n+2)^{-1}$ shows that for any polymonial $p(t)$,
    %
    \[ \int_0^1 p(t) [f(t) - t]\; dt = 0. \]
    %
    Applying some type of density theorem (the Stone-Weirstrass theorem, and its variants), we find that the family of all polynomials is dense in $L^2[0,1]$. Applying the continuity of the map $g \mapsto \int_0^1 g(t) [f(t) - t]$, we conclude that for all $g \in L^2[0,1]$,
    %
    \[ \int_0^1 g(t) [f(t) - t]\; dt = 0. \]
    %
    But setting $g(t) = \overline{f(t) - t}$, we conclude that
    %
    \[ \int_0^1 |f(t) - t|^2\; dt = 0. \]
    %
    This implies $f(t) = t$ for almost every $t$.
\end{solution}

\question (Fall 2021) Let $\{ f_n \}$ be a sequence of monotonic functions on $[0,1]$ converging to a function $f$ in measure. Show that $f$ coincides almost everywhere with a monotonic function $f_0$, and that $f_n(x) \to f_0(x)$ at every point of continuity of $f_0$.
\begin{solution}
	If the sequence $\{ f_n \}$ converges uniformly, then for each $0 \leq x < y \leq 1$, we would have
	%
	\[ f(y) - f(x) = \lim_n f_n(y) - f_n(x) \geq 0. \]
	%
	Thus $f$ is monotonic. This sequence does \emph{not} converge uniformly. But we utilize one of \emph{Littlewood's three principles}, namely, that every convergent sequence of functions is nearly uniformly convergent.

	We do this by first giving us a $\varepsilon$ of room. It suffices to show that for each $\varepsilon$, there exists a set $E(\varepsilon)$ such that if $x,y \in E(\varepsilon)$ and $x < y$, then
	%
	\[ f(y) - f(x) \geq - 2 \varepsilon. \]
	%
	If we set
	%
	\[ E_n(\varepsilon) = \{ x \in [0,1] : |f_n(x) - f(x)| \geq \varepsilon \} \]
	%
	then $|E_n(\varepsilon)| \to 1$. If $x < y$, with $x,y \in E_n$ for some $n$, then
	%
	\[ f(y) - f(x) \geq [f_n(y) - f_n(x)] - 2 \varepsilon \geq -2\varepsilon. \]
	%
	Thus if $E(\varepsilon) = \bigcup_n E_n(\varepsilon)$, then $|E(\varepsilon)^c| = 0$, and for $x,y \in E(\varepsilon)$ with $x < y$,
	%
	\[ f(y) - f(x) \geq - 2 \varepsilon. \]
	%
	The set $E = \bigcap E(1/2^k)$ is also a set with $|E^c| = 0$, and for any $x,y \in E$ with $x < y$,
	%
	\[ f(y) - f(x) \geq - 2 / 2^k \]
	%
	for all $k > 0$, so taking $k \to \infty$ gives
	%
	\[ f(y) - f(x) \geq 0. \]
	%
	Thus $f$ is monotonic when restricted to $E$, and thus agrees with a monotonic function $f_0: [0,1] \to \RR$ on $E$.

	It now suffices to show that $f_n(x_0) \to f_0(x_0)$, whenever $x_0$ is a continuity point of $f_0$. Here we use the same Littlewood's principle more explicitly, i.e. using Egorov's theorem: for any $\delta > 0$, there is $E_\delta \subset [0,1]$ with $|E_\delta|^c < \delta/2$, such that $f_n \to f_0$ uniformly on $E_\delta$. If $x_0$ is a continuity point of $f_0$, then for any $\varepsilon > 0$, there exists $\delta > 0$ such that if $|x - x_0| \leq \delta$, $|f(x) - f(x_0)| \leq \varepsilon$. Since $|E_\delta|^c < \delta/2$, there must exist
	%
	\[ x_0 - \delta < x_1 < x_0 < x_2 < x_0 + \delta \]
	%
	with $x_1,x_2 \in E_\delta$. But this implies that
	%
	\[ \limsup_n f_n(x_0) \leq \limsup_n f_n(x_2) = f_0(x_2) \leq f_0(x_0) + \varepsilon \]
	%
	and
	%
	\[ \liminf_n f_n(x_0) \geq \liminf_n f_n(x_1) = f_0(x_1) \geq f_0(x_0) - \varepsilon. \]
	%
	Since $\varepsilon$ was arbitrary, it is clear from these inequalities that $f_n(x_0)$ converges to $f_0(x_0)$.
\end{solution}

\newpage
\section{Day 5: Functional Analysis}

\question (Fall 2015) Find all $f \in L^2[0,\pi]$ such that
%
\[ \int_0^\pi |f(x) - \sin x|^2\; dx \leq \frac{4\pi}{9} \]
%
and
%
\[ \int_0^\pi |f(x) - \cos x|^2\; dx \leq \frac{\pi}{9} \]
\begin{solution}
    We note that $\sin x$ and $\cos x$ are orthogonal in $L^2[0,1]$. Thus we may consider a family of orthogonal vectors $\{ e_n \}$ such that $\{ \sin x, \cos x \} \cup \{ e_n \}$ is an orthogonal basis for $L^2[0,1]$. Thus there exists constants $a$, $b$, and $\{ c_n \}$ such that
    %
    \[ f(x) = a \sin x + b \cos x + \sum c_n e_n, \]
    %
    where the convergence of this sum is in the $L^2$ norm. Applying Parseval's theorem, we find that
    %
    \begin{align*}
        \int_0^\pi |f(x) - \sin x|^2\; dx &= |a - 1|^2 \int_0^\pi |\sin x|^2\; dx + |b|^2 \int_0^\pi |\cos x|^2\; dx + \sum |c_n|^2\\
        &= |a - 1|^2 (\pi/2) + |b|^2 (\pi/2) + \sum |c_n|^2.
    \end{align*}
    %
    Similarily,
    %
    \begin{align*}
        \int_0^\pi |f(x) - \cos x|^2\; dx &= |a|^2 (\pi/2) + |b - 1|^2 (\pi/2) + \sum_n |c_n|^2.
    \end{align*}
    %
    Next, we note that
    %
    \[ \int_0^\pi |\sin x - \cos x|^2 = \pi. \]
    %
    The triangle inequality thus implies that for any $f$ satisfying the requirements above, we actually have \emph{equality}, i.e.
    %
    \[ \int_0^\pi |f(x) - \sin x|^2\; dx = \frac{4\pi}{9} \]
    %
    and
    %
    \[ |f(x) - \cos x|^2\; dx = \frac{\pi}{9} \]
    %
    Thus we must find all families of coefficients $a$, $b$, and $\{ c_n \}$ such that
    %
    \[ |a - 1|^2 (\pi/2) + |b|^2 (\pi / 2) + \sum |c_n|^2 = 4 \pi / 9 \]
    %
    and
    %
    \[ |a|^2 (\pi/2) + |b - 1|^2 (\pi / 2) + \sum |c_n|^2 = \pi / 9. \]
    %
    Subtracting the second identity from the first, then multiplying by $2/\pi$, we conclude that
    %
    \[ (|a - 1|^2 - |a|^2) + (|b|^2 - |b-1|^2) = 2/3. \]
    %
    This equation can be simplified to
    %
    \[ \text{Re}(b) = \text{Re}(a) + 1/3. \]
    %
    %
    Substituting this into the first equation, we find that if $a = x + iy$, $b = (x + 1/3) + iz$, and $C = 2/\pi \sum c_n^2$, then
    %
    \[ 2x^2 - (4/3)x + (y^2 + z^2 + 2/9 + C) = 0. \]
    %
    The quadratic formula implies that there exists $x$ satisfying this equation for a fixed $y$ and $z$ if
    %
    \[ 16/9 - 8(y^2 + z^2 + 2/9 + C) \geq 0, \]
    %
    which can be rearranged to read
    %
    \[ 8(y^2 + z^2 + C) \leq 0. \]
    %
    This can only occur if $y = z = C = 0$, and it then follows from the quadratic formula that $x = 1/6$. Thus the only function $f$ satisfying this result is the function
    %
    \[ f(x) = (1/6) \sin x + (1/2) \cos x. \]
    %
    This completes the argument.

    Alternatively, let $u(x)= \sin(x)-f(x)$ and $v(x)=f(x)-\cos(x)$. Then

\begin{equation*}
  \norm{u+v}_{L^{2}}^{2} = \int_{0}^{\pi}\left( \sin(x)+\cos(x) \right)^{2}dx = \int_{0}^{\pi} \sin^{2}(x)+ \cos^{2}(x)dx =\pi.
\end{equation*}
By the triangle inequality and the hypotheses of the problem,

\begin{equation*}
  \sqrt{\pi}=\norm{u+v}_{L^{2}} \leq \norm{u}_{L^{2}} + \norm{v}_{L^{2}} \leq \frac{2}{3}\sqrt{\pi}+ \frac{1}{3}\sqrt{\pi}=\sqrt{\pi}.
\end{equation*}
Therefore
$\norm{u+v}_{L^{2}} = \norm{u}_{L^{2}} + \norm{v}_{L^{2}}$. Therefore
\begin{equation*}
  \left( \norm{u}_{L^{2}}+ \norm{v}_{L^{2}} \right)^{2} = \norm{u+v}_{L^{2}}^{2} = \langle u+v, u+v\rangle = \norm{u}^{2}_{L^{2}}+\norm{v}^{2}_{L^{2}} + 2\langle u,v\rangle.
\end{equation*}
Therefore since $  \left( \norm{u}_{L^{2}}+ \norm{v}_{L^{2}} \right)^{2}= \norm{u}_{L^{2}}^{2}+\norm{v}_{L^{2}}^{2}+2 \norm{u}_{L^{2}}\norm{v}_{L^{2}}$, it follows that $\norm{u}_{L^{2}}\norm{v}_{L^{2}}= \langle u, v \rangle$. Therefore by the Cauchy Schwarz inequality, $u$ and $v$ are linearly dependent (viz. the condition for equality in the statement of Cauchy Schwarz). That is, there exists $\alpha\in \R$ such that $\sin(x)-f(x)= \alpha \left( f(x)-\cos(x) \right)$ almost everywhere. Solving for $f$ gives
\begin{equation}\label{eq:form-of-f}
  f(x) = \frac{1}{1+\alpha}\sin(x)+ \frac{\alpha}{1+\alpha}\cos(x).
\end{equation}
It remains to determine $\alpha$. Since $\norm{u}_{L^{2}}+ \norm{v}_{L^{2}}=\sqrt{\pi}$, $\norm{u}_{L^{2}}\leq 2 \sqrt{\pi}/3$, and $\norm{v}_{L^{2}}\leq \sqrt{\pi}/3$, it follows that $\norm{u}^2_{L^{2}} = 4 \pi/9$. Therefore
\begin{align*}
  \frac{4}{9}\pi &= \int_{0}^{\pi}\left( \sin(x)- \frac{1}{1+\alpha}\sin(x) -\frac{\alpha}{1+\alpha}\cos(x)\right)^{2}dx\\
                 &= \left( \frac{\alpha}{1+\alpha} \right)^{2}\int_{0}^{\pi}\left( \sin(x)-\cos(x) \right)^{2}dx\\
                 &=  \left( \frac{\alpha}{1+\alpha} \right)^{2}\pi.
\end{align*}
This implies $\alpha = 2$ or $\alpha= -2/5$. A similar calculation using $\norm{v}^{2}_{L^{2}}= \pi /9$ implies that $\alpha = 2$ or $\alpha = -4$. The only value for $\alpha$ consistent with both of these conditions is $\alpha =2$. Therefore by \eqref{eq:form-of-f},
\begin{equation*}
  f(x) = \frac{1}{3}\sin(x)+ \frac{2}{3}\cos(x).
\end{equation*}
\end{solution}

\question (Spring 2017)
    Let $l^1(\mathbf{N})$ be the space of summable sequences, i.e.
    %
    \[ l^1(\mathbf{N}) = \{ x : \sum_{n = 1}^\infty |x_n| < \infty \}. \]
    %
    Let $\{ a_n \}$ be a sequence with $a_n \geq 0$ for all $n \in \mathbf{N}$ and consider the subset $K \subset l^1(\mathbf{N})$ defined by
    %
    \[ K = \{ x \in l^1(\mathbf{N}) : 0 \leq x_n \leq a_n\ \text{for all $n$} \}. \]
    %
    Show that $K$ is compact if and only if the sequence $\{ a_n \}$ itself belongs to $l^1(\mathbf{N})$.
\begin{solution}
    First suppose $K$ is compact. Then consider the family of sequences
    %
    \[ \{ x(k) = (a_1,\dots,a_k,0,\dots) \}. \]
    %
    Each sequence $x(k)$ belongs to $K$. Thus by compactness, this sequence has a convergent subsequence $x(k_i)$ converging to some sequence in $l^1(\mathbf{N})$. But $x(k_i)$ converges pointwise to $a$, and so $x(k_i)$ can only converge to $a$, which implies $a \in l^1(\mathbf{N})$.
    
    On the other hand, suppose $a \in l^1(\mathbf{N})$. Consider an arbitrary sequence $\{ x(k) \}$ in $K$. For any $N > 0$, the projection of $K$ onto the first $N$ elements of the sequence is compact, and thus we may inductively find subsequences $x(k_{N,i})$ for each $N$ such that $\{ k_{N,i} \}$ is a subsequence of $\{ k_{M,i} \}$ for each $M \leq N$, and the first $N$ elements of the sequences $x(k_{N,i})$ converge pointwise. Consider the subsequence of sequences $y(n) = x(k_{n,n})$. For each $N > 0$, this sequence is a subsequence of the sequence $\{ x(k_{N,i} \}$ and so we can conclude from this that $\{ y(n) \}$ converges pointwise to some sequence $y$. Since $K$ is defined pointwise, it is simple to see that $y \in K$. For each $\varepsilon > 0$, we pick $N > 0$ such that
    %
    \[ \sum_{k > N} a_k < \varepsilon. \]
    %
    Then exists $M$ such that for $n > M$, and $1 \leq k \leq N$, $|y(n)_k - y_k| \leq \varepsilon / N$. And thus
    %
    \[ \| y(n) - y \|_1 \leq \sum_{k = 1}^N |y(n)_k - y_k| + \sum_{n > N} |y(n)_k| + |y_k| \leq \sum_{k = 1}^N (\varepsilon / N) + \sum_{n > N} 2a_n \leq 3\varepsilon. \]
    %
    Taking $\varepsilon \to 0$ completes the proof.
\end{solution}

\question (Spring 2018, Spring 2021)
  Let $K$ be a continuous function on $[0,1]\times [0,1]$. Suppose that $g$ is a continuous function on $[0,1]$. Show that there exists a unique continuous function $f$ on $[0,1]$ such that
  \begin{equation*}
    f(x)=g(x)+\int_{0}^{x}f(y)K(x,y)dy
  \end{equation*}

  \begin{solution}
  	Define an operator $S$ on $C[0,1]$ by setting
  	%
  	\[ Sf(x) = \int_0^x K(x,y) f(y)\; dy. \]
  	%
    We can show that for any $f_{1},f_{2}\in C[0,1]$ and any $x\in [0,1]$,
    \begin{align*}
      |S^n f(x)|
      &\leq \norm{f}_{L^\infty[0,1]} M^{n}\int_{0}^{x}\int_{0}^{x_{1}}\cdots \int_{0}^{x_{n-1}}x_{n}dx_{n}\cdots dx_{1}\\
      &=\norm{f}_{L^\infty[0,1]} \frac{M^n x^{n}}{n! }\\
      &\leq \norm{f}_{L^\infty[0,1]} \frac{M^{n}}{n!}.
    \end{align*}
    %
    Thus $\| S^n \|_{L^\infty \to L^\infty}$ decays quickly as $n \to \infty$, which implies that we can employ the Neumann series to prove that the operator $I - S$ is invertible on $C[0,1]$, with
    %
    \[ (I - S)^{-1} = \sum_{k = 0}^\infty S^k. \]
    %
    But then for any $g \in C[0,1]$, the function $f = (I - S)^{-1} g$ is the unique function in $C[0,1]$ which solves the given equation.

    On the other hand, we can obtain another solution by applying the contraction mapping theorem. Define an integral operator $T$ by setting
    %
    \[ Tf(x) = g(x)+\int_{0}^{x}f(y)K(x,y)dy. \]
    %
    Since $g$ is continuous, $T:X\to X$ where $X=C[0,1]$, a Banach space with respect to the norm $\norm{f}_{L^\infty[0,1]} = \max_{x\in[0,1]} |f(x)|$. It suffices to show there is a unique $f$ such that $Tf = f$. Since $K$ is continuous and $[0,1]\times[0,1]$ is compact, there exists $M$ such that $|K(x,y)|\leq M$ for all $x,y\in [0,1]\times[0,1]$. The operator is \emph{not} a contraction operator on $C[0,1]$ if $M$ is sufficiently large. The fix is to consider $T$ as an operator on ``smaller'' Banach spaces, e.g. $C[0,x_1], C[x_1,x_2], \ldots$ etc, where $x_i-x_{i-1}$ are small enough. Given $x_0 < x_1$, define
    %
    \[ T_{x_0 x_1}(f)= g(x)+\int_{0}^{x}f(y)K(x,y)dy \]
    %
    on $C([x_0,x_1])$. Then
    %
    \[ \| T_{x_0 x_1} f_1 - T_{x_0 x_1} f_2 \|_{L^\infty[x_0,x_1]} \leq \int_{x_0}^{x_1} |K(x,y)|\; dy \cdot \| f_1 - f_2 \|_{L^\infty[x_0,x_1]} \leq M(x_1 - x_0) \| f_1 - f_2 \|_{L^\infty[0,x_0]}. \]
    %
    Thus if $x_1 - x_0 < 1/2M$, then we conclude that $T_{x_0 x_1}$ is a contraction map on $C([x_0,x_1])$. This means that for any such interval, there exists a unique $f_{x_0x_1} \in C[x_0,x_1]$ such that $T_{x_0 x_1} f = f$. But clearly if $f$ satisfies this property, then it's restriction to any subinterval also satisfies this property. This implies that any two solutions $f_{x_0x_1}$ and $f_{x_2x_3}$ agree on $[x_0,x_1] \cap [x_2,x_3]$. But the locality of continuity means we can patch these functions together to find $f \in C[0,1]$ such that $T_{x_0x_1} f = f$ for any $x_0$ and $x_1$ with $x_1 - x_0 < 1/2M$. But this implies that $Tf = f$.
\end{solution}

\question Let $\phi: \RR \to \RR$ be a continuous function with compact support.
\begin{parts}
    \part Prove that there exists a constant $A$ such that
    %
    \[ \| f * \phi \|_q \leq A \| f \|_p \]
    %
    for $1 \leq p \leq q \leq \infty$.
\begin{solution}
    Let us begin by showing that
    %
    \[ \| f * \phi \|_\infty \leq A \| f \|_p. \]
    %
    Indeed, H\"{o}lder's inequality implies that if $p^*$ is the conjugate to $p$, then
    %
    \begin{align*}
        \| f * \phi \|_{L^\infty_x} &= \left\| \int f(x-y) \phi(y)\; dy \right\|_{L^\infty_x}\\
        &\leq \left\| \left( \int |\phi(y)|^{p^*}\; dy \right)^{1/p^*} \cdot \left( \int |f(x-y)|^p\; dy \right)^{1/p} \right\|_{L^\infty_x}\\
        &\leq \| \phi \|_{L^{p^*}} \| f \|_{L^p}.
    \end{align*}
    %
    Next, we calculate using Minkowski's integral inequality that
    %
    \[ \| f * \phi \|_{L^p_x} = \left\| \int f(x-y) \phi(y)\; dy \right\|_{L^p_x} \leq \int |\phi(y)| \| f \|_{L^p}\; dy = \| \phi \|_{L^1} \| f \|_{L^p}. \]
    %
    Thus we have found a constant $A = \max_{1 \leq r \leq \infty} (\| \phi \|_{L^r})$ such that the result holds for $(p,q)$ of the form $(p,\infty)$, and of the form $(p,p)$. The remaining estimates lie `in-between' this range. One approach here is to apply the Riesz-Thorin interpolation theorem, which gives all the intermediary bounds $(p,q)$ for $p \leq q \leq \infty$. Alternatively, a sneaky application of H\"{o}lder's inequality shows that for $q \geq p$, if we fix $\alpha \in (0,1)$ such that $\alpha q = p$, then
    %
    \begin{align*}
        \| f * \phi \|_{L^q_x}^q &= \int |(f * \phi)(x)|^{\alpha q} |(f * \phi)(x)|^{(1 - \alpha)q}\\
        &\leq \left( \int |(f * \phi)(x)|^p |(f * \phi)(x)|^{(1 - \alpha)q}\; dx \right)\\
        &\leq \int |(f * \phi)(x)|^p\; dx \cdot \sup_x |(f * \phi)(x)|^{(1 - \alpha)q}\\
        &\leq \| f * \phi \|_{L^p_x}^p \| f * \phi \|_{L^\infty_x}^{(1 - \alpha)q}\\
        &\leq \left( A \| f \|_{L^p} \right)^p \left( A \| f \|_{L^p} \right)^{(1 - \alpha) q}\\
        &= A \| f \|_{L^p}.
    \end{align*}
\end{solution}
    
    \part Show by example that such general inequality cannot hold for $p > q$.
\begin{solution}
  It suffices, for any $R > 0$, to construct a function $f \in L^p$ such that
  %
  \[ \| f * \phi \|_{L^q} \geq R \| f \|_{L^p}. \]
  %
  One should think of bounds on $L^r$ for large $r$ as controlling how `peaked' a function can be, whereas bounds on $L^r$ for small $r$ control how `long' a function can be. Thus, intuitively, we must construct a function $f$ which is not peaked, but can be long (small in $L^p$ norm), into a function $f * \phi$ which is long (has large $L^q$ norm).
  
  Here is one approach that `almost' works for all $\phi$. A good choice for a non-peaked, long function is just a constant function supported on a large set. For instance, if $f(x) = \mathbf{I}(|x| \leq R)$, then $\| f \|_p \sim R^{1/p}$. If we assume that $\phi$ is supported on $|x| \leq S$, then for $|x| \leq R - S$,
  %
  \[ (\phi * f)(x) = \int \phi(x)\; dx. \]
  %
  Provided that $\int \phi(x)\; dx \neq 0$, this implies that $|(\phi * f)(x)| \gtrsim_\phi 1$ for $|x| \leq R - S$, which implies that for $R \geq 2S$,
  %
  \[ \| \phi * f \|_{L^q} \gtrsim_\phi \left( \int_{|x| \leq R - S} 1 \right)^{1/q}] \gtrsim (R - S)^{1/q} \gtrsim R^{1/q}. \]
  %
  Thus
  %
  \[ \| \phi * f \|_{L^q} \gtrsim_\phi R^{1/q - 1/p} \| f \|_{L^p}. \]
  %
  If $q < p$, then $1/q - 1/p > 0$, and so we may take $R \to \infty$ to show that the operator $Tf = \phi * f$ cannot be bounded. The only problem is that this idea cannot approach functions such that
  %
  \[ \int \phi(x)\; dx = 0. \]
  %
  If this is the case, in our examples above, $f * \phi = 0$ on $|x| \leq R - S$, which means $\| f * \phi \|_q \sim 1$, so this approach doesn't work. In order to approach this case, we choose $f$ more carefully so that $f * \phi$ cannot average out to zero on a large set. This choice is left to the reader.
  
  Here is an alternate approach, following \emph{Littlewood's Principle}. Fix a large integer $N > 0$, and set
  %
  \[ f(x) = \sum_{n = -N}^N \overline{\phi(2R \cdot n-x)}, \]
  %
  where we assume $\phi(x) = 0$ for $|x| \geq R$.  Then $f$ is not `peaked' (because $\phi$ is continuous, and not peaked), but supported on a very wide set. Indeed, we find that
  %
  \[ \| f \|_p \sim_\phi N^{1/p}. \]
  %
  For $|n| \leq N$, we compute that
  %
  \[ (\phi * f)(2Rn) = \int \phi(x) f(2Rn - x) = \int \phi(x) \overline{\phi(x)} = \| \phi \|_{L^1}. \]
  %
  By continuity of the quantities involved, there exists $\delta$ such that for $|x| \leq \delta$, $|(\phi * f)(x)| \geq \| \phi \|_{L^1} / 2$. Thus $\phi * f$ is non-zero on  a very wide set. More precisely, $|(\phi * f)| \gtrsim \| \phi \|_1$ on $N$ intervals of radius $\delta$, so
  %
  \[ \| \phi * f \|_q \gtrsim (N \delta)^{1/q} \| \phi \|_{L^1} \gtrsim_\delta N^{1/q}. \]
  %
  %
  Thus
  %
  \[ \| \phi * f \|_q \gtrsim_{\phi,\delta} N^{1/q - 1/p} \| f \|_p. \]
  %
  Taking $N \to \infty$ thus shows that the operator is unbounded.
\end{solution}
\end{parts}


\item (Fall 2016)
  Give an example of a non-empty closed subset of $L^{2}([0,1])$ that does not contain a vector of smallest norm. Prove your assertion.

\begin{solution}
	Let $\{ e_n \}$ be an orthonormal basis for $L^2[0,1]$, and define
	%
	\[ x_n = (1 + 1/n) e_n. \]
	%
	Then $\{ x_n \}$ is a closed subset of $L^2[0,1]$ (it is actually \emph{discrete}), and has no element of smallest norm.

\begin{comment}
  Let $e_{n} = \sqrt{2}\sin(n\pi x)$ for $n=1,2,\ldots$, and define $x_{n}= \left(\frac{n+1}{n}\right)e_{n}$. Then $\{x_{n}\}_{n=1}$ is a nonempty closed subset with no element of smallest norm. To prove this, we first prove two claims:

  

  \vt
  \noindent \textit{Claim 1:} $\{e_{n}\}_{n\geq 1}$ is an orthonormal set in $L^{2}([0,1])$.
  \begin{proof}[Proof of Claim 1:]
    Need to show two things: (1) that $\norm{e_{n}}_{L^{2}([0,1])}=1$ for all $n$, and (2) that $\langle e_{n},e_{m}\rangle$ whenever $m\neq n$. These can be shown by direct computation using trig identities:

    Suppose $n\geq 1$. Then using $\sin^{2}(\theta) = \frac{1}{2}\left( 1-\cos(2\theta) \right)$,

    \begin{equation*}
      \norm{e_{n}}_{L^{2}([0,1])}^{2} = \int_{0}^{1}2\sin^{2}(n\pi x)dx = \int_{0}^{1}1-\cos(n\pi x)dx=1.
    \end{equation*}
    This proves (1).
    Next, suppose $m\neq n$. Using $\sin(A)\sin(B) = \frac{1}{2}\left(\cos(A-B)-\cos(A+B)  \right)$,
    \begin{align*}
      \langle e_{n},e_{m}\rangle
      &= \int_{0}^{1}2\sin(n\pi x)\sin(m\pi x)dx\\
      &= \int_{0}^{1} \cos((n-m)\pi x) - \cos((n+m)\pi x)dx\\
      &=0.
    \end{align*}
    This proves (2), which completes the proof of the claim.
  \end{proof}

  \vt
  \noindent  \textit{Claim 2:} If $m\neq n$ then $\norm{x_{n}-x_{m}}_{L^{2}([0,1])}\geq \sqrt{2}$.
  \begin{proof}[Proof of Claim 2:]
    Suppose $m\neq n$. Denote $c_{n}= \frac{n+1}{n}$. Then 
    \begin{align*}
      \norm{x_{n}-x_{m}}^{2}_{L^{2}([0,1])}
      &= \langle x_{n}-x_{m}, x_{n}-x_{m}\rangle\\
      &= \langle c_{n} e_{n}-c_{m} e_{m},  c_{n}e_{n}- c_{m}e_{m}\rangle\\
      &=c_{n}^{2}\langle e_{n},e_{n}\rangle -2c_{m}c_{n}\langle e_{n},e_{m}\rangle + c_{m}^{2}\langle e_{m},e_{m}\rangle\\
      &= c_{n}^{2}+ c_{m}^{2}
    \end{align*}
    where the final equality is justified by Claim 1. It follows that
    \begin{equation*}
       \norm{x_{n}-x_{m}}_{L^{2}([0,1])} = \sqrt{c_{n}^{2}+c_{m}^{2}} = \sqrt{\left(1+ \frac{1}{n} \right)^{2}  + \left( 1+\frac{1}{m} \right)^{2}}\geq \sqrt{2}.
     \end{equation*}
     This proves claim 2.
  \end{proof}



Using Claim 1, we see that $\norm{x_{n}}_{L^2([0,1])} = \left( \frac{n+1}{n} \right)\norm{e_{n}}_{L^{2}([0,1])} = 1+\frac{1}{n}$, and from this it is clear that $(x_{n})$ has no element of smallest norm. The fact that $x_{n}$ is closed follows from Claim 2, which implies that any convergent sequence consisting of elements in $\left\{ x_{n} \right\}_{n}$ is eventually constant, and hence its limit point must again be an element in $\left\{ x_{n} \right\}_{n}$.
\end{comment}

  
\end{solution}






\newpage
\section{Day 6: Warm Up Problems}

\question Let $X$ be the set of all real-valued polynomials in a single variable. Prove that there does not exist a norm $\| \cdot \|$ on $X$ so that $(X, \| \cdot \|)$ is a Banach space.
\begin{solution}
	The fundamental reason is that $X$ has a countable basis, and no Banach space can have an infinite, countable basis. For each $n$, let $X_n$ be the vector space of polynomials of degree at most $n$. Then $X_n$ is a finite dimensional subspace of $X$. If $\| \cdot \|$ is any norm on $X$ which makes $X$ into a Banach space, then $X_n$ is closed (any finite dimensional subspace of a Banach space is closed). Since $X = \bigcup X_n$, the Baire category theorem implies that one of the sets $X_n$ has non-empty interior for some $n$. So $X_n$ contains a unit ball, which by translation, we may assume to be at the origin. So $X_n$ contains a ball $B_\delta = \{ x \in X: \| x \| < \delta \}$ for some $\delta > 0$. If $f \in X$ is a polynomial of degree greater than $n$ with $\| f \| = a$ for some $a > 0$, then the polynomial $g = (\delta / 2 a) f$ is contained in $B_\delta$, and thus $X_n$, but has degree greater than $n$, so that we reach a contradiction.
\end{solution}




\newpage
\section{Day 6: Functional Analysis}

\question (Fall 2019) Show that there is no sequence $\{ a_n \}$ of positive numbers such that $\sum a_n |c_n| < \infty$ if and only if $\{ c_n \}$ is a bounded sequence. Hint: Suppose that there exists a sequence and consider the map $T: l^\infty \to l^1$ given by $Tf(n) = a_n f(n)$. The set of $f$ such that $f(n) = 0$ for all but finitely many $n$ is dense in $l^1$ but not in $l^\infty$.
\begin{solution}
	We will prove the result by contradiction. Suppose such a sequence exists, and consider the operator $T$ defined in the hint. Clearly $T$ is linear. Also, $T$ is injective, since all elements of the sequence $\{ a_n \}$ are positive. We also claim that $T$ is surjective, given the property assumed of the sequence. Indeed, if $\sum b_n < \infty$, and we define $c_n = b_n/a_n$, then $\sum a_n |c_n| < \infty$, and thus $\{ c_n \}$ is a bounded sequence, and $Tc = b$. Finally, we claim that $T$ is bounded. Indeed, we have
    %
    \[ \| Tf \|_{l^1} = \sum |Tf(n)| = \sum |a_n| |f(n)| \leq \left( \sum |a_n| \right) \| f \|_{l^\infty}. \]
   
   %
   Thus we conclude that $T$ is an isomorphism (a consequence of the open mapping theorem is that any bounded, bijective linear oeprator is an isomorphism).  One can now argue that no isomorphism exists between $l^1$ and $l^\infty$ by finding a property that must be shared by isomorphic Banach spaces, but is not shared by $l^1$ and $l^\infty$; indeed, $l^1$ is separable, and $l^\infty$ is not separable, so the two spaces cannot be isomorphic.

   One can also argue more concretely. Because $T$ is an isomorphism, there exists a constants $C_1,C_2 > 0$ such that
   %
   \[ C_1 \| c \|_{l^\infty} \leq \| Tc \|_{l^1} \leq C_2 \| c \|_{l^\infty}. \]
   %
   Our goal is now to find a sequence $c$ so that the $l^1$ norm of $Tc$ is small, but the $l^\infty$ norm of $c$ is large. A simple way to do this is to set $c = (0,\dots,0,1,0,\dots)$, with only a single non-zero entry in the $n$th position. Then $Tc = (0,\dots,0,a_n,0,\dots)$, and so
   %
   \[ C_1 \leq |a_n| \leq C_2. \]
   %
   On the other hand, if $c = (1,\dots,1,1,1,\dots)$ with ones in each position, then $c \in l^\infty$, so $Tc = a$ lies in $l^1$. But the above inequality shows that $a$ \emph{cannot} be in $l^1$, because the sequence is lower bounded by $C_1 > 0$, and so $\sum |a_n| \geq \sum C_1 = \infty$.
\end{solution}

\question (Fall 2020)

Suppose that $X,Y$ and $Z$ are Banach spaces, and $T:X\times Y\to Z$ is a mapping such that:
\begin{enumerate}[(a)]
\item For each $x\in X$, the map $y\mapsto T(x,y)$ is a bounded linear map $Y\to Z$.
\item For each $y\in Y$, the map $x\mapsto T(x,y)$ is a bounded linear map $X\to Z$.
\end{enumerate}
Prove there exists a constant $C$ such that
\begin{equation*}
\norm{T(x,y)}_{Z}\leq C \norm{x}_{X}\norm{y}_{Y}
\end{equation*}

\begin{solution}
  Without loss of generality we may assume that $x,y$ are nonzero since if either $x=0$ or $y=0$, the inequality holds for any $C$. Further, we claim that it suffices to show the inequality holds for all $x,y$ satisfying $\norm{x}_{X}=1$ and $\norm{y}_{Y}=1.$ To see this, suppose $\norm{T(\tilde{x},\tilde{y})}_{Z}\leq C$ for all $\tilde{x}\in X$ and $\tilde{y}\in Y$ with $\norm{\tilde{x}}_{X}=1$, $\norm{\tilde{y}}_{Y}=1$. Then for any nonzero $x\in X$, $y\in Y$,
  \begin{equation*}
    \norm{T(x,y)}_{Z}= \norm{x}_{X}\norm{y}_{Y}\norm{T \left( \frac{x}{\norm{x}_{X}},\frac{y}{\norm{y}_{Y}} \right)}_{Z} \leq C \norm{x}_{X}\norm{y}_{Y}.
  \end{equation*}
  This proves the claim.

  Next, let for each $x\in X$, let $\phi_{x}:Y\to Z$ denote the map $y\mapsto T(x,y)$, and for each $y\in Y$, let $\psi_{y}:X\to Z$ denote the map $x\mapsto T(x,y)$. Let $B= \left\{ x\in X: \norm{x}_{X}=1 \right\}$. Then since $\phi_{x}(y)=T(x,y)=\psi_{y}(x)$ and since $\psi_{y}$ is assumed to be a bounded operator, it holds for all $y\in Y$ that
  \begin{equation*}
    \sup_{x\in B}\norm{\phi_{x}(y)}_{Z} = \sup_{x\in B}\norm{\psi_{y}(x)}_{Z} \leq \sup_{x\in B}\norm{\psi_{y}} \norm{x}_{X} = \norm{\psi_{y}} <\infty.
  \end{equation*}
  Therefore by the Uniform Boundedness Principle,
  \begin{equation*}
    \sup_{x\in B}\norm{\phi_{x}}= C <\infty.
  \end{equation*}
  Therefore for any $x,y$ with $\norm{x}_{X}=1$ and $\norm{y}_{Y}=1$,
  \begin{equation*}
    \norm{T(x,y)}_{Z} = \norm{\phi_{x}(y)}_{Z}\leq \norm{\phi_{x}}\norm{y}_{Y} = \norm{\phi_{x}} \leq C.
  \end{equation*}
\end{solution}

\question (Fall 2015) For $p \in (1,\infty)$, and for $f \in L^p(\RR)$ define
%
\[ Tf(x) = \int_0^1 f(x + y)\; dy. \]
%
\begin{parts}
    \part Show that $\| Tf \|_p \leq \| f \|_p$ and equality holds if and only if $f = 0$ almost everywhere.
    \begin{solution}
        We use Minkowski's inequality. For those unfamiliar, recall the triangle inequality, which for $1 \leq p \leq \infty$ says that
        %
        \[ \big\| \sum_n f_n \big\|_p \leq \sum_n \| f_n \|_p. \]
        %
        Minkowski's inequality occurs when we replace the sum with an integral, i.e. for $1 \leq p \leq \infty$,
        %
        \[ \left\| \int f_y\; dy \right\|_p \leq \int \| f_y \|_p\; dy. \]
        %
        In this case, we set $f_y(x) = f(x + y)$. Then
        %
        \[ Tf(x) = \int_0^1 f_y(x)\; dy. \]
        %
        Thus, using the fact that $\| f_y \|_p = \| f \|_p$, we conclude
        %
        \[ \| Tf \|_p = \left\| \int_0^1 f_y\; dy \right\|_p \leq \int_0^1 \| f_y \|_p\; dy = \int_0^1 \| f \|_p\; dy = \| f \|_p. \]
        %
        To address the case of equality, we recall the case of the triangle inequality, i.e. that
        %
        \[ \big\| \sum_n f_n \big\|_p = \sum_n \| f_n \|_p \]
        %
        if and only if $\{ f_n \}$ are all scalar multiples of one another. Similarily, \emph{Minkowski's inequality} is an equality if and only if $\{ f_y \}$ are all scalar multiples of one another \emph{almost everywhere}. Thus if $\| Tf \|_p = \| f \|_p$, then the application of Minkowski's inequality is an equality, which implies that there exists $c(y)$ such that $f_y(x) = c(y) f(x)$, i.e. for each $y$, $f(x + y) = c(y) f(x)$ for almost every $x$.

        This gives a function with very strange properties if $f \neq 0$. There are several ways of showing that assuming $f \neq 0$ leads to a contradiction. If
        %
        \[ \int_{-\infty}^\infty |f(x)|^p\; dx \neq 0, \]
        %
        Then we can find some $x_0 \in \RR$ with $f(x_0) \neq 0$. The dominated convergence theorem implies that for any $x \in \RR$,
        %
        \begin{align*}
            \| f \|_p^p &= \lim_{n \to \infty} \sum_{k = -n}^n \int_0^1 |f(x_0 + ky)|^p\; dy\\
            &= \lim_{n \to \infty} |f(x_0)|^p \int_0^1 \sum_{k = -n}^n |c(y)|^k\; dy.
        \end{align*}
        %
        The sum
        %
        \[ \sum_{k = -n}^n |c(y)|^k \]
        %
        converges monotonically in $n$ to $\infty$ everywhere except when $c(y) = 0$, where it is constantly zero in $n$. Thus by the monotone convergence theorem, if $c$ does not equal zero almost everywhere on $[0,1]$, then
        %
        \[ \| f \|_p^p = \infty, \]
        %
        which gives a contradiction. Thus $c = 0$ almost everywhere on $[0,1]$, and so as a result, $f = 0$ almost everywhere.

        Alternatively, for each $y$,
        %
        \begin{equation*}
          \int_{-\infty}^{\infty}\left| f(x) \right|^{p}dx 
          = \int_{-\infty}^{\infty}\left| f(x+y) \right|^{p}dx 
          = \int_{-\infty}^{\infty}\left| c(y)f(x) \right|^{p}dx 
          = |c(y)|^{p}\int_{-\infty}^{\infty}\left| f(x) \right|^{p}dx 
        \end{equation*}
        %
        If $f \neq 0$, then $|c(y)| = 1$ for all $y \in [0,1]$. But this means that for almost every $x \in \RR$, $|f(x+y)| = |c(y)| |f(x)| = |f(x)|$, and thus there exists $a \geq 0$ such that $|f(x)| = a$ for almost all $x \in \RR$. Since $f \in L^p(\RR)$, we must have $a = 0$, and thus $f = 0$ almost everywhere.
    \end{solution}
    
    \part (Fall 2015) Prove that the map $f \mapsto Tf - f$ does not map $L^p(\RR)$ onto $L^p(\RR)$.
    \begin{solution}
        First, we claim that $Tf - f$ is injective. Indeed, if $Tf = f$, then $\| Tf \|_p = \| f \|_p$, and by the last part of the problem, this implies that $f = 0$. If $Tf - f$ was surjective, then the open mapping theorem would therefore imply that there would exist a constant $C > 0$ such that for all $f \in L^p(\RR)$, $\| Tf - f \|_p \geq C \| f \|_p$. Our goal is to show this is not true.
        
        Intuitively, we wish to find a function $f$ such that $Tf - f$ is small. This would be possible if $Tf \approx f$, which would hold if $f$ was approximately constant. So we consider the `almost' constant function $f(x) = \mathbf{I}(|x| \leq R)$. Then $Tf(x)$ is supported on $|x| \leq R + 1$, $Tf(x) = f(x)$ for $|x| \leq R - 1$, and $\| Tf \|_\infty \leq 1$. Thus $Tf - f$ is supported on a set of total mass $O(1)$, and $\| Tf - f \|_\infty = O(1)$, so
        %
        \[ \| Tf - f \|_p \lesssim 1,  \]
        %
        independently of $R$. On the other hand, $\| f \|_p \sim R^{1/p}$, so it is impossible for a bound of the form $\| Tf - f \|_p \geq C \| f \|_p$ to hold.
    \end{solution}
\end{parts}



\question (Fall 2014)
\begin{parts}
\part For any $n \geq 1$ an integer, there exists two positive measures $\mu_{1}^{n},\mu_{2}^{n}$ supported on $[0,1]$ such that for any polynomial $P(x)$ with $\deg P(x)\leq n$ it holds:
\begin{equation*}
  P'(0) = \int_{0}^{1}P(x)d\mu^{n}_{1}(x)-\int_{0}^{1}P(x)d\mu_{2}^{n}(x).
\end{equation*}

\begin{solution}
	Let $X_n$ be the space of all real-valued polynomials on $[0,1]$ with degree at most $n$. Then $X_n$ is a finite dimensional linear subspace of $C[0,1]$. Let $T: X_n \to \RR$ be the operator $TP = P'(0)$. Then $T$ is bounded on $X_n$ since $X_n$ is finite dimensional. Thus by the Hahn-Banach theorem, we can find an extension of $T$ to a bounded functional on $C[0,1]$. The \emph{Reisz Representation Theorem} says that $C[0,1]^*$ is the space of all finite signed Borel measures on $[0,1]$. Thus there exists a finite signed Borel measure $\mu$ on $[0,1]$ such that for any $f \in C[0,1]$,
	%
	\[ Tf = \int f(x) d\mu(x). \]
	%
	In particular, if $P \in X_n$,
	%
	\[ P'(0) = \int P(x) d\mu(x). \]
	%
	Then we perform a \emph{Jordan decomposition} to write $\mu = \mu_1 - \mu_2$, where $\mu_1$ and $\mu_2$ are finite non-negative Borel measures, and this completes the construction.

	One can also define the measures explicitly, though the argument becomes more technical. This is Chun's solution from when they taught the SEP in 2016. We will show that there exists constants $c_{j}\in \R, x_{j}\in (0,1)$ with $0\leq j\leq n$ such that the measure $\mu^{n}$ defined by
  \begin{equation}\label{eq:mu-form}
    \mu^{n}:= \sum_{j=0}^{n}c_{j}\delta_{x_{j}}
  \end{equation}
  satisfies
  %
  \[ \int_0^1 P(x) d\mu^{(n)}(x) = P'(0) \]
  %
  for any polynomial $P \in X_n$. The result will then follow by taking
  \begin{equation*}
    \mu_{1}^{n} = \sum_{j=0}^{n}c_{j}^{+}\delta_{a_{j}} \quad \text{and}\quad    \mu_{2}^{n} = \sum_{j=0}^{n}c_{j}^{-}\delta_{a_{j}}
  \end{equation*}
  where $c_{j}^{+}=\max\{c_{j},0\}$ and $c^{-}= \max\{-c_{j},0\}$.

  Let us determine what conditions the measure $\mu^n$ must satisfy. If $P(x)=\sum_{k=0}^{n}a_{k}x^{k}$, then $P'(0)=a_{1}$. By \eqref{eq:mu-form}, $\int_{0}^{1}x^{k}d\mu^{n}(x) = \sum_{j=0}^{n}c_{j}x_{j}^{k}$ whenever $0\leq k\leq n$, and hence
  \begin{equation*}
    \int_{0}^{1}P(x)d\mu^{n}(x)
    = \int_{0}^{1}\sum_{k=0}^{n}a_{k}x^{k}d\mu^{n}(x)
    = \sum_{k=0}^{n}a_{k}\int_{0}^{1}x^{k}d\mu^{n}(x)
    = \sum_{k=0}^{n}\sum_{j=0}^{n}a_{k}c_{j}x_{j}^{k}.
  \end{equation*}
  Therefore $\int_{0}^{1}f(x)d\mu^{n}(x)=P'(0)$ holds if and only if
  \begin{equation*}
    \sum_{k=0}^{n}a_{k}\sum_{j=0}^{n}x_{j}^{k}=a_{1}
  \end{equation*}
  or equivalently,
  \begin{equation*}
    \sum_{j=0}^{n}c_{j}x_{j}^{k}=\left\{ \begin{array}{l@{\quad:\quad}l}
                                           1  &k=1 \\
                                           0  &k\neq 1
                                         \end{array}\right.
  \end{equation*}
  or equivalently
  \begin{equation}\label{eq:matrix-equation}
    \begin{pmatrix}
      1 & 1 & \cdots & 1 \\
      x_{0} & x_{1} & \cdots & x_{n} \\
      x_{0}^{2} & x_{1}^{2} & \cdots & x_{n}^{2}\\ 
      \vdots  & \vdots  & \ddots & \vdots  \\
      x_{0}^{n} & x_{1}^{n} & \cdots & x_{n}^{n} 
    \end{pmatrix}
    \begin{pmatrix}
      c_{0}\\
      c_{1}\\
      c_{2}\\
      \vdots\\
      c_{n}\\
    \end{pmatrix}
    =
    \begin{pmatrix}
      0\\
      1\\
      0\\
      \vdots\\
      0\\
    \end{pmatrix}.
  \end{equation}
  Define 
  \begin{equation*}
    Q(x_{0},\ldots,x_{n}):= \det
    \begin{pmatrix}
      1 & 1 & \cdots & 1 \\
      x_{0} & x_{1} & \cdots & x_{n} \\
      x_{0}^{2} & x_{1}^{2} & \cdots & x_{n}^{2}\\ 
      \vdots  & \vdots  & \ddots & \vdots  \\
      x_{0}^{n} & x_{1}^{n} & \cdots & x_{n}^{n} 
    \end{pmatrix}.
  \end{equation*}
  %
  The matrix equation necessarily has a solution $\vec{c}=(c_{0},\ldots,c_{n})\in \R^{n+1}$ if and only if we have $Q(x_{0},\ldots,x_{n})\neq 0$. Therefore to prove the existence of the desired $\mu^{n}$, it will suffice to show there exists $(x_{0},\ldots,x_{n})\in (0,1)^{n+1}$ such that $Q(x_{0},\ldots,x_{n})\neq 0$. But this follows because $Q$ is a polynomial, hence an entire function on $\CC^{n+1}$, and because it's derivatives do not vanish at the origin, i.e.
  %
  \[ \partial_{x_1} \partial_{x_2}^2 \dots \partial_{x_n}^n Q(0) \neq 0. \]
  %
  Thus $Q \neq 0$, so $Q$ cannot vanish on any open set. In particular, it cannot vanish on $(0,1)^{n+1}$.
\end{solution}

\part Does there exist two finite positive measures $\mu_{1},\mu_{2}$ supported on $[0,1]$ such that for any polynomial $P(x)$, it holds
\begin{equation*}
  P'(0) = \int_{0}^{1}P(x)d\mu_{1}(x) - \int_{0}^{1}P(x)d\mu_{2}(x)?
\end{equation*}

\begin{solution}
    There does not exist such a pair of measures. If the measures existed, and if $\mu = \mu_1 - \mu_2$, then $\mu$ corresponds to a continuous linear function $T$ on $C[0,1]$, such that $Tf = f'(0)$ for any polynomial $f$. But then $T$ cannot be continuous, for if $f_n = (1 - x)^n$, then $\| f_n \|_{L^\infty[0,1]} = 1$ for all $n$, whereas $Tf_n = n$.
\end{solution}
\end{parts}








\newpage
\section{Baire Category Notes}

The \emph{Baire Category Theorem} says that if $X$ is a complete metric space, then for any sequence $\{ U_n \}$ of open, dense subsets of $X$, $\bigcap_n U_n$ is dense in $X$. To make things less abstract, I like to think through this result in terms of logical properties:
%
\begin{itemize}
	\item A logical property $P$ of points in $X$ is \emph{stable} if, whenever $P(x_0)$ is true for some $x_0 \in X$, then $P(x)$ is true for $x$ in a neighborhood of $x_0$.

	\item A logical property $P$ of points in $X$ is \emph{unstable} if, whenever $P(x_0)$ is true for some $x_0 \in X$, then for any $\varepsilon > 0$, we can find $x \in X$ with $d(x,x_0) < \varepsilon$, such that $P(x)$ is false. One advantage of being quantitative here is that it suffices to show that the property is unstable for $x_0$ in a \emph{dense} subspace $X_0$ of $X$.
\end{itemize}
%
The Baire category theorem then says that, given a countable family of properties $\{ P_n \}$, such that $P_n$ is a stable property, and the \emph{negation} $\neg P_n$ of the property $P_n$ is unstable, then the set of points $x \in X$ such that $P_n(x)$ is true for all $n$ is a dense subset of $X$.

To see an example of this formulation of the theorem, let us go through the classic Baire category proof that there exists a continuous function on $[0,1]$, which is differentiable nowhere. The first challenge is to find countably many properties $\{ P_n \}$ of functions in $C[0,1]$ such that a function $f$ is differentiable nowhere if and only if $P_n(f)$ is true for all $n$, though we must also be careful to choose these properties to be stable. This leads to the family of properties $\{ P_{N,M} \}$, where $N$ and $M$ are positive integers, and $P_{N,M}(f)$ is true if, for any $x_0 \in [0,1]$, there exists $x_1 \in [0,1]$ with $0 < |x_1 - x_0| < 1/M$ and with
%
\[ |f(x_0) - f(x_1)| > N|x_0 - x_1|. \]
%
Let us now show that the properties $\{ P_{N,M} \}$ are both stable, and their complement is unstable:
%
\begin{itemize}
	\item Since $[0,1]$ is compact, and for any fixed $f$, $x_0$ and $x_1$ such that the above properties hold, we can keep $x_1$ constant as we vary $x_0$ in a small neighborhood, we can find a finite collection of points $\{ x_1(1),\dots, x_1(K) \}$ such that given any $x_0 \in [0,1]$, there is $k$ such that $0 < |x_0 - x_1(k)| < 1/M$ and $|f(x_0) - f(x_1)| > N|x_0 - x_1|$. Combined with the fact that we use a \emph{strict inequality} above, this can be used to show $P_{N,M}$ is a stable property.

	\item Conversely, the negation of the property $P_{N,M}$ is unstable. To see this, we note that $C^\infty[0,1]$ is dense in $C[0,1]$. Thus to show the property is unstable, it suffices to show it is unstable at any such function $f \in C^\infty[0,1]$. Fix $T > 0$ such that $f$ is Lipschitz of order $T$ for some large quantity $T \geq 1$. For any $R \geq 2M$, define a function $g_R \in C[0,1]$ such that for $1 \leq k \leq R$, $g(k/R) = (-1)^k (10NT/R)$, and then linearly interpolating between these values. Then $\| g_R \|_{L^\infty} \leq 10NT/R$. Now consider the function $f_R = f + g_R$. Now if $1/R \leq x \leq 2/R$,
	%
	\begin{align*}
		|f_R(k/R + x) - f_R(k/R)| &\geq |g_R(k/R + x) - g_R(k/R)| - |f(k/R + x) - f(k/R)|\\
		&\geq 10NT/R - |f(k/R + x) - f(k/R)|\\
		&\geq 10TN/R - 2T/R\\
		&\geq 5TN/R > N|x|.
	\end{align*}
	%
	Since $R \geq 2M$, this justifies that $P_{N,M}(f_R)$ is true. For any $\varepsilon > 0$, if $R \geq 10NT/\varepsilon$, then $\| f_R - f \|_{L^\infty} \leq \varepsilon$. Thus we see that the property $P_{N,M}$ is unstable.
\end{itemize}
%
The Baire category theorem then implies that the set of $f \in C[0,1]$ such that $P_{N,M}(f)$ is true for all $N$ and $M$ is dense in $C[0,1]$, which proves the existence of continuous functions differentiable at no point.




\newpage
\section{Day 7: Warm Up Problems}

\question (Fall 2017) Let $f_n$ be a sequence of real functions on $\RR$ such that each $f_n'$ is continuous on $\RR$. Suppose that as $n \to \infty$, $f_n$ converges to a function $f: \RR \to \RR$ pointwise, and $f_n'$ converges to a function $g$ pointwise.

Prove that there exists a non-empty interval $(a,b)$ and a constant $L < \infty$ such that
%
\[ |f(x) - f(y)| \leq L |x-y|. \]
%
Hint: Consider the sets $K_c = \{ x: \sup_n |f_n'(x)| \leq c \}$.
\begin{solution}
    For each $x$, $f_n'(x) \to f(x)$, so that the sequence of quantities $\{ f_n(x) \}$ is bounded. It follows that if
    %
    \[ K_c = \{ x : \sup_n |f_n'(x)| \leq c \}, \]
    %
    then $\bigcup_{c = 1}^\infty K_c = \RR$. Since the supremum of continuous functions is lower semi-continuous, each $K_c$ is a closed set. And thus the \emph{Baire category theorem} implies that there exists $L$ such that $K_L$ contains an interval. Thus there is an interval $(a,b)$ such that for all $x \in (a,b)$, $\sup |f_n'(x)| \leq L$. Thus by the mean value theorem
    %
    \[ |f(x) - f(y)| = \limsup_{n \to \infty} |f_n(x) - f_n(y)| \leq \limsup_{n \to \infty} |x - y| \| f_n \|_{L^\infty(a,b)} \leq L |x - y|. \]
\end{solution}




\newpage
\section{Day 7: Baire Category}

\item (Spring 2020)
  A \textbf{Hamel basis} for a vector space $X$ is a collection $\mathcal{H}\subset X$ of vectors such that $x\in X$ can be written uniquely as a finite linear combination of elements in $\mathcal{H}$. Prove that an infinite dimensional Banach space cannot have a countable Hamel basis. (Hint: otherwise the Banach space would be first category in itself.)

\begin{solution}
  Denote the norm of $X$ by $\norm{\cdot}$. Suppose $\mathcal{H}$ is countable. Then there exists an enumeration $f_{1},f_{2},\ldots$ of $\mathcal{H}$. Let $E_{n}= \text{span}\left\{ f_{1},\ldots,f_{n} \right\}$. By definition of a Hamel basis, $X=\bigcup_{n=1}^{\infty} E_{n}$. It will suffice to show that $E_{n}$ is nowhere dense, as this will imply that $X$---which is a complete metric space with respect to the norm topology---is a countable union of nowhere dense sets, contradicting the \emph{Baire Category theorem}. But this follows because \emph{any} finite dimensional subspace of a Banach space is closed, and if $E_n$ itself had nonempty interior, then it would contain a neighborhood of the origin, which is absorbing, and thus we would conclude that $E_n = X$.

  Let us argue in slightly more detail some steps of this argument:

  \noindent \textit{Lemma 1:} Any finite dimensional subspace $Y$ of a Banach space $X$ is closed.
  \begin{proof}
  	Since $Y$ is finite dimensional, it has some basis $\{ e_1, \dots, e_n \}$. If we define a norm
  	%
  	\[ \| \sum a_i e_i \|_Y = \left( \sum |a_i|^2 \right)^{1/2}, \]
  	%
  	then $Y$ is a Banach space with this norm, because it is easily seen to be isometric to $\RR^n$ or $\CC^n$ (depending on the underlying field). But all norms on a finite dimensional vector space are equivalent, so that $\| \cdot \|_Y$ is comparable to $\| \cdot \|_X$, and this shows $Y$ is a Banach space under the norm induced by $X$. But this implies that $Y$ is closed in $X$, since any sequence $\{ y_n \}$ converging to an element $x \in X$ is Cauchy, and thus converges in $Y$.
  \end{proof}

  \noindent \textit{Lemma 2:} If $Y$ is a proper subspace of a Banach space $X$, then $Y$ has nonempty interior in $X$.
  \begin{proof}
  	For each $x_0 \in X$ and $\varepsilon > 0$, let
  	%
  	\[ B_\varepsilon(x_0) = \{ x \in X: \| x - x_0 \| < \varepsilon \}. \]
  	%
  	Suppose that there exists $y_0 \in Y$ and $\varepsilon > 0$ such that $B_\varepsilon(y_0) \subset Y$. Then
  	%
  	\[ B_\varepsilon(0) = B_\varepsilon(y_0) - y_0 \subset Y - y_0 = Y. \]
  	%
  	If $r > 0$, then
  	%
  	\[ B_r(0) = (r / \varepsilon) \cdot B_\varepsilon(0) \subset (r / \varepsilon) \cdot Y = Y. \]
  	%
  	Thus $Y$ contains every ball centered at the origin. But this clearly means $X = Y$.
  \end{proof}
\end{solution}

\item (Fall 2016) Show that there is a continuous real valued function on $[0,1]$ that is not monotone on any open interval $(a,b)\subset [0,1]$.

% \begin{solution}
%   Suppose not. Let $E_{q,n}= \left\{ f\in C([0,1]): f \text{ monotone on }(q-1/n,q+1/n) \right\}.$ Then
%   \begin{equation}\label{eq:big-union}
%     C([0,1]) = \bigcup_{q\in \Q}\bigcup_{n\geq 1}E_{q,n}.
%   \end{equation}
%   We claim that each $E_{q,n}$ is closed, indeed suppose $(f_{k})$ is a sequence of monotone function on $(q-1/n,q+1/n)$ such that $f_{k}\to f$ uniformly. Then there exists a subsequence $(k_{j})$ with $k_{j}\to \infty$ and such that $f_{k_{j}}$ is an increasing function for all $j$ or $f_{k_{j}}$ is a decreasing function for all $j$. Without loss suppose that they are increasing functions. If $x\leq y$ then $f_{k_{j}}(x)-f_{k_{j}}(y)\leq $ for all $j$, and hence sending $j\to \infty$, we have $f(x)\leq f(y)$. Thus $f$ is increasing. This proves that $E_{q,n}$ is closed.
  
%   Since $C([0,1])$ is a complete metric space and  \eqref{eq:big-union} is a countable union of close sets, it follows by Baire's theorem that there exists some $q\in Q$ and $n\geq 1$ such that $E_{q,n}$ has nonempty interior. That is, there exists $f^{*}\in E_{q,n}$ and $\epsilon>0$ such that $\left\{ f\in C([0,1]): \norm{f^{*}-f}_{\infty} < \epsilon\right\}\subset E_{q,n}.$ Therefore every $f$ with $\norm{f^{*}-f}_{\infty}<\epsilon$ is monotone on the interval $(q-1/n,q+1/n)$. But this is absurd, since any function can by approximated uniformly by zig-zag functions (draw a picture). 

% \end{solution}

\begin{solution}
  Suppose not. Then we can write $C([0,1])$ as the following countable union:
  \begin{equation}
    C([0,1]) = \bigcup_{a,b\in \Q, a<b}E_{a,b}.
  \end{equation}
  where $E_{a,b}= \left\{ f\in C([0,1]): f \text{ monotone on }(a,b) \right\}.$
  
  We claim that each $E_{a,b}$ is closed. Indeed suppose $(f_{k})$ is a sequence of monotone function on $(a,b)$ such that $f_{k}\to f$ uniformly. Then there exists a subsequence $(k_{j})$ with $k_{j}\to \infty$ and such that $f_{k_{j}}$ is an increasing function for all $j$ or $f_{k_{j}}$ is a decreasing function for all $j$. Without loss suppose that they are increasing functions. If $x\leq y$ then $f_{k_{j}}(x)-f_{k_{j}}(y)\leq $ for all $j$, and hence sending $j\to \infty$, we have $f(x)\leq f(y)$. Thus $f$ is increasing. Thus $E_{a,b}$ is closed. This proves the claim.
  
  The $C[0,1]$ is a complete metric space which is the countable union of closed sets. It follows by Baire's theorem that there exists some $a,b\in Q$ with $a<b$ such that $E_{a,b}$ has nonempty interior. That is, there exists $f^{*}\in E_{a,b}$ and $\epsilon>0$ such that $\left\{ f\in C([0,1]): \norm{f^{*}-f}_{\infty} < \epsilon\right\}\subset E_{a,b}.$ Therefore every $f$ with $\norm{f^{*}-f}_{\infty}<\epsilon$ is monotone on the interval $(a,b)$. But this is absurd, since any function can by approximated uniformly by ``zig-zag functions''.

\end{solution}

\item (Spring 2014)
  Does there exist a sequence of continuous functions $f_{n}:[0,1]\to \R$ such that $f_{n}\to \chi_{\Q}$ pointwise?

\begin{solution}
  No. Suppose such a sequence $(f_{n})$ exists. Then we can write
  \begin{equation*}
    \left[ 0,1 \right] = \bigcup_{k\geq 1} \left\{ x\in [0,1]: \left| f_{n}(x)-f_{m}(x) \right| \leq \frac{1}{4} \text{ for all }m,n\geq k\right\}.
  \end{equation*}

  Since the right hand side is a countable union of closed sets, it follow by the Baire Category theorem taht there exists some $k_{0}$ and some $(a,b)$ such that for all $x\in (a,b)$,
  \begin{equation*}
    |f_{n}(x)-f_{m}(x)| \leq \frac{1}{4}\quad\text{ for all }n,m\geq k_{0}.
  \end{equation*}
  Sending $m\to \infty$,
  \begin{equation*}
    |f_{n}(x)-f(x)| \leq \frac{1}{4}\quad\text{ for all }n\geq k_{0}.
  \end{equation*}
  Let $n\geq k_{0}$. If $x\in (a,b)\backslash \Q$ then $|f_{n}(x)|\leq \frac{1}{4}$. But if $x\in (a,b)\cap \Q$ then $|f_{n}(x)-1|\geq \frac{1}{4}$, so that $|f_{n}(x)|\geq \frac{3}{4}$. But this contradicts the continuity of $f_{n}$.
\end{solution}


\item (Fall 2014)
  Let $X,Y$ be Banach spaces and $\left\{ T_{j,k}: j,k\in \mathbb{N} \right\}$ be a set of bounded linear transformations $X\to Y$. Suppose for each $k$, there exists $x\in X$ such that $\sup \left\{ \norm{T_{j,k}x}:  j\in \mathbb{N} \right\} = \infty.$ Then there is an $x\in X$ such that $\sup \left\{ \norm{T_{j,k} x} : j\in \mathbb{N}\right\} = \infty$ for all $k.$
  \begin{solution}
    Proof by contrapositive. Suppose that the conclusion does not hold: that is, suppose that for all $x\in X$ there exists some $k$ such that $\sup \{\norm{T_{j,k_{0}}x}:j\in\mathbb{N}\}<\infty$. Then we can write
\begin{equation*}
  X=\bigcup_{k= 1}^{\infty} \bigcup_{N= 1}^{\infty} \bigcap_{j= 1}^{\infty} \left\{ x\in X: \norm{T_{j,k}x}\leq N \right\}.
\end{equation*}
By the Baire Category theorem, there exists $k_{0},N_{0}\geq 1$ and some ball $\bar{B}_{r}(x_{0})= \{x\in X : \norm{x-x_0}\leq r\}$ with $r>1$ such that $$\sup_{j\geq 1}\norm{T_{j,k_{0}}x}\leq N_{0}$$ for all $x\in \bar{B}_r(x_0)$.

It will suffice to show that for this choice of $k_{0}$, we have $\sup \{\norm{T_{j,k_{0}}x}: j\in \mathbb{N}\}<\infty$ for all $x\in X$.

Let $x\in X$. There are two cases:

\textit{Case 1.} Suppose $\norm{x}\leq 1$. Then $rx+x_{0}\in \bar{B}_{r}(x_{0})$, and hence
\begin{align*}
  \norm{T_{j,k_{0}}x}
  &= \frac{1}{r}\norm{T_{j,k_{0}}(rx)}\\ 
  &=  \frac{1}{r} \norm{T_{j,k_{0}}(rx + x_{0})-T_{j,k_{0}}x_{0}}\\ 
  &\leq \frac{1}{r}\norm{T_{j,k_{0}}(rx + x_{0})} +\frac{1}{r}\norm{T_{j,k_{0}}x_{0}}\\ 
  &\leq 2N_{0}/r
\end{align*}
for all $j$.

\textit{Case 2.} Suppose $\norm{x}>1$. Let $x_1 = x/\norm{x}$. By Case 1,
\begin{equation*}
\norm{T_{j,k_0} x} = \norm{x} \norm{T_{j,k_0}x_1} \leq 2\norm{x}N_0/r
\end{equation*}
for all $j$. 

Combining the two cases, the result follows.
\end{solution}







\newpage
\section{Distribution Theory Notes}

Here we detail the very basics of distribution theory. The hope is that provided one knows these basics, then without having to study much distribution theory, one can turn many problems on the exam involving distributions into more basic analysis problems, to which one can apply the tools of basic analysis, measure theory, or functional analysis.

If $U \subset \RR^d$ is an open set, a distribution on $U$ is a \emph{continuous linear functional} $u$ on the vector space $\mathcal{D}(U) := C_c^\infty(U)$ of all smooth, compactly supported functions on $U$. Thus with each $\phi \in C_c^\infty(U)$, the functional $u$ associates a quantity $\langle u, \phi \rangle$, which we might also denote as
%
\[ \int u(x) \phi(x)\; dx. \]
%
The set of all distributions on $U$ is denoted $\mathcal{D}(U)^*$. There is an abstract theory of topological vector spaces that allows us to define what it means for $u$ to be continuous, but it is not necessary to learn this theory if one remembers a more practical definition. In order for $u$ to be continuous, one needs to show that for any compact set $K \subset U$, there exists an integer $N > 0$, possibly depending on $K$, such that for all $\phi \in C_c^\infty(U)$ with $\text{supp}(\phi) \subset K$,
%
\[ |\langle u, \phi \rangle| \lesssim_K \sup_{x \in K} \sup_{|\alpha| \leq N} |D^\alpha \phi(x)|. \]
%
To check you understand this definition, prove that the linear functional $u$ defined by setting
%
\[ \langle u, \phi \rangle = \sum_{n = 1}^\infty e^{e^{n}} D^{n!} \phi(n) \]
%
is a distribution. As other examples, for any locally integrable function $f: U \to \CC$, or any locally finite measure $\mu$, one can view $f$ and $\mu$ as distributions by defining
%
\[ \langle f, \phi \rangle = \int f(x) \phi(x)\; dx \quad\text{and} \langle \mu, \phi \rangle = \int \phi(x)\; d\mu(x). \]
%
Thus many mathematical objects in analysis are special cases of distributions. An important example is the Dirac delta distribution $\delta_x$, for any $x \in U$, such that $\langle \delta_x, \phi \rangle = \phi(x)$.

Surprisingly, one can take the formal derivative of \emph{any distribution}, by applying integration by parts. If $u$ is a distribution on $U$, we define it's partial derivatives $D^i u$, which are also distributions, by the formal definition
%
\[ \langle D^i u, \phi \rangle = - \langle u, D^i \phi \rangle. \]
%
As an example, we calculate that
%
\[ \langle D^i \delta_x, \phi \rangle = - \langle \delta_x, D^i \phi \rangle = - D^i \phi(x). \]
%
Thus the derivative of the Dirac delta at a point $x$ is the distribution that measures the derivatives of a given function at the point $x$. For more well behaved functions, the distributional derivative of a function will be equal to the derivative of the function.

A sequence of distributions $\{ u_n \}$ converges \emph{distributionally} to another distribution $u$ if, for any $\phi \in \mathcal{D}(U)$, the quantities $\langle u_n, \phi \rangle$ converge to $\langle u, \phi \rangle$. If this is the case, it also follows that the derivatives $\{ D^i u_n \}$ converge distributionally to $D^i u$.

Roughly speaking, if you start to get intuition about how to work with distributions, these facts will allow you to convert most problems about distributions to more basic problems about the convergence of numbers, or integrals, and so on, and so you can focus on trying to apply the more basic analytical tools.






%\newpage
%\section{Day 8: Warm Up Problems}

\newpage
\section{Day 8: Intro to Distribution}

\item (Fall 2020)
Consider the function $f:\R^{2}\to \R$ given by $f(x,y)=|x|$. Find $\left( \frac{\partial ^{2}}{\partial x^{2}}+ \frac{\partial ^{2}}{\partial y^{2}} \right)f$, where the derivative is taken in the sense of distributions.

\begin{solution}
  Let $\phi\in C_{c}^{\infty}(\R^{2})$. By definition of distributional derivative, 
  \begin{align*}
    \Big\langle  \left( \frac{\partial ^{2}}{\partial x^{2}}+ \frac{\partial ^{2}}{\partial y^{2}} \right)f, \phi\Big\rangle
    &= \Big\langle f, \left(\frac{\partial ^{2}}{\partial x^{2}}+ \frac{\partial ^{2}}{\partial y^{2}} \right) \phi \Big\rangle\\ 
    &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|x| \phi_{xx}(x,y)+ |x|\phi_{yy}(x,y)dxdy\\
    &= \underbrace{\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|x| \phi_{xx}(x,y)dxdy}_{A}+ \underbrace{\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|x|\phi_{yy}(x,y)dxdy}_{B}.
  \end{align*}
  Integrating by parts,
  \begin{align*}
    A&= \int_{-\infty}^{\infty}\left[\int_{0}^{\infty}x \phi_{xx}(x,y)dx - \int_{-\infty}^{0}x\phi_{xx}(x,y)\right]dy\\
     &= \int_{-\infty}^{\infty}\left[-\int_{0}^{\infty}\phi_{x}(x,y)dx + \int_{-\infty}^{0}\phi_{x}(x,y)\right]dy\\
     &= \int_{-\infty}^{\infty} -\left[ \phi(x,y) \right]^{x=\infty}_{x=0}+ \left[ \phi(x,y) \right]^{x=0}_{x=-\infty}dy\\
     &=2\int_{-\infty}^{\infty}\phi(0,y)dy
  \end{align*}
  Next, by Fubini's theorem,
  \begin{align*}
    B= \int_{-\infty}^{\infty}|x|\int_{-\infty}^{\infty}\phi_{yy}(x,y)dydx = 0
  \end{align*}
  because $\int_{-\infty}^{\infty}\phi_{yy}(x,y)dy = \left[ \phi_{y}(x,y) \right]^{y=\infty}_{y=-\infty}= 0$ for every $x$. It follows from our calcuations for $A$ and $B$ that
  \begin{equation*}
    \Big\langle  \left( \frac{\partial ^{2}}{\partial x^{2}}+ \frac{\partial ^{2}}{\partial y^{2}} \right)f, \phi\Big\rangle = 2\int_{-\infty}^{\infty}\phi(0,y)dy
  \end{equation*}
  for every $\phi$. Therefore $\left( \frac{\partial ^{2}}{\partial x^{2}}+ \frac{\partial ^{2}}{\partial y^{2}} \right)f= 2\delta(x)$ in the sense of distributions, where $\delta$ is the point mass at $x=0$.
\end{solution} 


\question (Spring 2017)
\begin{parts}
    \part Let $f \in L^1(\RR)$ and consider the sequence of distributions $T_n(x) = \sin(nx^2) f(x)$. Show that $\lim_{n \to \infty} T_n = 0$ in the sense of distributions.
    \begin{solution}
        We perform an integration by parts, which is always handy when dealing with oscillatory things. If $\Lambda_n(x) = \sin(nx^2)$, then
        %
        \begin{align*}
            \int \Lambda_n(x) \phi(x)\; dx &= \int \Lambda_n'(x) \frac{\phi(x)}{2n x}\\
            &= - \int \Lambda_n(x) \frac{d}{dx} \left( \frac{\phi(x)}{2nx} \right)\\
            &= - \int \Lambda_n(x) \left( \frac{\phi'(x)}{2nx} - \frac{\phi(x)}{2nx^2} \right).
        \end{align*}
        %
        Now performing a Taylor expansion, we can write
        %
        \[ \frac{\phi'(x)}{2nx} - \frac{\phi(x)}{2nx^2} = \frac{\phi'(0)}{2nx} - \frac{\phi(0)}{2nx^2} - \frac{\phi'(0)}{2nx} + O(1/n) = \frac{\phi'(0)}{2nx} + O(1/n). \]
        %
        But since $1/x$ is odd, and $\sin(nx^2)$ is even, we have
        %
        \[ \int \Lambda_n(x) \frac{\phi'(0)}{2nx}\; dx = 0. \]
        %
        Thus
        %
        \[ \int \Lambda_n(x) \phi(x) \lesssim_\phi O(1/n), \]
        %
        Taking $n \to \infty$ shows that $\Lambda_n$ converges to 0 distributionally.
    \end{solution}
    
    \part Find a distribution $T \in \mathcal{D}'(\RR)$ such that $T_n = \sin(nx^2)T$ does not converge to $0$ in the sense of distributions as $n \to \infty$.
    \begin{solution}
    		Let $T$ be the distribution defined by
    		%
    		\[ T(\phi) = \phi \left( \sqrt{\pi / 2} \right) \]
    		%
    		for every test function $\phi$. Then
    		%
    		\[ T_n(\phi) = T(\sin(nx^2)\phi(x))=\sin\left(\frac{n \pi}{2}\right)\phi\left(\sqrt{\frac{\pi}2}\right)= (-1)^n\phi\left(\sqrt{\frac{\pi}2}\right), \]
    		%
    		which does not converge as $n\to \infty$ if $\phi(\sqrt{\pi/2})\neq 0$.

%        One can find a distribution $T$ such that for $\phi$ supported away from the origin,
        %
%        \[ \int \phi(x) (1/x^2)\; dx. \]
        %
%        One choice is provided in a previous question. Consider a non-negative test function $\phi$ supported away from the origin (for simplicity, supported on $x > 0$), we have
        %
%        \[ \int \sin(nx^2) T(x) \phi(x)\; dx = \int \frac{\sin(nx^2) \phi(x)}{x^2}. \]
        %
%        For large $n$, we have $\sin(nx^2) \geq nx^2/2$ on the support of $\phi$, which implies that
        %
%        \[ \int \frac{\sin(nx^2) \phi(x)}{x^2} \geq n/2 \int \phi(x)\; dx.  \]
        %
%        This clearly does not converge to zero as $n \to \infty$.
    \end{solution}
\end{parts}

\question (Spring 2015)
\begin{parts}
    \part If $f \in C[0,1]$, and the distributional derivative $f'$ of $f$ on $(0,1)$ is in $L^1((0,1))$, prove that
    %
    \[ f(1) - f(0) = \int_0^1 f'(x)\; dx. \]
    \begin{solution}
        In order to relate $f'$ and $f$, since $f'$ is a distributional derivative, we need to integrate it against a test function, which reduces the question to a basic analysis problem. Fix $\delta > 0$ and consider $\phi_\delta \in C_c^\infty[0,1]$ with $\| \phi_\delta \|_\infty \leq 1$, $\phi_\delta(x) = 1$ for $\delta \leq x \leq 1-\delta$, and supported on $\delta/2 \leq x \leq 1 - \delta/2$, and with $\| \phi_\delta' \|_\infty \leq 10/\delta$. Then $\phi_\delta'$ is supported on $\delta/2 \leq x \leq \delta$ and $1-\delta/2 \leq x \leq 1-\delta$. Since $f'$ is integrable, we have
        %
        \[ \int_0^1 f'(x)\; dx = \lim_{\delta \to 0} \int_0^1 f'(x) \phi_\delta(x)\; dx. \]
        %
        Since $f'$ is a distributional derivative, we can apply an integration by parts, from which we conclude that
        %
        \[ \int_0^1 f'(x) \phi_\delta(x)\; dx = - \int_0^1 f(x) \phi_\delta'(x)\; dx. \]
        %
        We can write
        %
        \[ \int_0^1 f(x) \phi_\delta'(x)\; dx = \int_0^{\delta} f(x) \phi_\delta'(x)\; dx + \int_{1-\delta} f(x) \phi_\delta'(x)|; dx. \]
        %
        Since $f \in C[0,1]$, for each $\varepsilon > 0$, there exists $\delta_0 > 0$ such that for $x,y \in [0,1]$ with $|x-y| \leq \delta_0$, $|f(x) - f(y)| \leq \varepsilon$. If $\delta \leq \delta_0$, this implies that
        %
        \[ \left| \int_0^\delta [f(x) - f(0)] \phi_\delta'(x)\; dx \right| \leq \int_0^\delta \varepsilon |\phi_\delta'(x)| \leq \varepsilon. \]
        %
        Similarily,
        %
        \[ \left| \int_{1 - \delta}^1 [f(x) - f(1)] \phi_\delta'(x)\; dx \right| \leq \int_{1-\delta}^1 \varepsilon |\phi_\delta'(x)| \leq \varepsilon. \]
        %
        The fundamental theorem of calculus implies that
        %
        \[ \int_0^\delta f(0) \phi_\delta'(x) = f(0) [\phi_\delta(\delta) - \phi_\delta(0)] = f(0), \]
        %
        and
        %
        \[ \int_{1-\delta}^1 f(1) \phi_\delta'(x) = f(1) [\phi_\delta(1) - \phi_\delta(1-\delta)] = - f(1). \]
        %
        Putting these calculations together shows that
        %
        \[ \lim_{\delta \to 0} \int_0^1 f(x) \phi_\delta'(x)\; dx = f(0) - f(1), \]
        %
        and thus
        %
        \[ \int_0^1 f'(x)\; dx = - \lim_{\delta \to 0} \int_0^1 f(x) \phi_\delta'(x) = f(1) - f(0). \]
    \end{solution}
    
    \part Let $p \in [1,\infty)$ and let $F \subset C[0,1]$ be such that for each $f \in F$ we have $\| f \|_{L^1[0,1]} \leq 1$ and $\| f' \|_{L^p[0,1]} \leq 1$, where $f'$ is the distributional derivative of $f$. Prove that $F$ is precompact in $C[0,1]$, or find a counter-example.
    \begin{solution}
        Problems about pre-compactness in $C[0,1]$ normally want us to apply the Arzela-Ascoli theorem, so lets try that here. We wish to show there is a constant $C > 0$ such that for each $f \in C[0,1]$,
        %
        \[ \| f \|_\infty \leq C, \]
        %
        (uniform boundedness), and that for each $\varepsilon > 0$, there is $\delta > 0$ such that for $|x - y| \leq \delta$ and $f \in F$, $|f(x) - f(y)| \leq \varepsilon$ (equicontinuity).
        
        Using the last part of the problem, together with H\"{o}lder's inequality, if $p$ and $p^*$ are conjugates, then for $x < y$,
        %
        \[ |f(x) - f(y)| = \left| \int_x^y f'(x)\; dx \right| \leq \| f' \|_p \left( \int_x^y 1 \right)^{1/p^*} = |y - x|^{1/p^*} \| f' \|_p \leq |y - x|^{1/p^*}. \]
        %
        Thus $f$ satisfies a H\"{o}lder condition, uniformly over elements of $f$. This gives equicontinuity.
        
        To show $F$ is uniformly bounded, we must exploit the fact that $\| f \|_1 \leq 1$. Since elements of $K$ are H\"{o}lder continuous it suffices to show that $f(0)$ is bounded for all $f$ in $K$, since for all $x \in [0,1]$,
        %
        \[ |f(x)| \leq |f(x) - f(0)| + |f(0)| \lesssim |x|^{1/p^*} + |f(0)| \leq 1 + |f(0)| \]
        %
        Now for each $x \in [0,1]$,
        %
        \[ f(0) = f(x) - \int_0^x f'(y)\; dy \]
        %
        Integrating in $x$, we obtain that
        %
        \[ f(0) = \int_0^1 f(x) - \int_0^1 \int_0^x f'(y)\; dy\; dx. \]
        %
        We have
        %
        \[ \left|\int_0^1 f(x)\right| \leq \| f \|_1 \leq 1 \]
        %
        and by H\"{o}lder's inequality,
        %
        \[ \left|\int_0^1 \int_0^x f'(y)\; dy\; dx \right| \leq \left|\int_0^1 x^{1/p'} \right| \| f' \|_{L^p} \leq \frac{1}{1 + 1/p'}. \]
        %
        So $|f(0)| \leq 1 + 1/(1 + 1/p')$, which completes the proof of boundedness.

        Alternatively, we can prove uniform boundedness by contradiction. Suppose that $\| f \|_\infty \geq M$, where $M$ is large. Then there exists $x_0 \in [0,1]$ such  that $|f(x_0)| \geq M$. Using the H\"{o}lder condition, we find that $|f(x_0)| \geq M/2$ for $|y - x_0| \leq (M/2)^{p^*}$. And so we find that $\| f \|_1 \geq (M/2) \cdot 2 (M/2)^{p^*} \gtrsim_p M^{1 + p^*}$. Since $\| f \|_1 \leq 1$, this means that $M^{1 + p^*} \lesssim_p 1$, and so $M \lesssim_p 1$. This means we have shown that there exists a constant $C_p$ such that for all $f \in F$, $\| f \|_\infty \leq C_p$.
        
        We have shown that $K$ is an equicontinuous, bounded family of functions. So the hypotheses of the Arzela-Ascoli theorem apply, proving that $F$ is precompact.
    \end{solution}
\end{parts}


\item (Fall 2021, Spring 2016) Prove or disprove:
  \begin{parts}
    \part There exists a distribution $u\in \mathcal{D}(\R)$ so that the restriction to $(0,\infty)$ is given by
    \begin{equation*}
      \langle u, f \rangle = \int_{0}^{\infty}e^{1/x^{2}}f(x)dx
    \end{equation*}
    for all $f\in C^{\infty}(\R)$ which are compactly supported in $(0,\infty)$.
    \begin{solution}
    	Such a distribution does not exist. Suppose that such a distribution did exist. Then there would be $N > 0$ so that
    	%
    	\[ |\langle u, f \rangle| \lesssim \sup_{k \leq N} \| \partial^k \phi \|_{L^\infty(\RR)}, \]
    	%
    	whenever $\phi$ was supported in $[-10,10]$. We will construct a seqiemce $\phi_n$ so that the left hand side blows up faster than the right hand side, which will give a contradiction. The trick is to choose $\phi_n$ to be more and more concentrated at $0$ as $n \to \infty$.

    	Choose $\phi \in C_c^\infty(0,2)$ so that $\phi(x) = 1$ for all $x \in [1/2,1]$. Then define $\phi_n(x) = \phi(2^n x)$. Then $\phi_n(x) = 1$ for all $x \in I_n$, where $I_n = [1/2^{n+1}, 1/2^n]$. We have
      \begin{equation*}
        \langle u, \phi_{n}\rangle \geq \int_{I_{n}} e^{x^{-2}}dx \gtrsim e^{4^{n}} 2^{-n}
      \end{equation*}
      On the other hand, for any $k$,
      \begin{equation*}
        \| \partial^k \phi_n \|_{L^\infty(\RR)} = 2^{kn} \| \partial^k \phi \|_{L^\infty} \lesssim 2^{kn}.
      \end{equation*}
      %
      We therefore conclude that
      %
      \[ e^{4^n} 2^{-n} \lesssim 2^{N n}, \]
      %
      and taking $n \to \infty$ gives a contradiction.
    \end{solution}

    \part There exists a distribution $u\in \mathcal{D}'(\R)$ so that its restriction to $(0,\infty)$ is given by
    \begin{equation*}
      \langle u,f \rangle = \int_{0}^{\infty}x^{-2}e^{i/x^{2}}f(x)dx
    \end{equation*}
    for all $f\in C^{\infty}$ which are compactly supported in $(0,\infty)$.
    \begin{solution}
    	Such a distribution exists. The operator is clearly a linear functional on $\mathcal{D}(0,\infty)$, since the integral converges absolutely when $f$ is compactly supported away from $0$. We must show that $u$ is bounded uniformly near zero, so that we may apply the Hahn-Banach theorem to show that $u$ extends to a distribution on $\mathcal{D}(\RR)$.

    	Fix $f\in \mathcal{D}(\R)$ with $\text{supp}(f)\subset (0,\infty)$. Write $x^{-2}e^{ix^{-2}} = \frac{ix}{2}\frac{d}{dx}\left[ e^{ix^{-2}} \right]$, so that
      \begin{align*}
        \langle u , f \rangle &= \int_{0}^{\infty} \frac{d}{dx}\left[ e^{ix^{-2}} \right]\frac{ix}{2}f(x) dx\\
                              &= \int_{0}^{\infty} e^{ix^{-2}}\left(\frac{i}{2}f(x) + \frac{ix}{2}f'(x)\right) dx\\
                              &= \frac{i}{2}\int_{0}^{\infty} e^{ix^{-2}}\left(f(x) + xf'(x)\right) dx.
      \end{align*}
      and it is easy to verify that the right hand formula at the end of the calculuation defines a distribution for all $f\in \mathcal{D}(\R)$ simply by taking in absolute values, i.e. proving that for any compact set $K \subset [-M,M]$, if $\text{supp}(f) \subset K$, then
      %
      \[ |\langle u,f \rangle| \lesssim |K| [\| f \|_{L^\infty} + M \| f' \|_{L^\infty}] \lesssim_K \max_{k \leq 1} \| \partial^k f \|_{L^\infty}, \]
      %
      which proves $u$ is a bounded distribution on $\mathcal{D}(\RR)$ (the key property we proved is that the compact set $K$ is allowed to contain a neighborhood around $0$).
    \end{solution}
  \end{parts}





\question (Fall 2017) A distribution $T \in \mathcal{S}'(\RR^n)$ is said to be \emph{nonnegative} if $\langle T, \phi \rangle \geq 0$ for every test function $\phi \in \mathcal{S}(\RR^n)$ with $\phi(x) \geq 0$ for all $x \in \RR^n$.
\begin{parts}
    \part Suppose $f \in L^1_{\text{loc}}(\RR^n)$, and let $T_f$ be the distribution defined by $f$. Show that $T_f \geq 0$ if and only if $f \geq 0$ for almost all $x \in \RR^n$.
    \begin{solution}
        Suppose $f < 0$ on a set of positive measure. Then we may find $R > 0$ and $c > 0$ such that the set $E = \{ |x| \leq R: f(x) < -c \}$ has positive measure. Fix $\delta > 0$ and find an open set $U$ contained $E$ with $|U - E| \leq \delta$. Since $U$ is pre-compact, we can find a smooth function $\phi$ compactly supported on a set $V$ such that $f(x) = 1$ for $x \in U$, and $|V - U| \leq \delta$. Then
        %
        \[ \left| \int f(x) \phi(x)\; dx - \int_E f(x)\; dx \right| \leq \int_{V - U} |f(x)|\;dx. \]
        %
        Since $f$ is integrable, and $|V - E| \leq 2\delta$, for any fixed $\varepsilon > 0$, if $\delta$ is suitably small, then
        %
        \[ \int_{V - U} |f(x)|\; dx \leq \varepsilon. \]
        %
        On the other hand,
        %
        \[ \int_E f(x)\; dx \leq -c|E|, \]
        %
        and so
        %
        \[ 0 \leq \int f(x) \phi(x) \leq \varepsilon - c |E|. \]
        %
        Taking $\varepsilon \to 0$ gives a contradiction.
    \end{solution}
    
    \part Show that if $T_n \to T$ in the sense of distributions, and if $T_n \geq 0$ for all $n$, then $T \geq 0$.
    \begin{solution}
        If $\phi \geq 0$, then $\langle T_n, \phi \rangle \geq 0$ for all $n$ since $T_n \geq 0$. But $\langle T, \phi \rangle = \lim_n \langle T_n, \phi \rangle$, which implies that $\langle T,\phi \rangle \geq 0$.
    \end{solution}
    
    \part Suppose $\Phi: \RR \to \RR$ is a $C^2$ function with $\Phi'' \geq 0$ in $\RR$, and let $f \in C^2(\RR^n)$ have compact support. Show that $\Delta(\Phi(f(x)) \geq \Phi'(f(x)) \Delta f(x)$.
    \begin{solution}
        We calculate directly that
        %
        \[ \Delta(\Phi(f(x))) = \Phi'(f(x)) \Delta f + \sum_{i = 1}^n \Phi''(f(x)) \left( \frac{\partial f}{\partial x^i} \right)^2. \]
        %
        Since $\Phi''(f(x)) (\partial f / \partial x^i)^2 \geq 0$, this easily implies the inequality.
    \end{solution}
    
    \part Suppose $f \in C^2(\RR^n)$ has compact support. Show that $\Delta |f| \geq \text{sign}(f(x)) \Delta f(x)$ holds in the sense of distributions. (Hint use (c) with $\Phi(t) = \sqrt{\varepsilon + t^2})$.
    \begin{solution}
        For each $\varepsilon > 0$, if $\Phi_\varepsilon(t) = \sqrt{\varepsilon + t^2}$, then $\Phi_\varepsilon \in C^\infty(\RR)$, and the part (c) implies that
        %
        \[ \Delta(\sqrt{\varepsilon + f^2}) \geq \frac{f \cdot \Delta f}{\sqrt{\varepsilon + f^2}} \]
        %
        Our goal is to take $\varepsilon \to 0$ to show that $\Delta|f| \geq \text{sign}(f) \Delta f$.
        
        As $\varepsilon \to 0$, $\sqrt{\varepsilon + f^2}$ converges distributionally to $|f|$ by the dominated convergence theorem. Given $\phi \in C_c^\infty(\RR^d)$, $\phi(x) \sqrt{\varepsilon + f(x)^2} \to \phi(x) |f(x)|$ pointwise for every $x$, and is upper bounded by $2|\phi(x)| (1 + |f(x)|)$, which is integrable since $\phi$ is compactly supported. Thus
        %
        \[ \int \sqrt{\varepsilon + f(x)^2} \phi(x)\; dx \to \int |f(x)| \phi(x)|; dx. \]
        %
        Similarily, one can argue using the dominated convergence theorem that $f(x) \Delta f(x) / \sqrt{\varepsilon + f(x)^2}$ converges distributionally to $\text{sign}(f(x)) \Delta f(x)$, since it converges pointwise to this function, and is uniformly upper bounded in absolute value by $\Delta f(x)$. Applying (b) shows that $\Delta |f(x)| \geq \text{sign}(f(x)) \cdot \Delta f(x)$.
    \end{solution}
\end{parts}






\item (Spring 2020)
\begin{parts}
  \part Suppose $\Lambda$ is a distribution on $\R^{n}$ such that $\text{supp}(\Lambda)=\left\{ 0 \right\}.$ If $f\in C^{\infty}(\R^{n})$ satisfies $f(0)=0$, does it follow that $f\Lambda = 0$ as a distribution?
  \begin{solution}
    No. Let $n=1$, $f(x)=x$, and $\Lambda=D\delta$. It is clear that $\Lambda$ has support $\left\{ 0 \right\}$. But $\langle f\Lambda, \phi \rangle = \langle \Lambda , x\phi\rangle = \langle \delta, -(x\phi)'\rangle = \langle \delta, -\phi' - x\phi\rangle = -\phi'(0)$. Therefore $f\Lambda = -\delta$ in the sense of distributions, and hence is not the zero distribution.
  \end{solution}

  \part Suppose $\Lambda$ is a distribution on $\R^{n}$ such that $\text{supp}(\Lambda)\subseteq K$, where $K=\left\{ x\in \R^{n}: |x|\leq 1 \right\}$. If $f\in C^{\infty}(\R^{n})$ vanishes on $K$, does it follow that $f\Lambda =0$ as a distribution?
  \begin{solution}
    This part is true. Since $\text{supp}(\Lambda)$ is compact, it follows that $\Lambda$ is of finite order. Then $\Lambda = \sum_{\alpha\in \mathcal{A}}D^{\alpha}g_{\alpha}$ where $g_{\alpha}$ is a continuous function for each $\alpha\in \mathcal{A}$ and $\mathcal{A}$ is a finite set of multi-indices. Let $N = \max\left\{|\alpha|: \alpha\in \mathcal{A}\right\}$.

    Let $\phi\in C^{\infty}_{c}(\R^{n})$ and let $M>0$ such that $\text{supp}(\phi)\subseteq \left\{ x: |x|\leq M \right\}$. Then
    \begin{align*}
      \langle f\Lambda, \phi \rangle
      &=\langle \Lambda, f\phi \rangle\\
      &=\int_{\R^{n}} \sum_{\alpha\in \mathcal{A}}(-1)^{|\alpha|}g_{\alpha}(x)D^{\alpha}(f\phi)(x)dx\\
      &=\sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{\R^{n}} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi)dx
      &&\text{by Leibniz Formula}\\
      &=\sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{|x|>1} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi)dx
      &&\text{since $D^{\alpha-\beta}f(x)=0$ for all $x\in K$}\\
      &=\lim_{\epsilon\to 0^{+}}\sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{|x|>1+\epsilon} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi)dx
    \end{align*}
    We claim this limit is zero. To see this, we first define a cutoff function $\eta_1(x)\in C_c^{\infty}(\R^n)$ with $0\leq \eta_1\leq 1$ such that $\eta_1(x)= 0$ if $|x|<1$, and $\eta_1(x)= 1$ if $2\leq |x|\leq M$. Next, for each $\epsilon\in (0,1)$, define $\eta_{\epsilon}\in C_c^{\infty}$ satisfying
    \begin{equation*}
      \eta_{\epsilon}(x) = \left\{ \begin{array}{l@{\quad:\quad}l}
                                   0  & |x|\leq 1+\epsilon/2 \\
                                   \eta_{1}\left(\frac{2x}{\epsilon} - \frac{2}{\epsilon}\right) & 1+\epsilon/2 < |x| \leq 1+\epsilon \\
                                   1  & 1+\epsilon <|x|\leq M \\
                                   \text{smooth} & |x|>M
                                   \end{array}\right.
    \end{equation*}
    (Draw a picture for the $n=1$ case. This isn't as complicated as it looks.) 
    
    
    By the chain rule, $|D^{\alpha} \eta_{\epsilon}| \leq C\cdot 2^{|\alpha|}\epsilon^{-|\alpha|}$ where $C<\infty$ is an upper bound for the first order derivatives of $\eta_1$. Then taking $\phi_{\epsilon} = \eta_{\epsilon} \phi$,
    
    
    \begin{align*}
      \langle f\Lambda, \phi_{\epsilon}\rangle
      &= \sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi_{\epsilon})dx\\
      &\quad +  \sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{|x|>1+\epsilon} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi)dx
    \end{align*}
    On the other hand, $\text{supp}(f\phi_{\epsilon}) \subset \R^n\backslash B(0,1+\frac{\epsilon}{2})$. Therefore $f\phi_{\epsilon}\in \mathcal{D}(K^{c})$ and hence by definition of distribution support, $\langle f\Lambda, \phi_{\epsilon}\rangle = \langle \Lambda, f\phi_{\epsilon}\rangle =0$. Therefore
    \begin{align*}
      \langle f\Lambda, \phi \rangle
      &= \lim_{\epsilon\to 0^{+}}-\sum_{\alpha \geq \beta }(-1)^{|\alpha|}c_{\alpha\beta}\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi_{\epsilon})dx
    \end{align*}
    Therefore
    \begin{align} \label{dist-up-bound}
      |\langle f\Lambda, \phi \rangle|
      &\leq \limsup_{\epsilon\to 0^{+}}\sum_{\alpha \geq \beta } |c_{\alpha\beta}|\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} \left| g_{\alpha}(x)(D^{\alpha-\beta}f)(D^{\beta}\phi_{\epsilon})\right|dx.
    \end{align}
    In particular, for each $\epsilon>0$, using the Leibniz formula again
    \begin{align*}
    &\sum_{\alpha \geq \beta } |c_{\alpha\beta}|\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} \left| g_{\alpha}(D^{\alpha-\beta}f)(D^{\beta}\phi_{\epsilon})\right|dx \\
    &\leq \sum_{\alpha \geq \beta } |c_{\alpha\beta}|\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} \left| g_{\alpha}(D^{\alpha-\beta}f)\right|\left|\sum_{\beta\geq \gamma} c_{\gamma} D^{\gamma}\phi  D^{\gamma-\beta}\eta_{\epsilon} \right|dx \\
    &\leq \sum_{\alpha \geq \beta }  \sum_{\beta\geq \gamma} |c_{\alpha\beta}| |c_{\gamma}|\frac{C}{(\epsilon/2)^N}\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} \left| g_{\alpha}(D^{\alpha-\beta}f)\right| \left| D^{\gamma}\phi \right|dx.
    \end{align*}
    Therefore as $\epsilon\to 0^+$, we have $\frac{1}{(\epsilon/2)^N}\int_{1+ \epsilon\geq |x|>1+\frac{\epsilon}{2}} \left| g_{\alpha}(D^{\alpha-\beta}f)\right| \left| D^{\gamma}\phi \right|dx \to 0$ since $(D^{\alpha-\beta}f)(x)=0$ for all $x$ with $|x|=1$. Then it follows by \eqref{dist-up-bound} that $\langle f\Lambda, \phi\rangle = 0$. Since $\phi$ was arbitrary, $f\Lambda=0$ in the sense of distributions.
  \end{solution}
\end{parts}













\newpage
\section{Fourier Analysis / Tempered Distribution Notes}

With distribution theory, there are a couple simple facts that allow one to convert technical sounding problems into more basic problems. Fourier analysis consists of some more deep tools than this, so I'd recommend making a study of these tools separately. But the theory of Fourier analysis applied to tempered distributions is very similar to the theory of distributions, in the sense that if you know Fourier analysis, you can often convert problems involving tempered distributions into more basic problems about Fourier analysis / other analysis techniques.

If $f: \RR^d \to \CC$ is integrable, we define the Fourier transform $\widehat{f}: \RR^d \to \CC$ by the integral formula
%
\[ \widehat{f}(\xi) = \int f(x) e^{-2 \pi i \xi \cdot x}\; dx. \]
%
To avoid technical assumptions, we assume $f$ is in the \emph{Schwartz class} $\mathcal{S}(\RR^d)$, which consists of smooth functions $f \in C^\infty(\RR^d)$ which are rapidly decaying, i.e. such that for any $N > 0$ and $\alpha$, $|\partial_x^\alpha f(x)| \lesssim \langle x \rangle^{-N}$. The Fourier transform is then a bijection from $\mathcal{S}(\RR^d)$ to itself, with inverse given by
%
\[ \widecheck{g}(x) = \int g(\xi) e^{2 \pi i \xi \cdot x}\; d \xi. \]
%
One can then verify that the \emph{multiplication formula}
%
\[ \int \widehat{f}(\xi) g(\xi)\; d\xi = \int f(x) \widehat{g}(x)\; dx \]
%
holds. This will enable us to use duality to extend the definition of the Fourier transform to distributions.

Distributions on $\RR^d$ were defined as continuous linear functionals on $\mathcal{D}(\RR^d) = C_c^\infty(\RR^d)$. Some of these distributions extend to continuous linear functionals on $\mathcal{S}(\RR^d)$, which contains $\mathcal{D}(\RR^d)$ as a subclass. These distributions are called \emph{tempered}, and the class of all such tempered distributions is denoted $\mathcal{S}(\RR^d)^*$. Again, there is an abstract theory which determines when a linear functional on $\mathcal{S}(\RR^d)$ is continuous. But in practice, it suffices to verify the following: a tempered distribution $u$ is a functional that associates with each $\phi \in \mathcal{S}(\RR^d)$ a quantity $\langle u, \phi \rangle$, such that for some $N,M > 0$,
%
\[ |\langle u, \phi \rangle| \leq \sup_{|\alpha| \leq N} \sup_{x \in \RR^d} \frac{|D^\alpha \phi(x)|}{(1 + |x|)^M}. \]
%
If $u$ is a tempered distribution, we formally a tempered distribution $\widehat{u}$ by the formula $\langle \widehat{u}, \phi \rangle = \langle u, \widehat{\phi} \rangle$, so that the multiplication formula above holds. The Fourier transform is then a bijection of $\mathcal{S}(\RR^d)^*$, where the inverse Fourier transform is given by $\langle \widecheck{u}, \phi \rangle = \langle u, \widecheck{\phi} \rangle$. We now summarize several useful properties of the Fourier transform:
%
\begin{itemize}
	\item Because the Fourier transform is a bijection, if $\widehat{u} = 0$, then $u = 0$.

	\item \emph{The Riemann-Lebesuge Lemma}: If $f \in L^1(\RR^d)$, then $\widehat{f} \in C_0(\RR^d)$, and $\| \widehat{f} \|_{L^\infty(\RR^d)} \leq \| f \|_{L^1(\RR^d)}$.

	\item \emph{Parseval's Formula}: The Fourier transform is an isometry of $L^2(\RR^d)$. In particular, if $u$ is a tempered distribution, and $\widehat{u} \in L^2(\RR^d)$, then $u \in L^2(\RR^d)$.

	\item The Fourier transform of $D^\alpha u$ is equal to $(2 \pi i \xi)^\alpha \widehat{u}$.

	\item The Fourier transform of $(- 2 \pi i x)^\alpha u$ is equal to $D^\alpha \widehat{u}$.

	\item If we define the convolution of $\phi_1, \phi_2 \in \mathcal{S}(\RR^d)$ by setting
	%
	\[ (\phi_1 * \phi_2)(x) = \int \phi_1(y) \phi_2(x - y)\; dy, \]
	%
	then $\phi_1 * \phi_2 \in \mathcal{S}(\RR^d)$ is Schwartz, and it's Fourier transform is $\widehat{\phi_1} \cdot \widehat{\phi_2}$. There is a way to define the convolution of a distribution $u$ with a Schwartz function $\phi$, denoted $u * \phi$, which will be a tempered distribution with Fourier transform equal to $\widehat{u} \widehat{\phi}$.
\end{itemize}









\newpage
\section{Day 9: Warm Up Problems}

\question (Fall 2019)
    Let $s \in \RR$, and let $H^s(\RR)$ be the Sobolev space on $\RR$ with the norm
    %
    \[ \| u \|_{(s)} = \left( \int_{\RR} (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2\; d\xi \right)^{1/2} \]
    %
    where $\widehat{u}$ is the Fourier transform of $u$. Let $r < s < t$ be real numbers. Prove that for every $\varepsilon > 0$ there is $C > 0$ such that
    %
    \[ \| u \|_{(s)} \leq \varepsilon \| u \|_{(t)} + C \| u \|_{(r)} \]
    %
    for every $u \in H^t(\RR)$.
\begin{solution}
    Intuitively, we only need the $H^t$ norm to control $H^s$ for large frequencies $|\xi| \gtrsim 1$, since the $H^r$ norm gives enough control over low frequencies since $(1 + |\xi|)^s \approx 1 \approx (1 + |\xi|^2)^r$ when $|\xi| \lesssim 1$. Thus we fix $R > 0$, and write
    %
    \[ \int (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2\; d\xi = \int_{|\xi| \leq R} (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2 + \int_{|\xi| > R} (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2. \]
    %
    Now
    %
    \[ \int_{|\xi| \leq R} (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2\; d\xi \leq R^{s-r} \int_{|\xi| \leq R} (1 + |\xi|^2)^r |\widehat{u}(\xi)|^2\; d\xi \leq R^{s-r} \| u \|_{(r)}^2, \]
    %
    and
    %
    \[ \int_{|\xi| \geq R} (1 + |\xi|^2)^s |\widehat{u}(\xi)|^2\; d\xi \leq R^{s-t} \int_{|\xi| \geq R} (1 + |\xi|^2)^t |\widehat{u}(\xi)|^2\; d\xi \leq R^{s-t} \| u \|_{(t)}^2. \]
    %
    Thus we find that
    %
    \[ \| u \|_{(s)} \leq \sqrt{R^{s-r} \| u \|_{(r)}^2 + R^{s-t} \| u \|_{(t)}^2} \leq (2R^{s-r})^{1/2} \| u \|_{(r)} + (2R^{s-t})^{1/2} \| u \|_{(t)}.  \]
    %
    Given $\varepsilon > 0$, the proof is completed by picking $R \geq (2/\varepsilon^2)^{1/(t-s)}$.

    There is an alternate solution, using Young's inequality to obtain more explicit constants. If $A = 1 + |\xi|^2$, then Young's inequality for products implies that for $a,b > 0$, and any $p,q > 1$ with $1/p + 1/q = 1$,
    %
    \[ ab = (p \varepsilon^{2/p} a) ( p^{-1} \varepsilon^{-2/p} b ) \leq \varepsilon^2 a^p + \frac{(p^{-1} \varepsilon^{-2/p})^q}{q} b^q. \]
    %
    If we find $\theta \in [0,1]$ so that
    %
    \[ A^{s/2} = A^{\theta (t/2) + (1 - \theta) (r/2)} = A^{\theta (t/2)} A^{(1 - \theta) (r/2)} \]
    %
    then applying Young's product inequality with $p = 1/\theta$ and $q = 1/(1 - \theta)$ yields
    %
    \[ A^{s/2} \leq \varepsilon^2 A^{t/2} + C_{\varepsilon,r,s,t} A^{r/2}. \]
    %
    for some explicit constant $C_{\varepsilon,r,s,t}$. Setting $A = (1 + |\xi|)$, we find that
    %
    \[ \| u \|_{H^s}^2 \leq \varepsilon^2 \| u \|_{H^t}^2 + C_{\varepsilon,r,s,t} \| u \|_{H^s}^2. \]
    %
    Taking square roots yields that
    %
    \[ \| u \|_{H^s} \leq \sqrt{ \varepsilon^2 \| u \|_{H^t}^2 + C_{\varepsilon,r,s,t} \| u \|_{H^s}^2 } \leq \varepsilon \| u \|_{H^t} + C_{\varepsilon,r,s,t}^{1/2} \| u \|_{H^s}, \]
    %
    which completes the proof.
\end{solution}




\question (Fall 2015) Let $f$ be a tempered distribution on $\RR$ with Fourier transform
%
\[ \widehat{f}(\xi) = 1 + \xi^{12} + \sin \xi + \text{sign}(\xi). \]
%
Find $f$ and $f'$ (specify the definition of the Fourier transform you are using).
\begin{solution}
    I will be using the Fourier transform defined by
    %
    \[ \widehat{f}(\xi) = \int f(x) e^{-2 \pi i \xi \cdot x}\; d\xi. \]
    %
    The problem is really asking us to compute the inverse Fourier transform of $1$, $\xi^{12}$, $\sin \xi$, and $\text{sign}(\xi)$ individually.
    
    If $\delta$ is the Dirac delta function at the origin, then
    %
    \[ \widehat{\delta}(\xi) = \int e^{-2 \pi i \xi \cdot x} \delta(x)\; dx = 1. \]
    %
    Thus $\widehat{\delta} = 1$.
    
    To find a function $f$ whose Fourier transform is $\xi^{12}$, we recall the applying linear differential operators with constant coefficients on one side of the Fourier transform is the same as multiplying by a polynomial on the other side. Since $\xi^{12}$ is equal to $\xi^{12} \cdot \widehat{\delta}$, which implies that $f$ is a constant multiple of the distributional derivative $D^{12} \delta$. If we compute constants carefully, we find that
    %
    \[ f = \frac{1}{(2 \pi i)^{12}} D^{12} \delta = \frac{D^{12} \delta}{(2 \pi)^{12}}. \]
    %
    Thus we have found the inverse Fourier transform of $\xi^{12}$.

	 Next, we find a function whose Fourier transform is $\sin \xi$. A simple way to compute this is to write
    %
    \[ \sin \xi = \frac{e^{i \xi} - e^{-i \xi}}{2i}. \]
    %
    Since $e^{i \xi}$ is the Fourier transform of the Dirac delta $\delta_{-1/2\pi}$ at $-1/2\pi$, and $e^{-i \xi}$ is the Fourier transform of the Dirac delta $\delta_{1/2\pi}$ at $1/2\pi$, if we write $\Gamma(x) = \delta_{-1/2\pi}/2i - \delta_{1/2\pi}/2i$, then $\widehat{\Gamma}(\xi) = \sin \xi$.
    
    Finally, we find a function $f$ such that $\widehat{f}(\xi) = \text{sign}(\xi)$. Taking distributional derivatives, we find that
    %
    \[ D \widehat{f}(\xi) = 2 \delta. \]
    %
    Taking inverse Fourier transforms, we find that
    %
    \[ - 2 \pi i x f(x) = 2. \]
    %
    In particular, we might guess that
    %
    \[ f(x) = \frac{-1}{i \pi x}. \]
    %
    We must be slightly more careful, however, since integration against the singular function $1/\xi$ can be interpreted in several different ways. One choice is the \emph{principal value}
    %
    \[ \int \frac{\phi(\xi)}{\xi}\; d\xi = \lim_{\varepsilon \to 0} \int_{|\xi| \geq \varepsilon} \frac{\phi(\xi)}{\xi}, \]
    %
    which converges because $\phi$ is smooth. Any other choice differs from this principal value by a distribution supported at the origin, and thus differs by a constant multiple of the Dirac delta function (the only distribution supported at the origin which is homogeneous of degree $-1$), so we might as well guess that $f$ is given by the principal value formula above, and fix the function by adding or subtracting Dirac deltas if necessary. Thus we assume that
    %
    \[ \int f(x) \phi(x) = \lim_{\varepsilon \to 0} \frac{-1}{i \pi x}. \]
    %
    But we now calculate that
    %
    \begin{align*}
    	\int \widehat{f}(\xi) \phi(\xi)\; d\xi &= \frac{-1}{i \pi} \lim_{\varepsilon \to 0} \int_{|x| \geq \varepsilon} \frac{\widehat{\phi}(x)}{x}\; dx\\
    	&= \frac{1}{\pi} \int \phi(\xi) \lim_{\varepsilon \to 0} \int_\varepsilon^\infty \frac{\sin(2 \pi \xi x)}{x}\; dx\; d\xi.
    \end{align*}
    %
    Note that we may assume without loss of generality that $\phi$ is smooth and compactly supported, which justifies that we can apply Fubini's theorem, and to bring the limit into the integral, since then the inner integrals converge uniformly on the support of $\phi$. It is easy to see via a change of variables this limiting quantity is radial and odd in $\xi$, and thus this quantity is equal to
    %
    \[ \left( \frac{1}{\pi} \int \phi(\xi) \text{sign}(\xi)\; d\xi \right) \lim_{\varepsilon \to 0} \int_\varepsilon^\infty \frac{\sin(2 \pi x)}{x}\; dx\; d\xi. \]
    %
    One can explicitly calculate this integral, e.g. using contour integration techniques. But we can also now rely on a little bit of a cheat: we \emph{already know} from prior calculations that our choice of $f$ differs from the right answer by a distribution supported at the origin, which implies the limiting integral must be equal to $\pi$, for otherwise our answer will differ by a quantity that is supported on the entire real line. Thus we have chosen the right answer.

    Alternately, to verify that our choice of $f$ above is correct, it suffices only to plug in a Gaussian $\phi(x) = e^{- \pi |x|^2}$, which has the property that $\widehat{\phi}(\xi) = \phi(\xi)$, and then because the Gaussian is even, we find that
    %
    \begin{align*}
    	\int \widehat{f}(\xi) e^{- \pi |\xi|^2} &= \int f(x) e^{- \pi |x|^2}\\
    	&= \lim_{\varepsilon \to 0} \int_{|x| \geq \varepsilon} \frac{-1}{i \pi x} e^{- \pi |x|^2}\\
    	&= \lim_{\varepsilon \to 0} 0 = 0.
    \end{align*}
    %
    Since we know $\widehat{f}(\xi) = \text{sgn}(\xi) + C \delta$ for some constant $C$, we find that
    %
    \[ 0 = \int \widehat{f}(\xi) e^{- \pi |\xi|^2} = \int \text{sgn}(\xi) e^{- \pi |\xi|^2} + C = C. \]
    %
    Thus $f$ is actually the right answer.
    
    We finish by giving the rough idea of how to calculate the distributional derivative of $f$. One approach is to multiply by a power of $\xi$ on the Fourier transform side, and then take a Fourier transform, but we choose to work directly from the definition of the distributional derivative.
    
    First, we calculate directly using the definition of the distributional derivative that for any test function $\phi$,
    %
    \[ \int \delta_0'(x) \phi(x)\; dx = - \int \delta_0(x) \phi'(x)\; dx = - \phi'(0). \]
    %
    Thus $\delta_0'$ maps a test function $\phi$ to $-\phi'(0)$. Similarily, the derivative of $\delta_{-1/2\pi}$ maps $\phi$ to $-\phi'(-1/2\pi)$, and the derivative of $\delta_{1/2\pi}$ maps $\phi$ to $-\phi'(1/2\pi)$. This allows us to calculate the distributional derivative of $\Gamma$ simply. For the distribution $u: \phi \mapsto D^{12} \phi(0)$ we calculate
    %
    \[ \int u'(x) \phi(x)\; dx = -\int u(x) \phi'(x)\; dx = - D^{12} \phi'(0) = - D^{13}(0). \]
    %
    This enables us to calculate the distributional derivative of $\Lambda$. Finally,
    %
    \[ \int \text{p.v}(1/x)' \cdot \phi(x)\; dx = -\int \text{p.v}(1/x) \phi'(x)\; dx = - \lim_{\delta \to 0} \int_{|x| \geq \delta} (1/x) \phi'(x); dx. \]
    %
    Integration by parts shows that
    %
    \[ \int_\delta^\infty (1/x) \phi'(x)\; dx = - \phi(\delta) / \delta + \int_\delta^\infty (1/x^2) \phi(x)\; dx \]
    %
    and
    %
    \[ \int_{-\infty}^{-\delta} (1/x) \phi'(x)\; dx = -\phi(-\delta)/\delta + \int_{-\infty}^\delta (1/x^2) \phi(x)\; dx. \]
    %
    Thus
    %
    \begin{align*}
        \lim_{\delta \to 0} - \int_{|x| \geq \delta} (1/x) \phi'(x)\; dx &= \lim_{\delta \to 0} \frac{\phi(\delta) + \phi(-\delta)}{\delta} - \int_{|x| \geq \delta} (1/x^2) \phi(x)\; dx\\
        &= \lim_{\delta \to 0} \frac{2 \phi(0)}{\delta} - \int_{|x| \geq \delta} (1/x^2) \phi(x)\; dx.
    \end{align*}
    %
    Thus if we define the distribution $\text{f.p}(1/x^2)$ (the finite part distribution) as mapping a test function $\phi$ to
    %
    \[ \lim_{\delta \to 0} \int_{|x| \geq \delta} (1/x^2) \phi(x) - 2 \phi(0) / \delta, \]
    %
    then the derivative of $\text{p.v}(1/x)$ is $- \text{f.p}(1/x^2)$. This allows us to calculate the distributional derivative of $h$.
\end{solution}








\newpage 
\section{Day 9: Fourier Analysis + Distribution Theory}

\question (Spring 2017) Let $f \in L^1(\RR^n)$ be a function all of whose distributional derivatives $D^\alpha f$ of order $|\alpha| = m$ also belong to $L^1(\RR^n)$. Show that if $m > n$, then $f \in C(\RR^n)$.
\begin{solution}
    The duality between differentiation and multiplication by polynomials in the Fourier transform, and the fact that the Fourier transform of an $L^1$ function is in $L^\infty$, implies that for each $\alpha$ with $|\alpha| = m$,
    %
    \[ |\xi^\alpha \widehat{f}| \in L^\infty(\RR^n). \]
    %
    We have $\sum_{|\alpha| = m} |\xi^\alpha| \sim |\xi|^m$, so we conclude that $|\xi^m \widehat{f}| \in L^\infty$. Since $f \in L^1$, $\widehat{f} \in L^\infty$, so that we have a bound
    %
    \[ |\widehat{f}(\xi)| \lesssim \frac{1}{1 + |\xi|^m}. \]
    %
    Since $m > n$, this means $\widehat{f}$ is also integrable. The Fourier inversion formula thus implies that $f$ agrees almost everywhere with a bounded, continuous function.
\end{solution}

\question (Fall 2019) Let $f \in L^2(\RR)$. Define
%
\[ g(x) = \int_{-\infty}^\infty f(x-y) f(y)\; dy \]
%
Show that there exists a function $h \in L^1(\RR)$ such that
%
\[ g(\xi) = \int_{-\infty}^\infty e^{- i \xi x} h(x)\; dx, \]
%
i.e. $g$ is a Fourier transform of a function in $L^1(\RR)$. Hint: The following formal argument may be helpful:
%
\[ \widehat{g}(x) = \widehat{f * f}(x) = \widehat{f}(x)^2, \]
%
where $*$ denotes convolution, and $\widehat{\cdot}$ denotes the Fourier transform.
\begin{solution}
    If one only knows Fourier analysis in $L^1(\RR)$, the argument in the hint may be justified using the fact that $L^1(\RR) \cap L^2(\RR)$ is dense in $L^2(\RR)$, together with some continuity arguments.
    
    Young's convolution inequality implies that $g$ is a continuous function in $L^\infty(\RR)$. One cannot take the Lebesgue integral defining the Fourier transform for $g$, but one may take the distributional Fourier transform. If we consider a family of functions $\{ f_n \}$ lying in $L^2(\RR) \cap L^1(\RR)$, which converge to $f$ in $L^2(\RR)$ as $n \to \infty$, then Young's convolution inequality implies that $f_n * f_n$ converges to $f * f$ in $L^\infty(\RR)$. In particular, $f_n * f_n$ converges to $f * f$ distributionally. By continuity of the Fourier transform, $\widehat{f_n * f_n}$ converges to $\widehat{f * f}$, and $\widehat{f_n * f_n} = \widehat{f_n}^2$, which converges distributionally to $\widehat{f}^2$ (by Cauchy Schwarz it actually converges in $L^1(\RR)$, which is a stronger statement). Thus we conclude that $\widehat{g} = \widehat{f * f} = \widehat{f}^2$. But since the Fourier transform of $g$ lies in $L^1$, we know that $g$ acts as a distribution in exactly the same way as the distribution
    %
    \[ \xi \mapsto \int \widehat{f}(x)^2 e^{2 \pi i \xi \cdot x}. \]
    %
    Since $g$ is a continuous function, and the integral above defines a continuous function, it follows that for all $\xi$,
    %
    \[ g(\xi) = \int \widehat{f}(x)^2 e^{2 \pi i \xi \cdot x}. \]
\end{solution}

\question (Fall 2015) Recall that $H^s(\RR^n)$ is the Sobolev space consisting of all tempered distributions $g$ on $\RR^n$ for which the Fourier transform $\widehat{g}$ of $g$ is locally integrable and satisfies
%
\[ \int_{\RR^n} (1 + |\xi|^2)^s |\widehat{g}(\xi)|^2\; d\xi < \infty. \]
%
Let $u$ be a Schwartz function on $\RR^n$ and for $a \in \CC$, let
%
\[ f_a(x) = |x|^a u(x). \]
%
Show that if $\text{Re}(a) > - n/2$ and $s \in [0,\text{Re}(a) + n/2)$, then $f_a \in H^s(\RR^n)$.
\begin{solution}
    We wish to compute the Fourier transform of $f_a$, in order to apply the $H^s$ norm. $f_a$ is the product of a Schwartz function with $|x|^a$, so a useful avenue is to study the distributional derivative of $\Lambda(x) = |x|^a$, since $\widehat{f_a} = \widehat{\Lambda} * \widehat{u}$. The important properties of $\Lambda$ we will use is that $\Lambda$ acts as a $C^\infty$ functions on $\RR^n - \{ 0 \}$, is a radial function, and is \emph{homogeneous} of degree $a$. It follows from general Fourier analysis that $\widehat{\Lambda}$ acts as a $C^\infty$ function on $\RR^n - \{ 0 \}$, is a radial function, and is homogeneous of degree $-a-n$. Thus we can write $\widehat{\Lambda}(\lambda \theta) = \lambda^{-a - d} g(\theta)$ where $g$ is a $C^\infty$ function defined for $|\theta| = 1$, and $\lambda > 0$. Since $\text{Re}(a) > -n/2$, $\text{Re}(-a-n) < -n/2$. This implies that $\widehat{\Lambda}$ decays away from the origin. In particular, $\widehat{\Lambda}$ is $L^2$ away from the origin.

    Now we write
    %
    \[ |x|^a u(x) = T_1 u(x) + T_2 u(x), \]
    %
    where
    %
    \[ \widehat{T_1 u}(\xi) = \int_{|\eta| \leq 1} |\eta|^{-a-n} g(\eta/|\eta|) \widehat{u}(\xi - \eta)\; d\eta, \]
    %
    and
    %
    \[ \widehat{T_2 u}(\xi) = \int_{|\eta| \geq 1} |\eta|^{-a-n} g(\eta/|\eta|) \widehat{u}(\xi - \eta)\; d\eta. \]
    %
    Since $u$ is Schwartz, $\widehat{u}$ is also Schwartz. In particular, for $|\eta| \geq 2|\xi|$, $|\widehat{u}(\xi - \eta)| \lesssim_N |\eta|^{-N}$ for all $N > 0$, which implies that
    %
    \[ \left| \int_{|\eta| \geq 2|\xi|} |\eta|^{-a-n} g(\eta/|\eta|) \widehat{u}(\xi - \eta)\; d\eta \right| \lesssim_M |\xi|^{-M}. \]
    %
    For $1 \leq |\eta| \leq |\xi|/2$, $|\widehat{u}(\xi - \eta)| \lesssim_N |\xi|^{-N}$, which implies that
    %
    \[ \left| \int_{1 \leq |\eta| \leq |\xi|/2} |\eta|^{-a-n} g(\eta/|\eta|) \widehat{u}(\xi - \eta)\; d\eta \right| \lesssim_N |\xi|^{-N}. \]
    %
    For $|\xi|/2 \leq |\eta| \leq 2|\xi|$, we can really only use the bound $|\widehat{u}(\xi - \eta)| \lesssim 1$, which implies that
    %
    \[ \left| \int_{|\xi|/2 \leq |\eta| \leq |\xi|} |\eta|^{-a-n} g(\eta/|\eta|) \widehat{u}(\xi - \eta)\; d\eta \right| \lesssim |\xi|^{-\text{Re}(a)}. \]
    %
    Putting these bounds together shows that
    %
    \[ |\widehat{T_2 u}(\xi)| \lesssim |\xi|^{-\text{Re}(a)}, \]
    %
    which is sufficent to show that $T_2 u \in H^s$ for $0 \leq s \leq \text{Re}(a) + n/2$.
    
    To show $T_1 u \in H^s$, we `cheat' a little, since this is the average of $u$ over `low frequencies' (and a good intuition to have is that low frequencies are very smooth). More precisely, $\widehat{T_1 u} = \widehat{\Psi} * \widehat{u}$, where $\widehat{\Psi}$ is a compactly supported distribution. The Paley-Wiener theorem implies that $\Psi$ is a $C^\infty$ function, and there exists some $N > 0$ such that $|\Psi(x)| \lesssim_N 1 + |x|^N$. And $T_1 u = \Psi \cdot u$, so $T_1 u$ is therefore actually a \emph{Schwartz function}, and thus certainly in $H^s(\RR^n)$.
\end{solution}



\newpage
\section{Andreas Problems Warm Up Problem}

\question
\begin{parts}
	\part Let $\psi \in C_c(\RR)$. Show that
%
\[ \lim_{N \to \infty} \frac{1}{N} \int_0^\infty \frac{\psi(x/N)}{\sqrt{1 + x}} = 0. \]
\begin{solution}
	We apply the Lebesgue dominated convergence theorem. We can change variablse to write
	%
	\[ \frac{1}{N} \int_0^\infty \frac{\psi(x/N)}{\sqrt{1 + x}} = \int_0^\infty \frac{\psi(x)}{\sqrt{1 + Nx}}. \]
	%
	The integrand converges pointwise to zero, and is dominated by $\psi$, which is integrable, and so the result follows.
\end{solution}

\part Let
%
\[ J_N = \int_0^N e^{ix} \sqrt{1 + x}\; dx. \]
%
Does $\lim_{N \to \infty} J_N$ exist?
\begin{solution}
	Integrating by parts tells us that
	%
	\begin{align*}
		J_N &= -i \left[ e^{iN} (1 + N)^{1/2} - 1/\sqrt{2} - \frac{1}{2} \int_0^N e^{ix} (1 + x)^{-1/2} \right].
	\end{align*}
	%
	Another integration by parts tells us that
	%
	\[ \int_0^N e^{ix} (1 + x)^{-1/2} = -i \left[ e^{Nix} (1 + N)^{-1/2} - 1/\sqrt{2} + \frac{1}{2} \int_0^N e^{ix} (1 + x)^{-3/2} \right]. \]
	%
	Combining these bounds tells us that
	%
	\[ |J_N| \gtrsim N^{1/2} + O(1), \]
	%
	and so the quantities $J_N$ cannot converge as $N \to \infty$.
\end{solution}

\part Suppose $\chi \in C_c^2(\RR)$. Prove that
%
\[ \lim_{N \to \infty} \int_0^\infty \chi(x/N) e^{ix} \sqrt{1 + x}\; dx \]
%
exists, and calculate the limit.
\begin{solution}
	We integrate by parts twice to exploit the rapid oscillation of $e^{Nix}$, finding that
	%
	\begin{align*}
		N \int_0^\infty \chi(x) e^{Nix} \sqrt{1 + Nx} &= i \chi(0) + i \int_0^\infty e^{Nix} \left( \chi'(x) \sqrt{1 + Nx} + \frac{N}{2} \frac{\chi(x)}{\sqrt{1 + Nx}} \right)\\
		&= i \chi(0) - \frac{1}{N} \chi'(0) - \frac{1}{2} \chi(0)\\
		&\quad\quad - N^{-1} \int_0^\infty e^{Nix} \left( \chi''(x) \sqrt{1 + Nx} + N \chi'(x) \frac{1}{\sqrt{1 + Nx}} - \frac{N^2}{4} \frac{\chi(x)}{(1 + Nx)^{3/2}} \right).
	\end{align*}
	%
	This term is now equal to $(i-1/2) \chi(0) + O(1/\sqrt{N})$, and thus converges $(i - 1/2) \chi(0)$ as $N \to \infty$.
	% int_0^R sqrt(1 + x) e^{2 pi i xi x}
	% 1 + x = y^2
	% x = sqrt(y^2 - 1)
	% dx = 2y dy
\end{solution}

\end{parts}

\newpage
\section{Andreas Problems}

\question Let $f$ be a continuous function on $\RR$, and define
%
\[ F_n(x) = \int_0^x (x - t)^{n-1} f(t)\; dt. \]
%
Prove that $F_n$ is $n$ times differentiable, and find a simple formula for it's $n$th derivative.
\begin{solution}
	We calculate by differentiating under the integral sign that
	%
	\[ F_n'(x) = (n-1) F_{n-1}(x). \]
	%
	So $F_n^{(n)}(x) = (n-1) F_{n-1}^{(n-1)}(x)$. Since, by the fundamental theorem of calculus,
	%
	\[ F_1'(x) = f(x), \]
	%
	and so by induction, we can prove that for $n \geq 2$, $F_n^{(n)} = (n-1)! f(x)$.
\end{solution}

\question For $y > 0$, let
%
\[ f(x,y) = \sum_{n = 1}^\infty \frac{x}{x^2 + yn^2}. \]
%
\begin{parts}
	\part Show that for each $y > 0$, the limit $g(y) = \lim_{x \to \infty} f(x,y)$ exists, and find a formula for the limit.
	\begin{solution}
		When $x$ is large, we can take a Taylor expansion to write, for $n \leq |x| y^{-1/2}$,
		%
		\[ \frac{x}{x^2 + yn^2} = \frac{1}{x} \frac{1}{1 + yn^2/x^2}. \]
		%
		The sum looks hard to control, but might be easier to control with an integral. We note that
		%
		\[ \left| \frac{d}{dt} \frac{x}{x^2 + y t^2}\right| = \left| \frac{2xyt}{(x^2 + yt^2)^2} \right| \lesssim \min( ty/x^3, x/yt^3 ). \]
		% t^4y <= x^4
		% t <= x y^{-1/4}
		Since
		%
		\[ \sum_n \min(ny/x^3,x/yn^3) \lesssim \sum_{n \leq x y^{-1/4}} ny/x^3 + \sum_{n \geq x y^{-1/4}} x/yn^3 \lesssim y^{1/2}/x, \]
		%
		it follows by the mean value theorem that
		%
		\[ \left| \sum_{n = 1}^\infty \frac{x}{x^2 + yn^2} - \int_1^\infty \frac{x}{x^2 + yt^2}\; dt \right| \lesssim y^{1/2}/x, \]
		%
		which converges to zero as $x \to \infty$.
		%
		\[ \int_1^\infty \frac{x}{x^2 + yt^2}\; dt = y^{-1/2} \tan^{-1}( y^{1/2} t / x ) |_1^\infty = y^{-1/2} (\pi/2) - y^{-1/2} (\pi/4) = y^{-1/2} \pi / 4, \]
		%
		so we conclude that $g(y) = (\pi/4) y^{-1/2}$.
	\end{solution}

	\part Determine if $f$ converges to $g$ uniformly on $(0,\infty)$.
	\begin{solution}
		The error estimates in the last part should convince you that this is not the case. We can use these estimates to show this is not the case. For some $C > 0$, we actually have
		%
		\[ \frac{d}{dt} \frac{x}{x^2 + y t^2} \leq - C \min( ty/x^3, x/yt^3 ). \]
		%
		And so using the fundamental theorem of calculus, we can write
		%
		\[ \frac{x}{x^2 + yn^2} - \int_0^1 \frac{x}{x^2 + y(n + t)^2}\; dt = - \int_0^1 \int_0^t \frac{d}{ds} \frac{x}{x^2 + y(n + s)^2}\; ds\; dt \gtrsim \min(ty/x^3,x/yt^3). \]
		%
		Summing in $n$ gives that
		%
		\begin{align*}
			\sum_{n = 1}^\infty \frac{x}{x^2 + yn^2} - (\pi/4) y^{-1/2} &= \sum_{n = 1}^\infty \frac{x}{x^2 + yn^2} - \int_0^1 \frac{x}{x^2 + y(n + t)^2}\; dt\\
			&\gtrsim \sum_{n = 1}^\infty \min(ty/x^2,x/yt^3)\\
			&\gtrsim y^{-1/2}/x,
		\end{align*}
		%
		Since $y^{-1/2} \to \infty$ as $y \to 0$, the sum does not converge uniformly in $y$.
	\end{solution}
\end{parts}

\question \begin{parts}
	\part Find an explicit $\varepsilon > 0$ so that for every $x \in [0,1]$,
	%
	\[ |\sqrt{x} - \sqrt{x + \varepsilon}| \leq \frac{1}{200}. \]
	\begin{solution}
		A conjugation trick allows us to write
		%
		\[ \sqrt{x + \varepsilon} - \sqrt{x} = \frac{\varepsilon}{\sqrt{x + \varepsilon} + \sqrt{x}} \leq \varepsilon / 2 \sqrt{x}.  \]
		%
		Alternatively, this may be proved using the mean value theorem and differentiating $\sqrt{x}$. This bound is good for large $x$, i.e. for $x \geq 10000 \varepsilon^2$. For $x \leq 10000 \varepsilon^2$, we write
		%
		\[ \sqrt{x + \varepsilon} - \sqrt{x} \leq \sqrt{x + \varepsilon} \leq \sqrt{10000 \varepsilon^2 + \varepsilon}, \]
		%
		which is bounded by $1/200$ provided that
		%
		\[ 10000 \varepsilon^2 + \varepsilon \leq 1/200. \]
		%
		If $\varepsilon = 1/10000$, then
		%
		\[ 10000 \varepsilon^2 + \varepsilon \leq 2/10000 \leq 1/200, \]
		%
		so this choice of $\varepsilon$ suffices for the inequality to hold.
	\end{solution}

	\part Find an explicit integer $N$ such that there exists a polynomial $P$ of degree at most $N$ such that for $x \in [0,1]$,
	%
	\[ |\sqrt{x} - P(x)| \leq 1/100. \]
	\begin{solution}
		By the last part, and the triangle inequality, it suffices to find such a polynomial $P$ such that
		%
		\[ |\sqrt{x + \varepsilon} - P(x)| \leq 1/100. \]
		%
		The advantage of the $\varepsilon > 0$ is that $f(x) = \sqrt{x + \varepsilon}$ is \emph{differentiable} on $[0,1]$. In particular, if we take $P$ to be the Taylor series of $f(x)$ at $x = 0$ up to order $N$, then Taylor's theorem implies that for all $x \in [0,1]$, there exists $\xi \in [0,x]$ so that
		%
		\[ |\sqrt{x + \varepsilon} - P(x)| = \frac{f^{(N+1)}(\xi)}{(N+1)!} \xi^{N+1}. \]
		%
		Now the $N+1$th derivative of $f$ at $\xi$ is equal to
		%
		\[ (1/2)(-1/2)(-3/2)(-5/2) \cdot (1/2 - N) (\xi + \varepsilon)^{1/2-N} \]
		%
		which has magnitude at most $(1/2) N! (\xi + \varepsilon)^{1/2 - N}$. So
		%
		\[ |\sqrt{x + \varepsilon} - P(x)| \leq (1/2N) (\xi + \varepsilon)^{1/2 - N} \xi^{N+1} = (1/2N) \xi^{3/2} ( 1 + \varepsilon / \xi )^{1/2 - N} \leq 1/2N. \]
		%
		So taking $N = 50$ suffices.
	\end{solution}
\end{parts}

\question Determine all continuous $f: [0,2] \to \CC$ which satisfy $\int_0^2 f(x) x^n\; dx = 0$ for all $n \geq 0$.
\begin{solution}
	Since polynomials are dense in $C[0,2]$, and linearity, which implies $\int_0^2 f(x) p(x)\; dx = 0$ for all polynomials $p$, by taking polynomials which approximate $\overline{f}$ we can prove that $\int_0^2 |f(x)|^2 = \int_0^2 f(x) \overline{f(x)} = 0$. But this means $f(x) = 0$ for all $x \in [0,2]$.
\end{solution}

\question Does the series
%
\[ \sum_{n = 1}^\infty (-1)^n e^{-x/n} \frac{1}{n} \]
%
converge uniformly on $[0,\infty)$?
\begin{solution}
	If $f_x(y) = e^{-x/y} / y$, then
	%
	\[ f_x'(y) = e^{-x/y} y^{-2} [x/y -1], \]
	%
	%\[ f_x(n) - f_x(n-1) \gtrsim e^{-x/n} n^{-2} [ x / n - 1 ] \]
	% x >= 2n_0, then the difference is >> n^{-2}
	so $f_x$ is increasing for $y \leq x$, and decreasing for $y \geq x$. Now given $n_0$, by a telescoping type sum, since each term in the series below is increasing in magnitude, if $x \geq n_0 + 1$ we have
	%
	\[ \left| \sum_{n_0 \leq n \leq x} (-1)^n e^{-x/n} \frac{1}{n} \right| \lesssim 1/x \lesssim 1/n_0. \]
	%
	and
	%
	\[ \left| \sum_{x \leq n \leq \infty} (-1)^n e^{-x/n} \frac{1}{n} \right| \leq 1/x \lesssim 1/n_0. \]
	%
	On the other hand, if $x \leq n_0$, then the series is alternating and decreasing in magnitude for $n \geq n_0$, and so we can bound the series by the first term in the series, which gives
	%
	\[ \left| \sum_{n \geq n_0}^\infty (-1)^n e^{-x/n} \frac{1}{n} \right| \leq 1 / n_0. \]
	%
	For any $\varepsilon > 0$, taking $n_0 \geq 1/\varepsilon$ shows the series \emph{does} converge uniformly.
\end{solution}

\question Assume $\{ a_k \}$ is a positive sequence with $\sum a_k = \infty$. Given any bounded sequence $\{ b_k \}$ show that we can find an increasing sequence $\{ k_n \}$ so that $b_{k_n}$ converges as $n \to \infty$, and $\sum a_{k_n} = \infty$.
\begin{solution}
	TODO
\end{solution}

\question Let $\alpha \in \RR$ and define $u: (1,\infty) \to \RR$ be setting $u(x) = x^\alpha$. For which $\alpha$ do the differences
%
\[ \frac{u(x+h) - u(x)}{h} \to u'(x) \]
%
converge uniformly on $(1,\infty)$.
\begin{solution}
	Let us assume that we take the limit on the right, i.e. for $h > 0$. We can write
	%
	\[ \frac{(x + h)^\alpha - x^\alpha}{h} = \alpha \frac{1}{h} \int_x^{x + h} u^{\alpha - 1}\; du. \]
	%
	For $|h| \leq 1/2$ and $x \leq u \leq x + h$,
	%
	\[ u^{\alpha - 1} - x^{\alpha - 1} \sim x^{\alpha - 2} (u - x), \]
	%
	so
	%
	\[ |\frac{(x+h)^\alpha - x^\alpha}{h} - \alpha x^{\alpha - 1}| \sim h x^{\alpha - 2}. \]
	This converges to zero as $h \to 0$ uniformly in $x$ if and only if $\alpha \leq 2$, so that $x^{\alpha - 2}$ is bounded in $x$ for $x > 1$. The case $h < 0$ is similar.
\end{solution}

\question Let $f$ be real-valued differentiable function defined on the entire real line. Assume that
%
\[ \frac{f(x+h) - f(x)}{h} \to f'(x) \]
%
uniformly as $h \to 0^+$. Show that $f'$ is uniformly continuous. Must $f$ itself be uniformly continuous?
\begin{solution}
	TODO
\end{solution}

\question Let $u: \RR^3 \to \RR$ denote a smooth function and let $\Delta = \partial_x^2 + \partial_y^2 + \partial_z^2$ denote the Laplacian. Suppose $\Delta u = 1$ on $\RR^3$ and that $u(x,y,z) = x^3y^3$ on the sphere of radius $R$ centered at the origin. Compute $u(0,0,0)$.
\begin{solution}
	Without loss of generality, by rescaling $u$, we may assume that $R = 1$, since the general case may be understood by rescaling. Let
	%
	\[ G(x) = \frac{-1}{4\pi} \left[ \frac{R}{|x|} - 1 \right]. \]
	%
	Now for $r \neq 0$,
	%
	\[ \Delta G(r) = r^{-2} (r^2 G')' = G'' + 2r^{-1} G' = 0 \]
	%
	and
	%
	\[ \nabla G(x) = \frac{R x}{4\pi |x|^3}. \]
	%
	Applying Green's identity, we find that
	%
	\[ \int_{\varepsilon \leq |x| \leq R} G \Delta u - \Delta G u = R^{-1} \int_{|x| = R} (G \nabla u - \nabla G u) \cdot x - \varepsilon^{-1} \int_{|x| = \varepsilon} ( G \nabla u - \nabla G u ) \cdot x \]
	%
	Now because $G(x) = 0$ when $|x| = R$,
	%
	\[ R^{-1} \int_{|x| = R} G \nabla u \cdot x = 0 \]
	%
	and since $u$ is odd in $x$, by the symmetry of the sphere,
	%
	\[ R^{-1} \int_{|x| = R} (- \nabla G u) \cdot x = - \fint_{|x| = R} u(x)\; dx = 0. \]
	%
	We also calculate that
	%
	\[ \left| \varepsilon^{-1} \int_{|x| = \varepsilon} G \nabla u \cdot x \right| \lesssim \varepsilon \]
	%
	and
	%
	\[ \varepsilon^{-1} \int_{|x| = \varepsilon} (- \nabla G u) \cdot x = R \fint_{|x| = \varepsilon} u(x) = R u(0) + O(\varepsilon). \]
	%
	We have
	%
	\[ \int_{\varepsilon \leq |x| \leq R} G \Delta u = \frac{-1}{4 \pi} \int_\varepsilon^R r^2 \left[ \frac{R}{r} - 1 \right] = \frac{-R^3}{24 \pi} + O(\varepsilon) \]
	%
	and because $\Delta G = 0$,
	%
	\[ \int_{\varepsilon \leq |x| \leq R} \Delta G u = 0. \]
	%
	So Green's identity, taking $\varepsilon \to 0$, tells us that
	%
	\[ \frac{-R^3}{24 \pi} = R u(0), \]
	%
	or $u(0) = - R^2 / 24 \pi$.
\end{solution}

\question Calculate the line integral
%
\[ \int \frac{-y^3 dx + xy^2 dy}{(x^2 + y^2)^2} \]
%
along the plane curve defined by $10x^{12} + 22 y^8 = 240$, with the positive orientation.
\begin{solution}
	This curve seems like a tricky one to parameterize, so it's probably worth checking how easy it is to apply a Stoke's theorem to integrate on the interior of the curve. We calculate that if $f(x,y) = -y^3 / (x^2 + y^2)^2$ and $g(x,y) = xy^2 / (x^2 + y^2)^2$, then
	%
	\[ g_x = \frac{y^2}{(x^2 + y^2)^2} - \frac{4x^2y^2}{(x^2 + y^2)^3} = \frac{-3x^2y^2 + y^4}{(x^2 + y^2)^3} \]
	%
	and
	%
	\[ f_y = \frac{-3y^2}{(x^2 + y^2)^2} + \frac{4y^4}{(x^2 + y^2)^3} = \frac{-3x^2y^2 + y^4}{(x^2 + y^2)^3}. \]
	%
	So $g_x = f_y$, which tells us the differential form we're integrating on is exact, and so the line integral is zero.
\end{solution}

\question Let $\mathcal{D} \subset \RR^d$ be a compact, convex set containing the origin, with smooth boundary. For every $y \in \partial \mathcal{D}$ let $\alpha(x) \in [0,\pi)$ be the angle between the position vector $x$ and the outer normal vector $\mathfrak{n}(x)$. Let $\omega_d$ be the surface area of the unit sphere in $\RR^d$. Compute
%
\[ \frac{1}{\omega_d} \int_{\partial \mathcal{D}} \frac{\cos(\alpha(x))}{|x|^{d-1}}\; d\sigma(x). \]
\begin{solution}
	Given such a general set $\mathcal{D}$, the only feasible method for solving this problem seems to be applying some kind of variant of Stoke's theorem. The most likely form of the theorem is
	%
	\[ \int_{\partial \mathcal{D}} F \cdot \mathfrak{n} = \int_{\mathcal{D}} \nabla \cdot F. \]
	%
	Can we write the integral required in this form? We can write
	%
	\[ \cos(\alpha(x)) = \frac{\mathfrak{n}(x) \cdot x}{|x|}, \]
	%
	and so the integral \emph{is} in this form, if $F(x) = x / |x|^d$. We calculate that
	%
	\[ \nabla \cdot F(x) = d |x|^{-d} + x \cdot \nabla \{ |x|^{-d} \} = d/|x|^d + x \cdot -d x/|x|^{d+2} = d |x|^{-d} - d |x|^{-d} = 0. \]
	%
	So applying Stoke's theorem (or the divergence theorem, or whatever you'd like to call it), we conclude that the integral is equal to zero.
\end{solution}

\question Fix $f \in C_c^1(\RR)$, and let $b > 0$. Show
%
\[ A_b(x) = \lim_{\varepsilon \to 0^+} \int_{\RR - [-\varepsilon,b\varepsilon]} \frac{f(x-y)}{y}\; dy \]
%
exists for all $x \in \RR$. How do $A_b$ and $A_{b'}$ differ for $b \neq b'$.
\begin{solution}
	The integral is singular when $y = 0$, so we must exploit cancellation near that term using the smoothness of the function $f$. We can write $f(x - y) = f(x) - y \psi_x(y)$, where $\psi$ is continuous. Then for $\varepsilon \leq \varepsilon'$,
	%
	\[ \int_{[-\varepsilon',b \varepsilon'] \setminus [-\varepsilon,b \varepsilon]} \frac{f(x-y)}{y}\; dy = \int_{[-\varepsilon',b \varepsilon'] \setminus [-\varepsilon,b \varepsilon]} \frac{f(x)}{y} - \psi_x(y)\; dy. \]
	%
	By the boundedness of $\psi_x$,
	%
	\[ \left| \int_{[-\varepsilon',b \varepsilon'] \setminus [-\varepsilon,b \varepsilon]} \psi_x(y)\; dy \right| \lesssim \varepsilon' \]
	%
	A change of variables gives that
	%
	\begin{align*}
		\int_{[-\varepsilon',b \varepsilon'] \setminus [-\varepsilon,b \varepsilon]} \frac{f(x)}{y}\; dy &= \int_{[-1,b] \setminus [ -\varepsilon / \varepsilon', b \varepsilon / \varepsilon' ]} \frac{f(x)}{y}.\\
		&= f(x) [ \log(b) - \log( (-\varepsilon / \varepsilon') b ) - [\log(1) - \log((\varepsilon/\varepsilon'))] ]\\
		&= 0.
	\end{align*}
	%
	So, uniformly in $\varepsilon$,
	\[ \left| \int_{[-\varepsilon',b \varepsilon'] \setminus [-\varepsilon,b \varepsilon]} \frac{f(x-y)}{y}\; dy \right| \lesssim \varepsilon', \]
	%
	and we may now take $\varepsilon' \to 0$ to prove convergence of the integral.

	For $b \leq b'$, we have that
	%
	\begin{align*}
		&\int_{\RR - [-\varepsilon,b'\varepsilon]} \frac{f(x-y)}{y}\; dy - \int_{\RR - [-\varepsilon,b\varepsilon]} \frac{f(x-y)}{y}\; dy\\
		&\quad\quad = \int_{b \varepsilon}^{b' \varepsilon} \frac{f(x-y)}{y}\; dy\\
		&\quad\quad = \int_b^{b'} \frac{f(x - \varepsilon y)}{y}\; dy.
	\end{align*}
	%
	As $\varepsilon \to 0$, the integrand converges pointwise to $f(x) / y$, and the terms are bounded by a constant multiple of $1/y$, which is integrable on $[b,b']$. By the Lebesgue dominated convergence theorem, as $\varepsilon \to 0$ this integral converges to
	%
	\[ \int_b^{b'} \frac{f(x)}{y}\; dy. \]
	%
	Thus
	%
	\[ A_b(x) - A_{b'}(x) = \int_b^{b'} \frac{f(x)}{y}\; dy.  \]
\end{solution}

\question Prove or disprove that
%
\[ \lim_{\varepsilon \to 0} \int_{x^2 + y^2 \geq \varepsilon^2} \frac{f(x,y)}{(x + iy)^3}\; dx\; dy \]
%
exists for every $f \in C^2(\RR^2)$ with compact support. 
\begin{solution}
	The integral becomes singular near the origin as $\varepsilon \to 0$, and so the limit can only exist due to \emph{cancellation} near the origin. It might be useful to start testing the integral when $f(x,y) = x$ or $f(x,y) = y$, since one can often approximate a general function by such functions using a Taylor approximation. We have, using polar coordinates, and the orthogonality of  trigonometric functions oscillating at different frequencies,
	%
	\[ \int_{x^2 + y^2 = \varepsilon^2} \frac{x}{(x + iy)^3} = \varepsilon^{-2} \int_0^{2\pi} \cos(\theta) e^{-3i \theta}\;d \theta = 0. \]
	%
	But this means that
	%
	\[ \int_{\varepsilon^2 \leq x^2 + y^2 \leq \delta^2} \frac{x}{(x + iy)^3}\; dx\; dy = 0 \]
	%
	for any $\varepsilon$ and $\delta$. For similar reasons,
	%
	\[ \int_{\varepsilon^2 \leq x^2 + y^2 \leq \delta^2} \frac{y}{(x + iy)^3}\; dx\; dy = 0. \]
	%
	Because the functions $1/(x + iy)^3$, $x^2/(x + iy)^3$, $xy/(x + iy)^3$, and $y^2/(x + iy)^3$ are all \emph{odd}, for $g(x,y) \in \{ 1, x^2, xy, y^2 \}$,
	%
	\[ \int_{\varepsilon^2 \leq x^2 + y^2 \leq \delta^2} \frac{g(x,y)}{(x + iy)^3}\; dx\; dy = 0. \]
	%
	By linearity and the above bounds, we conclude that for \emph{any} quadratic polynomial $p(x,y)$,
	%
	\[ \int_{\varepsilon^2 \leq x^2 + y^2 \leq \delta^2} \frac{p(x,y)}{(x + iy)^3}\; dx\; dy = 0. \]
	%
	Now suppose $f$ is a $C^2$ function. If $p(x,y)$ is the quadratic polynomial obtained from the Taylor approximation of $f$ at the origin up to order two, then we can find $M > 0$ so that for $x^2 + y^2 \leq 1$, $|f(x,y) - p(x,y)| \leq M (x^2 + y^2)$. But this means that
	%
	\begin{align*}
		\int_{\varepsilon^2 \leq x^2 + y^2 \leq \delta^2} \frac{f(x,y)}{(x + iy)^3} &= \int_{\varepsilon^2 \leq x^2 + y^2 \leq \delta^2} \frac{f(x,y) - p(x,y)}{(x + iy)^3},
	\end{align*}
	%
	and
	%
	\[ \left| \int_{\varepsilon^2 \leq x^2 + y^2 \leq \delta^2} \frac{f(x,y) - p(x,y)}{(x + iy)^3} \right| \lesssim M \int_{\varepsilon^2 \leq x^2 + y^2 \leq \delta^2} \frac{1}{(x^2 + y^2)^{1/2}} \leq M \delta. \]
	%
	Taking $\delta \to 0$, we conclude that
	%
	\[ \lim\nolimits_{\delta \to 0^+} \limsup\nolimits_{\varepsilon < \delta} \int_{\varepsilon^2 \leq x^2 + y^2 \leq \delta^2} \frac{f(x,y)}{(x + iy)^3} = 0, \]
	%
	which proves that the singular integral above is well defined.
\end{solution}

\question Let
%
\[ s_N(x) = \sum_{n = 1}^N (-1)^n \frac{x^{3n}}{n^{2/3}}. \]
%
Prove that $s_N$ converges to a limit function on $[0,1]$, and $\| s - s_N \|_{L^\infty[0,1]} \lesssim N^{-2/3}$.
\begin{solution}
	The terms of the series are always decreasing, alternate in sign, and converge to zero, and thus converge by the Leibnitz alternating series. Reasoning à la the Leibnitz alternating series theorem, we conclude that the series is at most as large as it's initial term. Thus for any $N \leq M$ and $0 \leq x \leq 1$,
	%
	\[ |\sum_{n = N}^M (-1)^n \frac{x^{3n}}{n^{2/3}}| \leq |x|^{3N} N^{-2/3} \leq N^{-2/3}. \]
	%
	Taking $M \to \infty$, we thus conclude that $|s_N(x) - s(x)| \leq N^{-2/3}$.
\end{solution}

\question
\begin{parts}
	\part What is the volume of the region $\Omega$ in $\RR^n$ defined by
%
\[ \Omega = \{ x \in \RR^n: x_1,\dots,x_n > 0, 0 < x_1 + \cdots + x_n < 1 \} \]
	\begin{solution}
		It is simplest to calculate this volume using induction. Let $A_n(t)$ denote the volume $\Omega_{n,t}$ of the region in $\RR^n$ defined by $\{ x : x_1,\dots,x_n > 0, 0 < x_1 + \cdots + x_n < t \}$, and let $A_n = A_n(1)$. Then $t \Omega_{n,1} = \Omega_{n,t}$, so $A_n(t) = t^n A_n$. Fubini's theorem implies that
		%
		\[ A_{n+1} = \int_0^1 A_n(1-t)\; dt = A_n \int_0^1 (1 - t)^n\; dt = A_n / (n+1). \]
		%
		Since $\Omega_{1,1} = [0,1]$, $A_1 = 1$, and so we conclude by induction that $A_n = 1/n!$.
	\end{solution}

	\part What is the area of the parallelogram spanned by the vectors $(1,1,-1,1)$ and $(2,1,2,1)$ in $\R^4$?
	\begin{solution}
		The two vectors have length $\sqrt{ 4 }$ and $\sqrt{10}$, and if $\theta$ is the angle between the two vectors, then $\sqrt{40} \cos(\theta) = 2$, so $\cos(\theta) = 2 /\sqrt{40} = 1/\sqrt{10}$. The area of the parallelogram is thus the same as the area of the parallelogram spanned by the two vectors $\sqrt{4} (1,0)$ and $\sqrt{10} (\cos(\theta),\sin(\theta))$, which is equal to the magnitude of the determinant of the matrix obtained by stacking the vectors on top of one another, i.e. to
		%
		\[ \det \begin{pmatrix} \sqrt{4} & 0 \\ \sqrt{10} \cos(\theta) & \sqrt{10} \sin(\theta) \end{pmatrix} = \sqrt{40} \sin(\theta) = \sqrt{40} ( 1 - \cos^2(\theta) )^{1/2} = \sqrt{40} \sqrt{ 9/10} = \sqrt{36} = 6. \]
		%
		Alternatively, if we use the vectors $v$ and $w$ as columns to form a matrix $A$, then the determinant of the Gram matrix $A^T A$ is the area of the parallelogram. This might be expected, since the determinant of the matrix $A^T A$ is invariant under conjugation of $A$ by orthogonal matrices, and for vectors. Conjugating by the right matrices, we may assume that $v = (v_1,v_2,0,\cdots,0)$ and $w = (w_1,w_2,0,\cdots,0)$, and then write 
		%
		\[ A = \begin{pmatrix} \tilde{A} \\ 0 \end{pmatrix}, \]
		%
		then
		%
		\[ A^T A = \tilde{A}^T \tilde{A} \]
		%
		and so $\det(A^T A) = \det(\tilde{A}^T \tilde{A}) = \det(\tilde{A})^2$, and $\det(\tilde{A})$ is evidently seen to be the area of the parallogram spanned by $v$ and $w$ (it is essentially the definition of the determinant of a matrix). In this case the determinant is
		%
		\[ \begin{pmatrix} 1 & 1 & -1 & 1 \\ 2 & 1 & 2 & 1 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ 1 & 1 \\ -1 & 2 \\ 1 & 1 \end{pmatrix} = \begin{pmatrix} 4 & 2 \\ 2 & 10 \end{pmatrix}. \]
		%
		The determinant of this matrix is $36$, and the area of the parallelogram is the square root of this determinant, i.e. $6$. In other words, if two vectors have lengths $l_1$ and $l_2$, and dot product $a$, then the area of the parallelogram they generate is $\sqrt{l_1^2 l_2^2 - a^2}$.
	\end{solution}

	\part What is the volume of the box spanned by the vectors $(1,1,0,0,0)$, $(0,1,1,1,0)$ and $(0,0,1,-1,1)$ in $\R^5$?
	\begin{solution}
		We use the Gram Matrix as in the last problem, which is
		%
		\[ \begin{pmatrix} 1 & 1 & 0 & 0 & 0 \\ 0 & 1 & 1 & 1 & 0 \\ 0 & 0 & 1 & -1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{pmatrix} = \begin{pmatrix} 2 & 1 & 0 \\ 1 & 3 & 0 \\ 0 & 0 & 3 \end{pmatrix}. \]
		%
		The determinant of this matrix is $15$, so the volume is $\sqrt{15}$.
	\end{solution}
\end{parts}

\question Suppose $f$ is a positive, decreasing function on $(0,\infty)$. Let $\varepsilon > 0$ be a fixed, positive number
\begin{parts}
	\part Suppose that for $0 < x < \infty$, $f(2x) \leq 2^{-1-\varepsilon} f(x)$. Prove there is a constant $C$, depending only on $\varepsilon$, so that for all $a > 0$,
	%
	\[ \int_a^\infty f(x)\; dx \leq C a f(a). \]
	\begin{solution}
		We work dyadically. For $j \geq 0$ if we define
		%
		\[ A_j = \int_{2^j a}^{2^{j+1} a} f(x)\; dx, \]
		%
		then changing variables implies that
		%
		\[ A_j = 2 \int_{2^{j-1} a}^{2^j a} f(2x)\; dx \leq 2^{-\varepsilon} A_{j-1}. \]
		%
		So $A_j \leq 2^{-j\varepsilon} A_0$, and so
		%
		\[ \sum A_j \leq \sum 2^{-j \varepsilon} A_0 \lesssim_\varepsilon A_0. \]
		%
		All that remains is to prove that $A_0 \lesssim a f(a)$. But this follows because $f$ is decreasing, so
		%
		\[ A_0 = \int_a^{2a} f(x) \leq \int_a^{2a} f(a)\; dx = a f(a). \]
	\end{solution}

	\part Suppose that for all $0 < x < \infty$, $f(x) \leq 2^{1 - \varepsilon} f(2x)$. Prove there is a constant $C$ so that
	%
	\[ \int_0^a f(x)\; dx \leq C a f(a). \]
	\begin{solution}
		Work as in the last argument, i.e. proving that
		%
		\[ \int_{a/2^j}^{a/2^{j-1}} f(x)\; dx \leq 2^{-\varepsilon} \int_{a/2^{j-1}}^{a/2^{j-2}} f(x)\; dx, \]
		%
		and then interating this inequality to get geometric decay.
	\end{solution}

	\part Suppose that for all $0 < x < \infty$, $f(2x) \geq f(x) / 2$. Prove that $\int_1^\infty f(x)\; dx$ diverges.
	\begin{solution}
		The quantities $A_j = \int_{2^j}^{2^{j+1}} f(x)$ are increasing, which makes it impossible for $\sum A_j = \int_1^\infty f(x)\; dx$ to converge.
	\end{solution}
\end{parts}

\question For $a,b > 0$, let
%
\[ F(a,b) = \int_{-\infty}^\infty \frac{dx}{x^4 + (x - a)^4 + (x - b)^4}. \]
%
For which $p > 0$ is it true that
%
\[ \int_0^1 \int_0^1 F(a,b)^p\; da\; db < \infty? \]
Hint: First show that $F(a,b) \sim 1/b^3$ if $a \leq b$.
\begin{solution}
	Without loss of generality we may suppose that $a \leq b$. For $x \geq b/2$,
	%
	\[ x^4 + (x - a)^4 + (x - b)^4 \sim x^4 \]
	%
	and so
	%
	\[ \int_{b/2}^\infty \frac{1}{x^4 + (x - a)^4 + (x - b)^4} \sim \int_{b/2}^\infty \frac{1}{x^4} = 8/b^3. \]
	%
	For $x \leq b/2$,
	%
	\[ x^4 + (x - a)^4 + (x - b)^4 \sim (x - b)^4 \]
	%
	and so
	%
	\[ \int_{-\infty}^{b/2} \frac{1}{x^4 + (x - a)^4 + (x - b)^4} \sim \int_{-\infty}^{b/2} \frac{1}{(x - b)^4} = 8/b^3. \]
	%
	Thus $F(a,b) \sim 8/b^3$. So
	%
	\[ \int_0^1 \int_0^1 F(a,b)^p \sim \int_0^1 b^{-3p}\; db \]
	%
	which is finite if and only if $p < 1/3$.
\end{solution}

\question Let $\{ f_n \}$ be a sequence of continuous functions on $[0,1]$ and assume $\sup_n |f_n(x)| < \infty$ for every $x \in [0,1]$. Show there is an interval $(a,b) \subset [0,1]$ and $M > 0$ so that $|f_n(x)| \leq M$ for all $x \in (a,b)$ and all $n$.
\begin{solution}
	Let
	%
	\[ X_M = \{ x \in [0,1]: \sup_n f_n(x) \leq M \}. \]
	%
	Then $X_M$ is closed by the continuity of $\{ f_n \}$, and $\bigcup_M X_M = [0,1]$. The Baire category theorem thus guarantees that $(a,b) \in X_M$ for some $M > 0$. But this implies the required result.
\end{solution}

\question Consider a function $f: \RR \to \RR$. Prove that if the second derivative $f''(x_0)$ exists then
%
\[ \lim_{h \to 0} \frac{f(x_0 + h) - 2 f(x_0) + f(x_0 - h)}{h^2} = f''(x_0). \]
\begin{solution}
	TODO
\end{solution}

\question Let $f$ be defined on $[-2,2]$, so that
%
\[ \frac{f(b) - f(a)}{b - a} \leq \frac{f(c) - f(b)}{c - b} \leq A \]
%
whenever $-2 \leq a < b < c \leq 2$ (i.e. $f$ is a convex function), Show that there is $C \geq 0$ so that for $|h| \leq 1$<
%
\[ \int_{-1}^1 |f(x+h) + f(x - h) - 2f(x)| \leq C h^2. \]
\begin{solution}
	Applying convexity, we have
	%
	\[ f(x + h) - f(x) \geq f(x - h) - f(x), \]
	%
	so we can forget about the absolute value signs, and then
	%
	\[ \int_{-1}^1 f(x + h) + f(x - h) - 2f(x) = \int_0^h [f(1+t) - f(1 - t)] + [ f(-t) - f(t) ]\; dt. \]
	%
	Convexity implies that for
	%
	\[ \frac{f(1 + t) - f(1 - t)}{2t} \leq \frac{f(2) - f(1 + t)}{1 - t} \]
	%
	so that for $t \leq 1/2$,
	%
	\[ f(1 + t) - f(1 - t) \leq 8t \| f \|_{L^\infty[0,2]}. \]
	%
	Integration thus gives
	%
	\[ \int_0^h [ f(1+t) - f(1 - t) ] \leq 4h^2 \| f \|_{L^\infty[0,2]}. \]	
	%
	A similar argument bounds the integral of $f(-t) - f(t)$, completing the proof.
\end{solution}

\question Let $K$ be a continuous function on the unit square $Q = [0,1]^2$ with the property that $|K(x,y)| \leq 1$ for all $(x,y) \in Q$. Show that there is a continuous function $g$ on $[0,1]$ so that
%
\[ g(x) + \int_0^1 K(x,y) g(y)\; dy = \frac{e^x}{1 + x^2} \]
%
for $x \in [0,1]$.
\begin{solution}
	By compactness and continuity, $|K(x,y)| \leq 1 - \varepsilon$ for some $\varepsilon > 0$. Taking in absolute values to the function
	%
	\[ Tf(x) = \frac{e^x}{1 + x^2} - \int_0^1 K(x,y) g(y)\; dy \]
	%
	shows that $\| Tf - Tg \|_{L^\infty[0,1]} \leq (1 - \varepsilon) \| f - g \|_{L^\infty[0,1]}$. The Banach fixed point theorem then gives the existence and uniqueness of $g$.
\end{solution}

\question Prove there is a unique smooth function $f$ defined on $[0,1]$ which satisfies the integral equation
%
\[ f(x) + \int_0^x t \cos(tx) \frac{f(t)}{1 + f(t)^2} = 0 \]
%
for all $x \in [0,1]$.
\begin{solution}
	The idea is that $f(t) / (1 + f(t)^2)$ is \emph{close} to linear in $f$ when $f(t)$ is small, and the assumption forces $f(0) = 0$, so that the integral is linear for small $t$, which makes ocntorl easier. Define
	%
	\[ X_\delta = \{ f : \| f \|_{L^\infty[0,\delta]} \leq 1/100 \}. \]
	%
	Let $A(x) = x/(1 + x^2)$. Then for $|x|,|y| \leq 1/100$, taking a Taylor series in $1/(1 + x^2)$ and then using the mean value theorem gives that
	%
	\[ |A(x) - A(y)| = |\sum (-1)^k [ x^{2k+1} - y^{2k + 1} ]| \leq |x - y| \sum_{k = 0}^\infty \frac{2k + 1}{100^{2k}} \leq (1 + 1/100)|x - y|. \]
	%
	It follows that if we define
	%
	\[ Tf(x) = \int_0^x t \cos(tx) A(f(t))\; dt, \]
	%
	then provided that $|f(t)|, |g(t)| \leq 1/100$ for $0 \leq t \leq \delta$, taking in triangle inequalities shows that for $|x| \leq \delta$,
	%
	\[ |Tf(x) - Tg(x)| \leq (1 + 1/100) \int_0^x t |f(t) - g(t)|\; dt \leq (1/2 + 1/200) \| f - g \|_{L^\infty} \delta^2.  \]
	%
	This shows that $T$ is a contraction map on $[0,\delta]$, and thus by the Banach fixed point theorem has a unique fixed point.

	But now we can show that any function $f$ satisfying the integral formula above must be zero everywhere. Indeed, either $|f(t)| < 1/100$ for all $t \in [0,1]$, and the argument above applied with $\delta = 1$ show that $f = 0$, or there exists $\delta$ so that $|f(\delta)| = 1/100$, and then the argument applied with $X_\delta$ above gives a contradiction to the fact that $f$ does not vanish on $[0,\delta]$.
\end{solution}







\newpage
\section{Spring 2025 Qualifying Exam Questions}

\question Let $\{ a_n \}$ be a sequence of complex numbers and suppose $\sum_{n = 1}^\infty a_n$ converges. Prove that
%
\[ \lim_{N \to \infty} \sum_{k = 1}^N (1 - k/N) a_k = \sum_{n = 1}^\infty a_n. \]
\begin{solution}
	Let $S = \sum a_n$. Then
	%
	\[ \sum_{k = 1}^N (1 - k/n) a_k - S = \sum_{k = N+1}^\infty a_n - \frac{1}{N} \sum_{k = 1}^N k a_k. \]
	%
	The convergence of $\sum a_n$ implies $\sum_{k = N+1}^\infty a_n \to 0$ as $N \to \infty$. So it suffices to show that the term $B_n = N^{-1} \sum_{k = 1}^N k a_k$ converges to zero as $N \to \infty$. If $\sigma_{n,m} = \sum_{k = n}^m a_k$, then
	%
	\[ B_n = N^{-1} \sum_{k = 1}^N \sigma_{k,n}, \]
	%
	so $B_n$ is the average of the numbers $\sigma_{k,n}$, and so we must show that when $N$ is large, \emph{most} of these terms are small. The hope is that the terms for large $k$ will be small, and for small $k$ a mostly trivial bound should suffice (because we are only controlling average behaviour). Suppose $|a_n| \leq M$ for all $n$. For any $\varepsilon > 0$, there is $k_0$ so that for $k_0 \leq k \leq n$, $|\sigma_{k,n}| \leq \varepsilon$. This implies that for any $k \leq k_0$,
	%
	\[ |\sigma_{k,n} - S| = |\sigma_{1,n} - S| + k M \leq \varepsilon + k M \leq \varepsilon + k_0 M. \]
	%
	Therefore,
	%
	\[ |N^{-1} \sum_{k = k_0}^N \sigma_{k,n}| \leq \varepsilon, \]
	%
	and
	%
	\[ N^{-1} \sum_{k = 1}^{k_0} \sigma_{k,n} \leq (k_0/N) (\varepsilon + k_0 M). \]
	%
	Combinign these bounds and taking $N \to \infty$ gives
	%
	\[ \limsup_{N \to \infty} |B_N| \leq \varepsilon, \]
	%
	and taking $\varepsilon \to 0$ yields that $B_N \to 0$ as $N \to \infty$.
\end{solution}

\question Let
%
\[ f_n(x) = \int_1^n t e^{it^3 x}\; dt. \]
%
Let $\varepsilon > 0$. Prove the sequence $\{ f_n \}$ converges uniformly on $(\varepsilon,\infty)$.
\begin{solution}
	There's oscillation in this integral, so integration by parts should be useful. One integration by parts gives that for $m \leq n$,
	%
	\begin{align*}
		\int_m^n t e^{i t^3 x} &= \int_m^n \frac{1}{itx} [ e^{it^3 x} ]'\\
		&= \frac{e^{in^3 x}}{inx} - \frac{e^{mix}}{mix} + \frac{1}{ix} \int_m^n \frac{e^{it^3}}{t^2}.
	\end{align*}
	%
	Taking in absolute values gives that
	%
	\[ \left| \int_m^n t e^{it^3 x} \right| \lesssim \frac{1}{mx}. \]
	%
	But this inequality tells us that $\| f_n - f_m \|_{L^\infty(\varepsilon,\infty)} \lesssim 1/m \varepsilon$, which implies uniform convergence.
\end{solution}

\question Let $\RR^{n \times n}$ be the space of all real $n \times n$ matrices. Let $\| \cdot \|$ be any norm on this vector space. For $A \in \RR^{n \times n}$, let $\text{Tr}(A)$ be the trace of $A$. Prove that there are neighborhods $U$ and $V$ of the identity matrix $I$ so that for $B \in V$ there exists a unique $A \in U$ such that
%
\[ \frac{1}{n} \text{Tr}(A) A^5 = B. \]
\begin{solution}
	Since all norms on a finite dimensional vector space are equivalent, the norm $\| \cdot \|$ defines the same topology as identifying $\RR^{n \times n}$ with $\RR^{n^2}$ by taking the entries as coordinates. The result would then follow from the inverse function theorem from calculus if we could prove that the derivative of the left hand side was invertible at the origin. In our calculations below, we will use the operator norm as the norm, which we may do without loss of generality since all norms are equivalent. It is then immediate that $\| A^n \| = O( \| A \|^2 )$ for $n \geq 2$ and $\| \text{Tr}(A) A^n \| = O( \| A \|^2 )$ for $n \geq 1$. We thus see that
	%
	\begin{align*}
		\frac{1}{n} \text{Tr}(I + A) (I + A)^5 - I &= \frac{1}{n} ( n + \text{Tr}(A) ) (I + A)^5 - I\\
		&= [ I + 5 A + O(\| A \|^2) ] + [\text{Tr}(A) / n] I - I + O( \|A\|^2)\\
		&= ( \text{Tr}(A) / n ) I + 5 A + O( \| A \|^2).
	\end{align*}
	%
	We have thus computed the derivative of this map at the origin, and so it suffices to show that the map $A \mapsto ( \text{Tr}(A) / n ) I + 5 A$ is invertible. If
	%
	\[ ( \text{Tr}(A) / n ) I + 5 A = 0, \]
	%
	then $A = [- 5 \text{Tr}(A) / n] I$, so taking the trace of both sides yields that $\text{Tr}(A) = -5 \text{Tr}(A)$, yielding that $\text{Tr}(A) = 0$, and thus that $A = 0$. Thus we have proved invertibility, completing the proof.
\end{solution}

\question Let $f$ be a measurable function on $[0,1]$ with $f(x) > 0$ for all $x \in [0,1]$, and assume $\int_0^1 f(x)\; dx < \infty$. let $0 < \gamma < 1$. Prove
%
\[ \inf_{|E| = \gamma} \int_E f(x)\; dx > 0. \]
%
Here the infinum is taken over all measurable sets $E$ with Lebesgue measure $\gamma$.
\begin{solution}
	Let $F_j = \{ x : f(x) \sim 2^j \}$. Since $M = \int_0^1 f(x)\; dx$ is finite, we know that $\sum 2^j |F_j| \lesssim M$. Let $\gamma_n = \sum_{k \leq n} |F_k|$. Since $\gamma_n \to 0$ as $n \to -\infty$ by the Lebesgue dominated convergence theorem, it suffices to show that for each $n$,
	%
	\[ \inf_{|E| = \gamma_n} \int_E f(x)\; dx > 0. \]
	%
	We claim that
	%
	\[ \inf_{|E| = \gamma_n} \int_E f(x)\; dx \gtrsim \sum_{k \leq n} 2^k |F_k|, \]
	%
	which would prove what was required. If $E_j = E \cap F_j$, then
	%
	\[ \int_E f(x) \gtrsim \sum 2^j |E_j|. \]
	%
	If we write $a_k = |E_k|$, then $\sum a_k = \gamma_n$ and $a_k \leq |F_k|$, and the proof will follow if we can show
	%
	\[ \sum 2^k a_k \geq \sum_{k \leq n} 2^k |F_k|. \]
	%
	For any $\varepsilon > 0$, we may find $N \geq n$ so that $\sum a_k \geq \gamma_n - \varepsilon$,
	%
	\[ \sum 2^k a_k - \sum_{-N \leq j \leq N} 2^k a_k \leq \varepsilon, \]
	%
	such that $\gamma_n - \sum_{k = -N}^n |F_k| \leq \varepsilon$, and
	%
	\[ \sum_{k \leq n} 2^k |F_k| - \sum_{-N \leq k \leq n} 2^k |F_k| \leq \varepsilon. \]
	%
	For any fixed $N \geq n$, consider the minimizer $a_k^*$ to the problem of minimizing $\sum_{-N \leq k \leq N} 2^k a_k$ with $\sum a_k \geq \gamma_n - \varepsilon$ and with $a_k \leq |F_k|$ for each $k$. If we define $\delta_k = |F_k| - a_k^*$, and if $a_{k_0}^* > 0$, $\delta_k = 0$ for $k < k_0$ (for otherwise replacing $a_k^*$ with $a_k^* + \delta$ and replacing $a_{k_0}^*$ with $a_{k_0}^* - \delta$ for small $\delta$ will give a feasible solution which decreases the value of $\sum 2^k a_k$). But this implies that $a_k^* = |F_k|$ for all $k \leq n$, and so for a general sequence $\{ a_k \}$,
	%
	\[ \sum_{k = -N}^N 2^k a_k \geq \sum_{k = -N}^n 2^k |F_k| \geq \sum_{k = -\infty}^n 2^k |F_k| - \varepsilon. \]
	%
	Thus
	%
	\[ \sum 2^k a_k \geq \sum_{k = -N}^N 2^k a_k \geq \sum_{k = -\infty}^n 2^k |F_k| - \varepsilon. \]
	%
	Taking $\varepsilon \to 0$ completes the proof.
\end{solution}

\question Assume $\{ a_m \}$ and $\{ b_m \}$ are sequences in $l^2$. Prove that
%
\[ \sum_{k = 1}^\infty \sum_{m = k+1}^\infty \frac{a_k b_m}{m} < \infty. \]
\begin{solution}
	In order to apply Cauchy-Schwartz, we must somehow introduce a decay in $k$ so that we get convergence when we sum in $k$. So we write
	%
	\begin{align*}
		\sum_{k = 1}^\infty \sum_{m = k+1}^\infty \frac{a_k b_m}{m} &\lesssim  \sum_{k = 1}^\infty \sum_{m = k+1}^\infty \frac{a_k b_m (k/m)^{\delta/2} (m/k)^{\delta/2}}{m}\\
		&\lesssim \left( \sum_{k = 1}^\infty \sum_{m = k+1}^\infty \frac{a_k^2 k^\delta}{m^{1 + \delta}} \right)^{1/2} \left( \sum_{k = 1}^\infty \sum_{m = k+1}^\infty \frac{b_m^2}{m^{1 - \delta} k^\delta} \right)^{1/2}\\
		&\lesssim \left( \sum_{k = 1}^\infty a_k^2 k^\delta \sum_{m = k+1}^\infty \frac{1}{m^{1 + \delta}} \right)^{1/2} \left( \sum_{m = 1}^\infty b_m^2 m^{\delta - 1} \sum_{k = 1}^m \frac{1}{k^\delta} \right)^{1/2}\\
		&\lesssim ( \sum_{k = 1}^\infty a_k^2 )^{1/2} \left( \sum_{m = 1}^\infty b_m^2 \right)^{1/2} < \infty.
	\end{align*}
\end{solution}

\question Let $\mu$ be a positive, finite measure on $\RR$. Define
%
\[ G(x) = \mu((-\infty,x]) \]
%
and let $\beta \in \RR$. Evaluate
%
\[ \int_{-\infty}^\infty G(x + \beta) - G(x)\; dx. \]
\begin{solution}
	$G(x + \beta) - G(x) = \mu((x,x + \beta ])$, and so by Fubini's theorem,
	%
	\begin{align*}
		\int_{-\infty}^\infty G(x + \beta) - G(x)\; dx &= \int_{-\infty}^\infty \int_{(x,x + \beta]}\; d\mu\; dx\\
		&= \int_{-\infty}^\infty \int_{[y - \beta, y)}\; dx\; d\mu\\
		&= \int_{-\infty}^\infty \beta\; d\mu = \beta \mu(\RR).
	\end{align*}
\end{solution}

\question Let $f \in L^1(\TT)$ (i.e. 1 periodic with $\int_0^1 |f(x)|\; dx < \infty$), and suppose $f$ is differentiable at $x_0$. Prove
%
\[ \lim_{N \to \infty} \sum_{k = -N}^N \widehat{f}(k) e^{2 \pi i k x_0} = f(x_0). \]
\begin{solution}
	Assume without loss of generality that $x_0 \in (0,1)$. We can write
	%
	\[ \sum_{k = - N}^N \widehat{f}(k) e^{2 \pi i k x_0} = (D_N * f)(x_0), \]
	%
	where $D_N$ is the Dirichlet kernel, i.e.
	%
	\[ D_N(x) = \sum_{k = -N}^N e^{2 \pi i k x_0} = \frac{\sin((2 \pi (N + 1/2) x))}{\sin(2 \pi x / 2)}, \]
	%
	and
	%
	\[ (D_N * f)(x_0) = \int D_N(y) f(x_0 - y). \]
	%
	If $f$ vanishes in a neighborhood of $x_0$, then
	%
	\[ |(D_N * f)(x_0)| \lesssim \int_{|y| \gtrsim 1} D_N(y)\; dy \lesssim 1/N, \]
	%
	and so $(D_N * f)(x_0)$ converges to zero. So we may assume without loss of generality that $f$ is supported on $[x_0 - \delta, x_0 + \delta]$ for some $\delta < \min(|x_0|, 1 - |x_0|)$. Let $\eta$ be a smooth function supported on $(0,1)$ and equal to one for $|y| \leq \delta$. Then we can write $f(x_0 - y) = f(x_0) \eta(y) + y \psi(y)$, where $\psi$ is continuous and supported on $|y| \leq \delta$. Then
	%
	\[ (D_N * f)(x_0) - f(x_0) = f(x_0) \int D_N(y) [\eta(y) - 1]\; dy + \int \sin(2 \pi (N + 1/2) y ) \left[ \frac{y}{\sin(2 \pi y / 2)} \psi(y) \right]\; dy. \]
	%
	The first integral converges to $f(x_0)$ by an integration by parts. The second integral converges to zero by the Riemann-Lebesgue lemma on $\RR$.
\end{solution}

\question Let $\mathcal{F}$ be a set of $C_c^\infty$ functions on $\RR^n$ bounded in the topology of $C_c^\infty(\RR^n)$. Prove there exists a compact set $K$ such that for each $f \in \mathcal{F}$ is supported in $K$.
\begin{solution}
	I wouldn't recommend doing this problem unless you seriously know your topological vector space theory, i.e. remembering what it means for a set to be bounded in a topological vector space, and remembering the topology that defines the space $C_c^\infty(\RR^n)$ (it is \emph{not} a Frechet space, but rather an \emph{LF} space, an inductive limit of Frechet spaces).
%	Recall that a set is \emph{bounded} if for any neighborhood $B$ of the origin, there is $R > 0$ so that $\mathcal{F} \subset R B$.
\end{solution}

\question Define $g: \RR \to \RR$ by
%
\[ g(x) = \begin{cases} x^2 &: x \geq 0 \\ 0 &: x < 0 \end{cases} \]
%
Determine $g'$, $g''$, and $g''$, and find a simple expression for $f * g'''$ if $f \in C_c^\infty(\RR)$.
\begin{solution}
	An integration by parts will show that
	%
	\[ g'(x) = \begin{cases} 2x &: x \geq 0 \\ 0 &: x < 0 \end{cases}, \]
	%
	i.e. you must show that if $\phi \in C_c^\infty(\RR)$, then
	%
	\[ \int_0^\infty x^2 \phi'(x) = - \int_0^\infty 2x \phi(x)\; dx. \]
	%
	Next, we claim that
	%
	\[ g''(x) = \begin{cases} 2 &: x \geq 0 \\ 0 &: x < 0 \end{cases}. \]
	%
	This follows if we can show that if $\phi \in C_c^\infty(\RR)$, then
	%
	\[ \int_0^\infty x^2 \phi''(x) = \int_0^\infty 2 \phi(x)\; dx, \]
	%
	and follows from another integration by parts. Finally, we claim that $g'''(x) = 2 \delta_0$, where $\delta_0$ is the Dirac delta measure at the origin, i.e. the measure that assigns mass one to any set containing the origin, and zero to any set not containing the origin. Indeed, this follows if we can show that for $\phi \in C_c^\infty(\RR)$
	%
	\[ \int_0^\infty x^2 \phi'''(x) = - 2 \phi(0), \]
	%
	and follows by carefully integrating by parts \emph{and noting boundary terms}.

	Now we formally calculate the convolution $f * g'''$. If we define $f_x(y) = \phi(x - y)$, then the convolution $f * g'''$ is the distribution such that for $\phi \in C_c^\infty$,
	%
	\[ (f * g''') \{ \phi \} = \int_{-\infty}^\infty \phi(x) g''' \{ f_x \}\; dx. \]
	%
	Now
	%
	\begin{align*}
		\int_{-\infty}^\infty \phi(x) g''' \{ f_x \}\; dx &= \int_{-\infty}^\infty \phi(x) [ 2f_x(0) ]\\
		&= \int_{-\infty}^\infty \phi(x) f(x)\; dx.
	\end{align*}
	%
	So $f * g'''$ is obtained by integrating aginst $2f$, i.e. $f * g''' = 2f$.
\end{solution}

\newpage
\section{Fall 2024 Qualifying Exam Questions}

\question Let $K_n: [a,b] \times [a,b] \to \RR$ be a sequence of differentiable functions such that $\| K_n \|_{L^\infty}, \| \partial_1 K_n \|_{L^\infty}, \| \partial_2 K_n \|_{L^\infty} \leq 1$ for all $n$. For $f \in C[a,b]$, define
%
\[ A_n f(x) = \int_a^b K_n(x,t) f(t)\; dt. \]
\begin{parts}
	\part Prove that $\{ A_n f : \| f \|_{L^\infty} \leq 1, n \geq 1 \}$ is a totally bounded subset of $C[a,b]$.
	\begin{solution}
		The set is closed because $\{ A_n \}$ are uniformly bounded linear operations on $C[a,b]$, i.e. $\| A_n f \|_{L^\infty} \leq (b - a) \| f \|_{L^\infty}$ by the triangle inequality. We also need to prove the \emph{equicontinuity} of the set. Given $\varepsilon > 0$, and some $\delta > 0$, if $|x_1 - x_2| < \delta$, then, without loss of generality assuming $x_1 \leq x_2$, we have
		%
		\[ A_n f(x_2) - A_n f(x_1) = \int_{x_1}^{x_2} \int_a^b (\partial_1 K_n)(y,t) f(t)\; dy\; dt, \]
		%
		and using the triangle inequality gives $|A_n f(x_2) - A_n f(x_1)| \leq (x_2 - x_1) \| f \|_{L^\infty} \leq x_2 - x_1 \leq \delta$. So setting $\delta = \varepsilon$ gives equicontinuous, completing the proof that the set is totally bounded.
	\end{solution}

	\part If we drop the assumption that $\| \partial_2 K_n \|_{L^\infty} \leq 1$, is the set still totally bounded?
	\begin{solution}
		Yes. We never used this assumption in the proof above.
	\end{solution}

	\part If we drop the assumption that $\| \partial_1 K_n \|_{L^\infty \leq 1}$, is the set still totally bounded?
	\begin{solution}
		No. Then the set is not equicontinuous. If we take $a = 0$, $b = 1$, and choose $K_n(x,t)$ not depending on $t$ at all, but approximating $\mathbf{I}(x \leq 1/2)$, then the set cannot be totally bounded, since $A_n f_0(x)$ converges pointwise to $\mathbf{I}(x \leq 1/2) \int_0^1 f_0(t)\; dt$, which is \emph{not} continuous, and so $\{ A_n f_0 \}$ can have no subsequence which converges uniformly to something in $\{ A_n f \}$, so that the set $\{ A_n f \}$ is not compact.
	\end{solution}
\end{parts}

\question Show there is a unique function $f$ in $C[0,10]$ solving the equation
%
\[ f(x) = -15 + \cos(x) \int_0^x e^{e^{tx}} f(t)\; dt \]
%
for all $x \in [0,10]$.
\begin{solution}
	If $Tf(x) = -15 + \cos(x) \int_0^x e^{e^{tx}} f(t)\; dt$, and $\| f \|_L = \| e^{-Lx} f(x) \|_{L^\infty_x}$. Pick $A$ so that for $a \leq 100$, $e^a \leq 1 + Aa$. Then choose $L \geq 2 e^{1 + 100 A}$. Then
	%
	\[ e^{-Lx} \int_0^x e^{e^{tx} + Lt}\; dt \leq e^{-Lx} \int_0^x e^{1 + A tx + Lt}\ dt \leq e^{1 + Ax^2}{L} \leq 1/2. \]
	%
	Taking in absolute values then gives the trivial bound
	%
	\[ \| Tf - Tg \|_L \leq (1/2) \| f - g \|_L, \]
	%
	which shows the existence and uniqueness of a fixed, continuous $f$ with $\| f \|_L < \infty$. But \emph{any} continuous $f$ has $\| f \|_L < \infty$, which completes the proof.
\end{solution}

\question Let
%
\[ \Omega = \{ (x_1,x_2) \in \RR^2 : 0 < x_2 < x_1^2 \leq 1/2 \}. \]
%
Let $b \in \RR$ and let $f: \RR^2 \to \RR$ be defined by
%
\[ f(x) = |x|^{-b} |\log(|x|)|^{-\gamma} \]
%
Determine for which $b$ and $\gamma \in \RR$ the function $f$ is integrable on $\Omega$.
\begin{solution}
	The set $|\Omega|$ is contained in the ball of radius 10, for if $|x| \geq 10$, then either $x_1 \geq 5$ or $x_2 \geq 5$, and if $x \in \Omega$ then the bound $x_2 < x_1^2$ implies $x_1 \geq 5$, which gives a contradiction. So the failure of boudnedness only occurs if small level sets of $f$ have too large a measure. Suppose $j$ is large, and let $A_j = \{ x \in \Omega : |x| \sim 2^{-j} \}$. Then $x \in \Omega$ iff $x_0 < 2^{-2j}$ and $x_1 \sim 2^{-j}$. So $|A_j| \sim 2^{-3j}$. Now
	%
	\[ \int_\Omega f(x) \sim \sum_{j \gg 0} 2^{bj} j^{-\gamma} 2^{-3j}. \]
	%
	This series converges if and only if $b < 3$, or $b = 3$ and $\gamma > 1$.
\end{solution}

\question Suppose $f_n \in L^1(\RR)$ and $\{ \varepsilon_n \}$ is positive. Let $E_n = \{ x : |f_n(x)| \geq \varepsilon_n \}$ and assume $\sum |E_n| < \infty$.
\begin{parts}
	\part Prove $\lim_{n \to \infty} f_n = 0$ almost everywhere.
	\begin{solution}
		It follows from the Borel Cantelli lemma that almost every $x \in \RR$ is in at most finitely many of the sets $E_n$. But any $x$ which is in only finitely many $E_n$ has the property that \emph{eventually} $|f_n(x)| \leq \varepsilon_n$ for sufficiently large $n$, which implies convergence to zero.
	\end{solution}

	\part Prove that for every $\delta > 0$, there is $\Omega_\delta$ with $|\Omega_\delta| < \delta$ and $\lim_n f_n = 0$ uniformly on $\RR \setminus \Omega$.
	\begin{solution}
		This is essentially Egorov's theorem, so see a reference on measure theory for this proof.
	\end{solution}
\end{parts}

\question Let $E \subset \RR$. Suppose $g,f_n \in L^1(E)$, $\| f_n \|_{L^1(E)} \leq 1$ for each $n$, and $\lim_{n \to \infty} f_n = 0$ in measure. Prove that
%
\[ \lim_{n \to \infty} \int_E \sqrt{|f_n g|} = 0. \]
\begin{solution}
	We can convert this problem into a more `linear' problem, that for all $a_n, b \in L^2$ with $\| a_n \|_{L^2(E)} \leq 1$ for each $n$, and with $\lim_{n \to \infty} a_n = 0$,
	%
	\[ \lim_{n \to \infty} \int_E a_n b = 0. \]
	%
	If we can solve this problem, setting $a_n = \sqrt{|f_n|}$ and $b = \sqrt{|g|}$ would give the original statement.

	The functions
	%
	\[ T_n b = \int_E a_n b \]
	%
	are uniformly bounded on $L^2(E)$ by the Cauchy-Schwarz-Bunyakovski inequality. So it suffices to show that $T_n b \to 0$ for all $b$ in a dense subclass of $L^2(E)$. We will take the dense subclass of bounded, compactly supported functions. Applying convergence in measure, for any $\varepsilon > 0$ and $\delta > 0$, for sufficiently large $n$, the sets $\Omega_\varepsilon(n) = \{ x \in E : |a_n| \leq \varepsilon \}$ have $|\Omega_\varepsilon(n)| \leq \delta$. If $b$ is bounded and supported on $[-M,M]$, by the triangle inequality
	%
	\[ \int_{\Omega_\varepsilon(n)^c} a_n b \leq \| a_n \|_{L^\infty(\Omega_\varepsilon(n)^c)} \| b \|_{L^1(E)} \leq \varepsilon^{1/2} M^{1/2} \| b \|_{L^2(E)}. \]
	%
	Next, we can use Cauchy-Schwartz to write
	%
	\[ \int_{\Omega_\varepsilon(n)} a_n b \leq \| b \|_{L^\infty}^{1/2} |\Omega_\varepsilon(n)|^{1/2} \| a_n \|_{L^2(E)} \lesssim \delta \| b \|_{L^\infty}^{1/2}. \]
	%
	So
	%
	\[ \limsup_{n \to \infty} \int_E ab_n \lesssim \delta + \varepsilon, \]
	%
	and taking $\delta$ and $\varepsilon$ to zero gives
	%
	\[ \lim_{n \to \infty} \int a_n b = 0, \]
	%
	completing the proof.
\end{solution}

\question Suppose $\{ g_n \} \subset \mathcal{S}(\RR^2)$ and $\lim_{n \to \infty} \| g_n \|_{L^2(\RR^2)} = 0$. Show that there are $f_n \in C^2(\RR^2)$ such that
%
\[ \Delta f_n = f_n + g_n \]
%
and $\lim_{n \to \infty} f_n(0) = 0$, and $\lim_{n \to \infty} \| \partial_{x_1} \partial_{x_2} f_n \|_{L^2(\RR^2)} = 0$.
\begin{solution}
	Given $g_n$, if
	%
	\[ \Delta f_n = f_n + g_n, \]
	%
	then taking Fourier transforms of both sides yields that
	%
	\[ \widehat{f}_n(\xi) = - \frac{1}{1 + |\xi|^2} \widehat{g}_n(\xi). \]
	%
	If $g_n$ is a Schwartz function, then $\widehat{g}_n$ is a Schwartz function, which implies the right hand side defines a Schwartz function. If we define $f_n$ to be the inverse Fourier transform of the right hand side, then $f_n$ is a Schwartz function with $\Delta f_n = f_n + g_n$.

	By Cauchy Schwratz,
	%
	\[ \| f_n \|_{L^\infty} \leq \int \frac{|\widehat{g}_n(\xi)|}{1 + |\xi|^2}\; d\xi \lesssim \| \widehat{g}_n \|_{L^2(\RR^2)} = \| g_n \|_{L^2(\RR^2)}. \]
	%
	Thus $f_n \to 0$ uniformly as $n \to \infty$, and so certain $\lim_{n \to \infty} f_n(0) = 0$. The Fourier transform of $\partial_{x_1} \partial_{x_2} f_n$ is equal to
	%
	\[ \frac{- 4 \pi^2 \xi_1 \xi_2}{1 + |\xi|^2} \widehat{g}_n(\xi). \]
	%
	The fraction here is bounded uniformly in $\xi$. But this means that
	%
	\[ \| \partial_{x_1} \partial_{x_2} f \|_{L^2(\RR^2)} = \| \widehat{\partial_{x_1} \partial_{x_2} f} \|_{L^2(\RR^2)} \lesssim \| \widehat{g}_n \|_{L^2(\RR^2)} = \| g_n \|_{L^2(\RR^2)}. \]
	%
	Since $g_n \to 0$ in $L^2$, it follows that $\partial_{x_1} \partial_{x_2} f_n \to 0$ in $L^2$.

	The `reason' why all this works is the fact that the operator $1 - \Delta$ is an isometry between the Banach space
	%
	\[ H^2(\RR^2) = \{ f \in L^2(\RR^2): \int [ |\widehat{f}(\xi)| \langle \xi \rangle^2 ]^2\; d \xi < \infty \}. \]
	%
	and $L^2(\RR^2)$. Indeed, this essentially follows by definition of $H^2(\RR^2)$, since the Fourier transform of $(1 - \Delta) f$ is equal to $(1 + |\xi|^2) \widehat{f}(\xi)$, and we can apply Parseval's theorem. It follows by Parseval's theorem that
	%
	\[ \| f \|_{H^2(\RR^2)} \sim \sum_{|\alpha| \leq 2} \| \partial^\alpha f \|_{L^2(\RR^2)}. \]
	%
	So given $\{ g_n \}$, if we define $f_n = (1 - \Delta)^{-1} g_n$, then because $g_n \to 0$ in $L^2$, the functions $f_n \to 0$ in $H^2(\RR^2)$, which, in particular, imply that $\partial^\alpha f_n \to 0$ in $L^2$ for $|\alpha| \leq 2$, implying that $\partial_{x_1} \partial_{x_2} f_n \to 0$. And by the \emph{Sobolev embedding theorem}, $f_n \to 0$ uniformly if $f_n \to 0$ in $H^2(\RR^2)$ (it implies the result if $\{ f_n \}$ converge to 0 in $H^s(\RR^2)$ for any $s > 1$).
\end{solution}

\question Suppose
%
\[ \langle u, \phi \rangle = \int_0^2 \phi(0,t)\; dt \]
%
and
%
\[ \langle v, \phi \rangle = \int_0^2 \phi(t,0)\; dt. \]
%
Show that $u * v$ is an absolutely continuous measure $\mu$, and find $g \in L^1(\RR^2)$ so that $\mu = g\; dx$.
\begin{solution}
\begin{align*}
		\int (u * v)(x) \phi(x)\; dx &= \int u(y) v(x) \phi(x + y)\; dy\; dx\\
		&= \int u(y) \int_0^2 \phi(y_1 + t,y_2)\; dt\; dy\\
		&= \int_0^2 \left[ \int_0^2 \phi(t,s)\; dt \right]\; ds\\
		&= \int_0^2 \int_0^2 \phi(t,s)\; dt\; ds.
	\end{align*}
	%
	So $g = \mathbf{I}_{[0,2] \times [0,2]}$.
\end{solution}

\newpage
\section{More Distribution Theory Warm Up Question}

\question Suppose
%
\[ \langle u, \phi \rangle = \int_0^2 \phi(0,t)\; dt \]
%
and
%
\[ \langle v, \phi \rangle = \int_0^2 \phi(t,0)\; dt. \]
%
Show that $u * v$ is an absolutely continuous measure $\mu$, and find $g \in L^1(\RR^2)$ so that $\mu = g\; dx$.
\begin{solution}
		We calculate that
\begin{align*}
		\int (u * v)(x) \phi(x)\; dx &= \int u(y) v(x) \phi(x + y)\; dy\; dx\\
		&= \int u(y) \int_0^2 \phi(y_1 + t,y_2)\; dt\; dy\\
		&= \int_0^2 \left[ \int_0^2 \phi(t,s)\; dt \right]\; ds\\
		&= \int_0^2 \int_0^2 \phi(t,s)\; dt\; ds.
	\end{align*}
	%
	So $g = \mathbf{I}_{[0,2] \times [0,2]}$.
\end{solution}

\newpage
\section{More Distribution Theory Practice}

\question (January 2013) Let $T$ be a distribution on $\RR$. Set $\tau_a \phi(x) = \phi(x - a)$, and assume that $\langle T, \tau_a \phi \rangle = \langle T, \phi \rangle$ for all $a \in \RR$ and all test functions $\phi$. Prove that $T$ is constant (i.e. there is $C$ so that $\langle T, \phi \rangle = C \int \phi(x)\; dx$ for all test functions $\phi$).
\begin{solution}
	Taking limits, we have that for any test function $\phi$,
	%
	\[ \langle T, \phi' \rangle = 0. \]
	%
	Since any test function $\psi$ with $\int \psi = 0$ can be written as $\phi'$ for some test function $\phi$, it follows that $\langle T, \psi \rangle = 0$ whenever $\int \psi = 0$. This implies that the kernel of the function $\psi \mapsto \int \psi$ is contained in the kernel of $T$, viewed as linear functional on test functions. But by general linear algebra (no analysis required), this implies $T$ is a constant multiple of the integration functional, i.e. $\langle T, \psi \rangle = C \int \psi$ for some constant $C$, completing the proof.
\end{solution}

\question (2013) Let $\delta$ be the Dirac delta function at the origin, and let $\delta'$ be it's distributional derivative. Consider
%
\[ f_1(x) = \begin{cases} 0 & |x| > 1 \\ \sin(x^3) & |x| < 1 \end{cases} \]
%
and let $f_2 = \delta'$. For $j \in \{ 1, 2 \}$, find $\sup \{ s: f_j \in H^s(\RR) \}$, where
%
\[ H^s(\RR) = \{ f : \int [ |\widehat{f}(\xi)| \langle \xi \rangle^s ]^2\; d\xi < \infty \}. \]
\begin{solution}
	Directly from the definition, we can check that the Fourier transform of $\delta$ is the constant function $1$. By properties of the Fourier transform of the derivative of a distribution, the Fourier transform of $\delta'$ is $2 \pi \xi$. But this means that
	%
	\[ \int [ |f_2(\xi)| \langle \xi \rangle^s ]^2\; d\xi \sim 1 + \int_{|\xi| \geq 1} \xi^{2(s+1)}, \]
	%
	which is finite precisely when $s < -3/2$. So $\sup \{ s : f_j \in H^s(\RR) \} = -3/2$. This makes some intuitive sense, since the function $\text{sgn}(x)$ is the `second antiderivative' of $f_2$, in the sense that $\text{sgn}(x)'' = \delta'$, but does not have a derivative which is locally in $L^2$, but the `third antiderivative' $|x|$ does have a derivative locally in $L^2$.

	The function $f_1$ is integrable, so we can calculate it's Fourier transform pointwise, i.e. calculuating that
	%
	\begin{align*}
			\widehat{f}_1(\xi) &= \int_{-1}^1 \sin(x^3) e^{-2 \pi i \xi x}\; dx.
		\end{align*}
		%
		This doesn't look like something we can evaluate \emph{exactly}, but perhaps integration by parts will give a good estimate for $\widehat{f}_1$ when $\xi$ is large (we know that $\widehat{f}_1$ is bounded, and so the behaviour for large $\xi$ determines which of the spaces $H^s(\RR)$ that $f_1$ is contained in). When $\xi$ is large, $e^{-2 \pi i \xi x}$ is oscillating fast, which tells us we should antidifferentiate in this part of the argument, which yields that
		%
		\[ \widehat{f}_1(\xi) = \frac{\sin(1) \cos(2 \pi \xi)}{i \pi \xi} + \int_{-1}^1 3x \cos(x^3) \frac{e^{- 2\pi i \xi x}}{2 \pi i \xi}. \]
		%
		Integration by parts again yields that if $c = \sin(1) / i \pi$, then
		%
		\[ \widehat{f}_1(\xi) = c \cos(2 \pi \xi) / \xi + O(1/\xi^2). \]
		%
		Using this asymptotic formula, plugged into the integral defining $H^s(\RR)$, we see that $f_1 \in H^s(\RR)$ for $s < 1/2$.
\end{solution}

\question (Spring 2014) Does there exist a distribution $T$ such that for functions $\phi \in C_c^\infty(\RR)$ with $0 \not \in \text{supp}(\phi)$, $\langle T, \phi \rangle = \int \phi(x) |x|^{-d}$.
\begin{solution}
		Applying the Hahn Banach theorem, it suffices to show that the functional $\phi \mapsto \int \phi(x) |x|^{-d}$ is bounded on the subspace of functions $\phi$ supported away from zero. To do this, for each interval $[-M,M]$, we must find $k$ such that for functions supported on $[-M,M]$, the magnitude of $\int \phi(x) |x|^{-d}$ is controlled by the $L^\infty$ norm of the first $k$ derivatives of $\phi$. Setting $k = 0$ does not suffice, since $|x|^{-d}$ blows up near the origin, and upper bounds on $\phi$ do not suffice to `cancel out' this singularity. But we can work in polar coordinates, writing
		%
		\begin{align*}
				\int \phi(x) |x|^{-d} &= \int_0^\infty \int_{S^d} \phi( r x ) r^{-1}\; dx\; dr \\
				&= \int_0^\infty r^{-1} \int_{S^d} \left( \int_0^{rx} (\nabla \phi)(s) \cdot x\; ds \right)\; dx \;dr.
			\end{align*}
			%
			Now taking in absolute values yields an extra power of $r$ from integrating over the interval $[0,rx]$, so that we can obtain a bound
			%
			\[ \left| \int \phi(x) |x|^{-d} \right| \lesssim_M \| \nabla \phi \|_{L^\infty}. \]
			%
			So setting $k = 1$ suffices to prove boundedness, regardless of $M$. It makes sense that we can choose $k$ uniformly, because the `distribution' $|x|^{-d}$ only fails to be a smooth function at the origin, i.e. on a compact set, and so the choice of $k$ only depends on the number of derivatives on the test function required to obtain a bound on that compact set.

			Since the function $\phi \mapsto \int \phi(x) |x|^{-d}$ is bounded on the subset of test functions supported away from the origin, the Hahn-Banach theorem implies the existence of many bounded functionals which extend this functional to all test functionals. Any one of these distributions defines a distribution $T$ which we required to be found (importantly, $T$ is \emph{not unique}).
\end{solution}

\question (Fall 2003) Let $f: \RR^2 \to \RR$ be defined by
%
\[ f(x) = \max(1 - |x|^2,0). \]
%
Prove that $\Delta f + 4 \chi_\Omega \geq 0$, in the sense of distributions, i.e. that $\langle \Delta f + 4 \chi_\Omega, \psi \rangle \geq 0$ if $\psi \geq 0$.
\begin{solution}
		We use the definition of the distributional derivative to write
		%
		\[ \langle \Delta f + 4 \chi_\Omega, \psi \rangle = \int_{|x| \leq 1} (1 - |x|^2) \Delta \psi(x) + 4 \psi(x), \]
		%
		and we want to show this term is positive. In order to understand this integral, one of the Stoke's theorems may prove helpful to remove derivatives of $\Delta$ (so that we can eventually use the positivity of $\psi$ to prove this integral is non-negative). This is essentially the higher dimensional analogue of the integration by parts we had to use to calculate derivatives of distributions on the real line. We have
		%
		\[ \nabla \cdot \{ (1 - |x|^2) \nabla \psi \} = - 2 (\nabla \psi \cdot x) + (1 - |x|^2) \Delta \psi, \]
		%
		so by the divergence theorem,
		%
		\[ \int_{|x| \leq 1} (1 - |x|^2) \Delta \psi - 2 \int_{|x| \leq 1} (\nabla \psi(x) \cdot x) = \int_{|x| = 1} (1 - |x|^2) ( \nabla \psi(x) \cdot x )\; d\sigma(x) = 0. \]
		%
		Now by using polar coordinates and integration by parts, we can write
		%
		\begin{align*}
			2 \int_{|x| \leq 1} (\nabla \psi(x) \cdot x) &= 2 \int_{|x| = 1} \int_0^1 r^2 \nabla \psi(rx) \cdot x\\
			&= 2 \int_{|x| = 1} \left[ \psi(x) - \left( \int_0^1 2r \psi(rx)\; dr \right) \right]\; d\sigma(x).		\end{align*}
			%
			So we conclude that
			%
			\[ \int_{|x| \leq 1} (1 - |x|^2) \Delta \psi(x) = 2 \int_{|x| = 1} \psi(x) - 4 \int \psi(x)\; dx. \]
			%
			Thus
			%
			\[ \langle \Delta f + 4 \chi_\Omega, \psi \rangle = 2 \int_{|x| = 1} \psi(x)\; d\sigma(x), \]
			%
			and it is now clear that this quantity is non-negative if $\psi$ is non-negative.
\end{solution}









\newpage
\section{General Practice Warm Up Problems}

\question (Fall 2021) Let $\sigma$ be a Borel probability measure on $[0,1]$ satisfying
%
\begin{enumerate}
	\item $\sigma([1/3,2/3]) = 0$.
	\item $\sigma([a,b]) = \sigma([1-b,1-a])$ for any $0 \leq a < b \leq 1$.
	\item $\sigma([3a,3b]) = 2 \sigma([a,b])$ for any $a,b$ such that $0 \leq 3a < 3b \leq 1$.
\end{enumerate}
%
$\sigma$ is called the $1/3$ Cantor measure on $[0,1]$.
\begin{parts}
	\part Find $\sigma([0,1/8])$.
	\begin{solution}
		Property 2 implies that
		%
		\[ \sigma([0,1/3]) = \sigma([2/3,1]). \]
		%
		Since $\sigma$ is a probability measure, and $\sigma([1/3,2/3]) = 0$, this implies that $\sigma$ gives mass $1/2$ to each of these sets. Property 1 and 3 implies that
		%
		\[ \sigma([1/9,2/9]) = (1/2) \sigma([1/3,2/3]) = 0. \]
		%
		Since $1/9 < 1/8 < 2/9$, we find that
		%
		\[ \sigma([0,1/8]) = \sigma([0,1/9]). \]
		%
		But now Property 1 implies that
		%
		\[ \sigma([0,1/9]) = (1/2) \sigma([0,1/3]) = 1/4. \]
		%
		Thus $\sigma([0,1/8]) = 1/4$.
	\end{solution}

	\part Calculate the second moment of $\sigma$, i.e. the integral
	%
	\[ M = \int_0^1 x^2 d\sigma(x). \]
	\begin{solution}
		The trick to calculating $M$ is to exploit the recursive nature of the Cantor measure to write $M$ in terms of itself. First, we note that
		%
		\[ \int_0^1 x d\sigma(x) = 1/2. \]
		%
		To see why this is true, we note that Property 2. implies that $d\sigma(1 - x) = d\sigma(x)$, and so a change of variables gives that
		%
		\[ \int_0^1 x d\sigma(x) = \int_0^1 (1 - x) d\sigma(1 - x) = \int_0^1 (1 - x) d\sigma(x) = 1 - \int_0^1 x d\sigma(x). \]
		%
		Rearranging this equation gives the result.

		Now we have done this, let's address the second moment. Property 1. implies that we have $d \sigma(x/3) = (1/2) d\sigma(x)$, and so a change of variables gives that
		%
		\[ \int_0^{1/3} x^2 d\sigma(x) = \int_0^1 (x/3)^2 d\sigma(x/3) = (1/18) \int_0^1 x^2 d\sigma(x) = M/18. \]
		%
		Since $\sigma([1/3,2/3]) = 0$,
		%
		\[ \int_{1/3}^{2/3} x^2 d\sigma(x) = 0. \]
		%
		Finally, using Property 2. of the integral, we have $d \sigma(1 - x) = d \sigma(x)$, which gives that
		%
		\[ \int_{2/3}^1 x^2 d\sigma(x) = \int_0^{1/3} (1 - x)^2 d \sigma(1 - x) = \int_0^{1/3} (1 - 2x + x^2) d \sigma(x). \]
		%
		Now
		%
		\[ \int_0^{1/3} d\sigma(x) = \sigma([0,1/3]) = 1/2. \]
		%
		Another change of variables gives that
		%
		\[ \int_0^{1/3} x d\sigma(x) = \int_0^1 (x/3) d\sigma(x/3) = (1/6) \int_0^1 x d\sigma(x) = (1/12). \]
		%
		Finally, we have already calculated that
		%
		\[ \int_0^{1/3} x^2 d\sigma(x) = M/18. \]
		%
		Putting everything together, we conclude that
		%
		\[ \int_{2/3}^1 x^2 d\sigma(x) = 1 - 2/12 + M/18 = 1 - 1/6 + M/18. \]
		%
		Thus
		%
		\[ M = \int_0^1 x^2 d\sigma(x) = (M/18) + (0) + (1 - 1/6 + M/18) = M/9 + (5/6). \]
		%
		Rearranging gives
		%
		\[ M = 15/16. \]
	\end{solution}
\end{parts}

\question (Fall 2021) Find the spectrum of the linear operator $A$ on $L^2(\RR)$ defined as
%
\[ Af(x) = \int_{-\infty}^\infty \frac{f(y)}{1 + (x - y)^2}\; dy \]
%
(The spectrum of a linear operator $T$ is the closure of the set of all complex numbers $\lambda$ such that the operator $T - \lambda$ does not have a bounded inverse). Hint: It may be useful to find the Fourier transform of $1/(1 + x^2)$.
\begin{solution}
	That Fourier analysis might help us in this problem is suggest by the fact that $Af = K * f$, where
	%
	\[ K(x) = \frac{1}{1 + x^2}. \]
	%
	Thus the operator is a convolution operator. If $m = \widehat{K}$, then we can view $A$ as a \emph{Fourier multiplier operator}, i.e. such that
	%
	\[ \widehat{Af}(\xi) = m(\xi) \widehat{f}(\xi). \]
	%
	There are several ways to compute $K$. The first is to calculate it explicitly, writing
	%
	\[ m(\xi) = \int_{-\infty}^\infty \frac{e^{-2 \pi i \xi \cdot x}}{1 + x^2}\; dx \]
	%
	and apply contour integration. The second is to note that since
	%
	\[ \left( 1 - \frac{1}{4 \pi^2} (- 2 \pi i x)^2 \right) K = (1 + x^2) K = 1, \]
	%
	taking Fourier transforms on both sides gives that $m$ satisfies the differential equation
	%
	\[ D^2 m = (4 \pi^2) (m - \delta). \]
	%
	Solving this equation gives
	%
	\[ m(\xi) = A e^{2 \pi \xi} + B e^{-2 \pi \xi} + \pi [ e^{2 \pi x} H(x) - e^{-2 \pi x} H(x) ], \]
	% 
	Since $m$ is tempered, the constants must be chosen to avoid exponential increase near infinity. Thus we must set $A = \pi$, and $B = 0$, which gives
	%
	\[ m(\xi) = \pi e^{-2 \pi |\xi|}. \]
	%
	This viewpoint will help us find \emph{approximate eigenvalues} for $A$, which, by virtue of the fact that $A$ is self-adjoint, are the only barriers to $A - \lambda$ being invertible. Morally speaking, if $e_{\xi_0}(x) = e^{2 \pi i \xi_0 \cdot x}$, then $\widehat{e_{\xi_0}} = \delta_{\xi_0}$, and so the Fourier transform of $Ae_{\xi_0}$ \emph{should} be equal to $m(\xi_0) \delta_{\xi_0}$, and thus we should have
	%
	\[ A e_{\xi_0} = m(\xi_0) e_{\xi_0}. \]
	%
	Thus $e_{\xi_0}$ is an eigenfunction of $A$ with eigenvalue $m(\xi_0)$. \emph{However}, $e_{\xi_0}$ does not lie in $L^2(\RR)$, and in fact, $A$ has no eigenfunctions. To fix this, we simply approximate $e_{\xi_0}$ by elements of $L^2(\RR)$, i.e. considering a Schwartz function $\phi$ with $\| \phi \|_{L^2(\RR)} = 1$, with $\widehat{\phi}$ supported on $[-1,1]$, and with $\widehat{\phi} \geq 0$, and consider the functions
	%
	\[ e_{\xi_0,R}(x) = R^{-1/2} e^{2 \pi i \xi_0 \cdot x} \phi(x/R) \]
	%
	for $R > 0$. Then $\| e_{\xi_0,R} \|_{L^2(\RR)} = 1$ for all $R > 0$, and by applying Plancherel's formula,
	%
	\begin{align*}
		\| Ae_{\xi_0,R} - m(\xi_0) e_{\xi_0,R} \|_{L^2(\RR)} &= \| [m(\xi) - m(\xi_0) \widehat{e_{\xi_0,R}}(\xi) \|_{L^2_\xi(\RR)}\\
		&= R^{1/2} \left( \int |m(\xi) - m(\xi_0)|^2 |\widehat{\phi}(R (\xi - \xi_0))|^2\; d\xi \right)^{1/2}.
	\end{align*}
	%
	This integral is supported on $|\xi - \xi_0| \leq 1/R$, and on this region, by continuity of $m$, we have $|m(\xi) - m(\xi_0)| \lesssim_{\xi_0} 1/R$. Putting this into the integral above gives that
	%
	\[ \| Ae_{\xi_0,R} - m(\xi_0) e_{\xi_0,R} \|_{L^2(\RR)} \leq R^{-1/2} = R^{-1/2} \| e_{\xi_0,R} \|_{L^2(\RR)}. \]
	%
	Taking $R \to \infty$ shows that $A - m(\xi_0)$ cannot be an isomorphism, since if $T$ is an isomorphism, we have $\| Tf \|_{L^2(\RR)} \gtrsim \| f \|_{L^2(\RR)}$. Since the range of $m$ includes all positive numbers less than or equal to $\pi$, we have shown that the spectrum contains $[0,\pi]$.

	We claim this is \emph{all} of the spectrum of $A$. Indeed, if $\alpha \not \in [0,\pi]$, then $|m(\xi) - \alpha| \gtrsim 1$ for all $\xi \in \RR$, and so
	%
	\begin{align*}
		\| (A - \alpha) f \|_{L^2(\RR)} &= \| [m(\xi) - \alpha] \widehat{f}(\xi) \|_{L^2(\RR)} \gtrsim \| \widehat{f} \|_{L^2(\RR)} = \| f \|_{L^2(\RR)}.
	\end{align*}
	%
	Thus $A - \alpha$ is an isomorphism onto it's image, which must contain all of $L^2(\RR)$ because it has a continuous inverse, namely the operator $B_\alpha: L^2(\RR) \to L^2(\RR)$ defined such that for $f \in L^2(\RR)$,
	%
	\[ \widehat{B_\alpha f}(\xi) = \frac{\widehat{f}(\xi)}{m(\xi) - \alpha}. \]
\end{solution}

\newpage
\section{General Practice Problems}

\question (Fall 2017) Let $a_1, \dots, a_n > 0$. Let $f: \RR^n \to \RR$ be defined by
%
\[ f(x) = \frac{1}{1 + \sum |x_i|^{\alpha_i}}. \]
%
Determine for each $p > 0$ whether
%
\[ \int |f(x)|^p\; dx < \infty. \]
\begin{solution}
	Let $E_i = \{ x: x_i^{\alpha_i} \geq x_1^{\alpha_1}, \dots, x_n^{\alpha_n} \}$. On $E$, we have
	%
	\[ |f(x)|^p \sim \frac{1}{1 + |x_i|^{p \alpha_1}}. \]
	%
	Let us determine what values of $p > 0$ we have
	%
	\[ \int_{E_i} \frac{1}{1 + |x_i|^{p \alpha_1}} < \infty.  \]
	%
	Since $\RR^n = \bigcup E_i$, this will suffices to determine the range of $p$ required in the problem. Set $E_i(n) = E_i \cap \{ x : 2^{n-1} \leq x_1 \leq 2^n \}$. For $n > 0$, on $E_i(n)$ we have $|f(x)|^p \sim_p 1/2^{p \alpha_i n}$, and for $n < 0$ we have $|f(x)|^p \sim 1$. It is simple to calculate that $|E_i(n)| \sim 2^n 2^{n(\alpha_i/\alpha_2 + \dots + \alpha_i / \alpha_n)} = 2^{n \alpha_i (1/\alpha_1 + \dots + 1/\alpha_n)}$. For any value of $p$, we thus have
	%
	\[ \sum_{n \leq 0} \int_{E_i(n)} |f(x)|^p \sim \sum_{n < 0} 2^{n \alpha_i (1/\alpha_1 + \dots + 1/\alpha_n)} < \infty. \]
	%
	On the other hand, we have
	%
	\[ \sum_{n > 0} \int_{E_i(n)} |f(x)|^p \sim \sum_{n = 1}^\infty 2^{-p\alpha_i n} 2^{n \alpha_i(1/\alpha_1 + \dots + 1/\alpha_n)} = \sum_{n = 1}^\infty 2^{n \alpha_i(1/\alpha_1 + \dots + 1/\alpha_n -p)}. \]
	%
	This sum is finite if and only if $1/\alpha_1 + \dots + 1/\alpha_n < p$. Thus this is the range for which
	%
	\[ \int_{E_i} |f(x)|^p < \infty. \]
	%
	But this condition is invariant of $i$, so this is precisely the range for which
	%
	\[ \int |f(x)|^p < \infty. \]
\end{solution}

\begin{solution}

Let $\gamma=\frac{1}{\alpha_{1}}+\ldots+\frac{1}{\alpha_{n}}$. We claim the integral converges if and only if $p>\gamma$.

\vt
\textit{Case 1:} Suppose $p>\gamma$. Then
\begin{equation*}
  \int_{\R^{n}} |f(x)|^{p}dx = \int_{0}^{\infty}p t^{p-1}|E_{t}|dt
\end{equation*}
where $E_{t}= \left\{ x: f(x)>1 \right\}$. Since $E_{t}=\emptyset$ if $t\geq 1$, it follows that
\begin{equation}\label{eq:10000}
  \int_{\R^{n}} |f(x)|^{p}dx = \int_{0}^{1}p t^{p-1}|E_{t}|dt.
\end{equation}
Observe that
\begin{align*}
  E_{t}
  &= \left\{ x\in \R^{n} : \sum_{i=1}^{n}|x_{i}|^{\alpha_{i}}< \frac{1-t}{t} \right\} \\
  &\subseteq \bigcap_{i=1}^{n}\left\{ x\in \R^{n}: |x_{i}| < \left(\frac{1-t}{t}\right)^{\frac{1}{\alpha_{i}}} \right\}\\
  &= \left\{ x\in \R^{n}:  -\left(\frac{1-t}{t}\right)^{\frac{1}{\alpha_{i}}} < x_{i}< \left(\frac{1-t}{t}\right)^{\frac{1}{\alpha_{i}}} \text{ for all }i\right\}
\end{align*}

Since the set on the right hand side is a rectangle with Lebesgue measure $2^{n}\left(\frac{1-t}{t}\right)^{\gamma}$, it follows that
\begin{equation*}
  |E_{t}| \leq 2^{n}\left(\frac{1-t}{t}\right)^{\gamma}.
\end{equation*}
Combining this with equation \eqref{eq:10000} gives
\begin{equation*}
  \int_{\R^{n}} |f(x)|^{p}dx \leq 2^{n}p \int_{0}^{1}t^{p-1}(1-t)^{\gamma}t^{-\gamma}dt \leq 2^{n}p \int_{0}^{1}t^{p-1-\gamma}dt
\end{equation*}
which is integrable since $p>\gamma$.


\vt
\textit{Case 2:} Suppose $p\leq\gamma$. We will prove that $\int_{\R^{n}} |f|^{p}dx = +\infty$ by induction on $n$. If $n=1$,
\begin{equation*}
  \int_{\R} \(\frac{1}{1+ |x|^{\alpha_{1}}}\)^{p}dx \geq  \frac{1}{2}\int_{|x|>1} |x|^{-\alpha_{1}p}dx \geq \frac{1}{2}\int_{|x|>1}\frac{1}{|x|}dx= +\infty.
\end{equation*}

Next assume $n>1$. For convenience of notation we will write $R=1+ \sum_{i=2}|x_{i}|^{\alpha_{i}}$. We have
\begin{align*}
  \int_{\R^{n}}|f(x)|^{p}dx
  &= \int_{\R^{n-1}} \left[ \int_{\R} \left( \frac{1}{|x_{1}|^{\alpha_{1}}+ 1+ \sum_{i=2}|x_{i}|^{\alpha_{i}}} \right)^{p}dx_{1} \right]dx_{2}\cdots dx_{n}\\
  &=\int_{\R^{n-1}} \left[ \int_{\R} \left( \frac{1}{|x_{1}|^{\alpha_{1}}+ R} \right)^{p}dx_{1} \right]dx_{2}\cdots dx_{n}\\
  &\geq \int_{\R^{n-1}} \left[ \int_{|x_{1}|>R^{1/\alpha_{1}}} \left( \frac{1}{|x_{1}|^{\alpha_{1}}+ R} \right)^{p}dx_{1} \right]dx_{2}\cdots dx_{n}\\
  &\geq \frac{1}{2^{p}}\int_{\R^{n-1}} \left[ \int_{|x_{1}|>R^{1/\alpha_{1}}} \left( \frac{1}{|x_{1}|^{\alpha_{1}}} \right)^{p}dx_{1} \right]dx_{2}\cdots dx_{n}\\
  &= \frac{1}{2^{p-1}}\int_{\R^{n-1}} \int_{R^{1/\alpha_{1}}}^{\infty}x_{1}^{-p\alpha_{1}}dx_{1}\cdots dx_{n}.
\end{align*}
If $p\alpha_{1}\leq 1$ then $\int_{\R^{n-1}} \int_{R}^{\infty}x_{1}^{-p\alpha_{1}}dx_{1}= +\infty$ and we are done. On the other hand, if $p\alpha_{1}<1$, then by the above, we have
\begin{align*}
  \int_{\R^{n}}|f(x)|^{p}dx
  &= \frac{1}{2^{p-1}(1-p\alpha_{1})}\int_{\R^{n-1}} R^{\frac{1}{\alpha_{1}}-p}dx_{2}\cdots dx_{n}\\
  &= \frac{1}{2^{p-1}(1-p\alpha_{1})}\underbrace{\int_{\R^{n-1}} \left( \frac{1}{1+ \sum_{i=2}|x_{i}|^{\alpha_{i}}}
    \right)^{p-\frac{1}{\alpha_{1}}} dx_{2}\cdots dx_{n}}_{\text{Call this }A}
\end{align*}
However, since $p\geq \gamma$, it follows that $p-\frac{1}{\alpha_{1}}\geq \frac{1}{\alpha_{2}}+ \ldots + \frac{1}{\alpha_{n}}.$ Therefore by the induction hypothesis, the integral $A=+\infty$. Therefore $\int_{\R^{n}}|f(x)|^{p}dx = +\infty$.
\end{solution}


\question Let $f \in L^1(\RR)$. Let
%
\[ G(\lambda) = \int_{-\infty}^\infty e^{i \lambda t^2} f(t)\; dt. \]
%
Prove that $G$ is a continuous function and that $\lim_{\lambda \to \infty} G(\lambda) = 0$.
\begin{solution}
    It is simple to see that the operator $Tf = G$ is continuous from $L^1(\RR)$ to $L^\infty(\RR)$, with $\| Tf \|_\infty \leq \| f \|_1$. We claim that it thus suffices to show the result for $\phi \in C_c^\infty(\RR)$ supported away from the origin. Indeed, if $f \in L^1(\RR)$, we can find $\phi_n \in C_c^\infty(\RR)$ supported away from the origin, converging in $L^1(\RR)$ to $f$. Then $T\phi_n$ converges to $Tf$ uniformly, and $T\phi_n$ is continuous for each $n$, so $Tf$ is continuous. Moreover, if $\varepsilon > 0$, there is $n$ large enough that $\| f - \phi_n \|_1 \leq \varepsilon$. This means that $\| Tf - T\phi_n \|_\infty \leq \varepsilon$, and so
    %
    \[ \limsup_{\lambda \to \infty} |Tf(\lambda)| \leq \limsup_{\lambda \to \infty} |Tf(\lambda) - T\phi_n(\lambda)| + \limsup_{\lambda \to \infty} |T\phi_n(\lambda)| \leq \varepsilon + 0 \leq \varepsilon. \]
    %
    Taking $\varepsilon \to 0$ completes the argument. So now we show the result for $\phi \in C_c^\infty(\RR)$. Continuity here is easy because $\phi$ is compactly supported (in this case, $G$ is actually a smooth function). And to show that $T\phi(\lambda)$ converges to zero as $\lambda \to \infty$, we integrate by parts, writing
    %
    \[ T\phi(\lambda) = \int_{-\infty}^\infty e^{i \lambda t^2} \phi(t)\; dt = \int_{-\infty}^\infty \frac{\phi(t)}{2i \lambda t} \frac{d}{dt} \left( e^{i \lambda t^2} \right) = - \frac{1}{2i \lambda} \int_{-\infty}^\infty \frac{d}{dt} \left( \frac{\phi(t)}{t} \right) e^{i \lambda t^2}. \]
    %
    Since $\phi$ lies in $C^\infty_c$ and is supported away from the origin, so too is $\phi(t)/t$, and thus we find
    %
    \[ |T\phi(\lambda)| \leq \frac{1}{2\lambda} \int_{-\infty}^\infty \left| \frac{d}{dt} \left( \frac{\phi(t)}{t} \right) \right|\; dt \lesssim 1/\lambda. \]
    %
    Thus $T\phi(\lambda) \to 0$ as $\lambda \to \infty$.
\end{solution}

\question Let $(X,\mu)$ be a $\sigma$-finite measure space. Let $\{ f_n \}$ be a sequence of measurable functions and assume that $f_n \to f$ almost everywhere. Prove that there exists measurable $A_1,A_2,\dots \subset X$ such that $\mu(X - \bigcup_i A_i) = 0$, and such that $f_n|_{A_i} \to f|_{A_i}$ uniformly for each $i$.
\begin{solution}
    We may assume without loss of generality that $X$ is actually a \emph{finite} measure space, since if $\bigcup X_n = X$, with $X_n$ a finite measure space, then we can find $A_{n1}, A_{n2}, \dots$ such that $\mu(X_n - \bigcup_i A_{ni}) = 0$, such that $f_k$ converges uniformly on each $A_{ni}$, and then $\mu(X - \bigcup_{n,i} A_{ni}) = 0$. Normalizing, we assume $|X| = 1$.
    
    Now let
     %
    \[ E_n(k) = \{ x \in X : \sup_{m \geq n} |f_m(x) - f(x)| \leq 1/2^k \}. \]
    %
    Since $f_n \to f$ almost everywhere, as $n \to \infty$ and for a fixed $k > 0$, $\{ E_n(k) \}$ is an increasing sequence, and $X - \bigcup_n E_n(k)$ is a set of measure zero. Thus for each $r > 0$, we can pick $n_1$ such that $|X - E_{n_1}(1)| \leq 1/2^r$. Similarily, we can for each $k$ pick $n_k$ such that $|E_{n_{k-1}}(k-1) - E_{n_k}(k)| \leq 1 / 2^{k+r}$. If we define $A_r = \bigcap_k E_{n_k}(k)$, then $|X - A_r| \leq 1/2^{r-1}$, and by definition, $f_k$ converges uniformly on $A_r$ for each $r$ since for $n \geq n_k$, $|f_n(x) - f(x)| \leq 1/2^k$. Thus we have found the required sequence $\{ A_r \}$.
\end{solution}

\question (Spring 2017) Given for each function $f \in C^0(\RR^2)$ we define for each $y \in \RR$ a function $f_y \in C[0,1]$ by $f_y(x) = f(x,y)$. Assume that for each fixed $y$, the distributional derivative of $f_y \in \mathcal{D}'(\RR)$ defines a function $a_y \in L^p(\RR)$. Assume further that
%
\[ \| a_y \|_p \leq C < \infty \]
%
for some constant $C$ independent of $y$. Show that the distributional derivative $\partial_x f \in \mathcal{D}'(\RR^2)$ is in $L^p_{\text{loc}}(\RR^2)$, provided $1 < p \leq \infty$.
\begin{solution}
    We apply duality, which says that $\partial_x f$ is in $L^p_{\text{loc}}$ if and only if for each $R > 0$, and $\phi \in C_c^\infty(\RR^d)$ supported on $|z| \leq R$,
    %
    \[ \left| \int_{\RR^2} (\partial_x f)(z) \phi(z)\; dz \right| \lesssim_R \| \phi \|_{p^*}, \]
    %
    where $p^*$ is the conjugate to $p$. Now
    %
    \[  \int_{\RR^2} (\partial_x f)(z) \phi(z) = - \int f(z) (\partial_x \phi)(z)\; dz. \]
    %
    Since $f$ is continuous, and $\partial_x \phi$ is compactly supported on $|z| \leq R$, we can apply Fubini's theorem, writing
    %
    \[ \int f(z) (\partial_x \phi)(z)\; dz = \int_{-\infty}^\infty \int_{-\infty}^\infty f(x,y) (\partial_x \phi)(x,y)\; dx\; dy. \]
    %
    But applying integration by parts in just the $x$-variable, we have
    %
    \[ \int_{-\infty}^\infty f(x,y) (\partial_x \phi)(x,y)\; dx = - \int a_y(x) \phi(x,y)\; dx. \]
    %
    Thus we conclude that
    %
    \[ \int (\partial_x f)(z) \phi(z)\; dz = \int_{-\infty}^\infty \int_{-\infty}^\infty a_y(x) \phi(x,y)\; dx\; dy. \]
    %
    But by H\"{o}lder's inequality, if $p^*$ is the dual to $p$,
    %
    \begin{align*}
        \left| \int_{-\infty}^\infty \int_{-\infty}^\infty a_y(x) \phi(x,y)\; dx\; dy \right| &= \left| \int_{-R}^R \int_{-\infty}^\infty a_y(x) \phi(x,y)\; dx\; dy \right|\\
        &\leq \left( \int_{-R}^R \int_{-\infty}^\infty |a_y(x)|^p\; dx\; dy \right)^{1/p} \| \phi \|_{p^*}\\
        &= \left( \int_{-R}^R \| a_y \|_p^p\; dy \right)^{1/p} \| \phi \|_{p^*}\\
        &\leq C \cdot (2R)^{1/p} \| \phi \|_{p^*}\\
        &\lesssim_R \| \phi \|_{p^*}.
    \end{align*}
    %
    Thus we conclude that $\partial_x f \in L^p_{\text{loc}}$.
\end{solution}

\question (Spring 2020)
  Let $E\subset[0,1]$ be a measurable set with positive Lebesgue measure. Moreover, it satisfies the following property: As long as $x$ and $y$ belong to $E$, we know $\frac{x+y}{2}$ belongs to $E$. Prove that $E$ is an interval.
  
  
\begin{solution}
  Let $m=\inf E$ and $M = \sup E$. Since $|E|>0$, $E$ contains more than one element and hence $m<M$. We first prove two claims:

  \vt
  \noindent
  \textit{Claim 1:} $E$ is dense in $(m,M)$.
  \begin{proof}[Proof of Claim 1:]
    Let $z\in (m,M)$.
    By definition of infimum and supremum, there exists $u,v\in E$ such that $m \leq u<z<v\leq M$. Let $\delta>0$. It will suffice to show there exists an element $w\in E$ such that $|w-z|<\delta$. Define a sequence of intervals inductively by $I_{0}= [u,v]$ and if $I_{n-1}=[x,y]$, then
    \begin{equation*}
      I_{n} = \left\{
        \begin{array}{l@{\quad}l}
          \left[ \frac{x+y}{2}, y \right]  & \text{if } w\geq \frac{x+y}{2}\\
          \left[ x, \frac{x+y}{2} \right]  & \text{if } w< \frac{x+y}{2}
        \end{array}\right.
    \end{equation*}
     By construction, $w\in I_{n}$, and using the midpoint convexity of $E$, the endpoints of $I_{n}$ are elements of $E$. Choosing $N$ sufficiently large that $|I_{N}|=2^{-N}(v-u)<\delta$, it follows that $|w-\max I_{N}| < |\max I_{N}-\min I_{N}|= 2^{-N}(v-u)<\delta$. This proves the claim.
   \end{proof}

   \vt
   \noindent
   \textit{Claim 2:} $E$ contains a nonempty interval.
   \begin{proof}[Proof of Claim 2:]
     Denote by $E/2$ the set $\left\{ x/2 : x\in E   \right\}$. First observe that $|E/2|>0$. (Indeed, if $|E/2|=0$, then by absolute continuity of the function $x\mapsto 2x$, $|E|=0$, a contradiction.) We will show that $\frac{E}{2}+\frac{E}{2}$ contains a nonempty interval. The proof of the claim will then follow since $ E\supset \frac{E+E}{2}=\frac{E}{2}+\frac{E}{2}.$
     
     Let $h(x)= \int_{\R} \chi_A(x) \chi_A(x-y)dy$ where $A=E/2$. If $x\notin A+A$, then $h(x) = 0$. Therefore by contrapositive, if $\phi(x)>0$ then $x\in A+A$. Moreover, $h$ is continuous, so the set $\{x: h(x)>0\}$ is open and therefore contains an interval provided it is nonempty. Therefore it remains to show that $\{x: h(x)>0\}$ is nonempty. Indeed, by the Fubini-Tonelli theorem, $\int_{\R} h(x) dx = \int_A \int_{\R} \chi_A(x-y)dxdy = \int_A \int_{\R} \chi_A(x)dxdy = |A|^2>0$. Since $h$ is nonnegative function whose integral is positive, it follows that $h(x)>0$ for some $x$, as desired.
   \end{proof}

   We now proceed with the proof. By Claim 2, there exists a nonempty interval $(a,b)\subset E$. Let $b_{0} = \sup \left\{ r>0: (a,r)\subset E \right\}$ and $a_{0} = \inf \left\{ s>0 : (s,b) \subset E \right\}$. It will suffice to show that $a_{0}=m$ and  $b_{0}=M$, since this will imply that $(m,M)\subset E$, which is enough to conclude that $E$ is an interval. We shall show that $b_{0} = M$ as the proof that $a_{0}=m$ is similar.

   Suppose $b_{0}<M$. By Claim 1, there exists $b_{1}\in E$ such that $b_{0}<b_{1}< 2b_{0}-a$. Since $(a,b_{0})\subset E$ and $b_{1}\in E$, it follows that
   \begin{equation*}
     E\supset \left\{ \frac{b_{1}+s}{2}: a<s<b_{0} \right\} = \left\{ t: \frac{a+b_{1}}{2}<t<\frac{b_{0}+b_{1}}{2} \right\}= \left( \frac{a+b_{1}}{2},\frac{b_{0}+b_{1}}{2} \right).
   \end{equation*}
   Since $b_{1}<2b_{0}-a$, we have $\frac{a+b_{1}}{2}<b_{0}$. Therefore $\left( a, \frac{b_{0}+b_{1}}{2} \right)= (a,b_{0})\cup \left( \frac{a+b_{1}}{2},\frac{b_{0}+b_{1}}{2} \right)\subset E$. Moreover, since $b_{0}< b_{1}$, we have $\frac{b_{0}+b_{1}}{2}>b_{0}$. Therefore $b_{0}$ is not an upper bound for the set $\left\{ r>0: (a,r)\subset E \right\}$. But this is a contradiction since we defined $b_{0}$ to be the supremum of that set. Therefore $b_{0}=M$.
 \end{solution}

\question
\begin{parts}
    \part Does $p_N = \prod_{n = 2}^N (1 + (-1)^n/n)$ tend to a nonzero limit as $N \to \infty$.
    \begin{solution}
        We have
        %
        \[ \sum_{n = 2}^N \log(1 + (-1)^n/n) = \sum_{n = 2}^N (-1)^n/n + O(1/n^2). \]
        %
        The error terms converge absolutely, since $1/n^2$ is summable, and Leibniz's test implies since $(-1)^n/n$ is an alternating sequence decreasing in absolute value, that
        %
        \[ \sum_{n = 2}^N (-1)^n/n \]
        %
        converges as $N \to \infty$, and that the limiting value exceeds
        %
        \[ 1/2 - 1/3 > 0. \]
        %
        Thus taking exponentials shows that $p_N$ converges as $N \to \infty$.
    \end{solution}
    
    \part Does $q_N = \prod_{n = 2}^N (1 + (-1)^n/\sqrt{n})$ tend to a nonzero limit as $N \to \infty$.
    \begin{solution}
        We apply Taylor series, writing
        %
        \[ \log(1 + (-1)^n/\sqrt{n}) = (-1)^n/\sqrt{n} - 1/2n + O(1/n^{3/2}). \]
        %
        The error term is absolutely summable, and the sum of $(-1)^n/\sqrt{n}$ converges by Leibniz test as above. Thus the convergence of the sum
        %
        \[ \sum_{n = 2}^N \log(1 + (-1)^n/\sqrt{n}) \]
        %
        is equivalent to the convergence of the sum
        %
        \[ \sum_{n = 2}^N -1/2n. \]
        %
        But this sum converges to $-\infty$, so we find that, taking exponentials, $q_N \to 0$ as $N \to \infty$.
    \end{solution}
\end{parts}


\newpage
\section{General Practice 2: Warm Up Question}

\question (Fall 2015) Let $\chi \in C^\infty(\RR)$ have a compact support and define
%
\[ f_n(x) = n^2 \chi'(nx). \]
%
\begin{parts}
    \part Does $f_n$ converge in the sense of distributions as $n \to \infty$? If so, what is the limit?
    \begin{solution}
        Applying an integration by parts, we calculate that
        %
        \[ \int f_n(x) \phi(x)\; dx = \int n^2 \chi'(nx) \phi(x)|; dx = - \int n \chi(nx) \phi'(x)\; dx. \]
        %
        As $n \to \infty$, each function $x \mapsto n \chi(nx)$ has total mass one, but is concentrated in small and smaller neighborhoods of the origin. In particular, these functions operate as an approximation to the identity, so that
        %
        \[ - \int n \chi(nx) \phi'(x)\; dx = - \phi'(0). \]
        %
        Thus as $n \to \infty$, $f_n$ converges distributionally to the distribution $\phi \mapsto - \phi'(0)$.
    \end{solution}
    
    \part Let $p \in [1,\infty)$ and $g \in L^p(\RR)$ be such that the distributional derivative of $g$ also lies in $L^p(\RR)$. Does $f_n * g$ converge in $L^p(\RR)$ as $n \to \infty$? If so, what is the limit?
    \begin{solution}
        As mentioned before, integration by parts shows that
        %
        \[ (f_n * g)(x) = \int_{-\infty}^\infty f_n(y) g(x-y)\; dy = \int n \chi(ny) g'(x-y)\; dy = (n \chi(ny) * g')(x). \]
        %
        The function $n \chi(ny)$ is an approximation to the identity, and so as $n \to \infty$, $(f_n * g)$ converges in $L^p$ to $g'$.
    \end{solution}
\end{parts}

\question (Fall 2018)
  Prove or Disprove that in an infinite dimensional Banach space,
  \begin{enumerate}[(a)]
  \item every norm bounded set is weakly bounded,
  \item every norm closed set is weakly closed
  \item a norm bounded set has empty interior in the weak topology
  \end{enumerate}
  
\begin{solution}
    We first prove part (a). Let $X$ be a Banach space and supppose $E\subset X$ is norm bounded; i.e., that $\sup_{x\in E}\norm{x}=C<\infty$. Recall that to show $E$ is weakly bounded, we need to show that $\sup_{x\in E}|\phi(x)|<\infty$ for all $\phi\in X^{*}$. This follows immediately by definition of norm:
    \begin{equation*}
      \sup_{x\in E}|\phi(x)| \leq\sup_{x\in E} \norm{\phi}\norm{x} \leq C \norm{\phi}<\infty.
    \end{equation*}

    Next we prove (c). Let $X$ be an infinite-dimensional Banach space, and let $A\subset X$ be a norm-bounded set. Then there exists $\rho>0$ such that $A\subset B:=\left\{ x\in X: \norm{x}<\rho \right\}$. It suffices to show that $B$ has empty interior in the weak topology.
    
      By definition, the weak topology has a base consisting of sets of the form
    \begin{equation}\label{eq:1}
      N(x_{0},F,r)=\left\{ x\in X : |\phi(x-x_{0})|<r \mathrm{\ for\ all\ }\phi\in F\right\}
    \end{equation}
    where $x_{0}\in X$, $r>0$, and $F$ is a finite subset of $X^{*}$. Letting $K:= \bigcap_{\phi \in F} \ker(\phi)$, observe that
    \begin{equation*}
      x_{0}+ K \subset N(x_{0},F,r).
    \end{equation*}
    Since $\ker(\phi)$ is of co-dimension 1 in $X$ for every $\phi\in X^{*}$ and since $F$ is a finite set, it follows that $K$ is of finite codimension in $X$. Therefore since $X$ is infinite-dimensional, it follows that $\dim K >0$, and therefore $K$ contains a nonzero element $y$. Since $K$ is a subspace, it follows that $\alpha y\in K$ for any $\alpha\in \R$. This implies that $K$---and therefore $N(x_{0},F,r)$---contains elements of arbitrarily large norm. Therefore $N(x_{0},F,r)\not\subset B$. Therefore $B$ has empty interior in the weak topology. This proves statement (c).

    Incidentally, we have also shown that statement (b) is false: while $B^c$ is clearly norm closed, it is not weakly closed because $B$ is not weakly open.

    Finally we claim that the converse of statement (b)---i.e., that every weakly closed set is norm closed---is true. To see this, let $F$ be weakly closed. Then $F^c$ is weakly open, and hence can be written as a union of sets of the form given in equation \eqref{eq:1}. Moreover, since $x\mapsto |\phi(x-x_{0})|$ is continuous with respect to the norm topology, it follows that $N(x_{0},F,r)$ is open in the norm topology. Therefore $F^c$ is a union of norm open sets. Therefore $F^c$ is norm open, and hence $F$ is norm closed.

    Another way to prove the converse of statement (b) is the following. Suppose $F\subset X$ is weakly closed, and suppose that $x_{n}\in F$ converges in norm to $x\in X$. To show that $F$ is strongly closed, we need to show that $x\in F$. Using the linearity of $\phi$ and the definition of operator norm,
    \begin{equation*}
      |\phi(x_{n})-\phi(x)| = |\phi(x_{n}-x)|\leq \norm{\phi} \norm{x_{n}-x}
    \end{equation*}
    for every $\phi\in X^{*}$. Since the right-hand side converges to zero as $n\to\infty$, it follows that $\phi(x_{n})\to \phi(x)$ as well, i.e. that $x_{n}$ converges weakly to $x$. Therefore since $F$ is weakly closed, $x\in F$, as required.
\end{solution}


\newpage
\section{General Practice 2 Bonus Questions}

\question Suppose $f: [0,1]^2 \to \RR$ is continuous, and $\partial^2 f / \partial x^2$ exists pointwise on $[0,1]$, is continuous in the $x$ variable, and is bounded. Then $\partial f / \partial x$ is continuous.
\begin{solution}
	If $\partial_x f$ was not continuous, we could find $z \in [0,1]^2$ and $R > 0$ such that for any $\delta > 0$, we can find $w \in [0,1]^2$ with $|z - w| \leq \delta$ and with $| \partial_x f(w) - \partial x f(z)| \geq R$. Applying Taylor's formula and the boundedness and continuity of $\partial_x^2 f$, there exists $C > 0$, such that if $z + te_1 \in [0,1]^2$ and $w + te_1 \in [0,1]^2$
	%
	\[ |f(z + t e_1) - f(z) - t \partial_x f(z)| \leq C t^2. \]
	%
	and
	%
	\[ |f(w + te_1) - f(w) - t \partial_x f(w)| \leq C t^2. \]
	%
	Thus
	%
	\[ |f(z + t e_1) - f(w + te_1)| \geq |t| |\partial_x f(z) - \partial_x f(w)| - |f(z) - f(w)| - 2Ct^2 \geq |t| R - |f(z) - f(w)| - 2Ct^2 \]
	%
	Since $f$ is continuous, it is uniformly continuous. Thus for any $\varepsilon > 0$, there is $\delta > 0$ such that if $|v - w| \leq \delta$, then $|f(v) - f(w)| \leq \varepsilon$. But
	%
	\[ |(z + te_1) - (w + te_1)| = |z - w| \leq \delta, \]
	%
	so we conclude that $|f(z + te_1) - f(w + te_1)| \leq \varepsilon$ and $|f(z) - f(w)| \leq \varepsilon$, and thus that
	%
	\[ |t| R - 2Ct^2 \leq 2 \varepsilon \]
	%
	It is possible to pick $|t| = \min(1/10, R/4C)$. If $R/4C \geq 1/10$, then $R \geq 2C/5$, and so setting $t = 1/10$ gives
	%
	\[ 2 \varepsilon \geq R/10 - C/50 \geq C/50. \]
	%
	Thus $\varepsilon$ cannot be arbitrarily small, which gives a contradiction. If $R \leq 2C/5$, we can pick $|t| = R/4C$, which gives
	%
	\[ 2 \varepsilon \geq R^2/8C. \]
	%
	This again prevents $\varepsilon$ from being arbitrarily small.
\end{solution}

\question Let
%
\[ s_N(x) = \sum_{n = 1}^N (-1)^n \frac{x^{3n}}{n^{2/3}}. \]
%
Prove that $s_N(x)$ converges to a limit $s(x)$ on $[0,1]$, and that there is a constant $C > 0$ so that for all $N \geq 1$ the inequality
%
\[ \sup_{x \in [0,1]} |s_N(x) - s(x)|\leq C N^{-2/3} \]
%
holds.
\begin{solution}
    The convergence here becomes highly singular near $x = 1$, because the power series diverges for $|x| > 1$. Thus we cannot use normal power series techniques here. It seems we must exploit the oscillation of the series here to get explicit bounds since removing the $(-1)^n$ in the definition of the series causes the sum to diverge at $x = 1$. If we define
    %
    \[ S_N(x) = \sum_{n = 1}^N (-x^3)^n \]
    %
    for $N > 0$, and $S_0 = 0$, then the geometric series formula shows that there is $C > 0$ such that $|S_N(x)| \leq C$ for all $N$ and $x \in [0,1]$, and a summation by parts shows that
    %
    \[ \sum_{n = 1}^N (-1)^n \frac{x^{3n}}{n^{2/3}} = \sum_{n = 1}^N (S_n - S_{n-1}) \frac{1}{n^{2/3}} = \sum_{n = 1}^{N-1} S_n \left( \frac{1}{n^{2/3}} - \frac{1}{(n+3)^{2/3}} \right) + S_N / n^{2/3}. \]
    %
    Now a mean value theorem / Taylor series expansion implies that
    %
    \[ \left| \frac{1}{n^{2/3}} - \frac{1}{(n+3)^{2/3}} \right| \lesssim \frac{1}{n^{5/3}}, \]
    %
    which is summable, and so for $M \geq N$,
    %
    \[ |s_M(x) - s_N(x)| \lesssim \sum_{n = N}^{M-1} S_n / n^{5/3} + S_M / M^{2/3} \lesssim C/N^{2/3}. \]
    %
    This is sufficient to show the existence of the point wise limit (since it shows $s_N$ is a Cauchy sequence), and also shows the required inequality.
\end{solution}


\item (Spring 2016) Let $1<p<\infty$, and let $\chi_{[1-\frac{1}{n},1]}$ denote the characteristic function of $[1-\frac{1}{n},1]$. For which $\alpha\in \R$ does the sequences $n^{\alpha}\chi_{[1-\frac{1}{n},1]}$ converge weakly to $0$ in $L^{p}(\R)$?

\begin{solution}
  Case 1: Suppose $\alpha> 1/p$. Then $\norm{f_{n}}_{p}^{p} = \int_{1-\frac{1}{n}}^{1}n^{\alpha p}dx = n^{\alpha p - 1}\to \infty$ as $n\to\infty$. Since weakly convergent sequences are bounded in norm, it follows that $f_{n}$ does not weakly converge if $\alpha>1/p$.

  Case 2: Suppose $\alpha<1/p$. Let $g\in L^{q}(\R)$ where $\frac{1}{p}+\frac{1}{q}=1$. Using Holder's inequality,
  \begin{align*}
    \int f_{n}g &= \int_{1-\frac{1}{n}}^{1}n^{\alpha} g \leq \left( \int_{\R}n^{\alpha p}\chi_{[1-\frac{1}{n},1]} \right)^{\frac{1}{p}} \norm{g}_{q} = n^{\alpha-\frac{1}{p}}\norm{g}_{q}\to 0 \text{ as }n\to\infty.
  \end{align*}
  Since $g\in L^{q}$ was arbitrary, it follows that $f_{n}$ converges weakly to $0$ in $L^{p}$.

  Case 3: Suppose $\alpha=1/p$. Let $g\in L^{q}(\R)$ where $\frac{1}{p}+\frac{1}{q}=1$. By density of $C_{c}^{\infty}(\R)$ in $L^{q}(\R)$, there exists $\phi\in C_{c}^{\infty}(\R)$ with $\norm{\phi-g}_{q}<\epsilon$. Then
  \begin{align*}
    \left|\int_{\R }f_{n}g\right|
    &= \int_{\R} |f_{n}(g-\phi)| + \int_{\R} |f_{n}\phi|\\
    &\leq \norm{f_{n}}_{p}\underbrace{\norm{g-\phi}_{q}}_{<\epsilon} + \int_{1-\frac{1}{n}}^{1}n^{\frac{1}{p}}|\phi(x)|dx
    &&\text{ by Holder's inequality}\\
    &\leq \epsilon + \int_{1-\frac{1}{n}}^{1}n^{\frac{1}{p}}|\phi(x)|dx
    &&\text{ since }\norm{f_{n}}_{p}= 1\text{ for all }n\\
    &\leq \epsilon + \norm{\phi}_{\infty}\int_{1-\frac{1}{n}}^{1}n^{\frac{1}{p}}dx
    &&\text{ since }\phi\in C_{c}^{\infty}(\R)\\
    &= \epsilon + \norm{\phi}_{\infty}n^{\frac{1}{p}-1}.
  \end{align*}
  Since $\frac{1}{p}<1$, it follows that $\limsup_{n\to\infty}\left| \int_{\R}f_{n}g\right| \leq \epsilon$. Since $\epsilon>0$ and $g\in L^{q}(\R)$ were arbitrary, it follows that $\lim_{n\to\infty} \int f_{n}g = 0$ for all $g\in L^{q}$. So $f_{n}$ converges weakly to $0$ in $L^{p}$.
\end{solution}


\item (Spring 2018) Let $x_{n}$ be a sequence in a Hilbert space $H$. Suppose that $x_{n}$ converges to $x$ weakly. Prove that there is a subsequence $x_{n_{k}}$ such that
\begin{equation*}
  \frac{1}{N}\sum_{k=1}^{N}x_{n_{k}}
\end{equation*}
converges to $x$ (in norm) as $N\to\infty$. 

\begin{solution}
  (Note: this result is called the Banach-Saks theorem). Without loss of generality, assume that $x=0$. Since $x_{n}$ converges weakly, it follows that $x_{n}$ is bounded in norm (this is a consequence of the uniform boundedness principle). Let $C = \max_{n\geq 1}\norm{x_{n}}$. We construct a subsequence in the following manner. Let $x_{n_{1}}=x_{1}$. Having chosen $x_{n_{N}}$, choose $x_{n_{N+1}}$ so that
  \begin{equation*}
    \Big\langle \sum_{i=1}^{N}x_{n_{k}},x_{n_{N+1}}\Big\rangle \leq 2^{-(N+1)}.
  \end{equation*}
  Such an $x_{n_{N=1}}$ exists since $\langle y,x_{n}\rangle \to 0$ as $n\to \infty$ for all $y\in H$. We need to show that
  \begin{equation*}
    \norm{N^{-1}\sum_{k=1}^{N}x_{n_{k}}}\to 0 \text{ as }n\to\infty.
  \end{equation*}
  To see this, we have:
  \begin{align*}
    N^{-2}\Big\langle \sum_{k=1}^{N}x_{n_{k}}, \sum_{j=1}^{N}x_{n_{j}} \Big\rangle
    &= N^{-2}\sum_{j,k=1}^{N}\Big\langle x_{n_{k}}, x_{n_{j}}\Big\rangle\\
    &= N^{-2}\left( \sum_{j=1}^{N}\norm{x_{n_{j}}}^{2}+ \sum_{k=2}^{N}\sum_{j=1}^{k-1}\langle x_{n_{j}},x_{n_{k}}\rangle \right)\\
    &=N^{-2} \left(  \sum_{j=1}^{N}\norm{x_{n_{j}}}^{2}+ \sum_{k=2}^{N}\underbrace{\Big\langle \sum_{j=1}^{k-1} x_{n_{j}},x_{n_{k}}\Big\rangle}_{\leq 2^{-k}} \right)\\
    &\leq N^{-2} \left( C N + 1  \right) \to 0 \text{ as } N\to\infty.
  \end{align*}
\end{solution}

  
  
\question (Fall 2015) Let $E \subset \RR$ be a measurable set, such that $E + r = E$ for all $r \in \mathbf{Q}$. Show that $|E| = 0$ or $|E^c| = 0$.
\begin{solution}
    We apply the Lebesgue density theorem. Suppose $|E| \neq 0$. Fix $\delta > 0$. Then there exists $\varepsilon_0 > 0$ such that for $\varepsilon < \varepsilon_0$, we can find $a,b \in \mathbf{Q}$ with $b - a < \varepsilon$ with
    %
    \[ |E \cap (a,b)| \geq (1 - \delta) (b - a) \]
    %
    Set $\varepsilon_1 = b - a$. Because $E$ is invariant under translations in $\mathbf{Q}$,
    %
    \[ |E \cap (0,\varepsilon_1)| \geq (1 - \delta) \varepsilon_1. \]
    %
    Then $|E \cap (a,a + \varepsilon_1)| \geq (1 - \delta) \varepsilon_1$ for all $a \in \mathbf{Q}$. This implies that for any $x \in \RR$,
    %
    \[ \limsup_{\substack{x \in I\\|I| \to 0}} \frac{|E \cap I|}{|I|} \geq 1. \]
    %
    The Lebesgue density theorem thus implies that $|E^c| = 0$.
\end{solution}

\question (Spring 2015)
    Let $\{ r_n \} \in [0,1]$ be an arbitrary sequence, and define the function
    %
    \[ f(x) = \sum_{r_n < x} \frac{1}{2^n} \]
    %
    Show that $f$ is Borel measurable, find all it's points of discontinuity, and find $\int_0^1 f(x)\; dx$.
\begin{solution}
    Write
    %
    \[ f_n(x) = \sum_{k = 1}^n \mathbf{I}_{(r_n, \infty)} \cdot 2^{-n}. \]
    %
    Then for each $n$, $f_n$ is a simple function, and is thus measurable. Moreover, $f = \lim_{n \to \infty} f_n$, and this limit is monotone. The pointwise limit of measurable functions is measurable, so $f$ is measurable.
    
    Next, we look at the discontinuity points. We claim that the set of discontinuity points is \emph{precisely} the set of values $\{ r_n \}$. To see this, fix $\varepsilon > 0$, and note that
    %
    \[ f(x + \varepsilon) - f(x) = \sum_{x \leq r_n < x + \varepsilon} 2^{-n} \]
    %
    Suppose there exists some $n_0$ such that $x = r_{n_0}$. Then
    %
    \[ f(x + \varepsilon) - f(x) \geq 1/2^{n_0}, \]
    %
    and thus
    %
    \[ \limsup_{\varepsilon \to 0^+} f(x + \varepsilon) - f(x) \geq 1/2^{n_0}, \]
    %
    implying $f$ is not continuous at $x$. On the other hand, if $x \neq r_n$ for any $n$, then for any $N > 0$, there exists $\varepsilon_N$ such that for $\varepsilon < \varepsilon_N$, $\{ r_1, \dots, r_N \}$ is disjoint from $[x,x+\varepsilon)$. Therefore, for $\varepsilon < \varepsilon_0$,
    %
    \[ f(x + \varepsilon) - f(x) \leq \sum_{k = N+1}^\infty 2^{-n} = 2^{-N}. \]
    %
    Thus we conclude that
    %
    \[ \lim_{\varepsilon \to 0^+} f(x + \varepsilon) - f(x) = 0. \]
    %
    On the other hand, I claim that the left hand limit does not impact continuity, since for all $x \in [0,1]$,
    %
    \[ \lim_{\varepsilon \to 0^-} f(x) - f(x-\varepsilon) = 0. \]
    %
    Indeed, we have
    %
    \[ f(x) - f(x-\varepsilon) = \sum_{x - \varepsilon \leq r_n < x}. \]
    %
    If $N > 0$, there is $\varepsilon_0$ such that for $\varepsilon < \varepsilon_0$, $[x-\varepsilon,x)$, $\{ r_1, \dots, r_N \}$ is disjoint from $[x-\varepsilon,x)$, and thus for such $\varepsilon$,
    %
    \[ f(x) - f(x-\varepsilon) \leq 2^{-N}. \]
    %
    Taking $N \to \infty$ gives the limit, and completes the classification of discontinuity points.
    
    Finally, we note the monotone convergence theorem implies that
    %
    \[ \int_0^1 f(x)\; dx = \lim_{n \to \infty} \int_0^1 f_n(x)\; dx. \]
    %
    Since $f_n$ is a simple function, we easily evaluate
    %
    \[ \int_0^1 f_n(x)\; dx = \sum_{k = 1}^n (1 - r_k) \cdot 2^{-k}. \]
    %
    Thus
    %
    \[ \int_0^1 f(x)\; dx = \sum_{k = 1}^\infty (1 - r_k) \cdot 2^{-k}. \]
\end{solution}


\newpage
\section{General Practice 3: Warm Up Question}

\question (Spring 2021) Let $f$ be a $C^1$ function on $[0,\infty)$. Suppose that
%
\[ \int_0^\infty t |f'(t)|^2\; dt < \infty \]
%
and
%
\[ \lim_{T \to \infty} \frac{1}{T} \int_0^T f(t)\; dt = L. \]
%
Show that $f(t) \to L$ as $t \to \infty$.
\begin{solution}
    To relate $f$ and $f'$, we apply the fundamental theorem of calculus. Suppose that
    %
    \[ \int_R^\infty t |f'(t)|^2\; dt \leq \varepsilon. \]
    %
    Then if $S = R e^{1/\varepsilon^{1/2}}$, then for $s \in [R,S]$,
    %
    \[ |f(s) - f(R)| = \left| \int_R^s f'(t)\; dt \right| \leq \left( \int_R^s t |f'(t)|^2\; dt \right)^{1/2} \left( \int_R^s 1/t\; dt \right)^{1/2} \leq \varepsilon^{1/2} \log(s/R)^{1/2} \leq \varepsilon^{1/4}. \]
    %
    If $R$ is suitably large, then
    %
    \[ \left| \frac{1}{R} \int_0^R f(t)\; dt \right| \leq 2L. \]
    %
    Thus
    %
    \[ \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_0^R f(t)\; dt \right| \leq 2L e^{-1/\varepsilon^{1/2}}. \]
    %
    Next,
    %
    \begin{align*}
        \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_R^{Re^{1/\varepsilon^{1/2}}} f(t)\; dt &= \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_R^{Re^{1/\varepsilon^{1/2}}} f(R) + [f(t) - f(R)]\; dt\\
        &= \left( 1 - e^{-1/\varepsilon^{1/2}} \right) f(R) + \int_R^{R e^{1/\varepsilon^{1/2}}} [f(t) - f(R)]\; dt.
    \end{align*}
    %
    Again, for any $\delta > 0$, if $R$ is suitably large, then
    %
    \[ \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_0^{R e^{1/\varepsilon^{1/2}}} f(t)\; dt - L \right| \leq \delta \]
    %
    Combining all these inequalities yields that
    %
    \begin{align*}
        |L - f(R)| &= \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_0^{R e^{1/\varepsilon^{1/2}}} f(t)\; dt - f(R) \right| + \delta\\
        &= \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_R^{R e^{1/\varepsilon^{1/2}}} f(t)\; dt - f(R) \right| + \delta + 2Le^{-1/\varepsilon^{1/2}}\\
        &= \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_R^{R e^{1/\varepsilon^{1/2}}} [f(t) - f(R)]\; dt - e^{-1/\varepsilon^{1/2}} f(R) \right| + \delta + 2Le^{-1/\varepsilon^{1/2}}\\
        &\leq e^{-1/\varepsilon^{1/2}} |f(R)| + \delta + 2L e^{-1/\varepsilon^{1/2}} + \varepsilon^{1/4}.
    \end{align*}
    %
    The fact that
    %
    \[ |f(R)| \leq 2 \left| \frac{1}{R e^{1/\varepsilon^{1/2}}} \int_R^{Re^{1/\varepsilon^{1/2}}} f(t)\; dt \right| + 2 \left| \int_R^{R e^{1/\varepsilon^{1/2}}} [f(t) - f(R)]\; dt \right| \leq 2L + 2 \varepsilon^{1/4} \]
    %
    shows $f$ is bounded, which is sufficient to complete the proof.
\end{solution}


\begin{solution}
(Due to Anuk D.) Integration by parts with $u=f(t)$, $dv=1$ gives
\begin{equation*}
  \frac{1}{T}\int_{0}^{T}f(t)dt = f(T) - \frac{1}{T}\int_{0}^{T}tf'(t)dt.
\end{equation*}
Therefore it suffices to show that $\frac{1}{T}\int_{0}^{T}tf'(t)dt$ converges as $T\to \infty$. Indeed, by the Cauchy Schwarz inequality,
\begin{align*}
  \left|\frac{1}{T}\int_{0}^{T}tf'(t)dt\right|
  &= \left|\frac{1}{T}\int_{0}^{\sqrt{T}}\sqrt{t}\cdot\sqrt{t}f'(t)dt + \frac{1}{T}\int_{\sqrt{T}}^{T}\sqrt{t}\cdot \sqrt{t}f'(t)dt\right|\\
  & \leq \frac{1}{T}\left(\int_{0}^{\sqrt{T}}t dt \right)^{\frac{1}{2}}\left( \int_{0}^{\sqrt{T}}t |f'(t)|^{2}dt \right)^{\frac{1}{2}}+ \frac{1}{T}\left( \int_{\sqrt{T}}^{T}tdt \right)^{\frac{1}{2}}\left( \int_{\sqrt{T}}^{T}t|f'(t)|^{2}dt \right)^{\frac{1}{2}}\\
  &\leq \frac{1}{2 \sqrt{T}} \left(\int_{0}^{\infty}t|f'(t)|^{2} dt \right)^{\frac{1}{2}} + \frac{1}{T}\left( \frac{T^{2}-T}{2} \right)^{1/2}\left(\int_{\sqrt{T}}^{\infty}t|f'(t)|^{2}dt \right)^{\frac{1}{2}}\\
  &\leq \frac{C}{2 \sqrt{T}}  +\left(\int_{\sqrt{T}}^{\infty} t|f'(t)|^{2}dt \right)^{\frac{1}{2}} \to 0 \text{ as }T\to 0
\end{align*}
\end{solution}



\question (Fall 2013) Let $E= \left\{ (x_{1},x_{2}) : x_{1},x_{2}\in \R, x_{1}-x_{2}\in \Q\right\}$. Is it possible to find to Lebesgue measurable sets $A_{1},A_{2}\subset\R$ such that $|A_{1}|,|A_{2}|>0$, and $A_{1}\times A_{2}\subset E^{c}$?
\begin{solution}
  No, it is not possible. We will show that $|A_{1}|\cdot|A_{2}| = 0$ whenever $A_{1}\times A_{2}\subset E^{c}$. 

 \textbf{ Case 1:} Suppose $A_{1},A_{2}\subset [-n,n]$ for some positive integer $n$.
  Let $h(y)= \int_{\R}\chi_{A_{1}}(x)\chi_{A_{2}}(x+y)dx$. By Tonelli's Theorem,
  \begin{align*}
    \int_{\R}h(y)dy
    &=\int_{\R} \chi_{A_{1}}(x) \int_{\R} \chi_{A_{2}}(y+x)dy dx\\
    &=\int_{\R} \chi_{A_{1}}(x) \int_{\R} \chi_{A_{2}}(y)dy dx\\
    &=\left(\int_{\R} \chi_{A_{1}}(x)dx\right)\left( \int_{\R} \chi_{A_{2}}(y)dy\right)\\
    &=|A_{1}|\cdot |A_{2}|.
  \end{align*}

  Next we claim that $h$ is also continuous. This is by average continuity of the $L^{1}$ norm. For a detailed proof, let $\epsilon>0$. Since $\chi_{A_{2}}\in L^{1}(\R)$, there exists $\phi\in C_{c}(\R)$ with compact support $K\subset \R$ such that $\norm{\chi_{A_{2}}-\phi}_{L^{1}}<\epsilon/3$. Since $\phi$ is continuous on a compact set, it is uniformly continuous, so we may choose $\delta>0$ such that $|\phi(s)-\phi(t)|\leq\epsilon/3|K|$ whenever $|s-t|\leq\delta$. Then
  \begin{align*}
    |h(y+\delta)-h(y)|
    &= \int_{A_{1}}\chi_{A_{2}}(x+y+\delta)dx - \chi_{A_{2}}(x+y)dx\\
    &\leq \int_{A_{1}}|\chi_{A_{2}}(x+y+\delta) -\phi(x+y+\delta)|dx +\int_{A_{1}}|\phi(x+y+\delta)-\phi(x+y)|dx\\
    &\quad+\int_{A_{1}}|\phi(x+y) - \chi_{A_{2}}(x+y)|dx\\
    &\leq 2\int_{\R}|\chi_{A_{2}} -\phi|dx +\int_{K}|\phi(x+\delta)-\phi(x)|dx\\
    &\leq 2\epsilon/3 + \epsilon/3\\
    &= \epsilon.
  \end{align*}
  Since $\epsilon>0$ was arbitrary, $h$ is continuous.

  Finally we claim that $h(y)=0$ for all $y\in \Q$. Indeed, if $y\in \Q$ then for any $x$, we have $x-(y+x)\in \Q$ and hence $(x,y+x)\in E$. Therefore since $A_{1}\times A_{2}\subset E^{c}$, $(x,y+x)\notin A_{1}\times A_{2}$. Therefore 
  \begin{equation*}
    \chi_{A_{1}}(x)\chi_{A_{2}}(y+x)=0.
  \end{equation*}
  Since this holds for all $x\in \R$, it follows that $h(y)= 0$.
  We have shown that
  \begin{enumerate}
  \item $\int_{\R} h = |A_{1}|\cdot |A_{2}|$
  \item $h$ is continuous
  \item $h(y) = 0$ for all $y\in \Q$.
  \end{enumerate}
  By density of $\Q$ in $\R$, (2) and (3) imply that $h\equiv 0$, so that by (1), $|A_{1}|\cdot |A_{2}| = 0$.

  \textbf{Case 2:} Let $A_{1},A_{2}\subset \R$ be arbitrary measurable sets. By case 1,  $|A_{1}\cap [-n,n]|\cdot |A_{2}\cap [-n,n]| =0$ for all $n$. Therefore by continuity of Lebesgue measure,
  \begin{equation*}
    |A_{1}|\cdot|A_{2}| = \lim_{n\to\infty}|A_{1}\cap [-n,n]|\cdot |A_{2}\cap [-n,n]| =\lim_{n\to\infty}0=0.
  \end{equation*}
\end{solution}






\question (Spring 2021)
\begin{parts}
\part Let $H_1$ and $H_2$ be Hilbert spaces, and let $T:H_1\to H_2$ be a continuous linear operator. Give a precise definition of the adjoint operator $T^*$.
\begin{solution}
    The adjoint operator $T^*$ is a bounded operator from $H_2$ to $H_1$. For each $x \in H_2$, $T^*x$ is the unique element of $H_1$ such that for any $y \in H_1$, $\langle T^* x, y \rangle = \langle x, Ty \rangle$.
\end{solution}

\part Let $(a,b)\subset \R$ be a (possibly infinite) open interval. If $f\in L^2(a,b)$, explain what it means that the distributional derivative $f'$ is also in $L^2(a,b)$. 
\begin{solution}
    This means that there exists $g \in L^2(a,b)$, such that for any $\phi \in C_c^\infty(a,b)$,
    %
    \[ \int f(X) \phi'(x)\; dx = - \int g(x) \phi(x)\; dx. \]
\end{solution}

\part Let $\R_+$ denote the positive real axis $[0,\infty)$. Let $H^1(\R)$ (respectively $H^1(\R_+)$) be the space of real-valued functions $f\in L^2(\R)$  (respectively $f\in L^2(\R_+)$ such tha thte distributional derivative $f'$ is also in $L^2(\R)$ (respectively $L^2(\R_+)$). Then $H^1(\R)$ and $H^1(\R_+)$ are Hilbert spaces with inner product given by
\begin{align*}
\langle f, g \rangle_{H^1(\R)} &= \int_{\R} f(x)g(x)dx + \int_{\R} f'(x) g'(x)dx,\\
\langle f, g \rangle_{H^1(\R_+)} &= \int_{\R_+} f(x)g(x)dx + \int_{\R_+} f'(x) g'(x)dx
\end{align*}
Let $T:H^1(\R)\to H^1(\R_+)$ be the mapping given by the restriction. Compute exactly the adjoint operator $T^*$.
\begin{solution}
\begin{comment}
    Let $K$ be the kernel of $T$, and let $V$ be the orthogonal complement of $K$. We claim $T$ is a unitary map when restricted as a map from $V$ to $H^1(\RR_+)$. Indeed, if $f \in V$, then $f \in L^2(\RR)$, and since $\langle f, \phi \rangle = 0$ for any $\phi \in C_c^\infty(\RR)$ supported compactly on $(-\infty,0)$, we conclude via an integration by parts that the distribution $f - f''$ is supported on $x \geq 0$. Thus on $x \leq 0$, we can write $f(x) = A e^x + B e^{-x}$ on $x \leq 0$, and the fact that $f \in L^2(\RR)$ implies that $B = 0$.
    
    
    Any element of $H^1(\RR_+)$ is continuous, and has a well defined value at zero (a trace). Given $f \in H^1(\RR_+)$, define
    %
    \[ T^*f(x) = \begin{cases} f(0) e^x &: x < 0 \\ f(x) &: x > 0. \end{cases} \]
    %
    Then $T^* f$ is obviously square integrable, and
    %
    \begin{align*}
        \int T^*f(x) \phi(x)\; dx &= f(0) \int_{-\infty}^0 e^x \phi'(x)\; dx + \int_0^\infty f(x) \phi'(x)\; dx\\
        &= f(0) \left[ \phi'(0) - \int_{-\infty}^0 e^x \phi(x)\; dx \right] + \int_0^\infty f(x) \phi'(x)\; dx\\
        &= - f(0) \int_{-\infty}^0 e^x \phi(x)\; dx + \int_0^\infty f(x) \phi'(x)\; dx
    \end{align*}
    
    
    One can argue (Morrey's inequality for example, plus the density of $C_c^\infty(\RR)$ in $H^1(\RR)$), that if $g \in K$, then $g(0) = 0$.
\end{comment}
\end{solution}

\end{parts}







\newpage
\section{General Practice 3: Bonus Questions}

\question (Fall 2015) Let $(X, \mu)$ be a measure space, and let $f: X \to \RR$ be measurable. Then if $1 \leq p < r < q < \infty$ and there is $C < \infty$ such that
%
\[ \mu(\{ x : |f(x)| > \lambda \}) \leq \frac{C}{\lambda^p + \lambda^q} \]
%
for every $\lambda > 0$. Then $f \in L^r(\mu)$.
\begin{solution}
    We perform a dyadic decomposition, writing
    %
    \begin{align*}
        \int |f(x)|^r\; dx &= \sum_{k = -\infty}^\infty \int_{2^k < |f(x)| \leq 2^{k+1}} |f(x)|^r\; dx\\
        &\leq \sum_{k = -\infty}^\infty \mu(\{ 2^k < |f(x)|) \cdot 2^{kr}.
    \end{align*}
    %
    For $\lambda < 1$, $\lambda^p \leq \lambda^q$, so
    %
    \[ \mu(\{ 2^k < |f(x)| \}) \leq C/2^{kp}. \]
    %
    For $\lambda \geq 1$, $\lambda^q \leq \lambda^p$, so
    %
    \[ \mu(\{ 2^k < |f(x)| \}) \leq C/2^{kq}. \]
    %
    Thus we conclude
    %
    \[ \int |f(x)|^r\; dx \leq C \left( \sum_{k = -\infty}^0 2^{k(r - p)} + \sum_{k = 1}^\infty 2^{k(r - q)} \right). \]
    %
    Both of these sums converge, so
    %
    \[  \int |f(x)|^r\; dx \lesssim_{p,q} C < \infty. \]
\end{solution}

\begin{solution}
  \textit{Case 1:} Suppose that $(X,\mathcal{M},\mu)$ is $\sigma$-finite. This assumption of $\sigma$-finiteness allows us to apply the Fubini-Tonelli Theorem in the following calcuation:
  \begin{align*}
    \int_{X}|f(x)|^{r}dx
    &= \int_{X} \int_{0}^{|f(x)|}r t^{r-1}dt dx\\
    &=\int_{X} \int_{0}^{\infty}  rt^{r-1}\chi_{[ |f(x)|>t]}dtdx \\
    &=\int_{0}^{\infty}r t^{r-1}\int_{X} \chi_{[ |f(x)|>t]}dxdt &&\text{by Tonelli's Theorem}\\
    &=\int_{0}^{\infty}r t^{r-1}\mu\{x: |f(x)|>t\}dt\\ 
    &\leq Cr\int_{0}^{1}\frac{t^{r-1}}{t^{p}+t^{q}}dt + Cr\int_{1}^{\infty} \frac{t^{r-1}}{t^{p}+t^{q}}dt\\ 
    &\leq Cr\int_{0}^{1}t^{r-1-p}dt + Cr\int_{1}^{\infty} t^{r-1-q}dt
  \end{align*}
  and the right-hand side is finite since $1<p<r<q<\infty$.
  
  \vt
  \textit{Case 2:} Suppose $(X,\mathcal{M},\mu)$ is not $\sigma$-finite. Let $X'= \left\{ x\in X: |f(x)|>0 \right\}$. Also define $\mathcal{M}'=\left\{ M\cap X' : M\in \mathcal{M} \right\}$ (it is easy to see this is a $\sigma$-algebra) and define $\mu'$ as the restriction of $\mu$ to $\mathcal{M}'$. Then $(X',\mathcal{M}',\mu')$ is $\sigma$-finite because
  \begin{equation*}
    X'= \bigcup_{n=1}^{\infty}\left\{ x\in X': |f(x)|>\frac{1}{n} \right\}
  \end{equation*}
  and 
  \begin{equation*}
    \mu\left\{ x\in X': |f(x)|>\frac{1}{n} \right\} \leq \frac{C}{n^{-p}+n^{-q}}<\infty
  \end{equation*}
  for all $n$.

  Therefore by Case 1, $\int_{X'}|f(x)|^{r}dx<\infty$. And since $\int_{X}|f(x)|^{r}dx = \int_{X'}|f(x)|^{r}dx$, we are done. 
  \end{solution}

\question (Spring 2020)
Let $\sum_{n=1}^{\infty}a_{n}$ be a convergent series. Let $b_{n}\in\R$ be an increasing sequence with $\lim_{n\to\infty}b_{n}=\infty$. Show that
\begin{equation*}
  \lim_{n\to\infty} \frac{1}{b_{n}}\sum_{k=1}^{n}b_{k}a_{k} = 0.
\end{equation*}

\begin{solution}
  Let $k_{n}= \max \left\{ k\geq 1: \left| \sum_{i=1}^{k}a_{i}b_{i} \right| < \sqrt{b_{n}}\text{ and }k\leq n\right\}$. Since $\sqrt{b_{n}}\to\infty$ monotonically as $n\to \infty$, $k_{n}$ is defined for sufficiently large $n$, is increasing, and $k_{n}\to\infty$. For $n$ sufficiently large, we may write
\begin{equation*}
  \frac{1}{b_{n}}\sum_{k=1}^{n}b_{k}a_{k} =   \frac{1}{b_{n}}\sum_{k=1}^{k_{n}-1}b_{k}a_{k} +  \frac{1}{b_{n}}\sum_{k=k_{n}}^{n}b_{k}a_{k}
\end{equation*}
By choice of $k_{n}$, $\left|\frac{1}{b_{n}}\sum_{k=1}^{k_{n}-1}b_{k}a_{k}\right|\leq \frac{1}{\sqrt{b_{n}}}\to 0$ as $n\to\infty$, so it suffices to show that
\begin{equation*}
  \limn \frac{1}{b_{n}}\sum_{k=k_{n}}^{n}b_{k}a_{k} = 0.
\end{equation*}
Let $T_{k}= \sum_{i=1}^{k}a_{k}$ and let $a=\sum_{n=1}^{\infty}a_{n}$. Using the summation by parts formula,
\begin{align*}
  \frac{1}{b_{n}}\sum_{k=k_{n}}^{n}b_{k}a_{k}
  &= \frac{1}{b_{n}}\left[ T_{n}b_{n} - T_{k_{n}-1}b_{k_{n}}+ \sum_{k=k_{n}}^{n-1}T_{k}(b_{k}-b_{k+1}) \right]\\
  &=\frac{1}{b_{n}}\Bigg[ (T_{n}-a)b_{n} +ab_{n}- (T_{k_{n}-1}-a)b_{k_{n}}-ab_{k_{n}}\\
  &\quad +\left.\sum_{k=k_{n}}^{n-1}(T_{k}-a)(b_{k}-b_{k+1}) +a\sum_{k=k_{n}}^{n-1}(b_{k}-b_{k+1}) \right]\\
  &= \frac{1}{b_{n}}\left[ (T_{n}-a)b_{n} - (T_{k_{n}-1}-a)b_{k_{n}}+ \sum_{k=k_{n}}^{n-1}(T_{k}-a)(b_{k}-b_{k+1})\right]
\end{align*}
where for the last equality we used the fact that $\sum_{k=k_{n}}^{n-1}(b_{k}-b_{k+1})= b_{k_{n}}-b_{n}$. Let $M_{n}= \max_{k_{n}\leq k <n}|T_{k}-a|$. By the triangle inequality, and using $b_{k_{n}}\leq b_{n}$,
\begin{align*}
  \left| \frac{1}{b_{n}}\sum_{k=k_{n}}^{n}b_{k}a_{k} \right|
  &\leq 2\left| T_{n}-a \right| +  \frac{1}{b_{n}}\left|\sum_{k=k_{n}}^{n-1}(T_{k}-a)(b_{k}-b_{k+1})\right|\\
  &\leq 2\left| T_{n}-a \right| +  \frac{M_{n}}{b_{n}}\sum_{k=k_{n}}^{n-1}|b_{k}-b_{k+1}|\\
  &= 2\left| T_{n}-a \right| +  \frac{M_{n}}{b_{n}}\sum_{k=k_{n}}^{n-1}b_{k+1}-b_{k}\\
  &= 2\left| T_{n}-a \right| +  \frac{M_{n}}{b_{n}}(b_{n}-b_{k_{n}})\\
  &\leq 2\left| T_{n}-a \right| +  2M_{n}
\end{align*}
The result then follows since $T_{n}\to a$ and $M_{n}\to 0$ as $n\to \infty$. 
\end{solution}

\question (Spring 2021) Let $f_n \to f$ weakly in $L^2(\R)$ and $\norm{f_n}_2 \to \norm{f}_2$ as $n\to\infty$. Show that $f_n\to f$ strongly in $L^2(\R)$.

\begin{solution}
Since $f_n\to f$ weakly, therefore $\langle f_n, g \rangle \to \langle f, g \rangle$ for any $g\in L^2$. Since $\norm{f}_2 \leq \sup_{n}\norm{f_n}_2 <\infty$, we have $f\in L^2$. Therefore taking $g=f$, we have $\langle f_n,f\rangle \to \norm{f}_2^2$, so that
\begin{equation*}
\norm{f_n-f}_2^2 = \langle f_n-f,f_n-f\rangle = \norm{f_n}_2^2 -2 \langle f_n,f\rangle + \norm{f}^2_2 \to 0
\end{equation*}
as $n\to \infty$.
\end{solution}








\question (Spring 2017)
    Let $E \subset \RR^n$ be a set of finite, positive measure, and let $\{ t_k \}$ be a sequence with $\{ t_k \} > 0$ and $\lim_k t_k = 0$. Define, for $f \in L^p(\RR^n)$,
    %
    \[ Mf(x) = \sup_k \fint_{t_k E} |f(x-y)|\; dy. \]
    %
    Suppose furthermore that there is $C > 0$ such that
    %
    \[ | \{ x: Mf(x) > \lambda \}| \leq C \lambda^{-p} \| f \|_p^p. \]
    %
    Show that for every $f \in L^p(\RR^n)$,
    %
    \[ \lim_k \fint_{t_k E} f(x-y)\; dy = f(x). \]
    %
    for almost every $x \in \RR^d$.
\begin{solution}
    We note that the result is true for any $f \in C(\RR^n)$, i.e. for such functions, and any $x \in \RR$,
    %
    \[ \limsup_k \left| \fint_{t_k E} f(x) - f(x-y) \right| = 0. \]
    %
    Now given any $f \in L^p(\RR^n)$, for $1 \leq p < \infty$, fix $\varepsilon > 0$, and find $g \in C(\RR^n)$ with $\| f - g \|_p \leq \varepsilon$. Then
    %
    \[ |\{ M(f - g) > \lambda \}| \leq C \varepsilon^p \lambda^{-p}. \]
    %
    Thus
    %
    \begin{align*}
        &\left\{ \limsup_k \left| \fint_{t_k E} f(x) - f(x-y)\; dy \right| \geq \delta \right\}\\
        &\quad \subset \left\{ \limsup_k \left| \fint_{t_k E} f(x) - g(x) + g(x-y) - f(x-y)\; dy \right| \geq \delta \right\}\\
        &\quad \subset \left\{ x : |f(x) - g(x)| \geq \delta / 2 \right\} \cup \left\{ \limsup_k \left| \fint_{t_k E} g(x - y) - f(x-y)\; dy \right| \geq \delta/2 \right\}.
    \end{align*}
    %
    Now Markov's inequality implies that
    %
    \[ \left| \left\{ |f(x) - g(x)| \geq \delta / 2 \right\} \right| \leq \| f - g \|_p (\delta/2)^{-p} \lesssim_p \varepsilon \delta^{-p}. \]
    %
    and
    %
    \[ \left| \left\{ x: \limsup_k \left| \fint_{t_k E} g(x - y) - f(x-y)\; dy \right| \geq \delta/2 \right\} \right| \leq \left| \left\{ x: \left| M(g - f)(x) \right| \geq \delta/2 \right\} \right| \lesssim \varepsilon^p \delta^{-p}. \]
    %
    Thus we conclude that
    %
    \[ \left| \left\{ \limsup_k \left| \fint_{t_k E} f(x) - f(x-y)\; dy \right| \geq \delta \right\} \right| \lesssim (\varepsilon + \varepsilon^p) \delta^{-p}. \]
    %
    Taking $\varepsilon \to 0$ shows that for each $\delta > 0$,
    %
    \[ \left| \left\{ \limsup_k \left| \fint_{t_k E} f(x) - f(x-y)\; dy \right| \geq \delta \right\} \right| = 0. \]
    %
    Taking a union bound over $\delta = 1, 1/2, 1/4, \dots$ completes the proof.
\end{solution}




\question (Spring 2020)
  Let $f_{n}:[0,1]\to \R$ be a sequence of Lebesgue measurable functions such that $f_{n}$ converges to $f$ almost everywhere on $[0,1]$ and such that $\norm{f_{n}}_{L^{2}([0,1])}\leq 1$ for all $n$. Show that
  \begin{equation*}
    \limn \norm{f_{n}-f}_{L^{1}([0,1])}=0.
  \end{equation*}

\begin{solution}
  
  \textit{[This proof may border on trivializing the problem by invoking the Vitali convergence theorem, but I seem to have gotten away with a proof like this on my qual, so... -m]}. Since $|E|<\infty$, it follows trivially that $\{f_{n}\}$ is tight. Since $f_{n}\to f$ a.e., the result will then then follow by the Vitali Convergence Theorem provided that the sequence $\{f_{n}\}$ is uniformly integrable.
  We wish to show that $\left\{ f_{n} \right\}$ is uniformly integrable. Let $\epsilon>0$ and let $A\subset [0,1]$ be measurable. We need to show there exists a $\delta>0$ such that $\int_{A}|f_{n}| <\epsilon$ for all $n$ whenever $|A|<\delta$. By Cauchy-Schwarz inequality and the assumption that $\norm{f_{n}}_{L^{2}}\leq 1$ for all $n$,
  \begin{equation*}
    \int_{A}|f_{n}| = \int_{[0,1]}\chi_{A}|f_{n}| \leq |A|^{1/2}\norm{f_{n}}_{L^{2}} \leq |A|^{1/2}.
  \end{equation*}
  It follows that $\int_{A}|f_{n}|<\epsilon $ for all $n$ whenever $|A|<(\epsilon/2)^{2}$. Therefore $\left\{ f_{n} \right\}$ is uniformly integrable. This completes the proof.


Another (similar) proof is given:
\begin{proof}

  We first prove the following claim:
  \vt
  \noindent
  \textit{Claim 1:} $f_{n},f\in L^{1}([0,1])$ and $\norm{f_{n}}_{L^{1}},\norm{f}_{L^{1}}\leq 1$.
  \begin{proof}[Proof of Claim 1:]
    Since $x\mapsto x^{2}$ is convex and $[0,1]$ has measure 1, it follows by Jensen's inequality that
    \begin{equation*}
      \left(\int_{0}^{1}|f_{n}(x)|dx\right)^{2}\leq \int_{0}^{1}|f_{n}(x)|^{2}dx\leq 1.
    \end{equation*}
    Therefore $\norm{f_{n}}_{L^{1}}\leq 1$ and hence $f_{n}\in L^{1}([0,1])$ for all $n$. Since $f_{n}\to f$ a.e., it follows by Fatou's lemma that
    \begin{equation*}
      \int_{0}^{1}|f(x)|dx \leq \liminf_{n\to\infty}\int_{0}^{1}|f_{n}(x)|dx \leq 1.
    \end{equation*}
    Therefore $f\in L^{1}([0,1])$ with $\norm{f}_{L^{1}}\leq 1$  as well.
  \end{proof}



  By Egorov's Theorem, since $[0,1]$ has finite measure, there exists $E\subset [0,1]$ with $|E|<(\epsilon/4)^{2}$ such that $f_{n}\to f$ uniformly on $[0,1]\backslash E$. By uniform convergence, we may choose a positive integer $n_{0}$ such that $\left| f_{n}(x)-f(x) \right|< \epsilon/2$ for all $x\in [0,1]\backslash E$ and all $n\geq n_{0}$. Then for $n\geq n_{0}$,
  \begin{align*}
    \int_{0}^{1}\left| f_{n}-f \right|
    &= \int_{E} |f_{n}-f| + \int_{[0,1]\backslash E} |f_{n}-f|\\
    &< \int_{E} |f_{n}(x)-f(x)|dx + \epsilon/2\\
    &= \int_{[0,1]} \chi_{E}|f_{n}(x)|dx  + \int_{[0,1]}\chi_{E}|f(x)|dx + \epsilon/2 \\
    &\leq |E|^{1/2}\norm{f_{n}}_{L^{2}}+ |E|^{1/2}\norm{f}_{L^{2}} + \epsilon/2 &&\text{by Cauchy Schwarz}\\
    &\leq 2 |E|^{1/2}+ \epsilon/2 &&\text{by Claim 1}\\
    &< \epsilon/2+\epsilon/2\\
    &=\epsilon.
  \end{align*}
  It follows that $\limn \norm{f_{n}-f}_{L^{1}([0,1])}=0$.
\end{proof}
\end{solution}


\question (Fall 2017, Spring 2021) Let $f: \RR \to \RR$ be a compactly supported function that satisfies the H\"{o}lder condition with exponent $\beta \in (0,1)$, i.e. that there exists a constant $A < \infty$ such that for all $x,y \in \RR$, $|f(x) - f(y)| \leq A|x-y|^\beta$. Consider the function $g$ defined by
%
\[ g(x) = \int_{-\infty}^\infty \frac{f(y)}{|x-y|^\alpha}\; dy, \]
%
where $\alpha \in (0,\beta)$.
%
\begin{parts}
    \part Prove that $g$ is a continuous function at zero.
    \begin{solution}
        One way to see this immediately is to apply Young's inequality for convolution. More directly, we can write
        %
        \begin{align*}
            g(x) - g(0) = \int_{-\infty}^\infty f(y) \left( \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right)\; dy
        \end{align*}
        %
        For $|y| \leq |x|/2$, we have
        %
        \[ \left( \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right) \lesssim 1/|x|^\alpha. \]
        %
        Thus
        %
        \[ \left| \int_{|y| \leq |x|/2} f(y) \left( \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right)\; dy \right| \lesssim \| f \|_\infty |x|^{1-\alpha} \]
        %
        For $|y| \geq 2|x|$, we can apply Taylor's theorem / the mean value theorem to conclude that
        %
        \[ \left| \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right| \lesssim \frac{|x|}{|y|^{\alpha + 1}}. \]
        %
        This bound implies that
        %
        \[ \left| \int_{|y| \geq 2|x|} f(y) \left( \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right)\; dy \right| \lesssim \| f \|_\infty |x|^{1-\alpha}. \]
        %
        For $|x|/2 \leq |y| \leq 2|x|$, we have
        %
        \[ \left| \int_{|x|/2 \leq |y| \leq 2|x|} f(y) \left( \frac{1}{|x - y|^\alpha} - \frac{1}{|y|^\alpha} \right)\; dy \right| \leq 2 \| f \|_\infty \int_{|y| \leq 3|x|} \frac{1}{|y|^\alpha} \lesssim \| f \|_\infty |x|^{1-\alpha}. \]
        %
        These bounds together imply that
        %
        \[ |g(x) - g(0)| \lesssim \| f \|_\infty |x|^{1-\alpha}. \]
        %
        Thus $g(x) \to g(0)$ as $x \to 0$.
    \end{solution}
    
    \part Prove that $g$ is differentiable at zero. (Hint: Try the dominated convergence theorem).
    \begin{solution}
        Assume first that $f(0) = 0$. We have
        %
        \[ \frac{g(x) - g(0)}{x} = \int_{-\infty}^\infty f(y) \frac{1}{x} \left( \frac{1}{|y - x|^\alpha} - \frac{1}{|y|^\alpha} \right)\; dy. \]
        %
        The integrand here converges pointwise as $x \to 0$ to
        %
        \[ f(y) \alpha / |y|^{\alpha+1}, \]
        %
        which is integrable since the function is compactly supported and continuous away from the origin, and $|f(y)| \lesssim |y|^\beta$ near the origin. The mean value theorem implies that if $x \leq 1$,
        %
        \[ \left| \frac{1}{x} \left( \frac{1}{|y - x|^\alpha} - \frac{1}{|y|^\alpha} \right) \right| \lesssim 1/|y|^{\alpha+1} \]
        
        
        so we might expect that
        %
        \[ g'(0) = \int_{-\infty}^\infty f(y) \alpha / |y|^{\alpha+1}, \]
        %
        at least if the dominated convergence theorem
    \end{solution}
\end{parts}

\question (Fall 2018)
  Let $1<p\leq \infty$. Let $(X,\mathcal{M},\mu)$ be a finite measure speace. Let $\left\{ f_{n} \right\}$ be a sequence of measurable functions converging $\mu$-a.e. to the function $f$. Assume further that $\norm{f_{n}}_{p}\leq 1$ for all $n$. Prove that $f_{n}\to f$ as $n\to \infty$ in $L^{r}$ for all $1\leq r<p$.
\begin{solution}
  \textit{Case 1.} If $p=\infty$ then since $\norm{f}_{\infty}\leq 1$ and $f_{n}\to f$ a.e., it follows that $\norm{f}_{\infty}\leq 1$. Therefore by the triangle inequality and using the fact that $r\geq 1$, we have $|f_{n}-f|^{r}\leq (|f_{n}| + |f| )^{r}\leq 2^{r}$. Since the constant function $2^{r}$ is integrable on $X$ with respect to $\mu$, it follows by the dominated convergence theorem that $\lim_{n\to\infty}\int_{X} |f_{n}-f|^{r}d\mu = 0$. Therefore $f_{n}\to f$ in $L^{r}$.

  \textit{Case 2.} Suppose $1<p<\infty$. Let $\epsilon>0$, and let $\epsilon_{0} = 2^{-r} \epsilon^{\frac{p}{p-r}}$ (here we use the assumption that $p>r$ so that $\epsilon_{0}>0$ is defined). By Egorov's theorem there exists $A\subset X$ such that $\mu(A^{c})<\epsilon_{0}$ and $f_{n}\to f$ uniformly on $A$. Then
  \begin{equation*}
    \int_{X} |f_{n}-f|^{r} = \int_{A^{c}}|f_{n}-f|^{r}d\mu + \int_{A}|f_{n}-f|^{r}d\mu
  \end{equation*}
  Since $r\geq 1$, it follows that $|f_{n}-f|^{r}\to 0$ uniformly. Therefore $\lim_{n\to\infty }\int_{A}|f_{n}-f|^{r}d\mu =0$. Therefore
  \begin{equation}\label{eq:2}
    \limsup_{n\to\infty} \int_{X} |f_{n}-f|^{r} \leq \limsup_{n\to\infty} \int_{A^{c}}|f_{n}-f|^{r}d\mu
  \end{equation}
  By Holder's inequality,
  \begin{align*}
    \int_{A^{c}}|f_{n}-f|^{r}d\mu
    &= \int_{X} |f_{n}-f|^{r}\chi_{A^{c}} d\mu\\
    &\leq \left( \int_{X}|f_{n}-f|^{p} d\mu\right)^{r/p}\left( \int_{X}\chi_{A^{c}}d\mu  \right)^{\frac{p-r}{p}}\\
    &= \norm{f_{n}-f}_{p}^{r}\mu(A^{c})^{\frac{p-r}{r}}\\
    &= \epsilon\norm{f_{n}-f}_{p}^{r}\\
    &\leq \epsilon ( \norm{f_{n}}_{p}+ \norm{f}_{p})^{r}\\
    &\leq \epsilon \left( 1 + \norm{f}_{p} \right)^{r}
  \end{align*}
  Also, by Fatou's lemma, $\norm{f}_{p}\leq \liminf_{n\to\infty} \norm{f_{n}}_{p}\leq 1$. Therefore
  \begin{equation*}
    \int_{A^{c}} | f_{n}-f|^{r}d\mu \leq \epsilon 2^{r}
  \end{equation*}
  Therefore by \eqref{eq:2},
  \begin{equation*}
     \limsup_{n\to\infty} \int_{X} |f_{n}-f|^{r}d\mu  \leq \epsilon 2^{r}
   \end{equation*}
   Since $\epsilon>0$ was arbitrary, it follows that $\limsup_{n\to\infty} \int_{X} |f_{n}-f|^{r}d\mu=0$. Therefore $f_{n}\to f$ in $L^{r}$.
\end{solution}




\newpage
\section{Analysis Qualifying Exam 2022 Questions}

\question Let $x_1,\dots,x_{n+1}$ be pairwise distinct real numbers. Prove that there exists $C > 0$ such that if $P: \RR \to \RR$ is a polynomial with degree at most $n$, then
%
\[ \| P \|_{L^\infty[0,1]}\leq C \max \left( |P(x_1)|, \dots, |P(x_{n+1})| \right). \]
\begin{solution}
	Let $V_n$ be the vector space of degree $\leq n$ polynomials. The operator $T: V_n \to \RR^n$ given by
	%
	\[ TP = ( p(x_1), \dots, p(x_{n+1}) ) \]
	%
	is a bijection. To see that $T$ is injective, use the fact that a nonzero polynomial of degree $\leq n$ can have at most $n$ zeroes. That $T$ is surjective follows by the theory of polynomial interpolation, i.e. for any values $t_1,\dots,t_{n+1}$, the polynomial
	%
	\[ P(x) = \sum_{i = 1}^{n+1} t_i \cdot \frac{\prod_{j \neq i} (x - x_j)}{\prod_{j \neq i} (x_i - x_j)} \]
	%
	has $TP = (t_1,\dots,t_{n+1})$. Since $T$ is bijective, $T^{-1}: \RR^n \to V_n$ is well defined. As a linear map between two finite dimensional vector spaces, $T^{-1}$ is continuous from any norm on $\RR^n$, to any norm on $V_n$. If we equip $\RR^n$ with the $l^\infty$ norm, and $V_n$ with the $L^\infty[0,1]$ norm, then the result immediately follows.
\end{solution}

\question Given a real number $x$, let $\{ x \}$ denote the fractional part of $x$. Suppose $\alpha$ is an irrational number and define $T: [0,1] \to [0,1]$ by
%
\[ T(x) = \{ x + \alpha \}. \]
%
Prove: If $A \subset [0,1]$ is measurable and $T(A) = A$ then $|A| \in \{ 0, 1 \}$.
\begin{solution}
	Define $f: \RR \to \{ 0, 1 \}$ by setting $f(x) = \mathbf{I}(x \in A')$, where $A' = \bigcup_{n \in \ZZ} (A + n)$. Since $T(A) = A$, it follows that $f(x + \alpha) = f(x)$, i.e. $f$ is both $1$ periodic and $\alpha$ periodic. But applying the Fourier transform to both sides of this formula (viewing $f$ as a tempered distribution) yields that
	%
	\[ \widehat{f}(\xi) e^{2 \pi i \xi \alpha} = \widehat{f}(\xi), \]
	%
	i.e. that $\widehat{f}$ is supported on integer multiples of $2 \pi \alpha$. But the Poisson summation formula says precisely that $\widehat{f}$ is supported on the integer multiples of $2 \pi$, and the intersection of these two supports lies only at the origin. Thus $\widehat{f}$ is supported at the origin. So $f$ is a polynomial. But the only polynomial which is $\{ 0,1 \}$ valued is $f(x) = 1$ or $f(x) = 0$.
%	An easy (but lazy) way to prove the result is to apply the Weyl equidistribution theorem, which says that for any suitably regular function $f: [0,1] \to \RR$, and any $x \in [0,1]$,
	%
%	\[ \int_0^1 f(x)\; dx = \lim_{N \to \infty} \frac{1}{N} \sum_{k = 1}^N f ( \{ x + k \alpha \}  ). \]
	%
%	In particular, one can apply the result with $f(x)$

	Here is another method to prove the result, with the proof idea suggested by Yahui Qiu. Without loss of generality, we can consider the problem as showing that any subset $A$ of $\TT = \RR / \ZZ$ (we work with real numbers modulo the integers, so that we identify any real number with it's fractional part) with $A + \alpha = A$, for some irrational $\alpha$, has measure zero or measure one. If $|A| > 0$, then we can apply the Lebesgue density theorem to find a point $x_0 \in A$ of Lebesgue density. Thus
	%
	\[ \lim_{\delta \to 0} \frac{|A \cap (x_0 - \delta, x_0 + \delta)|}{2 \delta} = 1. \]
	%
	Since $A + \alpha = A$, we have $A + n\alpha = A$ for any $n \in \ZZ$, and
	%
	\begin{align*}
		\frac{|A \cap (x_0 + n \alpha - \delta, x_0 + n \alpha + \delta)}{2 \delta} &= \frac{|(A - n \alpha) \cap (x_0 - \delta, x_0 + \delta)|}{2 \delta}\\
		&= \frac{|A \cap (x_0 - \delta, x_0 + \delta)|}{2 \delta}.
	\end{align*}
	%
	Thus it follows that $x_n = x_0 + n \alpha$ is a point of Lebesgue density in $A$ for any $n \in \ZZ$, and moreover,
	%
	\[ \lim_{\delta \to 0} \inf_n \frac{|A \cap (x_n - \delta, x_n + \delta)|}{2 \delta} = 1. \]
	%
	We will now assume that $\{ x_n \}$ is dense in $\TT$, in order to argue that $|A| = 1$. We will justify this fact at the end of the argument. For any $\varepsilon > 0$, there exists $\delta_0$ such that for $\delta \leq \delta_0$, and any $n$,
	%
	\[  |A \cap (x_n - \delta, x_n + \delta)| \geq 2 \delta (1 - \varepsilon). \]
	%
	Since $\{ x_n \}$ is dense, we can cover $\TT$ by an almost disjoint family of intervals $I_1,\dots,I_N$, where $I_k$ has radius less than or equal to $\delta_0$, and center $x_{n_k}$ for some $n_k$. The inequality above states that for each $k$,
	%
	\[ |A \cap I_k| \geq (1 - \varepsilon) |I_k|. \]
	%
	But then it follows that
	%
	\begin{align*}
		|A| = |A \cap I_1| + \dots + |A \cap I_N| \geq (1 - \varepsilon) (|I_1| + \dots + |I_N|) = (1 - \varepsilon) |\TT| = 1 - \varepsilon.
	\end{align*}
	%
	Taking $\varepsilon \to 1$ completes the argument.

	Now lets justify when $\{ x_n \} = \{ x_0 + n \alpha \}$ is dense in $\TT$. By translation invariance, it suffices to show that $\{ n \alpha \}$ is dense in $\TT$. There are several ways to accomplish this. One easy (but lazy) method is to apply the Weyl equidistribution theorem, which gives this result automatically (and much more). A more basic method might be to apply the pigeonhole principle, which we now detail. For any $N$, we can divide $[0,1]$ up into $N$ intervals of length $1/N$. The pigeonhole principle implies that there exists $n_1 < n_2 \leq N + 1$ such that $n_1 \alpha$ and $n_2 \alpha$ both lie in the same interval, and thus in particular, $|(n_2 - n_1) \alpha| = |n_2 \alpha - n_1 \alpha| \leq 1/N$. Since $\alpha$ is irrational, $n_1 \alpha \neq n_2 \alpha$, so if $k = n_2 - n_1$, then we have
	%
	\[ 0 < |k \alpha| \leq 1/N. \]
	%
	Let $\delta = |k \alpha|$. Then the numbers $\{ m k\alpha : 1 \leq m \leq \lfloor 1/\delta \rfloor \}$ form an arithmetic progression in $(0,1)$, identified as a subset of $\TT$, spaced out at a distance smaller than $1/N$, with the first element $k \alpha$ lying at a distance beginning at a distance $t$ from $0$, and ending a distance at most $t$ from $1$. It follows that any point in $\TT$ lies a distance at most $t \leq 1/N$ from these values. Thus $N$ was arbitrary, we conclude the sequence is dense.
\end{solution}

\question Let $\{ f_n \}$ be a sequence of measurable, real-valued functions on a measure space $X$ such that $f_n \to f$ pointwise as $n \to \infty$, where $f: X \to \RR$, and suppose that for some constant $M > 0$,
%
\[ \int |f_n| d\mu \leq M\ \text{for all $n \in \mathbb{N}$}. \]
\begin{parts}
	\part Prove that
	%
	\[ \int |f|\; d\mu \leq M. \]
	\begin{solution}
		The result follows immediately from Fatou's lemma, i.e. that
		%
		\[ \int |f|\; d\mu = \int \liminf_n |f_n|\; d\mu \leq \liminf_n \int |f_n|\; d\mu \leq M. \]
	\end{solution}

	\part Give an example to show that we may have $\int |f_n|\; d\mu = M$ for every $n$, but $\int |f|\; d\mu < M$.
	\begin{solution}
		Let $X = \RR$, and let $f_n(x) = M \mathbf{I}(x \in [n,n+1])$. Then $f = 0$.
	\end{solution}

	\part Prove that
	%
	\[ \lim_{n \to \infty} \int ||f_n| - |f| - |f_n - f|| = 0. \]
	\begin{solution}
		The integrand converges pointwise to zero. Moreover, we have
		%
		\[ ||f_n| - |f| - |f_n - f|| = |f_n - f| + |f| - |f_n| \]
		%
		by virtue of the fact that
		%
		\[ |f_n - f| \geq |f_n| - |f|. \]
		%
		Now the triangle inequality gives that
		%
		\[ |f_n - f| + |f| - |f_n| \leq |f_n| + |f| + |f| - |f_n| \leq 2 |f|. \]
		%
		Thus the integrands are uniformly domainted by an \emph{integrable function}. The domainted convergence thus immediately gives the result.
	\end{solution}
\end{parts}

\question Consider the following equation for an unknown function $f: [0,1] \to \RR$:
%
\[ f(x) = g(x) + \lambda \int_0^1 (x - y)^2 f(y)\; dy + \frac{1}{2} \sin(f(x)). \]
%
Prove that there exists a number $\lambda_0 > 0$ such that for all $\lambda \in [0,\lambda_0)$ and all continuous functions $g$ on $[0,1]$, the equation has a unique continuous solution.
\begin{solution}
	Define an integral operator $T_g$ by setting
	%
	\[ T_g f(x) = g(x) + \frac{\sin(f(x))}{2} + \lambda \int_0^1 (x - y)^2 f(y)\; dy. \]
	%
	The problem is to find, and prove the uniqueness, of a fixed point for $T_g$. We calculate that
	%
	\begin{align*}
		\| T_g f_1 - T_g f_2 \|_{L^\infty[0,1]} &\leq \left\| \frac{\sin \circ f_1 - \sin \circ f_2}{2} \right\|_{L^\infty[0,1]} + \lambda \| f_1 - f_2 \|_{L^\infty[0,1]} \sup_{x \in [0,1]} \int_0^1 (x - y)^2\; dy\\
		&\leq (1/2) \| f_1 - f_2 \|_{L^\infty[0,1]} + \lambda \| f_1 - f_2 \|_{L^\infty[0,1]} \left( \sup_{x \in [0,1]} x^2 - x + 1/3 \right)\\
		&\leq (1/2 + \lambda / 3) \| f_1 - f_2 \|_{L^\infty[0,1]}.
	\end{align*}
	%
	If $\lambda < 1/6$, we conclude that $T_g$ is a contraction map, and thus has a unique fixed point.
\end{solution}

\question Given $\alpha \geq 0$, the $\alpha$-dimensional Hausdorff measure of a set $X \subset \RR^n$ is
%
\[ H^\alpha(X) = \liminf_{r \to 0} \left\{ \sum_{i = 1}^\infty r_i^\alpha : X \subset \bigcup_{i = 1}^\infty B(x_i,r_i), r_i < r\ \text{for all $i$} \right\} \]
%
(where $B(x,r)$ is the Euclidean ball with center $x$ and radius $r$) and the Hausdorff dimension is
%
\[ \dim_{\mathbf{H}}(X) = \inf \{ \alpha \geq 0: H^\alpha(X) = 0. \} \]
%
Prove:
%
\begin{parts}
	\part If $X \subset \RR^n$ and $\mu$ is a finite Borel measure on $X$ such that $\mu(X) > 0$ and $\mu(B(x,r)) \leq r^\alpha$ for all open balls $B(x,r)$, then $\dim_{\mathbf{H}}(X) \geq \alpha$.
	\begin{solution}
		Suppose $X$ is covered by a family of balls $\{ B(x_i,r_i) \}$. Then
		%
		\[ 1 \lesssim \mu(X) = \mu( \bigcup B(x_i,r_i) ) \leq \sum_i \mu ( B(x_i,r_i) ) \leq \sum_i r_i^\alpha. \]
		%
		Thus
		%
		\[ H^\alpha(X) = \liminf \sum_i r_i^\alpha \gtrsim 1, \]
		%
		which implies that $\dim_{\mathbf{H}}(X) \geq \alpha$.
	\end{solution}

	\part If $\mathbf{S}^1 = \{ (x,y) \in \RR^2: x^2 + y^2 = 1 \}$, then $\dim_{\mathbf{H}}(\mathbf{S}^1) = 1$.
	\begin{solution}
		For any $r$, we can cover $\mathbf{S}^1$ by $O(1/r)$ balls of radius $r$. Thus for $\alpha > 1$,
		%
		\[ H^\alpha(X) = \liminf_{r \to 0} O(1/r) r^\alpha = \liminf_{r \to 0} r^{\alpha - 1} = 0. \]
		%
		Thus $\dim_{\mathbf{H}}(\mathbf{S}^1) \leq 1$. We use the first part to prove that $\dim_{\mathbf{H}}(\mathbf{S}^1) \geq 1$, which will complete the proof. Namely, we let $\mu$ denote the angle measure on $\mathbf{S}^1$ divided by $1000$. Then it is easy to prove that $\mu(B(x,r)) \lesssim r$ for all balls $B$ - since the angle measure has mass $2 \pi / 1000$ we may assume $r \leq 1/10$, and then the intersection of $B(x,r)$ with $\mathbf{S}^1$ is an arc with angular length $O(r)$. Thus the above part of the problem gives the lower bound.
	\end{solution}
\end{parts}

\question Let $X = [0,1]$ with Lebesgue measure and $Y = [0,1]$ with counting measure. Give an example of an integrable function $f: X \times Y \to [0,\infty)$ for which Fubini's theorem does not apply.
\begin{solution}
	Let $f(x,y) = 1$ if $x = y$, and $f(x,y) = 0$ otherwise. Then
	%
	\[ \int_Y \int_X f(x,y)\; dx\; dy = \int_Y 0\; dy = 0, \]
	%
	whereas
	%
	\[ \int_X \int_Y f(x,y)\; dy\; dx = \int_X 1\; dx = 1. \]
\end{solution}


\question For $s > 1/2$ let $H^s(\RR^n)$ denote the Sobolev space
%
\[ H^s(\RR^n) = \{ f \in L^2(\RR^n): \int_{\RR^n} (1 + |\xi|^2)^s |\widehat{f}(\xi)|^2\; d\xi < \infty \}. \]
%
Use the Fourier transform to prove that if $u \in H^s(\RR^n)$ for $s > n/2$ then $u \in L^\infty(\RR^n)$ with the boundary
%
\[ \| u \|_{L^\infty(\RR^n)} \leq C \| u \|_{H^s(\RR^n)}. \]
%
for a constant $C$ depending only on $s$ and $n$.
\begin{solution}
	The Fourier inversion formula tells us that
	%
	\[ \| u \|_{L^\infty(\RR^n)} \leq \| \widehat{u} \|_{L^1(\RR^n)}. \]
	%
	But for $s > n/2$, we have
	%
	\[ \| \widehat{u} \|_{L^1(\RR^n)} \lesssim_s \| u \|_{H^s(\RR^n)}, \]
	%
	by virtue of Cauchy Schwarz, i.e. that
	%
	\[ \int |\widehat{u}(\xi)|\; d\xi = \int \left[ |\widehat{u}(\xi)| (1 + |\xi|^2)^{s/2} \right] \left[ (1 + |\xi^2)^{-s/2} \right] \leq \left( \int |\widehat{u}(\xi)|^2 (1 + |\xi|^2)^s \right)^{1/2} \left( \int (1 + |\xi|^2)^{-s} \right)^{1/2}. \]
	%
	The first integral is precisely the $H^s$ norm, and for large $\xi$, the right integrand is bounded, and essentially equal to $|\xi|^{-2s}$ for $|\xi| \geq 1$, which proves the second integral is finite, completing the proof.
\end{solution}


\question Assume that $X$ is a compact metric space and $T: X \to X$ is a continuous map. Let $M_1(T)$ denote the set of Borel probability measures on $X$ such that $T_* \mu = \mu$. Prove
%
\begin{parts}
	\part $M_1(T) \neq \emptyset$.
	\begin{solution}
		Fix any probability measure $\nu$. Then $T_*^k \nu$ is a probability measure for all $k \geq 0$. Define a probability measure $\mu_n$ such that
		%
		\[ \mu_n = \frac{1}{n} \sum_{k = 0}^{n-1} T_*^k \nu \]
		%
		Applying weak $*$ compactness, there exists a subsequence of these measures that converges in the weak $*$ topology to some probability measure $\mu$, i.e. $\mu = \lim_m \mu_{n_m}$. Then by continuity of the pushforward,
		%
		\[ T_* \mu = \lim_{m \to \infty} \frac{1}{n_m} \sum_{k = 0}^{n_m - 1} T_* T_*^k \nu = \lim_{m \to \infty} \frac{1}{n_m} \sum_{k = 0}^{n_m - 1} T_*^{k+1} \nu = \lim_{m \to \infty} [(n_m) / (n_m + 1)] (\mu_{n_m+1} - \nu). \]
		%
		The limit point of this 

		Let $\{ B_i \}$ be a countable basis of balls for $X$. It suffices to show that for any finite, disjoint collection of balls $\{ B_1, \dots, B_n \}$, we can find a Borel probability measure $\mu_n$ such that $\mu(T^{-1}(B_i)) = \mu(B_i)$, for then if we take a limit of maximal collections of balls whose radius tends to zero, and consider a subsequence of probability measures which converge weakly to some probability measure $\mu$, then it follows that $T_* \mu = \mu$. It does not take too much work to find a $\mu_n$ which works.


		We can break down the sets $\{ B_1,\dots,B_n, T^{-1}B_1,\dots, T^{-1}B_n \}$ into finitely many atomic parts $\{ A_1,\dots,A_m \}$, i.e. such that each set is a union of these parts, and such that the intersection of any subcollection of the sets is a finite union of these sets. The problem then just becomes a linear programming problem, i.e. each of the equations $\mu(T^{-1}(B_i)) = \mu(B_i)$ breaks down into an equation $\sum_{j \in \mathcal{S}_i'} c_j = \sum_{j \in \mathcal{S}_i} c_j$. where $c_i = 0$, subject to the contraint that $c_i \geq 0$.
	\end{solution}

	\part If $M_1(T) = \{ \mu \}$ consits of a single measure $\mu$, then
	%
	\[ \int_X f d \mu = \lim_{N \to \infty} \frac{1}{N} \sum_{n = 0}^{N-1} f \circ T^n(x) \]
	%
	for every continuous function $f: X \to \RR$ and $x \in X$.
	\begin{solution}
		The result above shows that every subsequence of the measures
		%
		\[ \mu_n = \frac{1}{N} \sum_{n = 0}^{N-1} f \circ T^n \]
		%
		has a further subsequence converging in the weak $*$ topology to the invariant measure $\mu$. But this is only possible if the sequence itself converges to $\mu$.
	\end{solution}
\end{parts}

\question Find the Fourier transform of the following function $f$ in $\RR^2$:
%
\[ f(x) = e^{i x \xi_0} |x - x_0|^{-1}. \]
\begin{solution}
	First, we note that if $g(x) = |x - x_0|^{-1}$, then
	%
	\[ \widehat{f}(\xi) = \widehat{g}(\xi - \xi_0 / 2 \pi). \]
	%
	But if $h(x) = |x|^{-1}$, then
	%
	\[ \widehat{g}(\xi) = \widehat{h}(\xi) e^{-2 \pi i \xi \cdot x_0}. \]
	%
	Thus it suffices to find the Fourier transform of $h$. But $h$ is radial, $C^\infty$ away from the origin, and homogeneous of degree $-1$. It follows that the Fourier transform of $h$ is radial, $C^\infty$ away from the origin, and homogeneous of degree $-1$. Since there are no such homogeneous functions supported at the origin, we conclude that there must exist a quantity $C$ such that
	%
	\[ \widehat{h}(\xi) = C |\xi|^{-1}. \]
	%
	To calculate $C$, it suffices to test $h$ against a Gaussian. Namely, if $\Phi(x) = e^{- \pi |x|^2}$, then $\Phi$ is it's own Fourier transform, and so the multiplication formula shows that
	%
	\[ C \int |\xi|^{-1} \Phi(\xi) = \int \widehat{h}(\xi) \Phi(\xi) = \int h(x) \Phi(x) = \int |x|^{-1} \Phi(x). \]
	%
	Thus we conclude that $C = 1$, i.e. that $h$ is it's own Fourier transform. Thus
	%
	\begin{align*}
		\widehat{f}(\xi) &= \widehat{g}(\xi - \xi_0 / 2 \pi)\\
		&= \widehat{h}(\xi - \xi_0 / 2\pi) e^{-2 \pi i \xi \cdot x_0} e^{i \xi_0 \cdot x}\\
		&= \frac{e^{-2 \pi i \xi \cdot x_0} e^{i \xi_0 \cdot x}}{|\xi - \xi_0 / 2\pi|}.
	\end{align*}
\end{solution}







\newpage
\section{Interchanging Limits Notes}

It is common that one wishes to the question of when one can justify the interchange of integral and limit, i.e. as in the case where $f_n \to f$ and one wishes to show of 
\begin{equation*}
  \lim_{n\to\infty} \int_E f_{n} = \int_E \lim_{n\to\infty}  f_{n}.
\end{equation*}
often arises. There are several tools for justifying such an \textit{interchange of limits}:

\begin{itemize}
  \item Monotone Convergence Theorem
  \item Uniform Convergence
  \item Lebesgue Dominated Convergence Theorem
  \item General Lebesgue Dominated Convergence Theorem
  \item Vitali Convergence Theorem
\end{itemize}

Of these theorems, the Vitali Convergence Theorem and Uniform convergence
require the domain of integration $E$ to be a set of finite measure. However, in that case, the Vitali
Convergence Theorem, which requires the $f_{n}$'s to be \textit{uniformly integrable}, provides a somewhat more general condition than the existence of a dominating function (and is often useful when a dominating function is difficult to find).
One condition which implies uniform integrability is 
\begin{equation*}
  \|f_{n}\|_{p}\leq C <\infty
\end{equation*}
for some $p>1$ and all $n$, (though if $p=1$ this condition is not sufficient). The proof of the Vitali Convergence theorem is essentially an application of Egorov's theorem (and was a qual question in Fall 2010).

If one needs only prove that a limit function $f = \lim_{n\to\infty} f_n$ is integrable, the above tools are sometimes overkill and one may be able to simply apply Fatou's lemma:
\begin{equation*}
  \int_{E} |f| \leq  \liminf_{n\to\infty}\int_{E} |f_{n}|
\end{equation*}
provided that one knows the right-hand side is finite.






\newpage
\section{Interchanging Limits Questions}

\question (Rice Qualifying Exam, Winter 2011) Let $\{ f_n \}$ be a sequence of Lebesgue measurable functions defined on $[0,1]$ such that $|f_n(x)| \leq 1$ for all $n \geq 1$ and all $0 < x \leq 1$, and
\[ \lim_{n \to \infty} f_n(x) = f(x) \]
%
exists for each $0 \leq x \leq 1$. Prove that
%
\[ \lim_{n \to \infty} \int_0^1 \frac{f_n(x)}{\sqrt{|x-1/n|}} = \int_0^1 \frac{f(x)}{\sqrt{x}}\; dx \]
\begin{solution}
    Let $g_n(x) = f_n(x) / \sqrt{|x - 1/n|}$. We wish to apply the Vitali convergence theorem. To do this, let us first show that the sequence $\{ g_n \}$ is uniformly integrable. Now for a fixed $M > 0$ and $n > 0$, since $|f_n(x)| \leq 1$, if $|g_n(x)| \geq M$, then $|x- 1/n| \leq 1/M^2$, and so
    %
    \[ \int_{|g_n(x)| \geq M} |g_n(x)| \leq \int_{|x - 1/n| \leq 1/M^2} \frac{1}{|x - 1/n|^{1/2}} \leq \int_{|x| \leq 1/M^2} \frac{1}{|x|^{1/2}} \lesssim M^{-1}. \]
    %
    Thus
    %
    \[ \lim_{M \to \infty} \sup_{|g_n(x)| \geq M} |g_n(x)| = 0. \]
    %
    To show $g_n$ converges to $g(x) = f(x) / \sqrt{x}$, it thus suffices to show that $g_n$ converges to $g$ in measure. Now $g_n$ clearly converges to $g$ except when $x = 0$, and thus $g_n$ converges to $g$ almost everywhere. But this implies $g_n$ converges to $g$ in measure (since we are working over a finite measure space), and thus $g_n$ converges to $g$ in $L^1[0,1]$.
\end{solution}

\question (Rice, Spring 2005) Compute
%
\begin{parts}
    \part $\lim_{n \to \infty} \int_0^\infty \frac{x^{n-2}}{1 + x^n}$.
    \begin{solution}
        We apply the dominated convergence theorem. For $0 \leq x \leq 1$, $x^{n-2} / 1 + x^n \leq 1$, and for $1 \leq x \leq \infty$, $x^{n-2} / (1 + x^n) \leq x^{n-2} / x^n \leq 1/x^2$. Thus the family of integrands $x^{n-2} / (1 + x^n)$ is domainated by an integrable function, namely the function
        %
        \[ \mathbf{I}(0 \leq x \leq 1) + \mathbf{I}(1 \leq x) \cdot 1/x^2. \]
        %
        The sequence converges pointwise to zero for $0 \leq x < 1$, and for $x > 1$,
        %
        \[ \lim_{n \to \infty} x^{n-2} / (1 + x^n) = \lim_{n \to \infty} 1 / (x^2 + 1/x^{n-2}) = 1/x^2. \]
        %
        Thus the domainated convergence theorem implies that
        %
        \[ \lim_{n \to \infty} \frac{x^{n-2}}{1 + x^n} = \int_1^\infty \frac{1}{x^2} = 1. \]
    \end{solution}
    
    \part $\lim_{n \to \infty} n \int_0^\infty \frac{\sin y}{y(1 + n^2 y)}\; dy$ (Hint: Substitute $x = ny$).
    \begin{solution}
        We have
        %
        \[ n \int_0^\infty \frac{\sin y}{y(1 + n^2 y)}\; dy = n \int_0^\infty \frac{\sin(y/n)}{y (1 + ny)}\; dy. \]
        %
        For $y \geq 1$,
        %
        \[ n \frac{\sin(y/n)}{y ( 1 + ny) } \leq n \frac{1}{y(ny)} \leq 1/y^2. \]
        %
        For $y \leq 1$, we employ the bound $\sin(y/n) \leq y/n$ to conclude that
        %
        \[ n \frac{\sin(y/n)}{y ( 1 + ny) } \leq \frac{1}{1 + ny} \leq 1. \]
        %
        These bounds imply that the family of integrands is uniformly integrable. Now for all $y \geq 0$,
        %
        \[ \lim_{n \to \infty} n \frac{\sin(y/n)}{y(1 + ny)} = \lim_{n \to \infty} \frac{1}{1 + ny} + O \left( n \frac{(y/n)^2}{y(1 + ny)} \right) = \lim_{n \to \infty} \frac{1}{1 + ny} + O \left( \frac{y}{n(1 + ny)} \right) = 0. \]
        %
        Thus the dominated convergence theorem implies that
        %
        \[ \lim_{n \to \infty} n \int_0^\infty \frac{\sin y}{y(1 + n^2 y)}\; dy = 0. \]
    \end{solution}
\end{parts}

\question (Spring 2017)
    Let $f: [0,\infty) \to \RR$ be a continuously differentiable function for which $\| f' \|_\infty < \infty$. Define, for $x > 0$,
    %
    \[ F(x) = \int_0^\infty f(x + yx) \psi(y)\; dy, \]
    %
    where $\psi$ satisfies
    %
    \[ \int_0^\infty |\psi(y)|\; dy \quad\text{and}\quad \int_0^\infty y \cdot |\psi(y)|\; dy < \infty. \]
    %
    Show that $F(x)$ is well defined for all $x \geq 0$, and that $F$ is continuously differentiable.
\begin{solution}
    The fundamental theorem of calculus implies that
    %
    \[ |f(x + yx)| = \left| f(x) + \int_x^{x+yx} f'(t)\; dt \right| = f(x) + O(yx). \]
    %
    Thus combined with the fact that $\int_0^\infty |\psi(y)|\; dy < \infty$ and $\int_0^\infty y \cdot |\psi(y)|\; dy < \infty$, this implies that
    %
    \[ \int_0^\infty |f(x + yx)| |\psi(y)|\; dy \lesssim |f(x)| + |x| < \infty, \]
    %
    so $F$ is well defined.
    
    To show that $F$ is continuously differentiable, we note that for a fixed $x \in [0,\infty)$, we calculate that, for $h$ with $0 \leq x + h$,
    %
    \begin{align*}
        \frac{F(x+h) - F(x)}{h} &= \int_0^\infty \frac{|f(x+yx + h(1 + y)) - f(x + yx)|}{h} \psi(y)\; dy.
    \end{align*}
    %
    Now
    %
    \[ \left| \frac{|f(x+yx + h(1 + y)) - f(x + yx)|}{h} \psi(y) \right| \lesssim (1 + y) |\psi(y)|. \]
    %
    Since $(1 + y) \psi(y)$ is integrable, the dominated convergence theorem implies that $F$ is differentiable at $x$, and
    %
    \[ F'(x) = \int_0^\infty f'(x + yx) (1 + y) \psi(y)\; dy. \]
    %
    Finally, we show $F'$ is continuous. We note that for any $\varepsilon > 0$, there exists $R > 0$ such that
    %
    \[ \int_R^\infty (1 + y) |\psi(y)|\; dy \leq \varepsilon. \]
    %
    Since $f'$ is continuous, it is uniformly continuous on $[0,2R(1 + x)]$. Thus there exists $\delta > 0$ such that for $|h| \leq \delta$, and $0 \leq y \leq R$, $|f'((x + yx) + h(1 + y)) - f(x + yx)| \leq \varepsilon$, and so
    %
    \begin{align*}
        F'(x+h) - F(x) &\lesssim \varepsilon + \int_0^R [f'((x + yx) + h(1 + y)) - f'(x + yx)] (1 + y) \psi(y)\; dy\\
        &\lesssim \varepsilon + \int_0^R \varepsilon (1 + y) \psi(y)\; dy \lesssim \varepsilon.
    \end{align*}
\end{solution}




\newpage
\section{Arzela-Ascoli Notes}

The key part of the Arzela-Ascoli theorem to know for the qual is the following: 

\begin{quote}
If $\{f_n\}\subset C[0,1]$ is a sequence which is uniformly bounded and equicontinuous, then $\{f_n\}$ has a uniformly convergent subsequence.
\end{quote} 

(Note that we can replace $[0,1]$ by any compact subset of $\R^d$. Also, there is a converse to the theorem, but I haven't seen it used in any qual problems. For a more general statement and discussion of this thorem, see the appendix to Rudin's \textit{Functional Analysis}.)
\begin{itemize}
\item By \textit{uniformly bounded}, we mean that $|f_n(x)|\leq C$ for all $x\in [0,1]$, $n\in \mathbb{N}$.

\item By \textit{equicontinuous}, we mean that for all $\epsilon>0$, there exists $\delta$ such that  $|f_n(x)-f_n(y)|<\epsilon$ whenever $|x-y|<\delta$ for all $n\in \mathbb{N}$.

\end{itemize}

A useful condition for demonstrating equicontinuity of a collection of functions is having some sort of bound on their derivatives (e.g. as in several of the problems below).


Recall that a subset $K$ of a metric space $X$ is \textit{sequentially compact} if every sequence in $K$ has a convergent subsequence whose limit belongs to $K$. For subsets of metric spaces, sequential compactness is equivalent to compactness. Similarly, $K$ is \textit{precompact} if and only if every sequence in $K$ has a convergent subsequence (but whose limit need not belong to $K$).


Let $X,Y$ be normed linear spaces. A linear operator $A:X\to Y$ is said to be \textit{compact} if it maps bounded sets to precompact sets.

When showing that a linear operator is compact, the following condition is often useful: 

A linear operator $A:X\to Y$ is compact if $(Ax_n)_{n=1}^{\infty}$ has a cauchy subsequence whenever $(x_n)_{n=1}^{\infty}$ is bounded in $X$.


\newpage
\section{Arzela-Ascoli Questions}

\question (From a UBC Math 321 Midterm) Let $\{ f_n \}$ be a sequence of functions in $C[a,b]$ with no uniformly convergent subsequence. Define
%
\[ F_n(x) = \int_a^x \sin(f_n(t))\; dt. \]
%
Does $\{ F_n \}$ has a uniformly convergent subsequence.
\begin{solution}
    The sequence $\{ F_n \}$ does have a uniformly convergent subsequence, by Arzela-Ascoli. Indeed, it is simple to see that $\| F_n \|_\infty \leq b - a$ for all $n > 0$, so the sequence $\{ F_n \}$ is uniformly bounded. The sequence is also equicontinuous, because
    \[ |F_n(x) - F_n(y)| \leq |x - y| \]
    %
    uniformly in $n$. Thus Arzela-Ascoli implies the existence of a uniformly convergent subsequence.
\end{solution}


\question (From a UBC Math 321 Midterm) Let $\{ f_n \}$ be a sequence of functions in $C[a,b]$ with no uniformly convergent subsequence. Define
%
\[ F_n(x) = \int_a^x \sin(f_n(t))\; dt. \]
%
Does $\{ F_n \}$ has a uniformly convergent subsequence.
\begin{solution}
    The sequence $\{ F_n \}$ does have a uniformly convergent subsequence, by Arzela-Ascoli. Indeed, it is simple to see that $\| F_n \|_\infty \leq b - a$ for all $n > 0$, so the sequence $\{ F_n \}$ is uniformly bounded. The sequence is also equicontinuous, because
    \[ |F_n(x) - F_n(y)| \leq |x - y| \]
    %
    uniformly in $n$. Thus Arzela-Ascoli implies the existence of a uniformly convergent subsequence.
\end{solution}

\question (Fall 2004) Let $f_{n}:[0,1]\to \R$ be a sequence of continuous functions whose derivatives $f'_{n}$ in the sense of distributions belong to $L^{2}(0,1)$. The functions also satisfy $f_{n}(0)=0$.
\begin{parts}
  \part
Assume that
\begin{equation*}
  \lim_{n\to\infty} \int_{0}^{1}f'_{n}(x)g(x)dx
\end{equation*}
exists for all $g\in L^{2}(0,1)$. Show that the $f_{n}$ converge uniformly as $n\to\infty$.
\begin{solution}
  Define $T_n(g) = \int_0^1 f_n'gdx$ for $g\in L^2(0,1)$. Then $T_n$ is clearly linear and by Cauchy-Schwarz inquality, $T_n$ is bounded. Then by the part (a) assumption, we have
  \begin{equation*}
  \sup \{ T_n(g) : n\geq 1 \} <\infty.
  \end{equation*}
  Therefore by the Uniform Boundedness Principle, 
  \begin{equation*}
  C:= \sup_n \norm{f_n'}_{L^2} <\infty
  \end{equation*}
  Uniform boundedness principle to obtain a uniform bound on $f_n'$. Then apply Arzela ascoli to a subsequence to obtain convergence of whole sequence. [Note the uniform boundedness of the derivatives suggests that we have uniform equicontinuity -- and hence maybe we can use the Arzela-Ascoli Theorem.] In particular, since $f'\in L^2[0,1]$, it follows that $f'\in L^1[0,1]$. Therefore
  \begin{equation}\label{newton-leibniz}
  f_n(x)-f_n(y) = \int_y^x f'_n(t)dt
  \end{equation}
  Taking $y=0$ and using $f_n(0)=0$, we have 
  \begin{equation*}
  |f_n(x)|\leq \int_0^x |f_n'(t)|dt \leq \int_0^1 |f_n'(t)|dt \leq \(\int_0^1 |f_n'(t)|^2dt\)^{\frac{1}{2}} \leq C.
  \end{equation*}
  Therefore $\{f_n\}_n$ is uniformly bounded. In addition, by \eqref{newton-leibniz} and Cauchy-Schwarz inequality, we have 
  \begin{equation*}
  |f_n(x)-f_n(y)| \leq \left| \int_y^x f_n'(t)dt\right| \leq \sqrt{x-y} \(\int_y^x |f_n'|^2dt \)^{\frac{1}{2}} \leq C \sqrt{x-y}.
  \end{equation*}
  Therefore $\{f_n\}$ is uniformly equi-continuous. Therefore by the Arzela-Ascoli theorem, $\{f_n\}_{n\in \mathbb{N}}$ is a relatively compact subset of $C([0,1])$. 
  
  Recall that by assumption $\lim_{n\to\infty} \int_{0}^{1}f'_{n}(x)g(x)dx$ exists for any $g\in L^2[0,1]$. Taking $g(x) = \chi_{[0,a]}$, we have $\lim_{n\to\infty} \int_{0}^{a}f'_{n}(x)dx=\limn f'_n(a)$ exists. That is, there exists some real-valued function $f$ such that $f_n\to f$ pointwise.  
  
  Suppose that $f_n$ does not converge uniformly to $f$. Then there exists a subsequence $n_k$ and $\epsilon_0>0$ such that 
  \begin{equation*}
  \sup_{x\in[0,1]} |f(x)-f_{n_k}(x)|\geq \epsilon_0
  \end{equation*}
  for all $k$. However, since $\{f_n\}_n$ is relatively compact, there exists a subsequence $f_{n_{k_j}}$ which is uniformly Cauchy--and therefore must converge uniformly to $f$. But this implies that
  \begin{equation*}
  \sup_{x\in[0,1]} |f(x)-f_{n_{k_j}}(x)|\to 0 \quad \text{as }j\to\infty
  \end{equation*}
  This is a contradiction.
\end{solution}

\part Assume that
\begin{equation*}
  \lim_{n\to\infty} \int_{0}^{1}f'_{n}(x)g(x)dx
\end{equation*}
exists for all $g\in C([0,1])$. Do we still have the $f_{n}$ converge uniformly?
\begin{solution}
  Maybe yes by density?
\end{solution}
\end{parts}

\question (Spring 2014) Consider the following operator
%
\[ Af(x) = \frac{1}{x \sqrt{1+|\log x|}} \int_{0}^{x}f(t)dt. \]
%
Is $A$ bounded as an operator from $L^{2}[0,1]$ to $L^{2}[0,1]$? Is it compact?
\begin{solution}
We first prove the following lemma: 

  \textit{Lemma 1 (Hardy's Inequality):} If $F(x)= \int_{0}^{x}f(t)dt$ then
  \begin{equation*}
    \int_{0}^{1}\left( \frac{F(x)}{x} \right)^{2}dx \leq 4 \int_{0}^{1}f(x)^2dx
  \end{equation*}
  \begin{proof}[Proof of Lemma:] 
    \begin{align*}
      \int_{0}^{1}\frac{F(x)^{2}}{x^{2}}dx
      &= - \left.\frac{F(x)^{2}}{x}\right|_{0}^{1} - \int_{0}^{1}\left( \frac{-1}{x} \right)2F(x)f(x)dx &&\text{Integration by parts}\\
      &\leq 2 \int_{0}^{1}\frac{F(x)}{x}f(x)dx\\
      &\leq 2 \left(  \int_{0}^{1}\frac{F(x)^{2}}{x^{2}}dx \right)^{\frac{1}{2}} \left( \int_{0}^{1}f(x)^{2}dx \right)^{\frac{1}{2}}&&\text{Cauchy-Schwarz}
    \end{align*}
    Dividing by $ \left(  \int_{0}^{1}\frac{F(x)^{2}}{x^{2}}dx \right)^{\frac{1}{2}}$ and squaring both sides gives the result.
  \end{proof}
  
  We will use Hardy's inequality to prove that $A$ is bounded. Letting $F(x) = \int_{0}^{x}f(t)dt$, we have
  \begin{align*}
    \norm{Af}_{2}^{2} = \int_{0}^{1}\frac{|F(x)|^{2}}{|x|^{2}} \frac{1}{1+|\log x|}dx \leq \int_{0}^{1}\frac{|F(x)|^{2}}{|x|^{2}}dx \leq 4 \norm{f}^{2}_{2}
  \end{align*}
  where the last inequality is due to Hardy's inequality. Since $A$ is clearly linear, it follows that $A$ is a bounded linear operator.
  
  Before we move onto the proof that $A$ is compact, here is an alternate proof of boundedness which does not use Hardy's inequality. We wish to show that $A:L^{2}[0,1]\to L^{2}[0,1]$ is
bounded. Let $f_{n}\in L^{2}[0,1]$ such that $f_{n}\to 0$ in $L^{2}[0,1],$ and
assume that $Af_{n}\to g$ in $L^{2}[0,1].$ By the closed graph theorem, it will
suffice to show that $g=0$ in $L^{2}[0,1].$  


Since $f_{n}\to 0$ in $L^{2}[0,1]$, there exists a subsequence $f_{n'}$ such
that $f_{n'}\to 0$ a.s. Therefore, by Fatou's Lemma,
\begin{align*}
  \int_{0}^{1}|g(x)|dx 
  &\leq \liminf_{n'\to\infty}\int_{0}^{1}|Af_{n'}(x)|dx \\
\end{align*}
(note that the integrals on both sides are well-defined since
$L^{2}[0,1]\subseteq L^{1}[0,1]$).
By the above equation, it will suffice to show that
\begin{equation*}
  \int_{0}^{1}|Af_{n'}(x)|dx \to 0
  \quad \text{as} \quad n'\to \infty
\end{equation*}
as it will then follow that $g=0$ a.e. Indeed, using the Fubini-Tonelli Theorem,
\begin{align*}
  \int_{0}^{1}|Af_{n'}(x)|dx
  &\leq \int_{0}^{1}\frac{1}{x \sqrt{1+|\log x|}} \int_{0}^{x}|f_{n'}(t)|dt dx\\ 
  &= \int_{0}^{1}|f_{n'}(t)| \int_{t}^{1}\frac{1}{x \sqrt{1+|\log x|}}  dx dt\\
  &\leq \int_{0}^{1}|f_{n'}(t)| \int_{t}^{1}\frac{1}{x}dxdt\\  
  &= -\int_{0}^{1}|f_{n'}(t)| \log t dt\\
  &\leq \left[ \int_{0}^{1}|f_{n'}(t)|^{2}dt  \right]^{\frac{1}{2}}\left[ \int_{0}^{1} |\log t|^{2} dt \right]^{\frac{1}{2}}
\end{align*}
where the last step is justified by the Cauchy-Schwarz inequality. Moreover,
using the substitution $u = -\log t$, we obtain
$\left[ \int_{0}^{1} |\log t|^{2} dt \right]^{\frac{1}{2}} =
\int_{0}^{\infty}u^{2}e^{-u}du=2$ by integration by parts. Therefore
\begin{equation*}
  \int_{0}^{1}|Af_{n'}(x)|dx \leq  2 \|f_{n'}\|_{L^{2}[0,1]} \to 0 \text{ as }n'\to \infty,
\end{equation*}
which completes the proof that $A$ is bounded.



  Next we show that $A$ is a compact operator. Let $\left\{ f_{n} \right\}_{n\geq 1}$ be a bounded sequence in $L^{2}[0,1]$. To show that $A$ is compact, it suffices to show that there exists a subsequence $\left\{ f_{n_{k}} \right\}_{k\geq 1}$ such that $\left\{ Af_{n_{k}} \right\}_{k\geq 1}$ converges in $L^{2}[0,1]$.

  
  \vt
  \textit{Claim 2.} Let $F_{n}(x):=\int_{0}^{x}f_{n}(t)dt$. Then $\{F_{n}\}_{n=1}^{\infty}$ is a relatively compact subset of $C[0,1]$ (with respect to the topology induced by the uniform norm).

  To prove claim 2, we apply the Arzela-Ascoli Theorem. First, since $\{f_{n}\}_{n}$ is bounded in $L^{2}[0,1]$, there exists some $C>0$ such that $\norm{f_{n}}_{L^{2}}\leq C$ for all $n$. Then
  \begin{equation*}
    |F_{n}(x)| \leq \int_{0}^{x}|f_{n}(x)|dx \leq \(\int_{0}^{1}|f_{n}(x)|^{2}dx \)^{\frac{1}{2}} \leq C <\infty.
  \end{equation*}
  Therefore $\left\{ F_{n} \right\}$ is uniformly bounded. It remains to show that $\left\{ F_{n} \right\}$ is equicontinuous. By the Cauchy-Schwarz inequality,
  \begin{equation*}
    \left| F_{n}(x)-F_{n}(y) \right| = \left| \int_{y}^{x}f_{n}(t)dt \right| \leq \norm{f_{n}}_{L^{2}} \sqrt{|x-y|} \leq C \sqrt{|x-y|}.
  \end{equation*}
  Therefore  $\left\{ F_{n} \right\}$ is equicontinuous. Therefore by the Arzela-Ascoli theorem, $\{F_{n}\}_{n=1}^{\infty}$ is a relatively compact. This proves Claim 2.

  By Claim 2, there exists a subsequence $F_{n_{k}}$ such that $\{F_{n_{k}}\}_{k}$ is uniformly Cauchy. We claim that $\{Af_{n_{k}}\}_{k\geq 1}$ converges in $L^{2}$-norm, which will complete the proof. Indeed,
  \begin{align*}
    \norm{Af_{n_{j}}-Af_{n_{k}}}_{L^{2}}^{2}
    &\leq \int_{0}^{1}\frac{|F_{n_{j}}(x)-F_{n_{k}}(x)|^{2}}{|x|^{2}\left( 1+|\log x| \right)} dx\\
    &= \int_{0}^{\delta}\frac{|F_{n_{j}}(x)-F_{n_{k}}(x)|^{2}}{|x|^{2}\left( 1+|\log x| \right)} dx + \int_{\delta}^{1}\frac{|F_{n_{j}}(x)-F_{n_{k}}(x)|^{2}}{|x|^{2}\left( 1+|\log x| \right)} dx\\
    &\leq \frac{1}{1+|\log \delta |} \underbrace{\int_{0}^{\delta}\frac{|F_{n_{j}}(x)-F_{n_{k}}(x)|^{2}}{|x|^{2}} dx}_{\text{apply Hardy's inequality again}} + \norm{F_{n_{j}}-F_{n_{k}}}_{L^{\infty}} \cdot\frac{1}{\delta^{2}}\\
    &\leq \frac{1}{1+|\log \delta |} \norm{f_{n_{j}}-f_{n_{k}}}_{L^{2}} + \frac{1}{\delta^{2}} \norm{F_{n_{j}}-F_{n_{k}}}_{L^{\infty}}\\
    &\leq \frac{2C}{1+|\log \delta|} + \frac{1}{\delta^{2}} \norm{F_{n_{j}}-F_{n_{k}}}_{L^{\infty}}.
  \end{align*}
  Let $\epsilon>0$. Choose $\delta$ sufficiently small that $\frac{2C}{1+|\log \delta|}<\epsilon/2$, and choose $N\geq 0$ sufficiently large that $\norm{F_{n_{j}}-F_{n_{k}}}_{L^{\infty}}\leq \delta^{2}\epsilon/2$ whenever $j,k\geq N$. So $j,k\geq N$ implies $\norm{Af_{n_{j}}-Af_{n_{k}}}_{L^{2}}^{2} \leq \epsilon/2 + \epsilon/2= \epsilon$.
  
  We have shown that $\left\{ Af_{n_{k}} \right\}_{k}$ is Cauchy in $L^2[0,1]$, as required.
\end{solution}


\question (Problem 36 from the 2017 SEP) Consider the Hilbert space $L^{2}([0,1])$ with inner product $(f,g):=\int_{0}^{1}f(t)\bar{g}(t)dt$. Let $\left\{ e_{n} \right\}_{n=1}^{\infty}$ be an orthonormal system of functions in $L^{2}([0,1])$.
\begin{parts}
  \part
Suppose that $e'_{n}\in L^{2}([0,1])$ for all $n\in\mathbb{N}$. Show that
\begin{equation*}
  \sup_{n}\max_{x\in[0,1]}|e_{n}'(x)| = \infty.
\end{equation*}

\begin{solution}
  Suppose not. Then there exists some $C<\infty$ such that
  \begin{equation*}
  \sup_n \max_{x\in [0,1]} \left| e'_{n}(x) \right| \leq C.
\end{equation*}
Therefore $|e_{n}(x)-e_{n}(y)| \leq C|x-y|$. Hence $\{e_{n}\}$ is equicontinuous.

By Chebychev's inequality,
\begin{equation*}
  \left| \left\{ x\in [0,1]: |e_{n}(x)|>\alpha \right\} \right|\leq \frac{\int_{0}^{1}e_{n}(x)^{2}dx}{\alpha^{2}}= \frac{1}{\alpha^{2}}.
\end{equation*}
Taking $\alpha=2$, we find that $\left| \left\{ x\in [0,1]: |e_{n}(x)|\leq 2 \right\} \right|>\frac{1}{4}$. Since every set with positive measure has at least one element, there exists some $x_{n}\in [0,1]$ such that $|e_{n}(x)|\leq 2$. Therefore for any $x\in [0,1]$ and any $n\in \mathbb{N}$, we have
\begin{equation*}
  |e_{n}(x)| = \left| e_{n}(x_{n})+\int_{x_{n}}^{x}e'_{n}(t)dt \right|\leq 2 + \left| x_{n}-x \right| \leq 3.
\end{equation*}
Therefore $\left\{ e_{n} \right\}$ is uniformly bounded. Therefore by the Arzela-Ascoli theorem, there exists some subsequence $(e_{n_{k}})$ such that $e_{n_{k}}\to f$ uniformly in $C[0,1]$ as $k\to \infty$. Therefore $e_{n_{k}}\to f$ in $L^{2}[0,1]$. But this contradicts the orthonormality of the sequence $\left\{ e_{n} \right\}_{n=1}^{\infty}$. (In particular, by orthonormality we have for all $m\neq n$: 
\begin{equation*}
  \norm{e_{n}-e_{m}}_{L^{2}} ^{2}= \langle e_{n}-e_{m},e_{n}-e_{m}\rangle = \langle e_{n},e_{n} \rangle - \langle e_{m},e_{n} \rangle - \langle e_{n},e_{m} \rangle + \langle e_{m},e_{m} \rangle =1-0-0+1 =2
\end{equation*}
and this implies that $\{e_{n}\}$ can have no subsequence which is Cauchy in $L^{2}$.)
\end{solution}

\part Suppose that $e_{n}$ is complete, which means $(g,e_{n})=0$ for all $n$ implies $g=0$ almost everywhere. Prove
\begin{equation*}
  \sum_{n=1}^{\infty}|e_{n}(x)|^{2}=\infty, \quad\text{almost everywhere.}
\end{equation*}

\begin{solution}
We first prove the following claim: 

  \textit{Claim 1.} If $E\subset [0,1]$ is measurable and $|E|>0$, then
  \begin{equation*}
    \int_{E} \sum_{n=1}^{\infty} \left| e_{n}(x) \right|^{2}dx \geq 1.
  \end{equation*}
  To prove the claim, observe that
  \begin{equation*}
    |E|= \norm{\chi_{E}}_{L^{2}}^{2}  = \sum_{n=1}^{\infty} (\chi_{E},e_{n})^{2} = \sum_{n=1}^{\infty} \left( \int_0^{1}\chi_{E}(x) e_{n}(x)dx \right)^{2}\leq \sum_{n=1}^{\infty} \(\int_{E} e_{n}^{2}dx \) |E|
  \end{equation*}
  where the second equality is by Completeness of $(e_{n})$ and the inequality is due to Cauchy-Schwarz. Dividing both sides by $|E|$ and applying the Fubini-Tonelli theorem proves the claim.

  Define
  \begin{equation*}
  A_M = \left\{ x:  \sum_{n=1}^{\infty} |e_n(x)|^2 \leq M\right\}.
  \end{equation*}
  Suppose there exists some $M>0$ such that $|A_M|>0$. Let $E_n\subset A_M$ with $0<|E_n|<1/n$. Then by Claim 1,
  
  $$1\leq \int_{E_n} \sum_{n=1}^{\infty} \left| e_{n}(x) \right|^{2}dx \leq \frac{M}{n} \to 0$$
  a contradiction. Therefore $|A_M|=0$ for all $M$. Since $\{A_M\}_M$ is an ascending sequence of sets, it follows by continuity of measure that
  
  $$\left|\left\{x: \sum_{n=1}^{\infty} |e_n(x)|^2 dx <\infty\right\}\right|=\left|\bigcup_{M=1}^{\infty} A_M\right| = \lim_{M\to\infty} |A_M| =0.$$
\end{solution}
\end{parts}








\newpage
\section{Bonus Day: Misc. Topics That Sometimes Occur on Exam}

\question (Fall 2024) Let $f: [0,1) \to \CC$ be Lebesgue integrable. For $n \in \NN$ and $x \in [0,1)$ let
%
\[ \EE_n f(x) = \fint_{I_n(x)} f(y)\; dy \]
%
where $I_n(x)$ is the unique interval of the form $[k2^{-n}, (k+1)2^{-n})$ with $k \in \Z$ that contains $x$.
\begin{parts}
	\part Let $f \in C[0,1)$ and assume $\lim_{x \to 1^-} f(x)$ exists. Prove that $\EE_n f$ converges to $f$ uniformly on $[0,1)$.
	\begin{solution}
		This is a question about proving $L^\infty$ convergence on a Banach space, so we can use the first fundamental principle of functional analysis, i.e. proving the result on a dense subclass. Since $\lim_{x \to 1^-} f(x)$, we can assume we are working with functions in $C[0,1]$, which in particular are bounded. It is simple to prove that $\| \EE_n f \|_{L^\infty} \leq \| f \|_{L^\infty}$ for all $f \in C[0,1]$ and all $n$. And if $f$ is smooth, then we can use the mean value theorem to show that $|\EE_n f(x) - f(x)| \lesssim 2^{-n}$, so that uniform convergence holds quantitatively for smooth functions. But now an approximation argument yields the result for all functions, i.e. given $f \in C[0,1]$ and $\varepsilon > 0$, find $g \in C^\infty[0,1]$ with $\| f - g \|_{L^\infty} \leq \varepsilon$. Then
		%
		\[ \| \EE_n f - \EE_n g \|_{L^\infty} = \| \EE_n (f - g) \|_{L^\infty} \leq \| f - g \|_{L^\infty} \leq \varepsilon \]
		%
		And for sufficiently large $n$ (depending on $g$), $\| \EE_n g - g \|_{L^\infty} \leq \varepsilon$, and so combining these three inequalities using the triangle inequality gives that for sufficiently large $n$, $\| \EE_n f - f \|_{L^\infty} \leq 3 \varepsilon$. Taking $\varepsilon \to 0$ shows uniform convergence holds.
	\end{solution}

	\part If $f \in L^1[0,1]$ prove that $\EE_n f$ converges to $f$ almost everywhere as $n \to \infty$.
	\begin{solution}
		We must now use the second fundamental principle of analysis, i.e. that an approximation argument will yield this result if we can establish it for a dense subclass of $L^1[0,1]$ (the continuous functions, as we did above), and if we can establish that the \emph{maximal operator} $Mf(x) = \max_n |\EE_n f(x)|$ is \emph{weakly bounded} from $L^1[0,1]$ to $L^{1,\infty}[0,1]$, i.e. if for all $t > 0$,
		%
		\[ \{ x : Mf(x) \geq t \} \lesssim t^{-1} \| f \|_{L^1[0,1]}. \]
		%
		To prove this inequality is quite simple. Write $\Omega_t = \{ x : Mf(x) \geq t \}$. If $Mf(x) \geq t$, then there is some $n$ so that $\EE_n f(x) \geq t/2$, and then for all $x' \in I_n(x)$, $Mf(x') \geq t/2$. For each $x \in \Omega_t$, pick $n_x$ and then let $I_x = I_n(x)$. Then $\Omega_t$ is covered by the intervals $\{ I_x \}$, and their union is contained in $\{ x : Mf(x) \geq t/2 \}$. Since the family of intervals $\{ I_x : x \in \Omega_t \}$ are dyadic, we can find $\Omega_t' \subset \Omega_t$ such that the intervals $\{ I_x : x \in \Omega_t' \}$ are almost disjoint but $\bigcup_{\Omega_t'} I_x = \bigcup \Omega_t I_x$ (this follows because dyadic intervals are nested within one another). But then using the identity
		%
		\[ |I_x| = \frac{\int_{I_x} |f(y)|}{\EE_n f(x)} \leq (2/t) \int_{I_x} |f(y)| \]
		% E_n f(x) = 1/|I_x| int_{I_x} f(y)\; dy
		\[ |\Omega_t| \leq \sum_{x \in \Omega_t'} |I_x| \leq (2/t) \sum_{x \in \Omega_t'} \int |f(y)| \leq (2/t) \| f \|_{L^1}. \]
		%
		The last inequality here crucially used the fact that the intervals we are summing over \emph{do not overlap}. This completes the proof.

		What is the approximation argument that proves almost everywhere convergence? Given $f \in L^1[0,1]$ and $\varepsilon > 0$, find $g \in C[0,1]$ with $\| f - g \|_{L^1} \leq \varepsilon$. Then for each $t > 0$,
		%
		\[ \{ x : M(f - g) \geq t \} \lesssim \varepsilon / t. \]
		%
		That means that outside a set of measure $\varepsilon / \delta$, $|\EE_n f(x) - \EE_n g(x)| \leq \delta$ holds \emph{for all $n$}. So
		%
		\[ g(x) - \delta \leq \limsup\nolimits_n \EE_n f(x) \leq g(x) + \delta \]
		%
		But Markov's inequality implies that $|g(x) - f(x)| \leq \delta$ outside a set of measure $\varepsilon / \delta$. So that the inequality 
		%
		\[ f(x) - 2 \delta \leq \limsup\nolimits_n \EE_n f(x) \leq f(x) + 2\delta \]
		%
		holds outside a set of measure $\varepsilon / \delta$. But $\varepsilon$ was arbitrary here, so taking $\varepsilon \to 0$, we conclude that for all $\delta > 0$, the inequality
		%
		\[ f(x) - 2 \delta \leq \limsup\nolimits_n \EE_n f(x) \leq f(x) + 2\delta \]
		%
		holds almost everywhere. But now taking $\delta \to 0$ shows that $\limsup\nolimits_n \EE_n f(x) = f(x)$ almost everywhere. A similar argument shows $\liminf_n \EE_n f(x) = f(x)$ almost everywhere, and so $\lim_n \EE_n f(x)$ exists almost everywhere and is equal to $f(x)$.
	\end{solution}
\end{parts}

\question Given a real number $x$, let $\{ x \}$ denote the fractional part of $x$. Suppose $\alpha$ is an irrational number and define $T: [0,1] \to [0,1]$ by
%
\[ T(x) = \{ x + \alpha \}. \]
%
Prove: If $A \subset [0,1]$ is measurable and $T(A) = A$ then $|A| \in \{ 0, 1 \}$.
\begin{solution}
	Define $f: \RR \to \{ 0, 1 \}$ by setting $f(x) = \mathbf{I}(x \in A)$. Since $T(A) = A$, it follows that $f(x + \alpha) = f(x)$, i.e. $f$ is both $[0,1]$ periodic and $\alpha$ periodic. But applying the Fourier transform to both sides of this formula (viewing $f$ as a tempered distribution) yields that
	%
	\[ \widehat{f}(\xi) e^{2 \pi i \xi \alpha} = \widehat{f}(\xi), \]
	%
	i.e. that $\widehat{f}$ is supported on integer multiples of $2 \pi \alpha$. But the Poisson summation formula says precisely that $\widehat{f}$ is supported on the integer multiples of $2 \pi$, and the intersection of these two supports lies only at the origin. Thus $\widehat{f}$ is supported at the origin. But the only locally integrable tempered function with this property is a constant function, so either $f(x) = 1$ almost everywhere, or $f(x) = 0$ almost everywhere, which yields the result.
%	An easy (but lazy) way to prove the result is to apply the Weyl equidistribution theorem, which says that for any suitably regular function $f: [0,1] \to \RR$, and any $x \in [0,1]$,
	%
%	\[ \int_0^1 f(x)\; dx = \lim_{N \to \infty} \frac{1}{N} \sum_{k = 1}^N f ( \{ x + k \alpha \}  ). \]
	%
%	In particular, one can apply the result with $f(x)$

	Here is another method to prove the result, with the proof idea suggested by Yahui Qiu. Without loss of generality, we can consider the problem as showing that any subset $A$ of $\TT = \RR / \ZZ$ (we work with real numbers modulo the integers, so that we identify any real number with it's fractional part) with $A + \alpha = A$, for some irrational $\alpha$, has measure zero or measure one. If $|A| > 0$, then we can apply the Lebesgue density theorem to find a point $x_0 \in A$ of Lebesgue density. Thus
	%
	\[ \lim_{\delta \to 0} \frac{|A \cap (x_0 - \delta, x_0 + \delta)|}{2 \delta} = 1. \]
	%
	Since $A + \alpha = A$, we have $A + n\alpha = A$ for any $n \in \ZZ$, and
	%
	\begin{align*}
		\frac{|A \cap (x_0 + n \alpha - \delta, x_0 + n \alpha + \delta)}{2 \delta} &= \frac{|(A - n \alpha) \cap (x_0 - \delta, x_0 + \delta)|}{2 \delta}\\
		&= \frac{|A \cap (x_0 - \delta, x_0 + \delta)|}{2 \delta}.
	\end{align*}
	%
	Thus it follows that $x_n = x_0 + n \alpha$ is a point of Lebesgue density in $A$ for any $n \in \ZZ$, and moreover,
	%
	\[ \lim_{\delta \to 0} \inf_n \frac{|A \cap (x_n - \delta, x_n + \delta)|}{2 \delta} = 1. \]
	%
	We will now assume that $\{ x_n \}$ is dense in $\TT$, in order to argue that $|A| = 1$. We will justify this fact at the end of the argument. For any $\varepsilon > 0$, there exists $\delta_0$ such that for $\delta \leq \delta_0$, and any $n$,
	%
	\[  |A \cap (x_n - \delta, x_n + \delta)| \geq 2 \delta (1 - \varepsilon). \]
	%
	Since $\{ x_n \}$ is dense, we can cover $\TT$ by an almost disjoint family of intervals $I_1,\dots,I_N$, where $I_k$ has radius less than or equal to $\delta_0$, and center $x_{n_k}$ for some $n_k$. The inequality above states that for each $k$,
	%
	\[ |A \cap I_k| \geq (1 - \varepsilon) |I_k|. \]
	%
	But then it follows that
	%
	\begin{align*}
		|A| = |A \cap I_1| + \dots + |A \cap I_N| \geq (1 - \varepsilon) (|I_1| + \dots + |I_N|) = (1 - \varepsilon) |\TT| = 1 - \varepsilon.
	\end{align*}
	%
	Taking $\varepsilon \to 1$ completes the argument.

	Now lets justify when $\{ x_n \} = \{ x_0 + n \alpha \}$ is dense in $\TT$. By translation invariance, it suffices to show that $\{ n \alpha \}$ is dense in $\TT$. There are several ways to accomplish this. One easy (but lazy) method is to apply the Weyl equidistribution theorem, which gives this result automatically (and much more). A more basic method might be to apply the pigeonhole principle, which we now detail. For any $N$, we can divide $[0,1]$ up into $N$ intervals of length $1/N$. The pigeonhole principle implies that there exists $n_1 < n_2 \leq N + 1$ such that $n_1 \alpha$ and $n_2 \alpha$ both lie in the same interval, and thus in particular, $|(n_2 - n_1) \alpha| = |n_2 \alpha - n_1 \alpha| \leq 1/N$. Since $\alpha$ is irrational, $n_1 \alpha \neq n_2 \alpha$, so if $k = n_2 - n_1$, then we have
	%
	\[ 0 < |k \alpha| \leq 1/N. \]
	%
	Let $\delta = |k \alpha|$. Then the numbers $\{ m k\alpha : 1 \leq m \leq \lfloor 1/\delta \rfloor \}$ form an arithmetic progression in $(0,1)$, identified as a subset of $\TT$, spaced out at a distance smaller than $1/N$, with the first element $k \alpha$ lying at a distance beginning at a distance $t$ from $0$, and ending a distance at most $t$ from $1$. It follows that any point in $\TT$ lies a distance at most $t \leq 1/N$ from these values. Thus $N$ was arbitrary, we conclude the sequence is dense.
\end{solution}

\question (Rice, Winter 2008) Is it possible to construct a measurable set $E \subset \RR$ of positive measure such that for any pair $a < b$, $|E \cap [a,b]| \leq 0.5 (b - a)$?
\begin{solution}
    No, since the Lebesgue differentiation theorem implies that if $E$ has positive measure, then there exists $x_0$ such that
    %
    \[ \limsup_{x_0 \in I} |E \cap I|/|I| = 1. \]
    %
    However, if $E$ is as above, then $|E \cap I| / |I| \leq 0.5$ for any interval $I$, so $x_0$ cannot possibly exist. Thus any $E$ satisfying the condition above must have measure zero.
\end{solution}

\question (Spring 2010) For $\lambda>0$, set
\begin{equation*}
  F(\lambda)= \int_{0}^{1}e^{-10\lambda x^{4}+\lambda x^{6}}dx
\end{equation*}
Prove there exists constants $A$ and $C>0$, such that $F(\lambda)=\frac{A}{\lambda^{\frac{1}{4}}} + E(\lambda)$ where $|E(\lambda)|\leq \frac{C}{\lambda^{\frac{1}{2}}}$.
\begin{solution}
  Supposing $A$ and $C$ exist, then $A=\lim_{\lambda\to\infty} \lambda^{\frac{1}{4}}F(\lambda)$. To determine the value of $A$, we make the $u$-substitution $u=\lambda^{\frac{1}{4}}x$, which gives
  \begin{equation}\label{eq:u-form-of-F}
    \lambda^{\frac{1}{4}}F(\lambda)= \int_{0}^{\lambda^{\frac{1}{4}}}e^{-10u^{4}+\lambda^{-\frac{1}{2}}u^{6}}du = \int_{0}^{\infty}\chi_{\(0,\lambda^{\frac{1}{4}}\)}e^{-10u^{4}+\lambda^{-\frac{1}{2}}u^{6}}du.
  \end{equation}
  Note that for $0<u<\lambda^{\frac{1}{4}}$, it holds that $\lambda^{-\frac{1}{2}}u^{6} \leq u^{4}$. It follows that
  \begin{equation*}
    \chi_{\(0,\lambda^{\frac{1}{4}}\)}e^{-10u^{4}+\lambda^{-\frac{1}{2}}u^{6}}\leq e^{-9u^{4}}.
  \end{equation*}
  Since the right-hand side is integrable and $\chi_{\(0,\lambda^{\frac{1}{4}}\)}e^{-10u^{4}+\lambda^{-\frac{1}{2}}u^{6}}\to \chi_{(0,\infty)}e^{-10u^{4}}$ pointwise as $\lambda\to\infty$, it follows by the Dominated Convergence Theorem that
  \begin{equation*}
    \lim_{\lambda\to\infty} \lambda^{\frac{1}{4}}F(\lambda) = \int_{0}^{\infty}e^{-10u^{4}}du.
  \end{equation*}
  So let $A:= \int_{0}^{\infty}e^{-10u^{4}}du$. We need to show that there exists some constant $C$ such that
  \begin{equation*}
     \left|\lambda^{\frac{1}{4}}F(\lambda) - A \right| \leq \frac{C}{\lambda^{\frac{1}{4}}}.
   \end{equation*}
In particular, by equation \eqref{eq:u-form-of-F} and the triangle inequality,
   \begin{align*}
     \left|\lambda^{\frac{1}{4}}F(\lambda) - A\right|
     &\leq \underbrace{\left|\int_{0}^{\lambda^{\frac{1}{4}}} e^{-10u^{4}}\left( e^{\lambda^{-\frac{1}{2}}u^{6}}-1 \right)du\right|}_{\text{Call this } g(\lambda)} + \underbrace{\int_{\lambda^{\frac{1}{4}}}^{\infty}e^{-10u^{4}}du}_{\text{Call this }h(\lambda)},
   \end{align*}
   so it will suffice to show that $\lambda^{\frac{1}{4}}g(\lambda)\leq C_{1}$ and $\lambda^{\frac{1}{4}}h(\lambda)\leq C_{2}$ for all $\lambda$.

  To estimate $\lambda^{\frac{1}{4}}g(\lambda)$, we use the inequality $|e^{x}-1|\leq |x|e^{x}$  (which holds for all $x>0$ since $|e^{x}-1|=|\int_{0}^{x}e^{t}dt|\leq xe^{x}$). Using this inequality, we have:
   \begin{align*}
     \lambda^{\frac{1}{4}}g(\lambda)
     &\leq \lambda^{\frac{1}{4}}\int_{0}^{\lambda^{\frac{1}{4}}} e^{-10u^{4}}\lambda^{-\frac{1}{2}}u^{6}e^{\lambda^{-\frac{1}{2}}u^{6}}du\\
     &=\lambda^{-\frac{1}{4}}\int_{0}^{\lambda^{\frac{1}{4}}} u^{6}e^{-10u^{4}+\lambda^{-\frac{1}{2}}u^{6}}du\\
     &\leq \int_{0}^{\lambda^{\frac{1}{4}}} u^{5}e^{-9u^{4}}du &&\text{using }u\leq \lambda^{\frac{1}{4}}\\
     &\leq  C_{1}
   \end{align*}
   where $C_{1} := \int_{0}^{\infty} u^{5}e^{-9u^{4}}du<\infty$.
   
   It remains to estimate $\lambda^{\frac{1}{4}}h(\lambda)$. Using $\lambda^{\frac{1}{4}}\leq u$, we have
   \begin{equation*}
     \lambda^{\frac{1}{4}}h(\lambda)
      = \lambda^{\frac{1}{4}}\int_{\lambda^{\frac{1}{4}}}^{\infty}e^{-10u^{4}}du
     < \int_{\lambda^{\frac{1}{4}}}^{\infty}ue^{-10 u^{4}}du < C_{2}
   \end{equation*}
   where $C_{2} := \int_{0}^{\infty}ue^{-10u^{4}}du<\infty$. This completes the proof.
 \end{solution}

\question (Fall 2010)
Let $I = [0,1]$ and define for $f\in L^{2}(I)$ the Fourier coefficients as $\hat{f}(k) =\int_{0}^{1}f(t)e^{-2\pi i k t}dt$ for any $k\in \Z$. 


\begin{parts}
\part Let $\mathcal{G}$ be the set of all $L^{2}(I)$ functions with the property that  $|\hat{f}(0)|\leq 1$ and  $|\hat{f}(k)|\leq |k|^{-3/5}$ for any $k\in \Z$, $k\neq 0$. Prove that $\mathcal{G}$ is a compact subset of $L^{2}(I)$.


\begin{solution}

  Let $f_{n}\in \mathcal{G}$. It will suffice to show that there exists a subsequence $f_{n_{j}}$ and an $f\in \mathcal{G}$ such that $f_{n_{j}}\to f$ in $L^{2}(I)$.

  We will apply Cantor's diagonal sequence argument to obtain a subsequence $n_{j}$ such that $\lim_{j\to\infty}\hat{f}_{n_{j}}(k)$ exists for all $k\in \Z$. The details of this argument are as follows:

  
  First, since $\{\hat{f}_{n}(0)\}_{n\in\mathbb{N}}$ is a bounded sequence of real numbers, there exists a subsequence $\{s(n,0)\}_{n\in \mathbb{N}}$ of $\mathbb{N}$ such that $\limn \hat{f}_{s(n,0)}(0)= c_{0}$ for some $c_{0}\in \R$.

  Next, since $|\hat{f}_{s(n,0)}(k)|\leq |k|^{-3/5}$ for any $k\in \Z\backslash \left\{ 0 \right\}$, it follows that  $\{\hat{f}_{s(n,0)}(1)\}_{n\in\mathbb{N}}$ and $\{\hat{f}_{s(n,0)}(-1)\}_{n\in\mathbb{N}}$ are both bounded sequences of real numbers. Therefore there exists a subsequence $\left\{ s(n,1) \right\}_{n\in \mathbb{N}}$ of  $\left\{ s(n,0) \right\}_{n\in \mathbb{N}}$ such that $\limn \hat{f}_{s(n,1)}(1) = c_{1}$ and $\limn \hat{f}_{s(n,1)}(-1) = c_{-1}$ for some $c_{1},c_{-1}\in \R$.
  
  In general, suppose we have chosen the subsequence $s(n,m-1)$. Then by the inequality $|\hat{f}(k)|\leq |k|^{-3/5}$, it follows that both $\{\hat{f}_{s(n,m-1)}(m)\}_{n\in\mathbb{N}}$ and $\{\hat{f}_{s(n,m-1)}(-m)\}_{n\in\mathbb{N}}$ are bounded sequences of real numbers, and therefore we may choose a further subsequence $\{s(n,m)\}_{n\in\mathbb{N}}$ of $\{s(n,m-1)\}_{n\in\mathbb{N}}$ such that 
  $\limn \hat{f}_{s(n,m)}(m) = c_{m}$ and $\limn \hat{f}_{s(n,m)}(-m) = c_{-m}$ for some $c_{m},c_{-m}\in \R$.

  We now have in our hands a nested sequence of subsequences
  \begin{equation*}
    \left\{ s(n,0) \right\}_{n\in\mathbb{N}}\supset  \left\{ s(n,1) \right\}_{n\in\mathbb{N}}\supset \left\{ s(n,2) \right\}_{n\in\mathbb{N}}\supset \cdots
  \end{equation*}
  Now consider the diagonal subsequence $\{s(n,n)\}_{n\in \mathbb{N}}$. By construction,
  \begin{equation}
    \label{eq:2}
    \lim_{n\to\infty} \hat{f}_{s(n,n)}(k) =c_{k}\in \R
  \end{equation}
  for all $k\in \Z$.

  Let $f(t)=\sum_{k\in\Z} c_{k}e^{2\pi i k t}$. By Parseval's identity and the inequality $|\hat{f}(k)|\leq |k|^{-3/5}$, we have
  \begin{equation*}
    \norm{f}^{2}_{L^{2}(I)} =  \sum_{k\in\Z}|\hat{f}(k)|^{2} \leq  \left( 1 + 2\sum_{k\geq 1}|k|^{-6/5} \right)<\infty
  \end{equation*}
  Therefore $f$ is well defined and in $L^{2}(I)$.
  
  It remains to show that $f_{s(n,n)}\to f$ in $L^{2}(I)$. To see this, observe that by Parseval's identity,
  \begin{align*}
    \norm{f_{s(n,n)}-f}_{L^{2}}^{2}
    &= \sum_{k\in \Z} \left| \hat{f}_{s(n,n)}(k)-c_{k} \right|^{2}\\
    &= \sum_{|k|\leq N} \left| \hat{f}_{s(n,n)}(k)-c_{k} \right|^{2} + \sum_{|k|> N} \left| \hat{f}_{s(n,n)}(k)-c_{k} \right|^{2}\\
    &\leq \sum_{|k|\leq N} \left| \hat{f}_{s(n,n)}(k)-c_{k} \right|^{2} + \sum_{|k|> N} \left| K \right|^{-6/5}
  \end{align*}
  Therefore
  \begin{align*}
    \limsup_{n\to\infty}  \norm{f_{s(n,n)}-f}_{L^{2}}^{2}
    &\leq \sum_{|k|> N} \left| K \right|^{-6/5} \to 0 \text{ as }N\to\infty
  \end{align*}
  Therefore $f_{s(n,n)}\to f$ in $L^{2}(I)$.
\end{solution}

\part Let $\mathcal{E}$ be the set of all $L^{2}(I)$ functions with the property that $\sum_{k}|\hat{f}(k)|^{5/3}\leq 2016^{-2016}$. Is $\mathcal{E}$ a compact subset of $L^{2}(I)$?

\begin{solution}
  $\mathcal{E}$ is not compact. To see this, let $f_{n}(t)= e^{2\pi i n t}$. Then $\hat{f_{n}}(k) = \delta(k-n)$ where $\delta$ is the Kronecker-delta function. If $\epsilon_{0} \in (0,2016^{-2016})$, then $\left\{ \epsilon_{0}f_{n} \right\}_{n=1}^{\infty}\subset \mathcal{E}$ but it is not compact because it contains no convergent subsequence.
\end{solution}
\end{parts}

\question (Fall 2011) Let $\ell^{2}(\mathbb{N})$ denote the Hilbert space of square summable sequences with inner product $(x,y)= \sum_{n=1}^{\infty}x_{n}y_{n}$, where $x=(x_{1},x_{2},\cdots)$ and $y=(y_{1},y_{2},\cdots)$.
\begin{parts}
\part What are the necessary and sufficient conditions on $\lambda_n>0$ for the set
\begin{equation*}
  S = \left\{ (x_{1},x_{2}\cdots) \in \ell^{2}(\mathbb{N}): |x_{n}| \leq \lambda_{n}, \forall n\right\}
\end{equation*}
to be compact in $\ell^{2}(\mathbb{N})$? 
\begin{solution}
    For any sequence $\{ x(1), x(2), \dots \}$ in $S$, and any fixed $i$, the sequence $\{ x(1)_i, x(2)_i, \dots \}$ is bounded, and thus has a convergent sub-sequence. Assuming that
    %
    \[ \lim_{N \to \infty} \sum_{i = N}^\infty \lambda_i^2 = 0, \]
    %
    we will use diagonalization to construct a convergent subsequence. This is necessary, because the sequence $\{ (\lambda_1,0, \dots), (\lambda_1,\lambda_2,0,\dots), (\lambda_1,\lambda_2,\lambda_3, 0, \dots), \dots \}$ only has a convergent subsequence if this property was true.
    
    We will define a family of elements $x(i,j) \in l^2(\mathbf{N})$ for $0 \leq i < \infty$ and $1 \leq j < \infty$, such that for $0 < k \leq i$, $x(i,j)_k$ converges as $j \to \infty$. Begin by setting $x(0,i) = x(i)$. Given $x(i_0,j)$, the sequence $x(i_0,j)_{i_0 + 1}$ is bounded in $j$, so we can find a sequence $y(j)$ such that $y(j)_{i_0 + 1}$ converges. Let $x_{i_0+1,j}$ be this convergent sub-sequence. Now define $y(k) = x(k,k)$. For each $i$, the sequence $\{ y(k) : k > i \}$ is a subsequence of $\{ x(i,j) \}$, and so $y(k)_i$ converges as $i \to \infty$ for each $i$. Thus for any fixed $K$,
    %
    \[ \limsup_{N \to \infty} \sup_{M > N} \sum_{j \leq K} |y(N)_j - y(M)_j|^2 = 0. \]
    %
    But
    %
    \[ \limsup_{N \to \infty} \sup_{M > N} \sum_{j > K} |y(N)_j - y(M)_j|^2 \leq 4 \sum_{j > K} \lambda_j^2. \]
    %
    Thus for any $K$,
    %
    \[ \limsup_{N \to \infty} \sum_{M > N} \| y(N) - y(M) \|_2 \leq 2 \left( \sum_{j > K} \lambda_j^2 \right)^{1/2} \]
    %
    Taking $K \to \infty$ gives
    %
    \[ \limsup_{N \to \infty} \sum_{M > N} \| y(N) - y(M) \|_2 = 0, \]
    %
    and so $\{ y(k) \}$ is a Cauchy sequence in $l^2(\mathbf{N})$, and is thus convergent.
\end{solution}

\part What are the necessary and sufficient conditions on $\mu_{n}>0$ for the set
\begin{equation*}
  \left\{ (x_{1},x_{2}\cdots) \in \ell^{2}(\mathbb{N}): \sum_{n}\frac{|x_{n}|^{2}}{\mu_{n}^{2}}\leq 1\right\}
\end{equation*}
to be compact in $\ell^{2}(\mathbb{N})$?
\begin{solution}
The necessary and sufficient condition is that $\mu_n\to 0$ as $n\to \infty$.

\vt
\textit{Proof of Necessity:} Suppose that $\mu_n\not\to 0$. Then there exists an $\epsilon>0$ and a subsequence $\{\mu_{n_j}\}$ with $\mu_{n_j}\geq \epsilon$.
Let $x^{(j)} = (0,\cdots,\mu_{n_j},0,\cdots)$. It is easy to check that this sequence has no convergent subsequence.

\vt
\textit{Proof of Sufficiency:} Suppose $\mu_n\to 0$. 


Let $\{x^{(n)}\}$ be a seqeunce with $x^{(n)}\in \left\{ (x_{1},x_{2}\cdots) \in \ell^{2}(\mathbb{N}): \sum_{n}\frac{|x_{n}|^{2}}{\mu_{n}^{2}}\leq 1\right\}$.

By a Cantor diagonalization argument, there exists a subsequence $\{x^{(n_j)}\}_j$ such that $x^{n_j}_k\to y_k\in \R$ for each $k$ as $j\to\infty$. 

We then check that that $y = (y_1,y_2,\cdots) \in \ell^2$ and that $x^{n_j}\to y$ in $\ell^2$.

\end{solution}
\end{parts}


\question Let $f:\R\to\R$ be a convex function, let $E=\left\{ x\in \R : f \text{ is not differentiable at }x\right\}$. Show that $E$ is at most countable.


\begin{solution}
For each $x_{0}\in \R$, define $f_{-}'(x_{0}) := \lim_{x\to x_{0}^{-}} \frac{f(x)-f(x_{0})}{x-x_{0}}$ and $f_{+}'(x_{0}) := \lim_{x\to x_{0}^{+}} \frac{f(x)-f(x_{0})}{x-x_{0}}$.

\vt
\textit{Claim 1:} Both $f_{-}'(x_{0})$ and $f_{+}'(x_{0})$ exist and are finite for all $x_{0}\in \R$. 


To prove this, we will demonstrate that $f_{-}'(x_{0})$ exists and is finite, as the case for $f_{+}'(x_{0})$ is similar. Recall the chordal slope property of convex functions: if $f$ is convex on $\R$, then for any $x_{1}<x_{2}<x_{3}$, the following inequality holds (draw a picture of this)
\begin{equation}\label{eq:chord-inequality}
  \frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}\leq \frac{f(x_{3})-f(x_{1})}{x_{3}-x_{1}}\leq \frac{f(x_{3})-f(x_{2})}{x_{3}-x_{2}}.
\end{equation}

The proof of \eqref{eq:chord-inequality} requires only the definition of convexity and a little bit of algebra.  Suppose $x_{1}<x_{2}<x_{3}$. Recall $f$ convex means that for all $a,b\in \R$ and all $0\leq \lambda\leq 1$, we have
  \begin{equation*}
    f(\lambda x + (1-\lambda)y)\leq \lambda f(x) + (1-\lambda)f(y).
  \end{equation*}
  Taking $x=x_{1}, y=x_{3}$ and $\lambda=\frac{x_{3}-x_{2}}{x_{3}-x_{1}}$ (so that $x_{2}= \lambda x_{1} + (1-\lambda)x_{3}$) in the above equation, we obtain 
\begin{equation*}
  \frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}\leq \frac{f(x_{3})-f(x_{2})}{x_{3}-x_{2}}.
\end{equation*}
Then, with a little more algebra, we can rewrite this inequalilty as the equivalent forms
\begin{equation*}
  \frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}\leq \frac{f(x_{3})-f(x_{1})}{x_{3}-x_{1}} \quad \text{and}\quad \frac{f(x_{1})-f(x_{3})}{x_{1}-x_{3}}\leq \frac{f(x_{2})-f(x_{3})}{x_{2}-x_{3}}.
\end{equation*}
This proves \eqref{eq:chord-inequality}

From \eqref{eq:chord-inequality}, we can deduce the following:
\begin{enumerate}
\item For each fixed $x_{0}\in \R$, the function $x\mapsto \frac{f(x)-f(x_{0})}{x-x_{0}}$ is increasing on $(-\infty,x_{0})$.
\item The family $\left\{   \frac{f(x)-f(x_{0})}{x-x_{0}}:  x\in (-\infty,x_{0})\right\}$ is bounded above by $\frac{f(x_{0}+1)-f(x_{0})}{(x_{0}+1)-x_{0}} = f(x_{0}+1)-f(x_{0})<\infty$.
\end{enumerate}
These two facts together imply that the limit
\begin{equation*}
  f_{-}'(x_{0}):=\lim_{x\to x_{0}^{-}}\frac{f(x)-f(x_{0})}{x-x_{0}}
\end{equation*}
is both increasing and bounded above, and therefore exists as a real number. The case for $f_{+}'(x_{0})$ is similar. This proves Claim 1.



By Claim 1, it follows that $f$ is not differentiable at a point $x\in \R$ if and only if $f'_{+}(x)\neq f'_{-}(x)$. Let $E=\left\{ x\in \R : f'_{-}(x)\neq f'_{+}(x) \right\}$. We need to show that $E$ is at most countable.


To see this, we first make the following observations, all of which follow from \eqref{eq:chord-inequality}:
\begin{enumerate}[(a)]
\item \label{item:3} $f'_{+}$ and $f'_{-}$ are both increasing functions
\item \label{item:5} $f'_{-}(x)\leq f'_{+}(x)$ for all $x\in\R$
\item \label{item:4} $f_{+}'(a) \leq \frac{f(b)-f(a)}{b-a}\leq f'_{-}(b)$ if $a<b$
\end{enumerate}

By \ref{item:3}, $f_{+}'$ and $f'_{-}$ are continuous at all but countably many points. That is, the set  $$D:=\left\{ x\in \R: f_{+}'\text{ is not continuous at }x \right\}\cup \left\{ x\in \R: f_{-}'\text{ is not continuous at }x \right\}$$ is countable. We will show that $E\subset D$, which will complete the proof.

Suppose $x\notin D$. We need to show that $x\notin E$. By \ref{item:5}, $E=\left\{ x\in \R : f'_{-}(x)< f'_{+}(x) \right\}$, so it will suffice to show that $f'_{-}(x)\geq f_{+}'(x)$.

Let $x_{n}$ be a sequence such that $x_{n}>x$ and $x_{n}\to x$. Since $x\notin D$,
\begin{equation*}
  f'_{-}(x)=\limn f'_{-}(x_{n})\geq f'_{+}(x)
\end{equation*}
where the final inequality is justified by \ref{item:4} since $x_{n}>x$.

(This proof was adapted from Royden and Fitzpatrick's \textit{Real Analysis} 4th ed.)
\end{solution}

\question (Fall 2015) Identify all $\alpha\in \R$ such that $\lim_{n\to\infty} \sin(2 \pi n \alpha)$ exists.

\begin{solution}
	Define $f(\alpha)=\sin(2\pi \alpha)$. It will suffice to characterize the $\alpha$ for which $\limn f(n\alpha)$ converges, since $\limn f(n\alpha)$ exists if and only if $\limn \sin(n2\pi \alpha)$ exists. Observe that $f$ is periodic: if $k\in \Z$ then $f(x)=\sin(2\pi x) = \sin(2\pi x +2\pi k) =f(x+k)$. By periodicity it suffices to consider $\alpha\in [0,1)$, since $\limn f(n\alpha)$ converges if and only if $\limn f(n(\alpha+k))$ converges for all $k\in \Z$. 

  \textit{Case 1.} Suppose $\alpha \notin \Q$. Let $\xi_{n}:= n\alpha - \lfloor n\alpha \rfloor$. Then by \emph{Weyl's equidistribution theorem}, the sequence $\{ \xi_{n} \}$ is equidistributed on $[0,1)$ and hence is dense. If we choose a sequence $n_k$ such that $\xi_{n_k} \to 3/4$ as $k \to \infty$, and another sequence $m_k$ such that $\xi_{m_k} \to 1/4$, then we conclude that by continuity,
  %
  \[ \liminf_{n \to \infty} f(n \alpha) \leq \liminf_{k \to \infty} f(n_k \alpha) = -1 \]
  %
  and
  %
  \[ \limsup_{n \to \infty} \sin(2 \pi n \alpha) \geq \limsup_{k \to \infty} \sin(2 \pi m_k) = 1. \]
  %
  This shows the $\liminf$ differs from the $\limsup$, so $\limn \sin(2 \pi n \alpha)$ cannot exist. Therefore $\sin(2\pi n \alpha)$ does not converge for any $\alpha\notin \Q$
  
  \textit{Case 2.} Suppose $\alpha\in\Q$. Write $\alpha = p/q$ where $p,q$ are integers with $\gcd(p,q)=1$. Let $x = 2\pi p/q$. Then the sequence $\xi_{n}:= n\alpha-\lfloor n \alpha \rfloor$ is periodic with $q$ distinct values, one of which is zero. Indeed,
  \begin{equation*}
    \xi_{q}=q \frac{p}{q} - \left\lfloor q \frac{p}{q} \right\rfloor = p-p = 0
  \end{equation*}
  and
  \begin{equation*}
    \xi_{q+1}= (q+1)\frac{p}{q} - \left\lfloor \frac{(q+1)p}{q} \right\rfloor = \frac{p}{q} - \left\lfloor \frac{p}{q}\right\rfloor = \xi_{1}.
  \end{equation*}
  Since $f(n\alpha) = f(\xi_{n})$, it follows that the seuence $\{f(n\alpha)\}_{n=1}^{\infty}$ is periodic and takes up to $q$ distinct values. Therefore, in order for $\limn f(n\alpha)$ to converge, we must have
  %
  \[ f(\xi_{1})=f(\xi_{2})=\ldots=f(\xi_{q})=f(0)=0. \]
  %
  By periodicity of $f$, this is equivalent to
  %
	\begin{equation*}
    f\(\frac{p}{q}\)= f\(\frac{2p}{q}\)=\ldots=f\left(\frac{(q-1)p}{q}\right)=f(p)
  \end{equation*}
  and
  \begin{equation*}
    f(p) = \sin(2\pi p)=0.
  \end{equation*}
  Therefore $\sin(2\pi p/q)=0$. This implies that $q=1$ or $q=2$. So either $\alpha =0$ or $1/2$ (mod 1). That is, $x=k\pi$ for $k\in \Z$.
\end{solution}


\newpage
\section{Spring 2021 Final Qualifying Exam}

\question (3) For a Lebesgue measurable subset $E$ of $\R$, denote $\textbf{1}_E$ the indicator function of $E$ (i.e. $\textbf{1}_E(x)=1$ for $x\in E$ and $\textbf{1}_E(x) = 0$ for $x\in E^c$).

Let $\{E_n: n\in \mathbb{N}\}$ be a family of Lebesgue measurable subsets of $\R$ with finite measure and let $f$ be a measurable function such that \begin{equation*}
\limn \int_{\R} | f(x)-\textbf{1}_{E_n}(x)| dx = 0.
\end{equation*}
Prove that $f$ is the indicator function of a measurable set.

\begin{solution}
For each $\epsilon>0$, 
\begin{equation*}
  \epsilon \mu \left\{ x: |f(x)-\mathbf{1}_{E_{n}}(x)| >\epsilon \right\}
  <  \int_{\left[|f-\mathbf{1}_{E_{n}}| >\epsilon \right]} |f(x)-\mathbf{1}_{E_{n}}(x)|dx
  \leq \int_{\R} |f(x)-\mathbf{1}_{E_{n}}(x)|dx \to 0
\end{equation*}
as $n\to \infty$.
Therefore $\mathbf{1}_{E_{n}}\to f$ in measure. By definition of convergence in measure, we may choose a subsequence $n_{k}$ such that
\begin{equation*}
  \mu \left\{ x: |f(x)-\mathbf{1}_{E_{n_{k}}}(x)|>\epsilon \right\} \leq 2^{-k}.
\end{equation*}
Therefore
\begin{equation*}
  \sum_{k\geq 1}\mu \left\{ x: |f(x)-\mathbf{1}_{E_{n_{k}}}(x)|>\epsilon \right\}<\infty.
\end{equation*}
Therefore by the Borel-Cantelli lemma,
\begin{equation*}
  \mu \left\{ x: |f(x)-\mathbf{1}_{E_{n_{k}}}(x)|>\epsilon \text{ for infinitely many }k \right\}=0.
\end{equation*}
In other words, $\lim_{k\to \infty} \mathbf{1}_{E_{n_{k}}}(x) = f(x)$ for a.e. $x\in \R$. Therefore $f$ is equal to zero or one almost everywhere, and as the a.e. limit of measureable functions, $f$ is also measurable. Therefore $f$ is the indicator function of a measurable set.

\end{solution}




\question (6) Let $f: \R\to\R$ be a compactly supported function that satisfies the Holder condition with exponent $\beta\in (0,1)$, i.e., there exists a constant $A<\infty$ such that $\forall x,y\in \R: |f(x)-f(y)|\leq A|x-y|^{\beta}$. Consider the function $g$ defined by
\begin{equation*}
g(x) = \int_{-\infty}^{\infty} \frac{f(y)}{|x-y|^{\alpha}} dy
\end{equation*}
where $\alpha \in (0,\beta).$
\begin{parts}
\part Prove that $g$ is a continuous function at $0$.
\begin{solution}
We will show that $|g(x)-g(0)|\to 0$ as $|x|\to 0$. Without loss of generality, we may assume that $|x|\leq 1$. Performing the $u$-substitution $u=y-x$, we may write
\begin{equation*}
g(x) = \int_{-\infty}^{\infty} \frac{f(x+y)}{|y|^{\alpha}} dy.
\end{equation*}
Therefore, taking $k>0$ sufficiently large that supp$(f)\subseteq [-k,k]$, we have
\begin{align*}
|g(x)-g(0)| 
&= \left|\int_{-\infty}^{\infty} \frac{f(x+y)-f(y)}{|y|^{\alpha}} dy\right|\\
&= \left|\int_{-(k+1)}^{k+1} \frac{f(x+y)-f(y)}{|y|^{\alpha}} dy\right| &&\text{since }|x|\leq 1\\
& \leq \int_{-(k+1)}^{k+1} \frac{|f(x+y)-f(y)|}{|y|^{\alpha}} dy\\
&\leq 2A|x|^{\beta} \int_0^{k+1} y^{-\alpha}dy \\
& = 2A |x|^{\beta} \frac{(k+1)^{1-\alpha}}{1-\alpha} \to 0 \text{ as } |x|\to0
\end{align*}
where we have used the fact that $\alpha<1$ so that the integral is finite.
\end{solution}
\part Prove that $g$ is differentiable at $0$. (Hint: Try the dominated convergence theorem).
\begin{solution}

So the idea is to use the so called generalized dominated convergence theorem, whose proof is exactly the same as the usual dominant convergence theorem. And the statement is the following:

\textit{Let $\{h_n\}$ be a family of $L^1$ function which converge almost everywhere to a $L^1$ function $h$, let $g_n$ be another family of $L^1$ functions such that $g_n\to g$ a.e., $|h_n|\le g_n$ for each $n$ and $|h|\le g$,  Suppose now we have $\lim \int g_n=\int g$, then $\lim\int h_n=\int h$.}

Now come back to our problem. First attempt would be to use the standard test, which means we just naively take the derivative of the inside function and then check integrability. We immediately encounter a problem because we don't have the desired integrability condition. In fact, we have $f(y)\cdot sgn(x-y)|x-y|^{-\alpha-1}$, which is not integrable.

So our second attempt would be to use the Holder condition to balance this singularity. Assume that supp$(f)\subset [-K,K]$ for some $K>1$. Let $(x_n)$ be a sequence of real numbers converging to zero. Without loss of generality, we may assume $|x_n|\leq1$ for all $n$. By the Mean Value Theorem, for each $n$ there exists some $\xi_n$ with $0\le |\xi_n|\le |x_n|$ such that $$\frac{1}{|y-x_n|^\alpha}-\frac{1}{|y|^\alpha}=\alpha\cdot sgn(y-\xi_n)\cdot |y-\xi_n|^{-\alpha-1}\cdot |x_n|.$$
Therefore
\begin{align}
\frac{g(x_n)-g(0)}{x_n} &=\int_{-K}^K \frac{f(y)-f(\xi_n)}{|y-x_n|^\alpha\cdot x_n}-\frac{f(y)-f(\xi_n)}{|y|^\alpha\cdot x_n}dy+E_n \nonumber\\
&=\int_{-K}^K \frac{f(y)-f(\xi_n)}{x_n}\cdot \(\frac{1}{|y-x_n|^{\alpha}}-\frac{1}{|y|^\alpha}\)dy+E_n \nonumber\\
&=\int_{-K}^K (f(y)-f(\xi_n))\cdot \alpha\cdot sgn(y-\xi_n)\cdot |y-\xi_n|^{-\alpha-1}dy+E_n \label{first-deriv-form}
\end{align}
where 
\begin{equation*}
E_{n}=\int_{-K}^K \frac{f(\xi_n)}{x_n}\(\frac{1}{|y-x_n|^\alpha}-\frac{1}{|y|^\alpha}\)dy.
\end{equation*}
In particular, we have 
\begin{align*}
|E_{n}|&=\left|\int_{-K-x_n}^{-K}\frac{f(\xi_n)}{x_n}\frac{1}{|y|^\alpha}dt-\int_{K-x_n}^{K}\frac{f(\xi_n)}{x_n}\frac{1}{|y|^\alpha}dy\right| = \left|\frac{f(\xi_n)}{x_n}\int_{K-x_n}^{K+x_n} \frac{1}{|y|^{\alpha}} dy\right| \leq \frac{C}{K^{\alpha}}.
\end{align*}
for $C>0$ sufficiently large. Therefore by the above estimate for $|E_n|$ and equation \eqref{first-deriv-form} we have:
\begin{equation}\label{deriv-equation}
\left|\frac{g(x_n)-g(0)}{x_n} - \int_{\R} h_n(y)dy\right| \leq \frac{C}{K^\alpha}
\end{equation}
where
$$h_n(y)=\chi_{[-K,K]}\cdot (f(y)-f(\xi_n))\cdot \alpha\cdot sgn(y-\xi_n)\cdot |y-\xi_n|^{-\alpha-1}.$$

Let $$h(y):= \lim_{n\to\infty} h_n(y) = \chi_{[-K,K]}\cdot(f(y)-f(0))\cdot \alpha\cdot sgn(y)\cdot |y|^{-\alpha-1}.$$

We wish to show that $\int_{\R}h_n(y)dy \to  \int_{\R}h(y)dt$ as $n\to\infty$. We now justify this by applying the generalized dominant convergence theorem. Define 
$$g_n=C\cdot |y-\xi_n|^{\beta-\alpha-1}\cdot \chi_{[-K,K]}$$ where $C$ is some large constant. Similarly, define 
$$g=C\cdot |y|^{\beta-\alpha-1}\cdot \chi_{[-K,K]}.$$ 
One easily sees that $|h_n|\le g_n$ and $|h|\le g$ by applying the Holder condition. Moreover, each $g_n$ is in $L^1$. Since $\xi_n\to 0$, we have $\lim \int g_n\to \int g$ and $g_n\to g$ a.e. Finally, invoke the generalized dominated convergence theorem, and it follows that $\limn \int h_n = \int h$, so that by equation \eqref{deriv-equation},
\begin{equation*}\limsup_{n\to\infty} \left|\frac{g(x_n)-g(0)}{x_n} - \int_{\R} h(y)dy\right| \leq \frac{C}{K^\alpha}\end{equation*}

Taking $K\to \infty$, it follows that
\begin{equation*}
\limn \frac{g(x_n)-g(0)}{x_n} = \int_{\R} (f(y)-f(0))\cdot \alpha\cdot sgn(y)\cdot |y|^{-\alpha-1}dy.
\end{equation*}
Since $(x_n)$ was an arbitrary sequence converging to zero, it follows that $$g'(0) = \int_{\R} (f(y)-f(0))\cdot \alpha\cdot sgn(y)\cdot |y|^{-\alpha-1}dy$$
which is finite.
\end{solution}
\end{parts}


\question[(Spring 2021)] A real valued function $f$ defined on $\RR$ belongs to the space $C^{1/2}(\RR)$ if and only if
%
\[ \sup_{x \in \RR} |f(x)| + \sup_{x \neq y} \frac{|f(x) - f(y)|}{\sqrt{|x - y|}} < \infty. \]
%
Prove that a function $f \in C^{1/2}(\RR)$ if and only if there exists a constant $C$ so that for every $\varepsilon > 0$, there is a bounded function $\varphi \in C^\infty(\RR)$ such that
%
\[ \sup_{x \in \RR} |f(x) - \varphi(x)| \leq C \varepsilon^{1/2} \quad\text{and}\quad \sup_{x \in \RR} \varepsilon^{1/2} |\varphi'(x)| \leq C. \]
\begin{solution}
    Suppose $f \in C^{1/2}(\RR)$. Let $\phi$ be a bump function symmetrical about the origin supported on $|x| \leq 1$, set $\phi_\varepsilon(y) = (1/\varepsilon) \phi(y/\varepsilon)$ and define $\varphi = f * \phi_\varepsilon$. Then $\varphi$ is bounded, and lies in $C^\infty(\RR)$. Since $\phi_\varepsilon(y)$ is supported on $|y| \leq \varepsilon$, We have
    %
    \[ f(x) - \varphi(x) = \int [f(x) - f(x-y)] \phi_\varepsilon(y) \lesssim \varepsilon^{1/2}. \]
    %
    We write (using the symmetry of $\phi$)
    %
    \[ \varphi'(x) = (1/\varepsilon)^2 \int_0^\infty \phi'(y/\varepsilon) [f(x-y) - f(x+y)]\; dy. \]
    %
    Now $|f(x-y) - f(x+y)| \lesssim \varepsilon^{1/2}$ for $|y| \leq \varepsilon$, the support of our integrand lies on $0 \leq y \leq \varepsilon$, and $|\phi'(y/\varepsilon)| \lesssim 1$. Thus it follows that $|\varphi'(x)| \lesssim \varepsilon^{1/2}$, which completes the proof.

    Now suppose the second condition. Given $\varphi$ satisfying the condition above, the derivative condition together with the fundamental theorem of calculus implies that
    %
    \[ |f(x) - f(y)| \leq |f(x) - \varphi(x)| + |\varphi(x) - \varphi(y)| + |\varphi(y) - f(y)| \leq 2C \varepsilon^{1/2} + C |x - y| / \varepsilon^{1/2}. \]
    %
    If we choose $\varepsilon = |x - y|$, then we conclude that
    %
    \[ |f(x) - f(y)| \leq 3C |x-y|^{1/2}, \]
    %
    so that
    %
    \[ \sup_{x \neq y} \frac{|f(x) - f(y)|}{|x-y|^{1/2}} \leq 3C. \]
    %
    The fact that $\varphi$ is bounded shows that if the condition holds for any $\varepsilon > 0$, then $f$ is bounded. Thus $f \in C^{1/2}(\RR)$.
\end{solution}





\question (Fall 2021) Let $f\in L^1(\mathbb{R})$  satisfy $\int_{a}^{b}f(x)dx=0$
for any two rational numbers $a$ and $b$, $a<b$. Does it follow that $f(x)=0$
for almost every $x$?

\begin{solution}
	Yes. One solution is to apply the Lebesgue differentiation theorem, though this trivializes the problem, so we present two other solutions here.

	If $f \neq 0$, then $f \neq 0$ when restricted to some interval $[-N,N]$. For any simple function $g \in L^\infty[-N,N]$ given as $\sum_{k = 1}^N a_k \chi_{I_k}$, where $I_k$ are intervals with rational endpoints, we have
	%
	\[ \int_a^b f(x) g(x)\; dx = 0. \]
	%
	Such functions are not dense in $L^\infty[-N,N]$, but they are `somewhat dense'. In particular, for any $f \in L^1[-N,N]$, and any $g \in L^\infty[-N,N]$, there exists a family of simple functions $\{ g_n \}$ of the form above such that
	%
	\[ \int f(x) g(x) = \lim_{n \to \infty} f(x) g_n(x)\; dx. \]
	%
	It follows from this that for any $g \in L^\infty[-N,N]$, and for the particular function $f$ above,
	%
	\[ \int f(x) g(x)\; dx = 0. \]
	%
	The Hahn-Banach theorem, and the fact that $L^\infty[-N,N]$ is the dual of $L^1[-N,N]$, shows that $f = 0$, giving a contradiction.

	To prove the `somewhat density' property specified above, we fix $f \in L^1[-N,N]$ and $g \in L^\infty[-N,N]$. Without loss of generality we may assume that $\| f \|_{L^1[-N,N]} = \| g \|_{L^\infty[-N,N]} = 1$. Then we apply Lusin's theorem: for each $n > 0$, there exists a compact set $E_n \subset [-N,N]$ such that $g$ is continuous when restricted to $E_n$, and $|E_n^c| < 1/n$. Because $f$ is integrable, for any $\varepsilon > 0$, there exists $n > 0$ such that if $|F| < 1/n$, then
	%
	\[ \int_{F} |f(x)|\; dx \leq \varepsilon. \]
	%
	If $n$ is suitably large it therefore follows that
	%
	\[ \int_{E_n^c} |f(x)|\; dx \leq \varepsilon. \]
	%
	Since $g$ is continuous on $E_n$ for each $n$, and $E_n$ is compact, it is uniformly continuous on $E_n$ for each $n$. Using this fact, we can find a simple function $g_n$ of the form above such that $\| g_n \|_{L^\infty[-N,N]} \leq 1$, and $\| g - g_n \|_{L^\infty(E_n)} \leq 1/n$. But this means that 
	%
	\begin{align*}
		\left| \int_{-N}^N f(x) (g - g_n)(x) \right| &\leq \| g - g_n \|_{L^\infty(E_n)} \int_{E_n} |f(x)|\; dx + 2 \int_{E_n^c} |f(x)|\\
		&\leq (1/n) + 2 \varepsilon.
	\end{align*}
	%
	Taking $n$ arbitrarily large allows us to take $\varepsilon$ arbitrarily small, which shows that
	%
	\[ \lim_{n \to \infty} \left| \int_{-N}^N f(x) g(x)\; dx - \int_{-N}^N f(x) g_n(x)\; dx \right| = 0, \]
	%
	which gives the result required.

Here is another approach to the proof, using the $\pi-\lambda$ theorem. The
$\pi-\lambda$ theorem states that if $\Pi$ is a $\pi$-system and $\mathcal{L}$
is a $\lambda$-system, then
\begin{equation*}
  \Pi\subseteq \mathcal{L}
  \quad \text{implies} \quad
  \sigma(\Pi) \subseteq \mathcal{L}.
\end{equation*}
Let $\mathcal{B}_{\mathbb{R}}$ denote the Borel $\sigma$-algebra on
$\mathbb{R}$. Let
\begin{equation*}
  \mathcal{L} 
  := \left\{ E \in  \mathcal{B}_{\mathbb{R}} :  \int_{E}f(x)dx  = 0 \right\}
\end{equation*}
and let 
\begin{equation*}
  \Pi := \left\{(a,b):a \leq b, \text{ and } a,b\in \mathbb{Q}\right\}.
\end{equation*}
Next, we make the following observations
\begin{enumerate}
  \item $\Pi$ is a $\pi$-system. (This is obvious).
  \item $\Pi\subseteq \mathcal{L}$. (This is given to us in the problem
  statement).
  \item $\mathcal{L}$ is a $\lambda$-system. (Just need to verify that
  $\mathcal{L}$ satisfies the definition of $\lambda$-system. Establishing that
  $\mathcal{L}$ is closed under countable unions requires use of the fact that
  $f$ is integrable in order to justify use of either continuity of integration
  or the dominated convergence theorem).
  \item $  \mathcal{B}_{\mathbb{R}}\subseteq \sigma(\Pi)$. (Since
  $\mathcal{B}_{\mathbb{R}}$ is by definition the minimal $\sigma$-algebra
  containing the open sets, it suffices to show that $\sigma(\Pi)$ contains the
  open sets. Therefore, since each open set in $\mathbb{R}$ can be written as a
  disjoint union of open intervals, it suffices to show that $\sigma(\Pi)$
  contains the open intervals. Indeed, if $(x,y)\subseteq \mathbb{R}$ is an open
  interval, then there exists sequences of rational numbers $(a_{n})$ and
  $(b_{n})$ such that $x\leq a_{n}\leq b_{n}\leq y$) for all $n$ and
  $a_{n}\to x$ and $b_{n}\to y$. Then $(x,y)= \cup_{n=1}^{\infty}(a_{n},b_{n})$.
  This implies $(x,y)\in \sigma(\Pi)$, as required.)
\end{enumerate}
Observations (1)-(3) justify the use of the $\pi-\lambda$ theorem, which implies
\begin{equation*}
  \sigma(\Pi)\subseteq \mathcal{L}
\end{equation*}
and hence by observation (4), it follows that
\begin{equation*}
  \mathcal{B}_{\mathbb{R}}\subseteq  \mathcal{L}.
\end{equation*}
But this implies that
\begin{equation}\label{eq:vanishing-borel-integrals}
  \int_{E}f(x)dx = 0 \quad\text{ for all}\quad E\in \mathcal{B}_{\mathbb{R}}.
\end{equation}
for all $E\in \mathcal{B}_{\mathbb{R}}$. In particular, let $E_{+}:= \left\{x:
  f(x)>0\right\}$. Since $f$ is measurable, it follows that $E_{+}\in \mathcal{B}_{\mathbb{R}}$
and hence by \eqref{eq:vanishing-borel-integrals} that $f(x)\leq 0$ a.e. A
similar argument using the set $E_{-}:= \left\{x: f(x)<0\right\}$ implies that
$f(x)\geq 0$ a.e. Therefore $f(x)=0$ a.e.
\end{solution}





\question (Spring 2020) Show that $\int_{0}^{\infty}\frac{\sin(x)}{x^{2/3}}dx$
converges. Determine whether the integral
\begin{equation*}
  \int_{1}^{\infty}\frac{\sin x}{\sin(x) +x^{2/3}} dx 
\end{equation*}
converges or not. Hint: use Taylor expansion.

\begin{solution}
For the first part, observe that
\begin{align*}
  \int_{0}^{\infty}\frac{\sin(x)}{x^{2/3}}dx
  &=  \int_{0}^{1}\frac{\sin(x)}{x^{2/3}}dx +  \int_{1}^{\infty}\frac{\sin(x)}{x^{2/3}}dx 
\end{align*}
We can use the inquality $\sin(x) \leq x$ for $x \geq 0$ to bound the first
integral on the right hand side. To show the second integral converges, we do
an integration by parts:
\begin{align*}
  \int_{1}^{\infty}\frac{\sin(x)}{x^{2/3}}dx
  &= \int_{1}^{\infty}\frac{d}{dx}\left[ -\cos(x) \right] \frac{1}{x^{2/3}}dx\\ 
  &= \cos(1) + \frac{2}{3}\int_{1}^{\infty}\cos(x) \frac{d}{dx} \left[  \frac{1}{x^{2/3}} \right] dx\\ 
  &= \cos(1) - \frac{2}{3}\int_{1}^{\infty} \frac{\cos x}{x^{5/3}} dx\\ 
\end{align*}
and this integral converges since $\left| \frac{\cos x}{x^{5/3}}  \right| \leq
\frac{1}{x^{5/3}}\in L^{1}(1,\infty)$.  

For the second part of the problem, we will show that the integral converges. For each $x>1$, let $a(x) = -\frac{\sin x}{x^{2/3}}$. Then
$|a(x)|<1$ for all $x>1$
% and, using the formula
% \begin{equation*}
%   \frac{1}{1-a} = 1 +a +a^{2}+\ldots + a^{n} + \frac{a^{n+1}}{1-a},
% \end{equation*}
% with $n=0$,  we have
\begin{equation*}
  \frac{1}{1-a} = 1 + \frac{a}{1-a}
\end{equation*}
Therefore 
\begin{align*}
  \int_{1}^{\infty}\frac{\sin x}{\sin(x) +x^{2/3}} dx 
  &= \int_{1}^{\infty} \frac{\sin x}{x^{2/3}} \left( \frac{1}{1+ \frac{\sin x}{x^{2/3}}} \right) dx \\
  &= \int_{1}^{\infty}  -a \left( \frac{1}{1-a} \right)dx\\ 
  &= \int_{1}^{\infty} -a -  \frac{a^{2}}{1-a} dx\\
  &= \int_{1}^{\infty} \frac{\sin x}{x^{2/3}}dx - \int_{1}^{\infty} \left( \frac{\sin x}{x^{2/3}} \right)^{2} \frac{1}{1+\frac{\sin x}{x^{2/3}}} dx
\end{align*}
The first integral is integrable by the first part of this problem. Next we
will show that the integral $\int_{1}^{\infty} \left( \frac{\sin x}{x^{2/3}}
\right)^{2} \frac{1}{1+\frac{\sin x}{x^{2/3}}} dx $
converges as well, which will complete the proof. To show this, it will suffice to
show that there exists a constant $C>0$ such that
$\frac{1}{1+\frac{\sin x}{x^{2/3}}} \leq C$ for all $x>1$, as it will then
follow that the integrand is dominated by an integrable function:
\begin{align*}
  \left|\left( \frac{\sin x}{x^{2/3}} \right)^{2} \frac{1}{1+\frac{\sin x}{x^{2/3}}} \right|
  \leq \frac{C}{x^{4/3}} \in L^{1}(1,\infty).
\end{align*}

Indeed, there are two cases:

\textit{Case 1.} Suppose $\sin(x) \geq 0$. Then $1+\frac{\sin x}{x^{2/3}}>1$ so
$\frac{1}{1+\frac{\sin x}{x^{2/3}}} \leq 1$.

\textit{Case 2.} Suppose $\sin(x) <0$. Then since we are assuming $x>1$, it must be the case that $x>\pi$. It then follows that
$\frac{\sin x}{x^{3/2}} \geq -\frac{1}{\pi^{3/2}}$ and hence
\begin{equation*}
  \frac{1}{1+ \frac{\sin x}{x^{2/3}}} \leq \frac{1}{1 - \frac{1}{\pi^{3/2}}}.
\end{equation*}
Therefore we may choose $C = \max \left\{ 1, \frac{1}{1 - \frac{1}{\pi^{3/2}}}  \right\}.$ This completes the proof.
\end{solution}











\question (Fall 2021) Let $\left\{f_{n}\right\}$ be a sequence of monotonic functions on $[0,1]$
converging to a function $f$ in measure (with respect to the Lebesgue measure).
Show that $f$ coincides almost everywhere with a monotonic function $f_{0}$ and
that $f_{n}(x)\to f_{0}(x)$ at every point of continuity of $f_{0}$. 
\begin{solution} 
Since $(f_{n})$ converges to $f$ in measure, there exists a
subsequence $(f_{n_{k}})$ converging to $f$ pointwise a.e. Letting
\begin{equation*}
  E_{0} := \left\{x\in [0,1] :\lim_{k\to\infty} f_{n_{k}}(x)=f(x)\right\}.
\end{equation*}
We claim that $f$ is monotone on $E$. To see this, there are two cases:

\textit{Case 1.} Suppose the $f_{n_{k}}$'s are all monotone increasing, or all
monotone decreasing, for $n$ sufficiently large. Without loss of generality, it
suffices to show the monotone increasing case since the monotone decreasing case
will follow by multipying all the functions in the sequence by $-1$. If $a<b$
then
\begin{equation*}
  f_{n_{k}}(a)\leq f_{n_{k}}(b)
\end{equation*}
for all $n$. Sending $k\to \infty$, we obtain
\begin{equation*}
  f(a) \leq f(b).
\end{equation*}


\textit{Case 2.} Suppose $f_{n_{k}}$ is monotone increasing for infinitely many
$k$ and $f_{n_{k}}$ is monotone decreasing for infinitely many $k$. Then there
exists two subsequences $(n_{k_{i}})$ and $(n_{k_{j}})$ of $(n_{k})$ such that
$f_{n_{k_{i}}}$ is increasing for all $i$ and $f_{n_{k_{j}}}$ is decreasing for
all $j$. Moreover, $\lim_{i \to \infty}f_{n_{k_{i}}}(x)= f(x) $ and
$\lim_{j \to \infty}f_{n_{k_{j}}}(x)= f(x) $ for all $x\in E$. Therefore by
similar reasoning as in Case 1, it follows that $f$ is both increasing and
decreasing on the set $E$. That is, $f$ is constant (and hence monotone) on $E$.


We have shown that $f$ is monotone on $E$. Without loss of generality, assume
that $f$ is monotone increasing, as the decreasing case will follow by a similar
argument. 


Define a function $f_{0}$ as

\begin{equation*}
  f_{0}(x) := \left\{ \begin{array}{l@{\quad:\quad}l} f(x) & x\in E\\
      \inf \left\{f(y) : x<y \text{ and } y\in E\right\}& x \notin E\end{array}\right.
\end{equation*}
Since $|E^{c}| =0$, therefore the functions $f$ and $f_{0}$ coincide a.e..
Moreover, it follows by definition of $f_{0}$ that $f_{0}$ is monotone
increasing. It remains only to show that $f_{n}(x)\to f_{0}(x)$ at every point
of continuity of $f_{0}$. We will use the following fun fact:

\begin{quote}
  A sequence of real numbers $(a_{n})$ converges
  to $a\in \mathbb{R}$ if and only every subsequence $(a_{n'})$ of $(a_{n})$ has a
  further subsequence $(a_{n''})$ which converges to $a$.
\end{quote}


Let $x$ be a point of continuity of $f_{0}$ and let $(f_{n'})$ be any
subsequence of $(f_{n})$. Since $f_{n'}\to f$ in measure it follows that there
exists a subsequence $(f_{n''})$ of $(f_{n'})$ such that $f_{n''} \to f$ a.e. It
will suffice to show that $f_{n''}(x)\to f(x)$.

By possibly taking a further subsequence, we may assume that the $f_{n''}$'s are
either all monotone increasing, or all monotone decreasing.


\textit{Case 1.} Suppose $f_{n''}$'s are all monotone increasing. Let
$\epsilon>0$ be arbitrary and let
\begin{equation*}
  E'' := \left\{x\in [0,1] :\lim_{n''\to\infty} f_{n''}(x) = f(x) \right\}.
\end{equation*}
Since $[0,1]\backslash E''$ has Lebesgue measure zero, $E''$ is dense in
$[0,1]$. Therefore, using the fact that $x$ is a point of continuity of $f_{0}$,
we may choose $a,b,c\in E''$ such that $a<b<x<c$ and
\begin{equation}
  \label{eq:3}
  f_{0}(x)-\epsilon
  < f_{0}(a)
  \leq f_{0}(b)
  \leq f_{0}(x)
  \leq f_{0}(c)
  \leq f_{0}(x) + \epsilon
\end{equation}
Therefore, since $f_{n''}(b)\to f_{0}(b)$, there exists a positive integer
$N_{1}$ such that
\begin{equation}
  \label{eq:2}
  f_{n''}(b)> f(x)-\epsilon \quad \text{whenever} \quad n'' \geq N_{1}.
\end{equation}
Also, since $f_{n''}(c)\to f(c)$, there exists a positive integer $N_{2}$ such
that
\begin{equation}
  \label{eq:3}
  f_{n''}(c) \leq  f(x)+ \epsilon \quad \text{whenever} \quad n'' \geq N_{2}
\end{equation}
Let $N_{0}= \max \left\{N_{1},N_{2}\right\}$. Then for all $n'' \geq N_{0}$,  we
have
\begin{align*}
  f(x)-\epsilon
  &< f_{n''}(b)
    &&\text{by \eqref{eq:2}}\\
  &\leq f_{n''}(x)
    &&\text{since $f_{n''}$ is monotone increasing}\\
  &\leq f_{n''}(c)
    &&\text{since $f_{n''}$ is monotone increasing}\\
  &\leq f(x)+\epsilon
    &&\text{by \eqref{eq:3}}.
\end{align*}
Therefore $f_{n''}(x)\to f(x)$.

\textit{Case 2.} Assume $f_{n''}$'s are all monotone decreasing. The argument is
similar to the previous case, but with \eqref{eq:2} replaced by
\begin{equation*}
  f_{n''}(c)>f(c)-\epsilon \geq f(x)-\epsilon
\end{equation*}
(with the second inequality following from the earlier assumption that $f$ is
monotone increasing and the fact that $x<c$), and \eqref{eq:3} replaced by
\begin{equation*}
  f_{n''}(c) \leq f(b) +\epsilon \leq f(x)+ \epsilon.
\end{equation*}
We then have, for sufficiently large $n''$, 
\begin{align*}
  f(x)-\epsilon
  &< f_{n''}(c)\\
  &\leq f_{n''}(x)
  &&\text{since $f_{n''}$ is monotone decreasing}\\
  &\leq f_{n''}(b)
  &&\text{since $f_{n''}$ is monotone decreasing}\\
  &\leq f(x)+\epsilon
\end{align*}
which proves the result.
\end{solution}

\question (Fall 2020) Let $f:[a,b] \to \mathbb{R}$ be a continuous function that is strictly
increasing. Prove that the inverse function $f^{-1}$ is absolutely continuous on
$[f(a),f(b)]$ if and only if
\begin{equation*}
  m(E) = 0 \quad \text{where} \quad E:= \left\{ x\in (a,b): f'(x)=0 \right\}.
\end{equation*}

\begin{solution}
Suppose $m(E)>0$. We will show that $f^{-1}$ is not absolutely continuous. Let
  $\delta>0$ be arbitrary. Since $f$ is increasing, it is differentiable a.e.
  (by Lebesgue's theorem) and its derivative $f'$ is integrable. (See, e.g. Royden and Fitzpatrick, \textit{Real Analysis}, 4th edition, p.113; the result is essentially an application of Fatou's lemma) Since a
  singleton set consisting of an integrable function is a \textit{uniformly
    integrable} set, it follows by definition of uniform integrability that
  there exists $\eta>0$ such that for any measurable set $A\subseteq [0,1]$,
  \begin{equation}\label{eq:4}
    \int_{A} f' < \delta \quad \text{whenever} \quad m(A)<\eta.
  \end{equation}
  Since $m(E)>0$, there exist a finite collection of pairwise disjoint intervals
  $(a_{1},b_{1}), (a_{2},b_{2}),\ldots, (a_{n},b_{n})$ with
  $U:= \bigcup_{k=1}^{n}(a_{k},b_{k})\subseteq [0,1]$ such that
  \begin{equation*}
    \sum_{k=1}^{n}b_{k}-a_{k}\geq \frac{m(E)}{2}
    \quad \text{and} \quad 
    m(U\backslash E)<\eta.
  \end{equation*}
  Let $c_{k}=f(a_{k})$ and $d_{k}=f(b_{k})$ for each $k=1,\ldots,n$. Then
  \begin{align*}
    \sum_{k=1}^{n}d_{k}-c_{k}
    &= \sum_{k=1}^{n}f(b_{k})-f(a_{k})\\ 
    &= \sum_{k=1}^{n} \int_{a_{k}}^{b_{k}}f'\\ 
    &= \int_{U} f'\\ 
    &= \int_{U\backslash E} f' +\int_{E \cap U} f'\\ 
    &= \int_{U\backslash E} f'
      &&\text{by definition of $E$}\\      
    &< \delta
      &&\text{by \eqref{eq:4}, since $m(u\backslash E)<\eta$.}
  \end{align*}
  However,
  \begin{align*}
    \sum_{k=1}^{n} f^{-1}(d_{k})-f^{-1}(c_{k})
    &= \sum_{k=1}^{n} b_{k}-a_{k} \geq \frac{m(E)}{2}>0.
  \end{align*}
  Therefore, $f^{-1}$ is not absolutely continuous.

  Next we prove the converse. We wish to show that $E$ has Lebesgue measure zero. Since $f^{-1}$ is absolutely continuous, there exists a function $g\in L^{1}[a,b]$ such that
  \begin{equation*}
    f^{-1}(x) = \int_{f(a)}^{x}g(t)dt 
  \end{equation*}
  for all $x\in [f(a),f(b)]$. Therefore $f^{-1}$ is differentiable a.e. Therefore
  \begin{equation} \label{eq:4-2020}
    m \left( \left\{ x: f^{-1} \text{ is not differentiable at }x \right\} \right)  = 0
  \end{equation}
  Since $x = f(f^{-1}(x))$, the chain rule implies that
  \begin{equation} \label{eq:5-2020}
    1= f'(f^{-1}(x))\cdot (f^{-1})'(x)
  \end{equation}
  %
  provided that $f^{-1}$ is differentiable at $f(x)$ and $f$ is differentiable
  at $x$. In particular this means that if $f^{-1}(x)\in E$ then $f^{-1}$ is not differentiable at $x$
  because otherwise equation \eqref{eq:5-2020} would yield $1=0$, a contradiction.
  Therefore
  \begin{equation*}
    \left\{ x: f^{-1}(x)\in E \right\} \subseteq  \left\{ x: f^{-1} \text{ is not differentiable at }x \right\}.
  \end{equation*}
  Therefore by \eqref{eq:4-2020} and monotonicity of Lebesgue measure,
  \begin{equation*}
    m(f[E])= m\left(\left\{ x:f^{-1}(x)\in E \right\}\right)=0.
  \end{equation*}
  Therefore since $f^{-1}$ is absolutely continuous, it satisfies the Luzin $N$
  property and hence the set
  \begin{equation*}
    E = f^{-1}[f[E]]
  \end{equation*}
  has Lebesuge measure zero.
\end{solution}







\newpage
\section{Questions Without Solutions}

\question (Fall 2017) Let $g: \RR^2 \to \RR$ be a function that has continuous partial derivatives in $\RR^2$. Define $\chi: \RR^3 \to \{ 0, 1 \}$ by $\chi(x) = 1$ if $x_3 > g(x_1,x_2)$, and $\chi(x) = 0$ otherwise. Compute the derivatives $\partial \chi / \partial x^i$ for $i = 1, 2, 3$.
\begin{solution}
		The function $\chi$ is constant away from the surface $x_3 = g(x_1,x_2)$, so we should expect the distributional derivatives of $\chi$ to be supported on this surface. Now for a test function $\phi$, using the fundamental theorem of calculus, we have
		%
		\begin{align*}
			\int \partial_3 \chi(x) \phi(x) &= - \int \int_{g(x_1,x_2)}^\infty \chi(x) \partial_3 \phi(x)\; dx_3 \; dx_1\; dx_2\\
			&= \int \chi(x_1,x_2, g(x_1,x_2)) \phi(x_1,x_2,g(x_1,x_2))\; dx_1\; dx_2.
		\end{align*}
		%
		Thus the distribution $\partial_3 \chi$ is equal to integration against the measure obtained by pulling back the Lebesgue measure on $\RR^2$ along the projection $\pi$ projecting the graph $x_3 = g(x_1,x_2)$ onto the $x_3$ axis.

		Now let $A(x_1,x_3) = \{ x_2 : g(x_1,x_2) = x_3\ \text{and}\ g(x_1,x_2 + \varepsilon) < x_3\ \text{for sufficiently small $\varepsilon > 0$} \}$, and let $B(x_1,x_3) = \{ x_2 : g(x_1,x_2) = x_3\ \text{and}\ g(x_1,x_2 - \varepsilon) < x_3\ \text{for sufficiently small $\varepsilon > 0$} \}$. Then applying the fundamental theorem of calculus, for each $x_1$ and $x_3$ we can write the set $\{ x_2 : x_3 > g(x_1,x_2) \}$ as a union of intervals. Then $A(x_1,x_3)$ is the set of left endpoints to the intervals, and $B(x_1,x_3)$ the right endpoints, and so by the fundamental theorem of calculus,
		%
		\begin{align*}
			\int \partial_2 \chi(x) \phi(x) &= - \int \chi(x) \partial_2 \phi(x)\; dx\\
			&= \int \left[\sum_{x_2 \in A(x_1,x_3)} \phi(x) - \sum_{x_2 \in B(x_1,x_3)} \phi(x) \right]\; dx_1\; dx_3
		\end{align*}
		%
		So if $\mu_1$ is the measure given for a Borel set $E$ by
		%
		\[ \mu_1(E) = \int \# \{ E \cap \{x_1 \} \times A(x_1,x_3) \times \{ x_3 \} \}\; dx_1\; dx_3, \]
		%
		and $\mu_2$ is the measure given by
		%
		\[ \mu_2(E) = \int \# \{ E \cap \{x_1 \} \times B(x_1,x_3) \times \{ x_3 \} \}\; dx_1\; dx_3, \]
		%
		then $\partial_2 \chi$ is equal to the measure $\mu_1 - \mu_2$.

		The case $\partial_1 \chi$ is similar to $\partial_2 \chi$.
\end{solution}

\question Let $\alpha \in (0,1)$, and for $f \in C[0,1]$, and $x \in [0,1]$, define
%
\[ (T_\alpha f)(x) = \int_0^1 \sin(x-y) |x-y|^{-\alpha} f(y)\; dy. \]
\begin{parts}
    \part Prove that $T_\alpha$ extends to a bounded operator on $L^2[0,1]$.
   	\begin{solution}
   		Using the bound $|\sin(x-y)| \leq |x - y|$, the kernel $K_\alpha(x,y) = \sin(x - y) |x - y|^{-\alpha}$ of $T_\alpha$ is $O( |x-y|^{1 - \alpha} )$, and is thus a bounded function. But then by Schur's test we conclude $T_\alpha$ is bounded on $L^2$, i.e. using H\"{o}lder's inequality we use Minkowski's integral inequality to conclude that
   		%
   		\[ \left| \int K_\alpha(x,y) f(y)\; dy \right| \lesssim \| K \|_{L^\infty[0,1]} \| f \|_{L^1[0,1]}. \]
   		%
   		Thus, squaring this and integrating in $x$, we conclude that
   		%
   		\[ \| T_\alpha f \|_{L^2[0,1]} \lesssim \| K \|_{L^\infty[0,1]} \| f \|_{L^1[0,1]} \leq \| K \|_{L^\infty[0,1]} \| f \|_{L^2[0,1]}, \]
   		%
   		proving boudnedness.
   	\end{solution}

    \part For which $\alpha \in (0,1)$ is $T_\alpha: L^2[0,1] \to L^2[0,1]$ a compact operator?
    \begin{solution}
    	The operator $T_\alpha$ is an integral operator with kernel
    	%
    	\[ K_\alpha(x,y) = \sin(x - y) |x - y|^{-\alpha}. \]
    	%
    	We calculate that
    	%
    	\begin{align*}
    		\| K_\alpha \|_{L^2_x[0,1] L^2_y[0,1]} &\leq \left( \int_0^1 \int_0^1 |x - y|^{2(1-\alpha)}\; dx\; dy \right)^{1/2}\\
    		&\lesssim \left( \int_0^{10} t^{2(1-\alpha)}\; dz \right)^{1/2}.
    	\end{align*}
    	%
    	For all $\alpha \in (0,1)$ this quantity is finite, and thus $T_\alpha$ is a Hilbert-Schmidt operator, and thus compact.
    \end{solution}
\end{parts}


%\newpage
%\section{Next Year Ideas}

%\begin{itemize}
%	\item Have Notes + A Day Purely on Lebesgue Differentiation / Indicator Function Type Arguments
%\end{itemize}


\end{questions}

\end{document}
